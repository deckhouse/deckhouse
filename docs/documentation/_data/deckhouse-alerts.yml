alerts:
    - name: CapsInstanceUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/caps-nodes.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        In MachineDeployment `{{ $labels.machine_deployment_name }}` number of unavailable instances is **{{ $value }}**. Take a look and check at the state of the instances in the cluster: `kubectl get instance -l node.deckhouse.io/group={{ $labels.machine_deployment_name }}`
      summary: |
        There are unavailable instances in the {{ $labels.machine_deployment_name }} MachineDeployment.
      severity: "8"
      markupFormat: markdown
    - name: CertificateSecretExpired
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificate.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Certificate in secret {{$labels.namespace}}/{{$labels.name}} expired.

        - If the certificate is manually managed, upload a newer one.
        - If the certificate is managed by cert-manager, try inspecting certificate resource, the recommended course of action:
          1. Retrieve certificate name from the secret: `cert=$(kubectl get secret -n {{$labels.namespace}} {{$labels.name}} -o 'jsonpath={.metadata.annotations.cert-manager\.io/certificate-name}')`
          2. View the status of the Certificate and try to figure out why it is not updated: `kubectl describe cert -m {{$labels.namespace}} "$cert"`
      summary: |
        Certificate expired
      severity: "8"
      markupFormat: markdown
    - name: CertificateSecretExpiredSoon
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificate.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Certificate in secret {{$labels.namespace}}/{{$labels.name}} will expire in less than 2 weeks

        - If the certificate is manually managed, upload a newer one.
        - If certificate is managed by cert-manager, try inspecting certificate resource, the recommended course of action:
          1. Retrieve certificate name from the secret: `cert=$(kubectl get secret -n {{$labels.namespace}} {{$labels.name}} -o 'jsonpath={.metadata.annotations.cert-manager\.io/certificate-name}')`
          2. View the status of the Certificate and try to figure out why it is not updated: `kubectl describe cert -n {{$labels.namespace}} "$cert"`
      summary: |
        Certificate will expire soon.
      severity: "8"
      markupFormat: markdown
    - name: CertmanagerCertificateExpired
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: Certificate {{$labels.exported_namespace}}/{{$labels.name}} expired
      summary: |
        Certificate expired
      severity: "4"
      markupFormat: default
    - name: CertmanagerCertificateExpiredSoon
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: The certificate {{$labels.exported_namespace}}/{{$labels.name}} will expire in less than 2 weeks
      summary: |
        Certificate will expire soon
      severity: "4"
      markupFormat: default
    - name: CertmanagerCertificateOrderErrors
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        Cermanager receives responses with the code `{{ $labels.status }}` on requesting {{ $labels.scheme }}://{{ $labels.host }}{{ $labels.path }}.

        It can affect certificates ordering and prolongation. Check certmanager logs for more info.
        `kubectl -n d8-cert-manager logs -l app=cert-manager -c cert-manager`
      summary: |
        Certmanager cannot order a certificate.
      severity: "5"
      markupFormat: default
    - name: CiliumAgentEndpointsNotReady
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Check what's going on: `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
      summary: |
        More than half of all known Endpoints are not ready in agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentMapPressureCritical
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: We've reached resource limit of eBPF maps. Consult with vendor for possible remediation steps.
      summary: |
        eBPF map {{ $labels.map_name }} is more than 90% full in agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentMetricNotFound
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Use the following commands to check what's going on:
        - `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
        - `kubectl -n {{ $labels.namespace }} exec -ti {{ $labels.pod }} cilium-health status`

        We need to cross-check the metrics with the neighboring agent.
        Also the absence of metrics is an indirect sign that new pods cannot be created on the node because of the inability to connect to the agent.
        It is important to get a more specific way of determining the above situation and create a more accurate alert for the inability to connect new pods to the agent.
      summary: |
        Some of the metrics are not coming from the agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentPolicyImportErrors
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Check what's going on: `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} fails to import policies.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentUnreachableHealthEndpoints
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Check what's going on: `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
      summary: |
        Some node's health endpoints are not reachable by agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: ClusterHasOrphanedDisks
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cloud data discoverer finds disks in the cloud for which there is no PersistentVolume in the cluster. You can manually delete these disks from your cloud:
          ID: {{ $labels.id }}, Name: {{ $labels.name }}
      summary: |
        Cloud data discoverer finds disks in the cloud for which there is no PersistentVolume in the cluster
      severity: "6"
      markupFormat: markdown
    - name: CniCiliumNonStandardVXLANPortFound
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/configmap.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        There is non-standard VXLAN port in Cilium config: `{{$labels.port}}` does not fit the recommended range (4298 if the virtualization module enabled or 4299 for regular deckhouse setup).

        Consider configuring the `tunnel-port` parameter in `cilium-configmap` ConfigMap (`d8-cni-cilium` namespace) according the recommended range. If you know why you need the non-standard port, just ignore the alert.
      summary: |
        There is non-standard VXLAN port in Cilium config
      severity: "4"
      markupFormat: markdown
    - name: CniCiliumOrphanEgressGatewayPolicyFound
      sourceFile: ee/modules/021-cni-cilium/monitoring/prometheus-rules/egressgatewaypolicies.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ee
      description: |
        There is orphan EgressGatewayPolicy in the cluster: with the name: `{{$labels.name}}` which has irrelevant EgressGateway name.

        It is recommended to check EgressGateway name in EgressGatewayPolicy resource: `{{$labels.egressgateway}}`
      summary: |
        Found orphan EgressGatewayPolicy with irrelevant EgressGateway name
      severity: "4"
      markupFormat: markdown
    - name: CPUStealHigh
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The CPU steal is too high on the {{ $labels.node }} Node in the last 30 minutes.

        Probably, some other component is stealing Node resources (e.g., a neighboring virtual machine). This may be the result of "overselling" the hypervisor. In other words, there are more virtual machines than the hypervisor can handle.
      summary: |
        CPU Steal on the {{ $labels.node }} Node is too high.
      severity: "4"
      markupFormat: default
    - name: CronJobAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Unable to login to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image in the `{{ $labels.namespace }}` Namespace; in the CronJob `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to login to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CronJobAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Insufficient privileges to pull the `{{ $labels.image }}` image using the `imagePullSecrets` specified in the `{{ $labels.namespace }}` Namespace; in the CronJob `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the imagePullSecrets specified.
      severity: "7"
      markupFormat: markdown
    - name: CronJobBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image name is spelled correctly: in the `{{ $labels.namespace }}` Namespace; in the CronJob `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image has incorrect name.
      severity: "7"
      markupFormat: markdown
    - name: CronJobFailed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: ""
      summary: |
        Job {{$labels.namespace}}/{{$labels.job_name}} failed in CronJob {{$labels.namespace}}/{{$labels.owner_name}}.
      severity: "5"
      markupFormat: default
    - name: CronJobImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image is available: in the `{{ $labels.namespace }}` Namespace; in the CronJob `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: CronJobRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The container registry is not available for the `{{ $labels.image }}` image: in the `{{ $labels.namespace }}` Namespace; in the CronJob `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CronJobSchedulingError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        CronJob {{$labels.namespace}}/{{$labels.cronjob}} failed to schedule on time.
        Schedule: "{{ printf "kube_cronjob_info{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | label "schedule" }}"
        Last schedule time: {{ printf "kube_cronjob_status_last_schedule_time{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | value | humanizeTimestamp }}%
        Projected next schedule time: {{ printf "kube_cronjob_next_schedule_time{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | value | humanizeTimestamp }}%
      summary: |
        CronJob {{$labels.namespace}}/{{$labels.cronjob}} failed to schedule on time.
      severity: "6"
      markupFormat: markdown
    - name: CronJobUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        An unknown error occurred for the  `{{ $labels.image }}` image
        in the `{{ $labels.namespace }}` Namespace;
        in the CronJob `{{ $labels.name }}`
        in the `{{ $labels.container }}` container in the registry.

        Refer to the exporter logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        An unknown error occurred for the  {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CustomPodMonitorFoundInCluster
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are PodMonitors in Deckhouse namespace that were not created by Deckhouse.

        Use the following command for filtering: `kubectl get podmonitors --all-namespaces -l heritage!=deckhouse`.

        They must be moved from Deckhouse namespace to user-spec namespace (was not labeled as `heritage: deckhouse`).

        The detailed description of the metric collecting process is available in the [documentation](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/300-prometheus/faq.html).
      summary: |
        There are PodMonitors in Deckhouse namespace that were not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: CustomServiceMonitorFoundInD8Namespace
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are ServiceMonitors in Deckhouse namespace that were not created by Deckhouse.

        Use the following command for filtering: `kubectl get servicemonitors --all-namespaces -l heritage!=deckhouse`.

        They must be moved from Deckhouse namespace to user-spec namespace (was not labeled as `heritage: deckhouse`).

        The detailed description of the metric collecting process is available in the [documentation](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/300-prometheus/faq.html).
      summary: |
        There are ServiceMonitors in Deckhouse namespace that were not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: D8AdmissionPolicyEngineNotBootstrapped
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/bootstrap.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        Admission policy engine module couldn't bootstrap. Please check that the module's components are up and running: `kubectl get pods -n d8-admission-policy-engine`.
        Also, it makes sense to check relevant logs in case there are missing constraint
        templates or not all CRD were created: `kubectl  logs -n d8-system -lapp=deckhouse --tail=1000 | grep admission-policy-engine`
      summary: |
        Admission policy engine module hasn't been bootstrapped for 10 minutes.
      severity: "7"
      markupFormat: markdown
    - name: D8BashibleApiserverLocked
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Check bashible-apiserver pods are up-to-date and running `kubectl -n d8-cloud-instance-manager get pods -l app=bashible-apiserver`
      summary: |
        Bashible-apiserver is locked for too long
      severity: "6"
      markupFormat: markdown
    - name: D8CertExporterPodIsNotReady
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-monitoring describe deploy cert-exporter`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-monitoring describe pod -l app=cert-exporter`
      summary: |
        The cert-exporter Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterPodIsNotRunning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-monitoring describe deploy cert-exporter`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-monitoring describe pod -l app=cert-exporter`
      summary: |
        The cert-exporter Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterTargetAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Check the Pod status: `kubectl -n d8-monitoring get pod -l app=cert-exporter`

        Or check the Pod logs: `kubectl -n d8-monitoring logs -l app=cert-exporter -c cert-exporter`
      summary: |
        There is no cert-exporter target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Check the Pod status: `kubectl -n d8-monitoring get pod -l app=cert-exporter`

        Or check the Pod logs: `kubectl -n d8-monitoring logs -l app=cert-exporter -c cert-exporter`
      summary: |
        Prometheus cannot scrape the cert-exporter metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8CloudDataDiscovererCloudRequestError
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cloud data discoverer cannot get data from cloud. See cloud data discoverer logs for more information:
        `kubectl -n {{ $labels.namespace }} logs deploy/cloud-data-discoverer`
      summary: |
        Cloud data discoverer cannot get data from cloud
      severity: "6"
      markupFormat: markdown
    - name: D8CloudDataDiscovererSaveError
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cloud data discoverer cannot save data to k8s resource. See cloud data discoverer logs for more information:
        `kubectl -n {{ $labels.namespace }} logs deploy/cloud-data-discoverer`
      summary: |
        Cloud data discoverer cannot save data to k8s resource
      severity: "6"
      markupFormat: markdown
    - name: D8ClusterAutoscalerManagerPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerPodIsNotRunning
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The {{$labels.pod}} Pod is {{$labels.phase}}.

        Run the following command to check its status: `kubectl -n {{$labels.namespace}} get pods {{$labels.pod}} -o json | jq .status`.
      summary: |
        The cluster-autoscaler Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerPodIsRestartingTooOften
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive cluster-autoscaler restarts indicate that something is wrong. Normally, it should be up and running all the time.

        Please, refer to the corresponding logs: `kubectl -n d8-cloud-instance-manager logs -f -l app=cluster-autoscaler -c cluster-autoscaler`.
      summary: |
        Too many cluster-autoscaler restarts have been detected.
      severity: "9"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTargetAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        Cluster-autoscaler automatically scales Nodes in the cluster; its unavailability will result in the inability
        to add new Nodes if there is a lack of resources to schedule Pods. In addition, the unavailability of cluster-autoscaler
        may result in over-spending due to provisioned but inactive cloud instances.

        The recommended course of action:
        1. Check the availability and status of cluster-autoscaler Pods: `kubectl -n d8-cloud-instance-manager get pods -l app=cluster-autoscaler`
        2. Check whether the cluster-autoscaler deployment is present: `kubectl -n d8-cloud-instance-manager get deploy cluster-autoscaler`
        3. Check the status of the cluster-autoscaler deployment: `kubectl -n d8-cloud-instance-manager describe deploy cluster-autoscaler`
      summary: |
        There is no cluster-autoscaler target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTargetDown
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape cluster-autoscaler's metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTooManyErrors
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cluster-autoscaler's scaling attempt resulted in an error from the cloud provider.

        Please, refer to the corresponding logs: `kubectl -n d8-cloud-instance-manager logs -f -l app=cluster-autoscaler -c cluster-autoscaler`.
      summary: |
        Cluster-autoscaler issues too many errors.
      severity: "8"
      markupFormat: markdown
    - name: D8ControlPlaneManagerPodNotRunning
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        Pod `d8-control-plane-manager` fails or not scheduled on Node {{ $labels.node }}

        Consider checking state of the `kube-system/d8-control-plane-manager` DaemonSet and its Pods:
        `kubectl -n kube-system get daemonset,pod --selector=app=d8-control-plane-manager`
      summary: |
        Controller Pod not running on Node {{ $labels.node }}
      severity: "6"
      markupFormat: markdown
    - name: D8CustomPrometheusRuleFoundInCluster
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are PrometheusRules in the cluster that were not created by Deckhouse.

        Use the following command for filtering: `kubectl get prometheusrules --all-namespaces -l heritage!=deckhouse`.

        They must be abandoned and replaced with the `CustomPrometheusRules` object.

        Please, refer to the [documentation](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/300-prometheus/faq.html#how-do-i-add-alerts-andor-recording-rules) for information about adding alerts and/or recording rules.
      summary: |
        There are PrometheusRules in the cluster that were not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseConfigInvalid
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse config contains errors.

        Please check Deckhouse logs by running `kubectl -n d8-system logs -f -l app=deckhouse`.

        Edit Deckhouse global configuration by running `kubectl edit mc global` or configuration of the specific module by running `kubectl edit mc <MODULE_NAME>`
      summary: |
        Deckhouse config is invalid.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotDeleteModule
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        Deckhouse is unable to delete the {{ $labels.module }} module.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotDiscoverModules
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        Deckhouse is unable to discover modules.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunGlobalHook
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        Deckhouse is unable to run the {{ $labels.hook }} global hook.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunModule
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        Deckhouse is unable to start the {{ $labels.module }} module.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunModuleHook
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        Deckhouse is unable to run the {{ $labels.module }}/{{ $labels.hook }} module hook.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseCustomTargetDown
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape custom metrics generated by Deckhouse hooks.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseDeprecatedConfigmapManagedByArgoCD
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The deckhouse configmap is no longer used.
        You need to remove configmap "d8-system/deckhouse" from ArgoCD
      summary: |
        Deprecated deckhouse configmap managed by Argo CD
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseGlobalHookFailsTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The {{ $labels.hook }} has failed in the last `__SCRAPE_INTERVAL_X_4__`.

        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        The {{ $labels.hook }} Deckhouse global hook crashes way too often.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseHasNoAccessToRegistry
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse is unable to connect to the registry (registry.deckhouse.io in most cases) to check for a new Docker image (checks are performed every 15 seconds). Deckhouse does not have access to the registry; automatic updates are not available.

        Usually, this alert means that the Deckhouse Pod is having difficulties with connecting to the Internet.
      summary: |
        Deckhouse is unable to connect to the registry.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseIsHung
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse is probably down since the `deckhouse_live_ticks` metric in Prometheus is no longer increasing (it is supposed to increment every 10 seconds).
      summary: |
        Deckhouse is down.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseIsNotOnReleaseChannel
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse is on a custom branch instead of one of the regular release channels.

        It is recommended that Deckhouse be subscribed to one of the following channels: `Alpha`, `Beta`, `EarlyAccess`, `Stable`, `RockSolid`.

        Use the command below to find out what release channel is currently in use: `kubectl -n d8-system  get deploy deckhouse -o json | jq '.spec.template.spec.containers[0].image' -r`

        Subscribe the cluster to one of the regular release channels.
      summary: |
        Deckhouse in the cluster is not subscribed to one of the regular release channels.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseModuleHookFailsTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The {{ $labels.hook }} hook of the {{ $labels.module }} module has failed in the last `__SCRAPE_INTERVAL_X_4__`.

        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        The {{ $labels.module }}/{{ $labels.hook }} Deckhouse hook crashes way too often.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseModuleUpdatePolicyNotFound
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module update policy not found for {{ $labels.module_release }}

        You need to remove label from MR: `kubectl label mr {{ $labels.module_release }} modules.deckhouse.io/update-policy-`.
        A new suitable policy will be detected automatically.
      summary: |
        Module update policy not found for {{ $labels.module_release }}
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhousePodIsNotReady
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        The Deckhouse Pod is NOT Ready.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhousePodIsNotRunning
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        The Deckhouse Pod is NOT Running.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhousePodIsRestartingTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive Deckhouse restarts indicate that something is wrong. Normally, Deckhouse should be up and running all the time.

        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        Excessive Deckhouse restarts detected.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseQueueIsHung
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse cannot finish processing of the {{ $labels.queue }} queue with {{ $value }} tasks piled up.

        Please, refer to the corresponding logs: `kubectl -n d8-system logs -f -l app=deckhouse`.
      summary: |
        The {{ $labels.queue }} Deckhouse queue has hung; there are {{ $value }} task(s) in the queue.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseSelfTargetAbsent
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        There is no Deckhouse target in Prometheus.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseSelfTargetDown
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Deckhouse metrics.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseWatchErrorOccurred
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Error occurred in the client-go informer, possible problems with connection to apiserver.

        Check Deckhouse logs for more information by running:
        `kubectl -n d8-system logs deploy/deckhouse | grep error | grep -i watch`

        This alert is an attempt to detect the correlation between the faulty snapshot invalidation
        and apiserver connection errors, especially for the handle-node-template hook in the node-manager module.
        Check the difference between the snapshot and actual node objects for this hook:
        `diff -u <(kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'|sort) <(kubectl -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module snapshots node-manager -o json | jq '."040-node-manager/hooks/handle_node_templates.go"' | jq '.nodes.snapshot[] | .filterResult.Name' -r | sort)`
      summary: |
        Possible apiserver connection error in the client-go informer, check logs and snapshots.
      severity: "5"
      markupFormat: markdown
    - name: D8DexAllTargetsDown
      sourceFile: modules/150-user-authn/monitoring/prometheus-rules/dex.yaml
      moduleUrl: 150-user-authn
      module: user-authn
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Dex metrics.
      severity: "6"
      markupFormat: markdown
    - name: D8EtcdDatabaseHighFragmentationRatio
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        The etcd database size in use on instance `{{ $labels.instance }}` is less than 50% of the actual allocated disk space, indicating potential fragmentation

        Possible solutions:
        - You can do defragmentation. Use the following command:
          `kubectl -n kube-system exec -ti etcd-{{ $labels.node }} -- /usr/bin/etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s`
      summary: |
        etcd database size in use is less than 50% of the actual allocated storage, indicating potential fragmentation, and the total storage size exceeds 75% of the configured quota.
      severity: "7"
      markupFormat: markdown
    - name: D8EtcdExcessiveDatabaseGrowth
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        Predicting that the etcd database will run out of disk space in the next 1 day on instance `{{ $labels.instance }}` based on 6h growth rate.

        Please check and take action as it might be disruptive.
      summary: |
        etcd cluster database growing very fast.
      severity: "4"
      markupFormat: markdown
    - name: D8GrafanaDeploymentReplicasUnavailable
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The number of Grafana replicas is less than the specified number.

        The Deployment is in the MinimumReplicasUnavailable state.

        Run the following command to check the status of the Deployment: `kubectl -n d8-monitoring get deployment grafana -o json | jq .status`.

        Run the following command to check the status of the Pods: `kubectl -n d8-monitoring get pods -l app=grafana -o json | jq '.items[] | {(.metadata.name):.status}'`.
      summary: |
        One or more Grafana Pods are NOT Running.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaDeprecatedCustomDashboardDefinition
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The `grafana-dashboard-definitions-custom` ConfigMap was found in the `d8-monitoring` namespace. This means that the deprecated method of registering custom dashboards in Grafana is being used.

        **This method is no longer used!**
        Please, use the custom [GrafanaDashboardDefinition](https://github.com/deckhouse/deckhouse/blob/main/modules/300-prometheus/docs/internal/GRAFANA_DASHBOARD_DEVELOPMENT.md) resource instead.
      summary: |
        The deprecated ConfigMap for defining Grafana dashboards is detected.
      severity: "9"
      markupFormat: markdown
    - name: D8GrafanaPodIsNotReady
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: ""
      summary: |
        The Grafana Pod is NOT Ready.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaPodIsRestartingTooOften
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive Grafana restarts indicate that something is wrong. Normally, Grafana should be up and running all the time.

        Please, refer to the corresponding logs: `kubectl -n d8-monitoring logs -f -l app=grafana -c grafana`.
      summary: |
        Excessive Grafana restarts are detected.
      severity: "9"
      markupFormat: markdown
    - name: D8GrafanaTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        Grafana visualizes metrics collected by Prometheus. Grafana is critical for some tasks,
        such as monitoring the state of applications and the cluster as a whole. Additionally,
        Grafana unavailability can negatively impact users who actively use it in their work.

        The recommended course of action:
        1. Check the availability and status of Grafana Pods: `kubectl -n d8-monitoring get pods -l app=grafana`;
        2. Check the availability of the Grafana Deployment: `kubectl -n d8-monitoring get deployment grafana`;
        3. Examine the status of the Grafana Deployment: `kubectl -n d8-monitoring describe deployment grafana`.
      summary: |
        There is no Grafana target in Prometheus.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaTargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Grafana metrics.
      severity: "6"
      markupFormat: markdown
    - name: D8HasModuleConfigAllowedToDisable
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        ModuleConfig is waiting for disable.

        It is recommended to keep clean your module configurations from approve annotations.

        If you ignore this alert and do not clear the annotation, it may cause the module to be accidentally removed from the cluster.

        Removing a module from a cluster can lead to a number of irreparable consequences.

        Please run `kubectl annotate moduleconfig {{ $labels.module }} modules.deckhouse.io/allow-disable-` to stop this alert.
      summary: |
        ModuleConfig annotation for allow to disable is setted.
      severity: "4"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterMalfunctioning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The `image-availability-exporter` failed to perform any checks for the availability of images in the registry for over 20 minutes.

        You need to analyze its logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        image-availability-exporter has crashed.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterPodIsNotReady
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The images listed in the `image` field are not checked for availability in the container registry.

        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-monitoring describe deploy image-availability-exporter`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-monitoring describe pod -l app=image-availability-exporter`
      summary: |
        The image-availability-exporter Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterPodIsNotRunning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The images listed in the `image` field are not checked for availability in the container registry.

        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-monitoring describe deploy image-availability-exporter`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-monitoring describe pod -l app=image-availability-exporter`
      summary: |
        The image-availability-exporter Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterTargetAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Check the Pod status: `kubectl -n d8-monitoring get pod -l app=image-availability-exporter`

        Or check the Pod logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        There is no image-availability-exporter target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Check the Pod status: `kubectl -n d8-monitoring get pod -l app=image-availability-exporter`

        Or check the Pod logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        Prometheus cannot scrape the image-availability-exporter metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioActualDataPlaneVersionNotEqualDesired
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are Pods in Namespace `{{$labels.namespace}}` with istio data-plane version `{{$labels.version}}`, but the desired one is `{{$labels.desired_version}}`.
        Impact — istio version is to change after Pod restarting.
        Cheat sheet:
        ```
        ### namespace-wide configuration
        # istio.io/rev=vXYZ — use specific revision
        # istio-injection=enabled — use global revision
        kubectl get ns {{$labels.namespace}} --show-labels

        ### pod-wide configuration
        kubectl -n {{$labels.namespace}} get pods -l istio.io/rev={{$labels.desired_revision}}
        ```
      summary: |
        There are Pods with istio data-plane version {{$labels.version}}, but desired version is {{$labels.desired_version}}
      severity: "8"
      markupFormat: markdown
    - name: D8IstioActualVersionIsNotInstalled
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods with injected sidecar with version `{{$labels.version}}` (revision `{{$labels.revision}}`) in namespace `{{$labels.namespace}}`, but the control-plane version isn't installed. Consider installing it or change the Namespace or Pod configuration.
        Impact — Pods have lost their sync with k8s state.
        Getting orphaned pods:
        ```
        kubectl -n {{ $labels.namespace }} get pods -l 'service.istio.io/canonical-name' -o json | jq --arg revision {{ $labels.revision }} '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson | .revision == $revision) | .metadata.name'
        ```
      summary: |
        control-plane version for Pod with already injected sidecar isn't installed
      severity: "4"
      markupFormat: markdown
    - name: D8IstioAdditionalControlplaneDoesntWork
      sourceFile: modules/110-istio/monitoring/prometheus-rules/controlplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Additional istio controlplane `{{$labels.label_istio_io_rev}}` doesn' work.
        Impact — sidecar injection for Pods with `{{$labels.label_istio_io_rev}}` revision doesn't work.
        ```
        kubectl get pods -n d8-istio -l istio.io/rev={{$labels.label_istio_io_rev}}
        ```
      summary: |
        Additional controlplane doesn't work.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioDataPlaneVersionMismatch
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are Pods in `{{$labels.namespace}}` namespace with istio data-plane version `{{$labels.full_version}}` which differ from control-plane one `{{$labels.desired_full_version}}`.
        Consider restarting affected Pods, use PromQL query to get the list:
        ```
        max by (namespace, dataplane_pod) (d8_istio_dataplane_metadata{full_version="{{$labels.full_version}}"})
        ```
        Also consider using the automatic istio data-plane update described in the documentation: https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/110-istio/examples.html#upgrading-istio
      summary: |
        There are Pods with data-plane version different from control-plane one.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioDataPlaneWithoutIstioInjectionConfigured
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are Pods in `{{$labels.namespace}}` Namespace with istio sidecars, but the istio-injection isn't configured.
        Impact — Pods will lose their istio sidecars after re-creation.
        Getting affected Pods:
        ```
        kubectl -n {{$labels.namespace}} get pods -o json | jq -r --arg revision {{$labels.revision}} '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson | .revision == $revision) | .metadata.name'
        ```
      summary: |
        There are Pods with istio sidecars, but without istio-injection configured
      severity: "4"
      markupFormat: markdown
    - name: D8IstioDeprecatedIstioVersionInstalled
      sourceFile: modules/110-istio/monitoring/prometheus-rules/versions.tpl
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        There is deprecated istio version `{{$labels.version}}` installed.
        Impact — version support will be removed in future deckhouse releases. The higher alert severity — the higher probability of support cancelling.
        Upgrading instructions — https://deckhouse.io/documentation//modules/110-istio/examples.html#upgrading-istio.
      summary: |
        There is deprecated istio version installed
      severity: undefined
      markupFormat: markdown
    - name: D8IstioDesiredVersionIsNotInstalled
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There is desired istio control plane version `{{$labels.desired_version}}` (revision `{{$labels.revision}}`) configured for pods in namespace `{{$labels.namespace}}`, but the version isn't installed. Consider installing it or change the Namespace or Pod configuration.
        Impact — Pods won't be able to re-create in the `{{$labels.namespace}}` Namespace.
        Cheat sheet:
        ```
        ### namespace-wide configuration
        # istio.io/rev=vXYZ — use specific revision
        # istio-injection=enabled — use global revision
        kubectl get ns {{$labels.namespace}} --show-labels

        ### pod-wide configuration
        kubectl -n {{$labels.namespace}} get pods -l istio.io/rev={{$labels.revision}}
        ```
      summary: |
        Desired control-plane version isn't installed
      severity: "6"
      markupFormat: markdown
    - name: D8IstioFederationMetadataEndpointDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/federation.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        Metadata endpoint `{{$labels.endpoint}}` for IstioFederation `{{$labels.federation_name}}` has failed to fetch by d8 hook.
        Reproducing request to public endpoint:
        ```
        curl {{$labels.endpoint}}
        ```
        Reproducing request to private endpoints (run from deckhouse pod):
        ```
        KEY="$(deckhouse-controller module values istio -o json | jq -r .internal.remoteAuthnKeypair.priv)"
        LOCAL_CLUSTER_UUID="$(deckhouse-controller module values -g istio -o json | jq -r .global.discovery.clusterUUID)"
        REMOTE_CLUSTER_UUID="$(kubectl get istiofederation {{$labels.federation_name}} -o json | jq -r .status.metadataCache.public.clusterUUID)"
        TOKEN="$(deckhouse-controller helper gen-jwt --private-key-path <(echo "$KEY") --claim iss=d8-istio --claim sub=$LOCAL_CLUSTER_UUID --claim aud=$REMOTE_CLUSTER_UUID --claim scope=private-federation --ttl 1h)"
        curl -H "Authorization: Bearer $TOKEN" {{$labels.endpoint}}
        ```
      summary: |
        Federation metadata endpoint failed
      severity: "6"
      markupFormat: markdown
    - name: D8IstioGlobalControlplaneDoesntWork
      sourceFile: modules/110-istio/monitoring/prometheus-rules/controlplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Global istio controlplane `{{$labels.label_istio_io_rev}}` doesn' work.
        Impact — sidecar injection for Pods with global revision doesn't work, validating webhook for istio resources is absent.
        ```
        kubectl get pods -n d8-istio -l istio.io/rev={{$labels.label_istio_io_rev}}
        ```
      summary: |
        Global controlplane doesn't work.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioMulticlusterMetadataEndpointDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        Metadata endpoint `{{$labels.endpoint}}` for IstioMulticluster `{{$labels.multicluster_name}}` has failed to fetch by d8 hook.
        Reproducing request to public endpoint:
        ```
        curl {{$labels.endpoint}}
        ```
        Reproducing request to private endpoints (run from deckhouse pod):
        ```
        KEY="$(deckhouse-controller module values istio -o json | jq -r .internal.remoteAuthnKeypair.priv)"
        LOCAL_CLUSTER_UUID="$(deckhouse-controller module values -g istio -o json | jq -r .global.discovery.clusterUUID)"
        REMOTE_CLUSTER_UUID="$(kubectl get istiomulticluster {{$labels.multicluster_name}} -o json | jq -r .status.metadataCache.public.clusterUUID)"
        TOKEN="$(deckhouse-controller helper gen-jwt --private-key-path <(echo "$KEY") --claim iss=d8-istio --claim sub=$LOCAL_CLUSTER_UUID --claim aud=$REMOTE_CLUSTER_UUID --claim scope=private-multicluster --ttl 1h)"
        curl -H "Authorization: Bearer $TOKEN" {{$labels.endpoint}}
        ```
      summary: |
        Multicluster metadata endpoint failed
      severity: "6"
      markupFormat: markdown
    - name: D8IstioMulticlusterRemoteAPIHostDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        Remote api host `{{$labels.api_host}}` for IstioMulticluster `{{$labels.multicluster_name}}` has failed healthcheck by d8 monitoring hook.

        Reproducing (run from deckhouse pod):
        ```
        TOKEN="$(deckhouse-controller module values istio -o json | jq -r --arg ah {{$labels.api_host}} '.internal.multiclusters[]| select(.apiHost == $ah)| .apiJWT ')"
        curl -H "Authorization: Bearer $TOKEN" https://{{$labels.api_host}}/version
        ```
      summary: |
        Multicluster remote api host failed
      severity: "6"
      markupFormat: markdown
    - name: D8IstioOperatorReconcileError
      sourceFile: modules/110-istio/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        There is some error in istio-operator reconcilation loop. Please check the logs out:

        ```kubectl -n d8-istio logs -l app=operator,revision={{$labels.revision}}```
      summary: |
        istio-operator is unable to reconcile istio control-plane setup.
      severity: "5"
      markupFormat: markdown
    - name: D8IstioPodsWithoutIstioSidecar
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There is a Pod `{{$labels.dataplane_pod}}` in `{{$labels.namespace}}` Namespace without istio sidecars, but the istio-injection is configured.
        Getting affected Pods:
        ```
        kubectl -n {{$labels.namespace}} get pods -l '!service.istio.io/canonical-name' -o json | jq -r '.items[] | select(.metadata.annotations."sidecar.istio.io/inject" != "false") | .metadata.name'
        ```
      summary: |
        There are Pods without istio sidecars, but with istio-injection configured
      severity: "4"
      markupFormat: markdown
    - name: D8IstioVersionIsIncompatibleWithK8sVersion
      sourceFile: modules/110-istio/monitoring/prometheus-rules/versions.tpl
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        The current istio version `{{$labels.istio_version}}` may not work properly with the current k8s version `{{$labels.k8s_version}}`, because it is unsupported officially.
        Please upgrade istio as soon as possible.
        Upgrading instructions — https://deckhouse.io/documentation//modules/110-istio/examples.html#upgrading-istio.
      summary: |
        The installed istio version is incompatible with the k8s version
      severity: "3"
      markupFormat: markdown
    - name: D8KubeEtcdDatabaseSizeCloseToTheLimit
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        The size of the etcd database on `{{ $labels.node }}` has almost exceeded.
        Possibly there are a lot of events (e.g. Pod evictions) or a high number of other resources are created in the cluster recently.

        Possible solutions:
        - You can do defragmentation. Use next command:
        `kubectl -n kube-system exec -ti etcd-{{ $labels.node }} -- /usr/bin/etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s`
        - Increase node memory. Begin from 24 GB `quota-backend-bytes` will be increased on 1G every extra 8 GB node memory.
          For example:
          Node Memory  quota-backend-bytes
          16GB         2147483648 (2GB)
          24GB         3221225472 (3GB)
          32GB         4294967296 (4GB)
          40GB         5368709120 (5GB)
          48GB         6442450944 (6GB)
          56GB         7516192768 (7GB)
          64GB         8589934592 (8GB)
          72GB         8589934592 (8GB)
          ....
      summary: |
        etcd db size is close to the limit
      severity: "3"
      markupFormat: markdown
    - name: D8KubernetesVersionIsDeprecated
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        Current kubernetes version "{{ $labels.k8s_version }}" is deprecated, and its support will be removed within 6 months

        Please migrate to the next kubernetes version (at least 1.27)

        Check how to update the Kubernetes version in the cluster here - https://deckhouse.io/documentation/deckhouse-faq.html#how-do-i-upgrade-the-kubernetes-version-in-a-cluster
      summary: |
        Kubernetes version &quot;{{ $labels.k8s_version }}&quot; is deprecated
      severity: "7"
      markupFormat: markdown
    - name: D8LogShipperAgentNotScheduledInCluster
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        A number of log-shipper-agents are not scheduled.

        To check the state of the `d8-log-shipper/log-shipper-agent` DaemonSet:
        ```shell
        kubectl -n d8-log-shipper get daemonsets --selector=app=log-shipper
        ```

        To check the state of the `d8-log-shipper/log-shipper-agent` Pods:
        ```shell
        kubectl -n d8-log-shipper get pods --selector=app=log-shipper-agent
        ```

        The following command might help figuring out problematic nodes given you are aware where the DaemonSet should be scheduled in the first place:
        ```
        kubectl -n d8-log-shipper get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="log-shipper-agent")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Pods of log-shipper-agent cannot be scheduled in the cluster.
      severity: "7"
      markupFormat: markdown
    - name: D8LogShipperClusterLogDestinationD8LokiAuthorizationRequired
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/warnings.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |-
        Found ClusterLogDestination resource {{$labels.resource_name}} without authorization params.
        You should [add](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/460-log-shipper/faq.html#how-to-add-authorization-to-the-clusterlogdestination-resource) authorization params to the ClusterLogDestination resource.
      summary: |
        Required authorization params for ClusterLogDestination.
      severity: "9"
      markupFormat: markdown
    - name: D8LogShipperCollectLogErrors
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        The `{{ $labels.host }}` log-shipper agent on the `{{ $labels.node }}` node failed to collect metrics for more than 10 minutes.
        The reason is `{{ $labels.error_type }}` errors occurred during the `{{ $labels.stage }}` stage while reading `{{ $labels.component_type }}`.

        Consider checking logs of the pod or follow advanced debug instructions.
        `kubectl -n d8-log-shipper logs {{ $labels.host }}` -c vector
      summary: |
        Pods of log-shipper-agent cannot collect logs to the {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8LogShipperDestinationErrors
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Logs do not reach their destination, the `{{ $labels.host }}` log-shipper agent on the {{ $labels.node }} node cannot send logs for more than 10 minutes.
        The reason is `{{ $labels.error_type }}` errors occurred during the `{{ $labels.stage }}` stage while sending logs to `{{ $labels.component_type }}`.

        Consider checking logs of the pod or follow advanced debug instructions.
        `kubectl -n d8-log-shipper logs {{ $labels.host }}` -c vector
      summary: |
        Pods of log-shipper-agent cannot send logs to the {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8LogShipperLogsDroppedByRateLimit
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Rate limit rules are applied, log-shipper agent on the {{ $labels.node }} node is dropping logs for more than 10 minutes.

        Consider checking logs of the pod or follow advanced debug instructions.
        `kubectl -n d8-log-shipper get pods -o wide | grep {{ $labels.node }}`
      summary: |
        Pods of log-shipper-agent drop logs to the {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsNotRunning
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The {{$labels.pod}} Pod is {{$labels.phase}}.

        Run the following command to check the status of the Pod: `kubectl -n {{$labels.namespace}} get pods {{$labels.pod}} -o json | jq .status`.
      summary: |
        The machine-controller-manager Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsRestartingTooOften
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive machine-controller-manager restarts indicate that something is wrong. Normally, it should be up and running all the time.

        Please, refer to the logs: `kubectl -n d8-cloud-instance-manager logs -f -l app=machine-controller-manager -c controller`.
      summary: |
        The machine-controller-manager module restarts too often.
      severity: "9"
      markupFormat: markdown
    - name: D8MachineControllerManagerTargetAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        Machine controller manager manages ephemeral Nodes in the cluster. Its unavailability will result in the inability to add/delete Nodes.

        The recommended course of action:
        1. Check the availability and status of `machine-controller-manager` Pods: `kubectl -n d8-cloud-instance-manager get pods -l app=machine-controller-manager`;
        2. Check the availability of the `machine-controller-manager` Deployment: `kubectl -n d8-cloud-instance-manager get deploy machine-controller-manager`;
        3. Check the status of the `machine-controller-manager` Deployment: `kubectl -n d8-cloud-instance-manager describe deploy machine-controller-manager`.
      summary: |
        There is no machine-controller-manager target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerTargetDown
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape machine-controller-manager's metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8MetalLBBGPSessionDown
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        {{ $labels.job }} — MetalLB {{ $labels.container }} on {{ $labels.pod}} has BGP session {{ $labels.peer }} down.
        Details are in logs:
        ```
        kubectl -n d8-metallb logs daemonset/speaker -c speaker
        ```
      summary: |
        MetalLB BGP session down.
      severity: "4"
      markupFormat: markdown
    - name: D8MetalLBConfigNotLoaded
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        {{ $labels.job }} — MetalLB {{ $labels.container }} on {{ $labels.pod}} has not loaded.
        To figure out the problem, check controller logs:
        ```
        kubectl -n d8-metallb logs deploy/controller -c controller
        ```
      summary: |
        MetalLB config not loaded.
      severity: "4"
      markupFormat: markdown
    - name: D8MetalLBConfigStale
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        {{ $labels.job }} — MetalLB {{ $labels.container }} on {{ $labels.pod}} has run on a stale configuration, because the latest config failed to load.
        To figure out the problem, check controller logs:
        ```
        kubectl -n d8-metallb logs deploy/controller -c controller
        ```
      summary: |
        MetalLB running on a stale configuration, because the latest config failed to load.
      severity: "4"
      markupFormat: markdown
    - name: D8NeedDecreaseEtcdQuotaBackendBytes
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        Deckhouse can increase `quota-backend-bytes` only.
        It happens when control-plane nodes memory was reduced.
        If is true, you should set quota-backend-bytes manually with `controlPlaneManager.etcd.maxDbSize` configuration parameter.
        Before set new value, please check current DB usage on every control-plane node:
        ```
        for pod in $(kubectl get pod -n kube-system -l component=etcd,tier=control-plane -o name); do kubectl -n kube-system exec -ti "$pod" -- /usr/bin/etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key endpoint status -w json | jq --arg a "$pod" -r '.[0].Status.dbSize / 1024 / 1024 | tostring | $a + ": " + . + " MB"'; done
        ```
        Recommendations:
        - `controlPlaneManager.etcd.maxDbSize` maximum value is 8 GB.
        - If control-plane nodes have less than 24 GB, use 2 GB for `controlPlaneManager.etcd.maxDbSize`.
        - For >= 24GB increase value on 1GB every extra 8 GB.
          Node Memory  quota-backend-bytes
          16GB         2147483648 (2GB)
          24GB         3221225472 (3GB)
          32GB         4294967296 (4GB)
          40GB         5368709120 (5GB)
          48GB         6442450944 (6GB)
          56GB         7516192768 (7GB)
          64GB         8589934592 (8GB)
          72GB         8589934592 (8GB)
          ....
      summary: |
        Deckhouse considers that quota-backend-bytes should be reduced.
      severity: "6"
      markupFormat: markdown
    - name: D8NginxIngressKruiseControllerPodIsRestartingTooOften
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The number of restarts in the last hour: {{ $value }}.
        Excessive kruise controller restarts indicate that something is wrong. Normally, it should be up and running all the time.

        The recommended course of action:
        1. Check any events regarding kruise-controller-manager in d8-ingress-nginx namespace
        in case there were some issues there related to the nodes the manager runs on or memory shortage (OOM):  `kubectl -n d8-ingress-nginx get events | grep kruise-controller-manager`
        2. Analyze the controller's pods' descriptions to check which containers were restarted
        and what were the possible reasons (exit codes, etc.): `kubectl -n d8-ingress-nginx describe pod -lapp=kruise,control-plane=controller-manager`
        3. In case `kruise` container was restarted, list relevant logs of the container to check
        if there were some meaningful errors there: `kubectl -n d8-ingress-nginx logs -lapp=kruise,control-plane=controller-manager -c kruise`
      summary: |
        Too many kruise controller restarts have been detected in d8-ingress-nginx namespace.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeGroupIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for Nodes of the {{ $labels.node_group }} group; Nodes have learned about the update. However, no Node can get approval to start updating.

        Most likely, there is a problem with the `update_approval` hook of the `node-manager` module.
      summary: |
        The {{ $labels.node_group }} node group is not handling the update correctly.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeHasDeprecatedOSVersion
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/node-os-requirements.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Some nodes have deprecated OS versions. Please update nodes to actual OS version.

        To observe affected nodes use the expr `kube_node_info{os_image=~"Ubuntu 18.04.*|Debian GNU/Linux 10.*|CentOS Linux 7.*"}` in Prometheus.
      summary: |
        Nodes have deprecated OS versions.
      severity: "4"
      markupFormat: markdown
    - name: D8NodeHasUnmetKernelRequirements
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/kernel-requirements.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Some nodes have unmet kernel constraints. This means that some modules cannot be run on that nodes.
        Current kernel constraint requirements:
        For Cilium module kernel should be >= 4.9.17.
        For Cilium with Istio modules kernel should be >= 5.7.
        For Cilium with OpenVPN modules kernel should be >= 5.7.
        For Cilium with Node-local-dns modules kernel should be >= 5.7.

        To observe affected nodes use the expr `d8_node_kernel_does_not_satisfy_requirements == 1` in Prometheus.
      summary: |
        Nodes have unmet kernel requirements
      severity: "4"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group; the Node has learned about the update, requested and received approval, started the update, ran into a step that causes possible downtime. The update manager (the update_approval hook of node-group module) performed the update, and the Node received downtime approval. However, there is no success message about the update.

        Here is how you can view Bashible logs on the Node:
        ```shell
        journalctl -fu bashible
        ```
      summary: |
        The {{ $labels.node }} Node cannot complete the update.
      severity: "7"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group}; the Node has learned about the update, requested and received approval, but cannot complete the update.

        Here is how you can view Bashible logs on the Node:
        ```shell
        journalctl -fu bashible
        ```
      summary: |
        The {{ $labels.node }} Node cannot complete the update.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group but it has not received the update nor trying to.

        Most likely Bashible for some reason is not handling the update correctly. At this point, it must add the `update.node.deckhouse.io/waiting-for-approval` annotation to the Node so that it can be approved.

        You can find out the most current version of the update using this command:
        ```shell
        kubectl -n d8-cloud-instance-manager get secret configuration-checksums -o jsonpath={.data.{{ $labels.node_group }}} | base64 -d
        ```

        Use the following command to find out the version on the Node:
        ```shell
        kubectl get node {{ $labels.node }} -o jsonpath='{.metadata.annotations.node\.deckhouse\.io/configuration-checksum}'
        ```

        Here is how you can view Bashible logs on the Node:
        ```shell
        journalctl -fu bashible
        ```
      summary: |
        The {{ $labels.node }} Node does not update.
      severity: "9"
      markupFormat: markdown
    - name: D8NodeIsUnmanaged
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-unmanaged.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The {{ $labels.node }} Node is not managed by the [node-manager](http://documentation.example.com/modules/040-node-manager/) module.

        The recommended actions are as follows:
        - Follow these instructions to clean up the node and add it to the cluster: http://documentation.example.com/modules/040-node-manager/faq.html#how-to-clean-up-a-node-for-adding-to-the-cluster
      summary: |
        The {{ $labels.node }} Node is not managed by the node-manager module.
      severity: "9"
      markupFormat: markdown
    - name: D8NodeUpdateStuckWaitingForDisruptionApproval
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group; the Node has learned about the update, requested and received approval, started the update, and ran into a stage that causes possible downtime. For some reason, the Node cannot get that approval (it is issued fully automatically by the `update_approval` hook of the `node-manager`).
      summary: |
        The {{ $labels.node }} Node cannot get disruption approval.
      severity: "8"
      markupFormat: markdown
    - name: D8OkmeterAgentPodIsNotReady
      sourceFile: modules/500-okmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-okmeter
      module: okmeter
      edition: ce
      description: ""
      summary: |
        Okmeter agent is not Ready
      severity: "6"
      markupFormat: markdown
    - name: D8OldPrometheusCustomTargetFormat
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        Services with the `prometheus-custom-target` label are used to collect metrics in the cluster.

        Use the following command for filtering: `kubectl get service --all-namespaces --show-labels | grep prometheus-custom-target`.

        Note that the label format has changed. You need to change the `prometheus-custom-target` label to `prometheus.deckhouse.io/custom-target`.

        For more information, refer to the [documentation](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/300-prometheus/faq.html).
      summary: |
        Services with the prometheus-custom-target label are used to collect metrics in the cluster.
      severity: "9"
      markupFormat: markdown
    - name: D8OldPrometheusTargetFormat
      sourceFile: ee/fe/modules/340-monitoring-applications/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-applications
      module: monitoring-applications
      edition: fe
      description: |-
        Services with the `prometheus-target` label are used to collect metrics in the cluster.

        Use the following command to filter them: `kubectl get service --all-namespaces --show-labels | grep prometheus-target`

        Note that the label format has changed. You need to replace the `prometheus-target` label with `prometheus.deckhouse.io/target`.
      summary: |
        Services with the prometheus-target label are used to collect metrics in the cluster.
      severity: "6"
      markupFormat: markdown
    - name: D8ProblematicNodeGroupConfiguration
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for Nodes of the {{ $labels.node_group }} group; Nodes have learned about the update. However, {{ $labels.node }} Node cannot be updated.

        Node {{ $labels.node }} has no `node.deckhouse.io/configuration-checksum` annotation.
        Perhaps the bootstrap process of the Node did not complete correctly. Check the `cloud-init` logs (/var/log/cloud-init-output.log) of the Node.
        There is probably a problematic NodeGroupConfiguration resource for {{ $labels.node_group }} NodeGroup.
      summary: |
        The {{ $labels.node }} Node cannot begin the update.
      severity: "8"
      markupFormat: markdown
    - name: D8PrometheusLongtermFederationTargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/longterm-target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: prometheus-longterm cannot scrape "/federate" endpoint from Prometheus. Check error cause in prometheus-longterm WebUI or logs.
      summary: |
        prometheus-longterm cannot scrape prometheus.
      severity: "5"
      markupFormat: default
    - name: D8PrometheusLongtermTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        This Prometheus component is only used to display historical data and is not crucial. However, if its unavailability will last long enough, you will not be able to view the statistics.

        Usually, Pods of this type have problems because of disk unavailability (e.g., the disk cannot be mounted to a Node for some reason).

        The recommended course of action:
        1. Take a look at the StatefulSet data: `kubectl -n d8-monitoring describe statefulset prometheus-longterm`;
        2. Explore its PVC (if used): `kubectl -n d8-monitoring describe pvc prometheus-longterm-db-prometheus-longterm-0`;
        3. Explore the Pod's state: `kubectl -n d8-monitoring describe pod prometheus-longterm-0`.
      summary: |
        There is no prometheus-longterm target in Prometheus.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorPodIsNotReady
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action:
        1. Analyze the Deployment info: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`;
        2. Examine the status of the Pod and try to figure out why it is not running: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`.
      summary: |
        The prometheus-operator Pod is NOT Ready.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorPodIsNotRunning
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action:
        1. Analyze the Deployment info: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`;
        2. Examine the status of the Pod and try to figure out why it is not running: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`.
      summary: |
        The prometheus-operator Pod is NOT Running.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorTargetAbsent
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action is to analyze the deployment information: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`.
      summary: |
        There is no prometheus-operator target in Prometheus.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorTargetDown
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |-
        The `prometheus-operator` Pod is not available.

        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action:
        1. Analyze the Deployment info: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`;
        2. Examine the status of the Pod and try to figure out why it is not running: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`.
      summary: |
        Prometheus is unable to scrape prometheus-operator metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8ReservedNodeLabelOrTaintFound
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/reserved-domain.tpl
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        Node {{ $labels.name }} uses:
        - reserved `metadata.labels` *node-role.deckhouse.io/* with ending not in `(system|frontend|monitoring|_deckhouse_module_name_)`
        - or reserved `spec.taints` *dedicated.deckhouse.io* with values not in `(system|frontend|monitoring|_deckhouse_module_name_)`

        [Get instructions on how to fix it here](http://documentation.example.com/modules/040-node-manager/faq.html#how-do-i-allocate-nodes-to-specific-loads).
      summary: |
        Node {{ $labels.name }} needs fixing up
      severity: "6"
      markupFormat: markdown
    - name: D8RuntimeAuditEngineNotScheduledInCluster
      sourceFile: ee/modules/650-runtime-audit-engine/monitoring/prometheus-rules/runtime-audit-engine.yaml
      moduleUrl: 650-runtime-audit-engine
      module: runtime-audit-engine
      edition: ee
      description: |
        A number of runtime-audit-engine pods are not scheduled.
        Security audit is not fully operational.

        Consider checking state of the d8-runtime-audit-engine/runtime-audit-engine DaemonSet.
        `kubectl -n d8-runtime-audit-engine get daemonset,pod --selector=app=runtime-audit-engine`
        Get a list of nodes that have pods in an not Ready state.
        ```
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Pods of runtime-audit-engine cannot be scheduled in the cluster.
      severity: "4"
      markupFormat: markdown
    - name: D8SecretCopierDeprecatedLabels
      sourceFile: modules/600-secret-copier/monitoring/prometheus-rules/deprecated-label.yaml
      moduleUrl: 600-secret-copier
      module: secret-copier
      edition: ce
      description: |-
        The [secrets copier module](https://github.com/deckhouse/deckhouse/tree/main/modules/600-secret-copier/) has changed the service label for the original secrets in the `default` namespace.

        Soon we will abandon the old `antiopa-secret-copier: "yes"` label.

        You have to replace the `antiopa-secret-copier: "yes"` label with  `secret-copier.deckhouse.io/enabled: ""` for all secrets that the `secret-copier` module uses in the `default` namespace.
      summary: |
        Obsolete antiopa_secret_copier=yes label has been found.
      severity: "9"
      markupFormat: markdown
    - name: D8SmokeMiniNotBoundPersistentVolumeClaims
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        {{ $labels.persistentvolumeclaim }} persistent volume claim status is {{ $labels.phase }}.

        There is a problem with pv provisioning. Check the status of the pvc o find the problem:
        `kubectl -n d8-upmeter get pvc {{ $labels.persistentvolumeclaim }}`

        If you have no disk provisioning system in the cluster,
        you can disable ordering volumes for the some-mini through the module settings.
      summary: |
        Smoke-mini has unbound or lost persistent volume claims.
      severity: "9"
      markupFormat: markdown
    - name: D8SnapshotControllerPodIsNotReady
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-controller`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-controller`
      summary: |
        The snapshot-controller Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotControllerPodIsNotRunning
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-controller`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-controller`
      summary: |
        The snapshot-controller Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotControllerTargetAbsent
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Check the Pod status: `kubectl -n d8-snapshot-controller get pod -l app=snapshot-controller`
        2. Or check the Pod logs: `kubectl -n d8-snapshot-controller logs -l app=snapshot-controller -c snapshot-controller`
      summary: |
        There is no snapshot-controller target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotControllerTargetDown
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Check the Pod status: `kubectl -n d8-snapshot-controller get pod -l app=snapshot-controller`
        2. Or check the Pod logs: `kubectl -n d8-snapshot-controller logs -l app=snapshot-controller -c snapshot-controller`
      summary: |
        Prometheus cannot scrape the snapshot-controller metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotValidationWebhookPodIsNotReady
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-validation-webhook.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-validation-webhook`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-validation-webhook`
      summary: |
        The snapshot-validation-webhook Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotValidationWebhookPodIsNotRunning
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-validation-webhook.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-validation-webhook`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-validation-webhook`
      summary: |
        The snapshot-validation-webhook Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterClusterStateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Real Kubernetes cluster state is `{{ $labels.status }}` comparing to Terraform state.

        It's important to make them equal.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter cluster state changed
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterClusterStateError
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter can't check difference between Kubernetes cluster state and Terraform state.

        Probably, it occurred because Terraform-state-exporter had failed to run terraform with current state and config.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter cluster state error
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterHasErrors
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Errors occurred while terraform-state-exporter working.

        Check pods logs to get more details: `kubectl -n d8-system logs -l app=terraform-state-exporter -c exporter`
      summary: |
        Terraform-state-exporter has errors
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeStateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Real Node `{{ $labels.node_group }}/{{ $labels.name }}` state is `{{ $labels.status }}` comparing to Terraform state.

        It's important to make them equal.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter node state changed
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeStateError
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter can't check difference between Node `{{ $labels.node_group }}/{{ $labels.name }}` state and Terraform state.

        Probably, it occurred because Terraform-manager had failed to run terraform with current state and config.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter node state error
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeTemplateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter found difference between node template from cluster provider configuration and from NodeGroup `{{ $labels.name }}`.
        Node template is `{{ $labels.status }}`.

        First, run the `dhctl terraform check` command to check what will change.
        Use `dhctl converge` command or manually adjust NodeGroup settings to fix the issue.
      summary: |
        Terraform-state-exporter node template changed
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterPodIsNotReady
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter doesn't check the difference between real Kubernetes cluster state and Terraform state.

        Pease, check:
        1. Deployment description: `kubectl -n d8-system describe deploy terraform-state-exporter`
        2. Pod status: `kubectl -n d8-system describe pod -l app=terraform-state-exporter`
      summary: |
        Pod terraform-state-exporter is not Ready
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterPodIsNotRunning
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter doesn't check the difference between real Kubernetes cluster state and Terraform state.

        Pease, check:
        1. Deployment description: `kubectl -n d8-system describe deploy terraform-state-exporter`
        2. Pod status: `kubectl -n d8-system describe pod -l app=terraform-state-exporter`
      summary: |
        Pod terraform-state-exporter is not Running
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterTargetAbsent
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        To get more details:
        Check pods state: `kubectl -n d8-system get pod -l app=terraform-state-exporter` or logs: `kubectl -n d8-system logs -l app=terraform-state-exporter -c exporter`
      summary: |
        Prometheus has no terraform-state-exporter target
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterTargetDown
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        To get more details:
        Check pods state: `kubectl -n d8-system get pod -l app=terraform-state-exporter` or logs: `kubectl -n d8-system logs -l app=terraform-state-exporter -c exporter`
      summary: |
        Prometheus can't scrape terraform-state-exporter
      severity: "8"
      markupFormat: markdown
    - name: D8TricksterTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The following modules use this component:
        * `prometheus-metrics-adapter` — the unavailability of the component means that HPA (auto scaling) is not running and you cannot view resource consumption using `kubectl`;
        * `vertical-pod-autoscaler` — this module is quite capable of surviving a short-term unavailability, as VPA looks at the consumption history for 8 days;
        * `grafana` — by default, all dashboards use Trickster for caching requests to Prometheus. You can retrieve data directly from Prometheus (bypassing the Trickster). However, this may lead to high memory usage by Prometheus and, hence, to its unavailability.

        The recommended course of action:
        1. Analyze the Deployment information: `kubectl -n d8-monitoring describe deployment trickster`;
        2. Analyze the Pod information: `kubectl -n d8-monitoring describe pod -l app=trickster`;
        3. Usually, Trickster is unavailable due to Prometheus-related issues because the Trickster's readinessProbe checks the Prometheus availability. Thus, make sure that Prometheus is running: `kubectl -n d8-monitoring describe pod -l app.kubernetes.io/name=prometheus,prometheus=main`.
      summary: |
        There is no Trickster target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: D8TricksterTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The following modules use this component:
        * `prometheus-metrics-adapter` — the unavailability of the component means that HPA (auto scaling) is not running and you cannot view resource consumption using `kubectl`;
        * `vertical-pod-autoscaler` — this module is quite capable of surviving a short-term unavailability, as VPA looks at the consumption history for 8 days;
        * `grafana` — by default, all dashboards use Trickster for caching requests to Prometheus. You can retrieve data directly from Prometheus (bypassing the Trickster). However, this may lead to high memory usage by Prometheus and, hence, to unavailability.

        The recommended course of action:
        1. Analyze the Deployment stats: `kubectl -n d8-monitoring describe deployment trickster`;
        2. Analyze the Pod stats: `kubectl -n d8-monitoring describe pod -l app=trickster`;
        3. Usually, Trickster is unavailable due to Prometheus-related issues because the Trickster's readinessProbe checks the Prometheus availability. Thus, make sure that Prometheus is running: `kubectl -n d8-monitoring describe pod -l app.kubernetes.io/name=prometheus,prometheus=main`.
      summary: |
        There is no Trickster target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: D8UpmeterAgentPodIsNotReady
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: ""
      summary: |
        Upmeter agent is not Ready
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterAgentReplicasUnavailable
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |-
        Check DaemonSet status:
        `kubectl -n d8-upmeter get daemonset upmeter-agent -o json | jq .status`

        Check the status of its pod:
        `kubectl -n d8-upmeter get pods -l app=upmeter-agent -o json | jq '.items[] | {(.metadata.name):.status}'`
      summary: |
        One or more Upmeter agent pods is NOT Running
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageConfigmap
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Probe configmaps found.

        Upmeter agents should clean ConfigMaps produced by control-plane/basic probe. There should not be more
        configmaps than master nodes (upmeter-agent is a DaemonSet with master nodeSelector). Also, they should be
        deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the configmaps were left by old
        upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "basic-functionality") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional.

        3. Delete configmaps manually:

        `kubectl -n d8-upmeter delete cm -l heritage=upmeter`
      summary: |
        Garbage produced by basic probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageDeployment
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe deployments count per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean Deployments produced by control-plane/controller-manager probe. There should not
        be more deployments than master nodes (upmeter-agent is a DaemonSet with master nodeSelector).
        Also, they should be deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the deployments were left by old
        upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "controller-manager") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional, kube-controller-manager in particular.

        3. Delete deployments manually:

        `kubectl -n d8-upmeter delete deploy -l heritage=upmeter`
      summary: |
        Garbage produced by controller-manager probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageNamespaces
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe namespace per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean namespaces produced by control-plane/namespace probe. There should not be more
        of these namespaces than master nodes (upmeter-agent is a DaemonSet with master nodeSelector).
        Also, they should be deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the namespaces were left
        by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "namespace") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional.

        3. Delete namespaces manually: `kubectl -n d8-upmeter delete ns -l heritage=upmeter`
      summary: |
        Garbage produced by namespace probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbagePods
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe pods count per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean Pods produced by control-plane/scheduler probe. There should not be more
        of these pods than master nodes (upmeter-agent is a DaemonSet with master nodeSelector). Also, they should be
        deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the pods were left
        by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "scheduler") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional.

        3. Delete pods manually:

        `kubectl -n d8-upmeter delete po -l upmeter-probe=scheduler`
      summary: |
        Garbage produced by scheduler probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbagePodsFromDeployments
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe pods count per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean Deployments produced by control-plane/controller-manager probe,
        and hence kube-controller-manager should clean their pods. There should not be more of these pods than
        master nodes (upmeter-agent is a DaemonSet with master nodeSelector). Also, they should be
        deleted within seconds.

        This might be an indication of a problem with kube-apiserver or kube-controller-manager. Or, probably,
        the pods were left by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "controller-manager") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional, kube-controller-manager in particular.

        3. Delete pods manually:

        `kubectl -n d8-upmeter delete po -l upmeter-probe=controller-manager`
      summary: |
        Garbage produced by controller-manager probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageSecretsByCertManager
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Probe secrets found.

        Upmeter agents should clean certificates, and thus secrets produced by cert-manager should clean, too.
        There should not be more secrets than master nodes (upmeter-agent is a DaemonSet with master nodeSelector).
        Also, they should be deleted within seconds.

        This might be an indication of a problem with kube-apiserver, or cert-manager, or upmeter itself.
        It is also possible, that the secrets were left by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "cert-manager") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane and cert-manager are functional.

        3. Delete certificates manually, and secrets, if needed:

        ```
        kubectl -n d8-upmeter delete certificate -l upmeter-probe=cert-manager
        kubectl -n d8-upmeter get secret -ojson | jq -r '.items[] | .metadata.name' | grep upmeter-cm-probe | xargs -n 1 -- kubectl -n d8-upmeter delete secret
        ```
      summary: |
        Garbage produced by cert-manager probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterServerPodIsNotReady
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: ""
      summary: |
        Upmeter server is not Ready
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterServerPodIsRestartingTooOften
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Restarts for the last hour: {{ $value }}.

        Upmeter server should not restart too often. It should always be running and collecting episodes.
        Check its logs to find the problem:
        `kubectl -n d8-upmeter logs -f upmeter-0 upmeter`
      summary: |
        Upmeter server is restarting too often.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterServerReplicasUnavailable
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |-
        Check StatefulSet status:
        `kubectl -n d8-upmeter get statefulset upmeter -o json | jq .status`

        Check the status of its pod:
        `kubectl -n d8-upmeter get pods upmeter-0 -o json | jq '.items[] | {(.metadata.name):.status}'`
      summary: |
        One or more Upmeter server pods is NOT Running
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterSmokeMiniMoreThanOnePVxPVC
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The number of unnecessary smoke-mini PVs: {{ $value }}.

        Smoke-mini PVs should be deleted when released. Probably smoke-mini storage class has Retain policy by default,
        or there is CSI/cloud issue.

        These PVs have no valuable data on them an should be deleted.

        The list of PVs: `kubectl get pv | grep disk-smoke-mini`.
      summary: |
        Unnecessary smoke-mini volumes in cluster
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterTooManyHookProbeObjects
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average UpmeterHookProbe count per upmeter-agent pod is {{ $value }}, but should be strictly 1.

        Some of the objects were left by old upmeter-agent pods due to Upmeter update or downscale.

        Leave only newest objects corresponding to upemter-agent pods, when the reason it investigated.

        See `kubectl get upmeterhookprobes.deckhouse.io`.
      summary: |
        Too many UpmeterHookProbe objects in cluster
      severity: "9"
      markupFormat: markdown
    - name: D8YandexNatInstanceConnectionsQuotaUtilization
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/nat-instance.tpl
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: Nat-instance connections quota should be increased by Yandex technical support.
      summary: |
        Yandex nat-instance connections quota utilization is above 85% over the last 5 minutes.
      severity: "4"
      markupFormat: markdown
    - name: DaemonSetAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Unable to login to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image in the `{{ $labels.namespace }}` Namespace; in the DaemonSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to login to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Insufficient privileges to pull the `{{ $labels.image }}` image using the `imagePullSecrets` specified in the `{{ $labels.namespace }}` Namespace; in the DaemonSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the imagePullSecrets specified.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image name is spelled correctly: in the `{{ $labels.namespace }}` Namespace; in the DaemonSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image has incorrect name.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image is available: in the `{{ $labels.namespace }}` Namespace; in the DaemonSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The container registry is not available for the `{{ $labels.image }}` image: in the `{{ $labels.namespace }}` Namespace; in the DaemonSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        An unknown error occurred for the  `{{ $labels.image }}` image
        in the `{{ $labels.namespace }}` Namespace;
        in the DaemonSet `{{ $labels.name }}`
        in the `{{ $labels.container }}` container in the registry.

        Refer to the exporter logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        An unknown error occurred for the  {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeadMansSwitch
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.
      summary: |
        Alerting DeadMansSwitch
      severity: "4"
      markupFormat: default
    - name: DeckhouseModuleUseEmptyDir
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/emptydir.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Deckhouse module {{ $labels.module_name }} use emptydir as storage.
      summary: |
        Deckhouse module {{ $labels.module_name }} use emptydir as storage.
      severity: "9"
      markupFormat: markdown
    - name: DeckhouseReleaseDisruptionApprovalRequired
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse release contains disruption update.

        You can figure out more details by running `kubectl describe DeckhouseRelease {{ $labels.name }}`.
        If you are ready to deploy this release, run: `kubectl annotate DeckhouseRelease {{ $labels.name }} release.deckhouse.io/disruption-approved=true`.
      summary: |
        Deckhouse release disruption approval required.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseReleaseIsBlocked
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse release requirements is not met.

        Please run `kubectl describe DeckhouseRelease {{ $labels.name }}` for details.
      summary: |
        Deckhouse release requirements unmet.
      severity: "5"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse release is waiting for manual approval.

        Please run `kubectl patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'` for confirmation.
      summary: |
        Deckhouse release is waiting for manual approval.
      severity: "3"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse release is waiting for manual approval.

        Please run `kubectl patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'` for confirmation.
      summary: |
        Deckhouse release is waiting for manual approval.
      severity: "6"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse release is waiting for manual approval.

        Please run `kubectl patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'` for confirmation.
      summary: |
        Deckhouse release is waiting for manual approval.
      severity: "9"
      markupFormat: markdown
    - name: DeckhouseUpdating
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Deckhouse is being updated.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseUpdatingFailed
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Failed to update Deckhouse.

        Next version minor/path Deckhouse image is not available in the registry or the image is corrupted.
        Actual version: {{ $labels.version }}.

        Make sure that the next version Deckhouse image is available in the registry.
      summary: |
        Deckhouse updating is failed.
      severity: "4"
      markupFormat: markdown
    - name: DeploymentAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Unable to login to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image in the `{{ $labels.namespace }}` Namespace; in the Deployment `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to login to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Insufficient privileges to pull the `{{ $labels.image }}` image using the `imagePullSecrets` specified in the `{{ $labels.namespace }}` Namespace; in the Deployment `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the imagePullSecrets specified.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image name is spelled correctly: in the `{{ $labels.namespace }}` Namespace; in the Deployment `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image has incorrect name.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentGenerationMismatch
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kube-state-metrics.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Observed deployment generation does not match expected one for deployment {{$labels.namespace}}/{{$labels.deployment}}
      summary: |
        Deployment is outdated
      severity: "4"
      markupFormat: default
    - name: DeploymentImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image is available: in the `{{ $labels.namespace }}` Namespace; in the Deployment `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The container registry is not available for the `{{ $labels.image }}` image: in the `{{ $labels.namespace }}` Namespace; in the Deployment `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        An unknown error occurred for the  `{{ $labels.image }}` image
        in the `{{ $labels.namespace }}` Namespace;
        in the Deployment `{{ $labels.name }}`
        in the `{{ $labels.container }}` container in the registry.

        Refer to the exporter logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        An unknown error occurred for the  {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeprecatedGeoIPVersion
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/deprecated-geoip-version.tpl
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        There is an IngressNginxController and/or an Ingress object that utilize(s) Nginx GeoIPv1 module's variables. The module is deprecated and its support is discontinued from Ingess Nginx Controller of version 1.10 and higher. It's recommend to upgrade your configuration to use [GeoIPv2 module](http://documentation.example.com/modules/402-ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-geoip2).
        Use the following command to get the list of the IngressNginxControllers that contain GeoIPv1 variables:
        `kubectl  get ingressnginxcontrollers.deckhouse.io -o json | jq '.items[] | select(..|strings | test("\\$geoip_(country_(code3|code|name)|area_code|city_continent_code|city_country_(code3|code|name)|dma_code|latitude|longitude|region|region_name|city|postal_code|org)([^_a-zA-Z0-9]|$)+")) | .metadata.name'`

        Use the following command to get the list of the Ingress objects that contain GeoIPv1 variables:
        `kubectl  get ingress -A -o json | jq '.items[] | select(..|strings | test("\\$geoip_(country_(code3|code|name)|area_code|city_continent_code|city_country_(code3|code|name)|dma_code|latitude|longitude|region|region_name|city|postal_code|org)([^_a-zA-Z0-9]|$)+")) | "\(.metadata.namespace)/\(.metadata.name)"' | sort | uniq`
      summary: |
        Deprecated GeoIP version 1 is being used in the cluster.
      severity: "9"
      markupFormat: markdown
    - name: EarlyOOMPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/early-oom.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod has detected unavailable PSI subsystem. Check logs for additional information: kubectl -n d8-cloud-instance-manager logs {{$labels.pod}} Possible actions to resolve the problem: * Upgrade kernel to version 4.20 or higher. * Enable Pressure Stall Information. * Disable early oom.
      severity: "8"
      markupFormat: markdown
    - name: EbpfExporterKernelNotSupported
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/ebpf-exporter.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        The BTF module required for ebpf_exporter is missing in the kernel. Possible actions to resolve the problem: * Built kernel with BTF type information info. * Disable ebpf_exporter
      severity: "8"
      markupFormat: markdown
    - name: ExtendedMonitoringDeprecatatedAnnotation
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/deprecated-annotation.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: ""
      summary: |
        Deprecated extended-monitoring.flant.com/enabled annotations are used in cluster. Migrate to extended-monitoring.deckhouse.io/enabled label ASAP. Check d8_deprecated_legacy_annotation metric in Prometheus to get list of all usages.
      severity: "4"
      markupFormat: markdown
    - name: ExtendedMonitoringTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/self.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Pod with extended-monitoring exporter is unavailable.

        Following alerts will not be fired:
          * About lack of the space and inodes on volumes
          * CPU overloads and throttling of containers
          * 500 errors on ingress
          * Replicas quantity of controllers (alerts about the insufficient amount of replicas of  Deployment, StatefulSet, DaemonSet)
          * And [others](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/340-extended-monitoring/)

        To debug, execute the following commands:
          1. `kubectl -n d8-monitoring describe deploy extended-monitoring-exporter`
          2. `kubectl -n d8-monitoring describe pod -l app=extended-monitoring-exporter`
      summary: |
        Extended-monitoring is down
      severity: "5"
      markupFormat: markdown
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $labels.job }}: {{ $labels.instance }} instance will exhaust in file/socket descriptors within the next hour'
      summary: |
        file descriptors soon exhausted
      severity: "3"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} instance will exhaust in file/socket descriptors within the next hour'
      summary: |
        file descriptors soon exhausted
      severity: "3"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $labels.job }}: {{ $labels.instance }} instance will exhaust in file/socket descriptors within the next 4 hours'
      summary: |
        file descriptors soon exhausted
      severity: "4"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} instance will exhaust in file/socket descriptors within the next 4 hours'
      summary: |
        file descriptors soon exhausted
      severity: "4"
      markupFormat: default
    - name: GrafanaDashboardAlertRulesDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before updating to Grafana 10, it's required to migrate an outdated alerts from Grafana to the external alertmanager (or exporter-alertmanager stack)
        To list all deprecated alert rules use the expr `sum by (dashboard, panel, alert_rule) (d8_grafana_dashboards_deprecated_alert_rule) > 0`

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        Deprecated Grafana alerts have been found.
      severity: "8"
      markupFormat: markdown
    - name: GrafanaDashboardPanelIntervalDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before updating to Grafana 10, it's required to rewrite an outdated expressions that uses `$interval_rv`, `interval_sx3` or `interval_sx4` to `$__rate_interval`
        To list all deprecated panel intervals use the expr `sum by (dashboard, panel, interval) (d8_grafana_dashboards_deprecated_interval) > 0`

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        Deprecated Grafana panel intervals have been found.
      severity: "8"
      markupFormat: markdown
    - name: GrafanaDashboardPluginsDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before updating to Grafana 10, it's required to check if currently installed plugins will work correctly with Grafana 10
        To list all potentially outdated plugins use the expr `sum by (dashboard, panel, plugin) (d8_grafana_dashboards_deprecated_plugin) > 0`

        Plugin "flant-statusmap-panel" is being deprecated and won't be supported in the near future
        We recommend you to migrate to the State Timeline plugin: https://grafana.com/docs/grafana/latest/panels-visualizations/visualizations/state-timeline/

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        Deprecated Grafana plugins have been found.
      severity: "8"
      markupFormat: markdown
    - name: HelmReleasesHasResourcesWithDeprecatedVersions
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/helm/deprecated-versions.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        To observe all resources use the expr `max by (helm_release_namespace, helm_release_name, helm_version, resource_namespace, resource_name, api_version, kind, k8s_version) (resource_versions_compatibility) == 1` in Prometheus.

        You can find more details for migration in the deprecation guide: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v{{ $labels.k8s_version | reReplaceAll "\\." "-" }}.

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        At least one HELM release contains resources with deprecated apiVersion, which will be removed in Kubernetes v{{ $labels.k8s_version }}.
      severity: "5"
      markupFormat: markdown
    - name: HelmReleasesHasResourcesWithUnsupportedVersions
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/helm/deprecated-versions.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        To observe all resources use the expr `max by (helm_release_namespace, helm_release_name, helm_version, resource_namespace, resource_name, api_version, kind, k8s_version) (resource_versions_compatibility) == 2` in Prometheus.

        You can find more details for migration in the deprecation guide: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v{{ $labels.k8s_version | reReplaceAll "\\." "-" }}.

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        At least one HELM release contains resources with unsupported apiVersion for Kubernetes v{{ $labels.k8s_version }}.
      severity: "4"
      markupFormat: markdown
    - name: IngressResponses5xx
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/ingress.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} with Service name "{{$labels.service}}" and port "{{$labels.service_port}}" has more than {{ printf "extended_monitoring_ingress_threshold{threshold=\"5xx-critical\", namespace=\"%s\", ingress=\"%s\"}" $labels.namespace $labels.ingress | query | first | value }}% 5xx responses from backend.

        Currently at: {{ .Value }}%
      summary: |
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} has more than {{ printf &quot;extended_monitoring_ingress_threshold{threshold=&quot;5xx-critical&quot;, namespace=&quot;%s&quot;, ingress=&quot;%s&quot;}&quot; $labels.namespace $labels.ingress | query | first | value }}% 5xx responses from backend.
      severity: "4"
      markupFormat: default
    - name: IngressResponses5xx
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/ingress.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} with Service name "{{$labels.service}}" and port "{{$labels.service_port}}" has more than {{ printf "extended_monitoring_ingress_threshold{threshold=\"5xx-warning\", namespace=\"%s\", ingress=\"%s\"}" $labels.namespace $labels.ingress | query | first | value }}% 5xx responses from backend.

        Currently at: {{ .Value }}%
      summary: |
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} has more than {{ printf &quot;extended_monitoring_ingress_threshold{threshold=&quot;5xx-warning&quot;, namespace=&quot;%s&quot;, ingress=&quot;%s&quot;}&quot; $labels.namespace $labels.ingress | query | first | value }}% 5xx responses from backend.
      severity: "5"
      markupFormat: default
    - name: IstioIrrelevantExternalServiceFound
      sourceFile: modules/110-istio/monitoring/prometheus-rules/services.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        There is service in the namespace: `{{$labels.namespace}}` with the name: `{{$labels.name}}` which has irrelevant ports spec.
        .spec.ports[] do not make any sense for services with a type `ExternalName` but
        istio renders for External Services with ports listener "0.0.0.0:port" which catch all the traffic to the port. It is a problem for services out of istio registry.

        It is recommended to get rid of ports section (`.spec.ports`). It is safe.
      summary: |
        Found external service with irrelevant ports spec
      severity: "5"
      markupFormat: markdown
    - name: K8SApiserverDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: No API servers are reachable or all have disappeared from service discovery
      summary: |
        No API servers are reachable
      severity: "3"
      markupFormat: default
    - name: K8sCertificateExpiration
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Some clients connect to {{$labels.component}} with certificate which expiring soon (less than 1 day) on node {{$labels.component}}.

        You need to use `kubeadm` to check control plane certificates.
        1. Install kubeadm: `apt install kubeadm=1.24.*`.
        2. Check certificates: `kubeadm alpha certs check-expiration`

        To check kubelet certificates, on each node you need to:
        1. Check kubelet config:
        ```
        ps aux \
          | grep "/usr/bin/kubelet" \
          | grep -o -e "--kubeconfig=\S*" \
          | cut -f2 -d"=" \
          | xargs cat
        ```
        2. Find field `client-certificate` or `client-certificate-data`
        3. Check certificate using openssl

        There are no tools to help you find other stale kubeconfigs.
        It will be better for you to enable `control-plane-manager` module to be able to debug in this case.
      summary: |
        Kubernetes has API clients with soon expiring certificates
      severity: "5"
      markupFormat: markdown
    - name: K8sCertificateExpiration
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Some clients connect to {{$labels.component}} with certificate which expiring soon (less than 7 days) on node {{$labels.node}}.

        You need to use `kubeadm` to check control plane certificates.
        1. Install kubeadm: `apt install kubeadm=1.24.*`.
        2. Check certificates: `kubeadm alpha certs check-expiration`

        To check kubelet certificates, on each node you need to:
        1. Check kubelet config:
        ```
        ps aux \
          | grep "/usr/bin/kubelet" \
          | grep -o -e "--kubeconfig=\S*" \
          | cut -f2 -d"=" \
          | xargs cat
        ```
        2. Find field `client-certificate` or `client-certificate-data`
        3. Check certificate using openssl

        There are no tools to help you find other stale kubeconfigs.
        It will be better for you to enable `control-plane-manager` module to be able to debug in this case.
      summary: |
        Kubernetes has API clients with soon expiring certificates
      severity: "6"
      markupFormat: markdown
    - name: K8SControllerManagerTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-controller-manager.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: There is no running kube-controller-manager. Deployments and replication controllers are not making progress.
      summary: |
        Controller manager is down
      severity: "3"
      markupFormat: default
    - name: K8SKubeletDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus failed to scrape {{ $value }}% of kubelets.
      summary: |
        Many kubelets cannot be scraped
      severity: "3"
      markupFormat: default
    - name: K8SKubeletDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus failed to scrape {{ $value }}% of kubelets.
      summary: |
        A few kubelets cannot be scraped
      severity: "4"
      markupFormat: default
    - name: K8SKubeletTooManyPods
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Kubelet {{ $labels.node }} is running {{ $value }} pods, close to the limit of {{ printf "kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\",unit=\"integer\",node=\"%s\"}" $labels.node | query | first | value }}
      summary: |
        Kubelet is close to pod limit
      severity: "7"
      markupFormat: default
    - name: K8SManyNodesNotReady
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $value }}% of Kubernetes nodes are not ready'
      summary: |
        Too many nodes are not ready
      severity: "3"
      markupFormat: default
    - name: K8SNodeNotReady
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: The Kubelet on {{ $labels.node }} has not checked in with the API, or has set itself to NotReady, for more than 10 minutes
      summary: |
        Node status is NotReady
      severity: "3"
      markupFormat: default
    - name: K8SSchedulerTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-scheduler.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: There is no running K8S scheduler. New pods are not being assigned to nodes.
      summary: |
        Scheduler is down
      severity: "3"
      markupFormat: default
    - name: K8STooManyNodes
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/nodes.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: Cluster is running {{ $value }} nodes, close to the maximum amount of {{ print "d8_max_nodes_amount{}" | query | first | value }} nodes.
      summary: |
        Nodes amount is close to the maximum allowed amount.
      severity: "7"
      markupFormat: default
    - name: KubeEtcdHighFsyncDurations
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        In the last 15 minutes, the 99th percentile of the fsync duration for WAL files is longer than 0.5 seconds: {{ $value }}.

        Possible causes:
        1. High latency of the disk where the etcd data is located;
        2. High CPU usage on the Node.
      summary: |
        Synching (fsync) WAL files to disk is slow.
      severity: "7"
      markupFormat: markdown
    - name: KubeEtcdHighNumberOfLeaderChanges
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        There were {{ $value }} leader re-elections for the etcd cluster member running on the {{ $labels.node }} Node in the last 10 minutes.

        Possible causes:
        1. High latency of the disk where the etcd data is located;
        2. High CPU usage on the Node;
        3. Degradation of network connectivity between cluster members in the multi-master mode.
      summary: |
        The etcd cluster re-elects the leader too often.
      severity: "5"
      markupFormat: markdown
    - name: KubeEtcdInsufficientMembers
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Check the status of the etcd pods: `kubectl -n kube-system get pod -l component=etcd`.
      summary: |
        There are insufficient members in the etcd cluster; the cluster will fail if one of the remaining members will become unavailable.
      severity: "4"
      markupFormat: markdown
    - name: KubeEtcdNoLeader
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Check the status of the etcd Pods: `kubectl -n kube-system get pod -l component=etcd | grep {{ $labels.node }}`.
      summary: |
        The etcd cluster member running on the {{ $labels.node }} Node has lost the leader.
      severity: "4"
      markupFormat: markdown
    - name: KubeEtcdTargetAbsent
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Check the status of the etcd Pods: `kubectl -n kube-system get pod -l component=etcd` or Prometheus logs: `kubectl -n d8-monitoring logs -l app.kubernetes.io/name=prometheus -c prometheus`
      summary: |
        There is no etcd target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: KubeEtcdTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Check the status of the etcd Pods: `kubectl -n kube-system get pod -l component=etcd` or Prometheus logs: `kubectl -n d8-monitoring logs -l app.kubernetes.io/name=prometheus -c prometheus`.
      summary: |
        Prometheus is unable to scrape etcd metrics.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        No more free bytes on imagefs (filesystem that the container runtime uses for storing images and container writable layers) on node {{$labels.node}} mountpoint {{$labels.mountpoint}}.
      summary: |
        No more free bytes on imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of imagefs (filesystem that the container runtime uses for storing images and container writable layers) on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_imagefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Hard eviction of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Close to hard eviction threshold of imagefs (filesystem that the container runtime uses for storing images and container writable layers) on node {{$labels.node}} mountpoint {{$labels.mountpoint}}.

        Threshold at: {{ printf "kubelet_eviction_imagefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Close to hard eviction threshold of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "7"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of imagefs (filesystem that the container runtime uses for storing images and container writable layers) on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_imagefs_bytes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Soft eviction of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No more free inodes on imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_imagefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Hard eviction of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Close to hard eviction threshold of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.

        Threshold at: {{ printf "kubelet_eviction_imagefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Close to hard eviction threshold of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "7"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_imagefs_inodes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Soft eviction of imagefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No more free space on nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "5"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_nodefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Hard eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Close to hard eviction threshold of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.

        Threshold at: {{ printf "kubelet_eviction_nodefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Close to hard eviction threshold of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "7"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_nodefs_bytes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Soft eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No more free inodes on nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.
      severity: "5"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_nodefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Hard eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Close to hard eviction threshold of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint.

        Threshold at: {{ printf "kubelet_eviction_nodefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Close to hard eviction threshold of nodefs on the {{$labels.node}} Node at the  {{$labels.mountpoint}} mountpoint.
      severity: "7"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of nodefs on the {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.

        Threshold at: {{ printf "kubelet_eviction_nodefs_inodes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Currently at: {{ .Value }}%
      summary: |
        Soft eviction of nodefs on the  {{$labels.node}} Node at the {{$labels.mountpoint}} mountpoint is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesAPFRejectRequests
      sourceFile: modules/011-flow-schema/monitoring/prometheus-rules/flow-schema.yaml
      moduleUrl: 011-flow-schema
      module: flow-schema
      edition: ce
      description: |
        To show APF schema queue requests, use the expr `apiserver_flowcontrol_current_inqueue_requests{flow_schema="d8-serviceaccounts"}`.

        Attention: This is an experimental alert!
      summary: |
        APF flow schema d8-serviceaccounts has rejected API requests.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesCoreDNSHasCriticalErrors
      sourceFile: modules/042-kube-dns/monitoring/prometheus-rules/kubernetes/dns.yaml
      moduleUrl: 042-kube-dns
      module: kube-dns
      edition: ce
      description: |-
        CoreDNS pod {{$labels.pod}} has at least one critical error.
        To debug the problem, look into container logs: `kubectl -n kube-system logs {{$labels.pod}}`
      summary: |
        CoreDNS has critical errors.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDaemonSetNotUpToDate
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        There are {{ .Value }} outdated Pods in the {{ $labels.namespace }}/{{ $labels.daemonset }} DaemonSet for the last 15 minutes.

        The recommended course of action:
        1. Check the DaemonSet's status: `kubectl -n {{ $labels.namespace }} get ds {{ $labels.daemonset }}`
        2. Analyze the DaemonSet's description: `kubectl -n {{ $labels.namespace }} describe ds {{ $labels.daemonset }}`
        3. If the `Number of Nodes Scheduled with Up-to-date Pods` parameter does not match
        `Current Number of Nodes Scheduled`, check the DaemonSet's updateStrategy:
        `kubectl -n {{ $labels.namespace }} get ds {{ $labels.daemonset }} -o json | jq '.spec.updateStrategy'`
        4. Note that if the OnDelete updateStrategy is set, the DaemonSet gets only updated when Pods are deleted.
      summary: |
        There are {{ .Value }} outdated Pods in the {{ $labels.namespace }}/{{ $labels.daemonset }} DaemonSet for the last 15 minutes.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesDaemonSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Count of available replicas in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} is at zero.

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}

        This command might help figuring out problematic nodes given you are aware where the DaemonSet should be scheduled in the first place (using label selector for pods might be of help, too):

        ```
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Count of available replicas in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} is at zero.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDaemonSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Count of unavailable replicas in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} is above threshold.
        Currently at: {{ .Value }} unavailable replica(s)
        Threshold at: {{ printf "extended_monitoring_daemonset_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", daemonset=\"%s\"}" $labels.namespace $labels.daemonset | query | first | value }} unavailable replica(s)

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}

        This command might help figuring out problematic nodes given you are aware where the DaemonSet should be scheduled in the first place (using label selector for pods might be of help, too):

        ```
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Count of unavailable replicas in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} is above threshold.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesDeploymentReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Count of available replicas in Deployment {{$labels.namespace}}/{{$labels.deployment}} is at zero.

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"Deployment\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        Count of available replicas in Deployment {{$labels.namespace}}/{{$labels.deployment}} is at zero.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDeploymentReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Count of unavailable replicas in Deployment {{$labels.namespace}}/{{$labels.deployment}} is violating "spec.strategy.rollingupdate.maxunavailable".

        Currently at: {{ .Value }} unavailable replica(s)
        Threshold at: {{ printf "extended_monitoring_deployment_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", deployment=\"%s\"}" $labels.namespace $labels.deployment | query | first | value }} unavailable replica(s)

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"Deployment\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        Count of unavailable replicas in Deployment {{$labels.namespace}}/{{$labels.deployment}} is violating &quot;spec.strategy.rollingupdate.maxunavailable&quot;.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesDnsTargetDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/kube-dns.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Prometheus is unable to collect metrics from kube-dns. Thus its status is unknown.

        To debug the problem, use the following commands:
        1. `kubectl -n kube-system describe deployment -l k8s-app=kube-dns`
        2. `kubectl -n kube-system describe pod -l k8s-app=kube-dns`
      summary: |
        Kube-dns or CoreDNS are not under monitoring.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesStatefulSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Count of ready replicas in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}} at zero.

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"StatefulSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        Count of ready replicas in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}} at zero.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesStatefulSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Count of unavailable replicas in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}} above threshold.

        Currently at: {{ .Value }} unavailable replica(s)
        Threshold at: {{ printf "extended_monitoring_statefulset_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", statefulset=\"%s\"}" $labels.namespace $labels.statefulset | query | first | value }} unavailable replica(s)

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"StatefulSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        Count of unavailable replicas in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}} above threshold.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesVersionEndOfLife
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        Current kubernetes version "{{ $labels.k8s_version }}" support will be removed in the next Deckhouse release (1.58).

        Please migrate to the next kubernetes version (at least 1.24) as soon as possible.

        Check how to update the Kubernetes version in the cluster here - https://deckhouse.io/documentation/deckhouse-faq.html#how-do-i-upgrade-the-kubernetes-version-in-a-cluster
      summary: |
        Kubernetes version &quot;{{ $labels.k8s_version }}&quot; has reached End Of Life.
      severity: "4"
      markupFormat: markdown
    - name: KubeStateMetricsDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kube-state-metrics.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        There are no metrics about cluster resources for 5 minutes.

        Most alerts an monitroing panels aren't working.

        To debug the problem:
        1. Check kube-state-metrics pods: `kubectl -n d8-monitoring describe pod -l app=kube-state-metrics`
        2. Check its logs: `kubectl -n d8-monitoring describe deploy kube-state-metrics`
      summary: |
        Kube-state-metrics is not working in the cluster.
      severity: "3"
      markupFormat: markdown
    - name: L2LoadBalancerModuleDeprecated
      sourceFile: ee/modules/381-l2-load-balancer/monitoring/prometheus-rules/services.yaml
      moduleUrl: 381-l2-load-balancer
      module: l2-load-balancer
      edition: ee
      description: |
        The L2LoadBalancer module is deprecated and will be removed in a future release. Disable the module and use MetalLB module in L2 mode.
      summary: |
        L2LoadBalancer module is deprecated
      severity: "3"
      markupFormat: markdown
    - name: L2LoadBalancerOrphanServiceFound
      sourceFile: ee/modules/381-l2-load-balancer/monitoring/prometheus-rules/services.yaml
      moduleUrl: 381-l2-load-balancer
      module: l2-load-balancer
      edition: ee
      description: |
        There is orphan service in the namespace: `{{$labels.namespace}}` with the name: `{{$labels.name}}` which has irrelevant L2LoadBalancer name.

        It is recommended to check L2LoadBalancer name in annotations (`network.deckhouse.io/l2-load-balancer-name`).
      summary: |
        Found orphan service with irrelevant L2LoadBalancer name
      severity: "4"
      markupFormat: markdown
    - name: LoadAverageHigh
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: For the last 5 minutes, the load average on the {{ $labels.node }} Node has been higher than {{ printf "extended_monitoring_node_threshold{threshold=\"load-average-per-core-critical\", node=\"%s\"}" $labels.node | query | first | value }} per core. There are more processes in the queue than the CPU can handle; probably, some process has created too many threads or child processes, or the CPU is overloaded.
      summary: |
        The load average on the {{ $labels.node }} Node is too high.
      severity: "4"
      markupFormat: markdown
    - name: LoadAverageHigh
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: For the last 30 minutes, the load average on the {{ $labels.node }} Node has been higher or equal to {{ printf "extended_monitoring_node_threshold{threshold=\"load-average-per-core-warning\", node=\"%s\"}" $labels.node | query | first | value }} per core. There are more processes in the queue than the CPU can handle; probably, some process has created too many threads or child processes, or the CPU is overloaded.
      summary: |
        The load average on the {{ $labels.node }} Node is too high.
      severity: "5"
      markupFormat: markdown
    - name: LoadBalancerServiceWithoutExternalIP
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/loadbalancer.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        One or more services with the LoadBalancer type cannot get an external address.

        The list of services can be obtained with the following command:
        kubectl get svc -Ao json | jq -r '.items[] | select(.spec.type == "LoadBalancer") | select(.status.loadBalancer.ingress[0].ip == null) | "namespace: \(.metadata.namespace), name: \(.metadata.name), ip: \(.status.loadBalancer.ingress[0].ip)"'
        Check the cloud-controller-manager logs in the 'd8-cloud-provider-*' namespace
        If you are using a bare-metal cluster with the metallb module enabled, check that the address space of the pool has not been exhausted.
      summary: |
        A load balancer has not been created.
      severity: "4"
      markupFormat: default
    - name: MigrationRequiredFromRBDInTreeProvisionerToCSIDriver
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/migration-alerts.tpl
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To migrate volumes use this script https://github.com/deckhouse/deckhouse/blob//modules/031-ceph-csi/tools/rbd-in-tree-to-ceph-csi-migration-helper.sh
        A description of how the migration is performed can be found here https://github.com/deckhouse/deckhouse/blob//modules/031-ceph-csi/docs/internal/INTREE_MIGRATION.md
      summary: |
        Storage class {{ $labels.storageclass }} uses the deprecated rbd provisioner. It is necessary to migrate the volumes to the Ceph CSI driver.
      severity: "9"
      markupFormat: markdown
    - name: ModuleConfigDeprecated
      sourceFile: modules/810-documentation/monitoring/prometheus-rules/deprecated-mc.yaml
      moduleUrl: 810-documentation
      module: documentation
      edition: ce
      description: |-
        The module `deckhouse-web` was renamed to the `documentation`.

        The new ModuleConfig `documentation` was generated automatically. Please, remove deprecated ModuleConfig `deckhouse-web` from the CI deploy process and delete it: `kubectl delete mc deckhouse-web`.
      summary: |
        Deprecated ModuleConfig was found.
      severity: "9"
      markupFormat: markdown
    - name: ModuleReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module release is waiting for manual approval.

        Please run `kubectl annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"` for confirmation.
      summary: |
        Module release is waiting for manual approval.
      severity: "6"
      markupFormat: markdown
    - name: NATInstanceWithDeprecatedAvailabilityZone
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/deprecated_availability_zone.yaml
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: |
        Availability zone `ru-central1-c` is deprecated by Yandex.Cloud.
        You should migrate your NAT Instance to `ru-central1-a` or `ru-central1-b` zone.

        You can use the following instructions to migrate.

        **IMPORTANT** The following actions are destructive changes and cause downtime (typically a several tens of minutes, also it depending on the response time of Yandex Cloud).

        1. Migrate NAT Instance.

            Get `providerClusterConfiguration.withNATInstance`:
            ```
            kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.cloudProviderYandex.internal.providerClusterConfiguration.withNATInstance'
            ```

            1. If you specified `withNATInstance.natInstanceInternalAddress` and/or `withNATInstance.internalSubnetID` in providerClusterConfiguration, you need to remove them with the following command:

                ```
                kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller edit provider-cluster-configuration
                ```

            1. If you specified `withNATInstance.externalSubnetID` and/or `withNATInstance.natInstanceExternalAddress` in providerClusterConfiguration, you need to change these to the appropriate values.

                You can get address and subnetID from Yandex.Cloud console or with CLI

                Change `withNATInstance.externalSubnetID` and `withNATInstance.natInstanceExternalAddress` with the following command:
                ```
                kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller edit provider-cluster-configuration
                ```

        1. Run the appropriate edition and version of the Deckhouse installer container on the **local** machine (change the container registry address if necessary) and do converge.

            1. Get edition and version of the Deckhouse:

                ```
                DH_VERSION=$(kubectl -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}')
                DH_EDITION=$(kubectl -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]')
                echo "DH_VERSION=$DH_VERSION DH_EDITION=$DH_EDITION"
                ```

            1. Run the installer:

                ```
                docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
                ```

            1. Do converge:

                ```
                dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
                ```

        1. Update route table

            1. Get route table name

                ```
                kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.global.clusterConfiguration.cloud.prefix'
                ```

            1. Get NAT Instance name:

                ```
                kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.cloudProviderYandex.internal.providerDiscoveryData.natInstanceName'
                ```

            1. Get NAT Instance internal IP

                ```
                yc compute instance list | grep -e "INTERNAL IP" -e <NAT_INSTANCE_NAME_FROM_PREVIOUS_STEP>
                ```

            1. Update route

                ```
                yc vpc route-table update --name <ROUTE_TABLE_NAME_FROM_PREVIOUS_STEP> --route "destination=0.0.0.0/0,next-hop=<NAT_INSTANCE_INTERNAL_IP_FROM_PREVIOUS_STEP>"
                ```
      summary: |
        NAT Instance {{ $labels.name }} is in deprecated availability zone.
      severity: "9"
      markupFormat: markdown
    - name: NginxIngressConfigTestFailed
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The configuration testing (nginx -t) of the {{ $labels.controller }} Ingress controller in the {{ $labels.controller_namespace }} Namespace has failed.

        The recommended course of action:
        1. Check controllers logs: `kubectl -n {{ $labels.controller_namespace }} logs {{ $labels.controller_pod }} -c controller`;
        2. Find the newest Ingress in the cluster: `kubectl get ingress --all-namespaces --sort-by="metadata.creationTimestamp"`;
        3. Probably, there is an error in configuration-snippet or server-snippet.
      summary: |
        Config test failed on NGINX Ingress {{ $labels.controller }} in the {{ $labels.controller_namespace }} Namespace.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressDaemonSetNotUpToDate
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        There are {{ .Value }} outdated Pods in the {{ $labels.namespace }}/{{ $labels.daemonset }} Ingress Nginx DaemonSet for the last 20 minutes.

        The recommended course of action:
        1. Check the DaemonSet's status: `kubectl -n {{ $labels.namespace }} get ads {{ $labels.daemonset }}`
        2. Analyze the DaemonSet's description: `kubectl -n {{ $labels.namespace }} describe ads {{ $labels.daemonset }}`
        3. If the `Number of Nodes Scheduled with Up-to-date Pods` parameter does not match
        `Current Number of Nodes Scheduled`, check the pertinent Ingress Nginx Controller's 'nodeSelector' and 'toleration' settings,
        and compare them to the relevant nodes' 'labels' and 'taints' settings
      summary: |
        There are {{ .Value }} outdated Pods in the {{ $labels.namespace }}/{{ $labels.daemonset }} Ingress Nginx DaemonSet for the last 20 minutes.
      severity: "9"
      markupFormat: markdown
    - name: NginxIngressDaemonSetReplicasUnavailable
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Count of available replicas in NGINX Ingress DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} is at zero.

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}

        This command might help figuring out problematic nodes given you are aware where the DaemonSet should be scheduled in the first place (using label selector for pods might be of help, too):

        ```
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Count of available replicas in NGINX Ingress DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} is at zero.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressDaemonSetReplicasUnavailable
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Some replicas of NGINX Ingress DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} are unavailable.
        Currently at: {{ .Value }} unavailable replica(s)

        List of unavailable Pod(s): {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}

        This command might help figuring out problematic nodes given you are aware where the DaemonSet should be scheduled in the first place (using label selector for pods might be of help, too):

        ```
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Some replicas of NGINX Ingress DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} are unavailable.
      severity: "6"
      markupFormat: markdown
    - name: NginxIngressPodIsRestartingTooOften
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The number of restarts in the last hour: {{ $value }}.
        Excessive NGINX Ingress restarts indicate that something is wrong. Normally, it should be up and running all the time.
      summary: |
        Too many NGINX Ingress restarts have been detected.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressProtobufExporterHasErrors
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The Ingress Nginx sidecar container with `protobuf_exporter` has {{ $labels.type }} errors.

        Please, check Ingress controller's logs:
        `kubectl -n d8-ingress-nginx logs $(kubectl -n d8-ingress-nginx get pods -l app=controller,name={{ $labels.controller }} -o wide | grep {{ $labels.node }} | awk '{print $1}') -c protobuf-exporter`.
      summary: |
        The Ingress Nginx sidecar container with protobuf_exporter has {{ $labels.type }} errors.
      severity: "8"
      markupFormat: markdown
    - name: NginxIngressSslExpired
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        SSL certificate for {{ $labels.host }} in {{ $labels.namespace }} has expired.
        You can verify the certificate with the `kubectl -n {{ $labels.namespace }} get secret {{ $labels.secret_name }} -o json | jq -r '.data."tls.crt" | @base64d' | openssl x509 -noout -alias -subject -issuer -dates` command.

        https://{{ $labels.host }} version of site doesn't work!
      summary: |
        Certificate has expired.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressSslWillExpire
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        SSL certificate for {{ $labels.host }} in {{ $labels.namespace }} will expire in less than 2 weeks.
        You can verify the certificate with the `kubectl -n {{ $labels.namespace }} get secret {{ $labels.secret_name }} -o json | jq -r '.data."tls.crt" | @base64d' | openssl x509 -noout -alias -subject -issuer -dates` command.
      summary: |
        Certificate expires soon.
      severity: "5"
      markupFormat: markdown
    - name: NodeConntrackTableFull
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The `conntrack` table on the {{ $labels.node }} Node is full!

        No new connections are created or accepted on the Node; note that this may result in strange software issues.

        The recommended course of action is to identify the source of "excess" `conntrack` entries using Okmeter or Grafana charts.
      summary: |
        The conntrack table is full.
      severity: "3"
      markupFormat: markdown
    - name: NodeConntrackTableFull
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The conntrack table on the {{ $labels.node }} is {{ $value }}% of the maximum size.

        There's nothing to worry about yet if the `conntrack` table is only 70-80 percent full. However, if it runs out, you will experience problems with new connections while the software will behave strangely.

        The recommended course of action is to identify the source of "excess" `conntrack` entries using Okmeter or Grafana charts.
      summary: |
        The conntrack table is close to the maximum size.
      severity: "4"
      markupFormat: markdown
    - name: NodeDiskBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: ""
      summary: |
        Node disk &quot;{{$labels.device}}&quot; on mountpoint &quot;{{$labels.mountpoint}}&quot; is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-bytes-critical&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of storage capacity. Currently at: {{ .Value }}%
      severity: "5"
      markupFormat: markdown
    - name: NodeDiskBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Node disk "{{$labels.device}}" on mountpoint "{{$labels.mountpoint}}" is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-bytes-warning\", node=\"%s\"}" $labels.node | query | first | value }}% of the storage capacity.
        Currently at: {{ .Value }}%

        Retrieve the disk usage info on the node: `ncdu -x {{$labels.mountpoint}}'

        If the output shows high disk usage in the /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/ directory, use the following command to show the pods with the highest usage:
        `crictl stats -o json | jq '.stats[] | select((.writableLayer.usedBytes.value | tonumber) > 1073741824) | { meta: .attributes.labels, diskUsage: ((.writableLayer.usedBytes.value | tonumber) / 1073741824 * 100 | round / 100 | tostring + " GiB")}'`
      summary: |
        Node disk &quot;{{$labels.device}}&quot; on mountpoint &quot;{{$labels.mountpoint}}&quot; is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-bytes-warning&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of the storage capacity. Currently at: {{ .Value }}%
      severity: "6"
      markupFormat: markdown
    - name: NodeDiskInodesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: ""
      summary: |
        Node disk &quot;{{$labels.device}}&quot; on mountpoint &quot;{{$labels.mountpoint}}&quot; is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-inodes-critical&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of storage capacity. Currently at: {{ .Value }}%
      severity: "5"
      markupFormat: markdown
    - name: NodeDiskInodesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: ""
      summary: |
        Node disk &quot;{{$labels.device}}&quot; on mountpoint &quot;{{$labels.mountpoint}}&quot; is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-inodes-warning&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of storage capacity. Currently at: {{ .Value }}%
      severity: "6"
      markupFormat: markdown
    - name: NodeExporterDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus could not scrape a node-exporter for more than 10m, or node-exporters have disappeared from discovery
      summary: |
        Prometheus could not scrape a node-exporter
      severity: "3"
      markupFormat: default
    - name: NodeFilesystemIsRO
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-ro-fs.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The file system on the node has switched to read-only mode.

        See the node logs to find out the cause and fix it.
      summary: |
        The file system of the node is in read-only mode.
      severity: "4"
      markupFormat: default
    - name: NodeGroupHasStaticInternalNetworkCIDRsField
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group-deprecate.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Internal network CIDRs setting now located in the static cluster configuration.
        Delete this field from NodeGroup {{ $labels.name }} to fix this alert.
        Do not worry, it has been already migrated to another place.
      summary: |
        NodeGroup {{ $labels.name }} has deprecated filed spec.static.internalNetworkCIDRs
      severity: "9"
      markupFormat: markdown
    - name: NodeGroupMasterTaintIsAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        `master` node group has no `node-role.kubernetes.io/control-plane` taint. Probably control-plane nodes are misconfigured
        and are able to run not only control-plane Pods. Please, add:
        ```yaml
          nodeTemplate:
            taints:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
        ```
        to the `master` node group spec.
        `key: node-role.kubernetes.io/master` taint was deprecated and will have no effect in Kubernetes 1.24+.
      summary: |
        The 'master' node group does not contain desired taint.
      severity: "4"
      markupFormat: markdown
    - name: NodeGroupNodeWithDeprecatedAvailabilityZone
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/deprecated_availability_zone.yaml
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: |
        Availability zone `ru-central1-c` is deprecated by Yandex.Cloud.
        You should migrate your Nodes, Disks and LoadBalancers to `ru-central1-a`, `ru-central1-b` or `ru-central1-d` (introduced in v1.56).
        To check which Nodes should be migrated, use `kubectl get node -l "topology.kubernetes.io/zone=ru-central1-c"` command.

        You can use [Yandex Migration Guide](https://cloud.yandex.com/en/docs/overview/concepts/zone-migration) (mostly applicable to the `ru-central1-d' zone only).

        **IMPORTANT** You cannot migrate public IP addresses between zones. Check out the Yandex Migration Guide for details.
      summary: |
        NodeGroup {{ $labels.node_group }} contains Nodes with deprecated availability zone.
      severity: "9"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Probably, machine-controller-manager is unable to create a machine using the cloud provider module. Possible causes:
          1. Cloud provider limits on available resources;
          2. No access to the cloud provider API;
          3. Cloud provider or instance class misconfiguration;
          4. Problems with bootstrapping the Machine.

        The recommended course of action:
          1. Run `kubectl get ng {{ $labels.node_group }} -o yaml`. In the `.status.lastMachineFailures` field you can find all errors related to the creation of Machines;
          2. The absence of Machines in the list that have been in Pending status for more than a couple of minutes means that Machines are continuously being created and deleted because of some error:
          `kubectl -n d8-cloud-instance-manager get machine`;
          3. Refer to the Machine description if the logs do not include error messages and the Machine continues to be Pending:
          `kubectl -n d8-cloud-instance-manager get machine <machine_name> -o json | jq .status.bootstrapStatus`;
          4. The output similar to the one below means that you have to use nc to examine the bootstrap logs:
             ```json
             {
               "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
               "tcpEndpoint": "192.168.199.158"
             }
             ```
          5. The absence of information about the endpoint for getting logs means that `cloudInit` is not working correctly. This may be due to the incorrect configuration of the instance class for the cloud provider.
      summary: |
        There are no available instances in the {{ $labels.node_group }} node group.
      severity: "7"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Possibly, autoscaler has provisioned too many Nodes. Take a look at the state of the Machine in the cluster.
        Probably, machine-controller-manager is unable to create a machine using the cloud provider module. Possible causes:
          1. Cloud provider limits on available resources;
          2. No access to the cloud provider API;
          3. Cloud provider or instance class misconfiguration;
          4. Problems with bootstrapping the Machine.

        The recommended course of action:
          1. Run `kubectl get ng {{ $labels.node_group }} -o yaml`. In the `.status.lastMachineFailures` field you can find all errors related to the creation of Machines;
          2. The absence of Machines in the list that have been in Pending status for more than a couple of minutes means that Machines are continuously being created and deleted because of some error:
          `kubectl -n d8-cloud-instance-manager get machine`;
          3. Refer to the Machine description if the logs do not include error messages and the Machine continues to be Pending:
          `kubectl -n d8-cloud-instance-manager get machine <machine_name> -o json | jq .status.bootstrapStatus`;
          4. The output similar to the one below means that you have to use nc to examine the bootstrap logs:
             ```json
             {
               "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
               "tcpEndpoint": "192.168.199.158"
             }
             ```
          5. The absence of information about the endpoint for getting logs means that `cloudInit` is not working correctly. This may be due to the incorrect configuration of the instance class for the cloud provider.
      summary: |
        The number of simultaneously unavailable instances in the {{ $labels.node_group }} node group exceeds the allowed value.
      severity: "8"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of unavailable instances is {{ $value }}. See the relevant alerts for more information.
        Probably, machine-controller-manager is unable to create a machine using the cloud provider module. Possible causes:
          1. Cloud provider limits on available resources;
          2. No access to the cloud provider API;
          3. Cloud provider or instance class misconfiguration;
          4. Problems with bootstrapping the Machine.

        The recommended course of action:
          1. Run `kubectl get ng {{ $labels.node_group }} -o yaml`. In the `.status.lastMachineFailures` field you can find all errors related to the creation of Machines;
          2. The absence of Machines in the list that have been in Pending status for more than a couple of minutes means that Machines are continuously being created and deleted because of some error:
          `kubectl -n d8-cloud-instance-manager get machine`;
          3. Refer to the Machine description if the logs do not include error messages and the Machine continues to be Pending:
          `kubectl -n d8-cloud-instance-manager get machine <machine_name> -o json | jq .status.bootstrapStatus`;
          4. The output similar to the one below means that you have to use nc to examine the bootstrap logs:
             ```json
             {
               "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
               "tcpEndpoint": "192.168.199.158"
             }
             ```
          5. The absence of information about the endpoint for getting logs means that `cloudInit` is not working correctly. This may be due to the incorrect configuration of the instance class for the cloud provider.
      summary: |
        There are unavailable instances in the {{ $labels.node_group }} node group.
      severity: "8"
      markupFormat: markdown
    - name: NodePingPacketLoss
      sourceFile: modules/340-monitoring-ping/monitoring/prometheus-rules/node-ping.yaml
      moduleUrl: 340-monitoring-ping
      module: monitoring-ping
      edition: ce
      description: ICMP packet loss to node {{$labels.destination_node}} is more than 5%
      summary: |
        Ping loss more than 5%
      severity: "4"
      markupFormat: default
    - name: NodeRequiresDisruptionApprovalForUpdate
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for Nodes and the {{ $labels.node }} Node of the {{ $labels.node_group }} group has learned about the update, requested and received approval, started the update, and ran into a step that causes possible downtime.

        You have to manually approve the disruption since the `Manual` mode is active in the group settings (`disruptions.approvalMode`).

        Grant approval to the Node using the `update.node.deckhouse.io/disruption-approved=` annotation if it is ready for unsafe updates (e.g., drained).

        **Caution!!!** The Node will not be drained automatically since the manual mode is enabled (`disruptions.approvalMode: Manual`).

        **Caution!!!** No need to drain the master node.

        * Use the following commands to drain the Node and grant it update approval:
          ```shell
          kubectl drain {{ $labels.node }} --delete-local-data=true --ignore-daemonsets=true --force=true &&
            kubectl annotate node {{ $labels.node }} update.node.deckhouse.io/disruption-approved=
          ```
        * Note that you need to **uncordon the node** after the update is complete (i.e., after removing the `update.node.deckhouse.io/approved` annotation).
          ```
          while kubectl get node {{ $labels.node }} -o json | jq -e '.metadata.annotations | has("update.node.deckhouse.io/approved")' > /dev/null; do sleep 1; done
          kubectl uncordon {{ $labels.node }}
          ```

        Note that if there are several Nodes in a NodeGroup, you will need to repeat this operation for each Node since only one Node can be updated at a time. Perhaps it makes sense to temporarily enable the automatic disruption approval mode (`disruptions.approvalMode: Automatic`).
      summary: |
        The {{ $labels.node }} Node requires disruption approval to proceed with the update
      severity: "8"
      markupFormat: markdown
    - name: NodeStuckInDraining
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The {{ $labels.node }} Node of the {{ $labels.node_group }} NodeGroup stuck in draining.

        You can get more info by running: `kubectl -n default get event --field-selector involvedObject.name={{ $labels.node }},reason=DrainFailed --sort-by='.metadata.creationTimestamp'`

        The error message is: {{ $labels.message }}
      summary: |
        The {{ $labels.node }} Node is stuck in draining.
      severity: "6"
      markupFormat: markdown
    - name: NodeStuckInDrainingForDisruptionDuringUpdate
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} NodeGroup. The Node has learned about the update, requested and received approval, started the update, ran into a step that causes possible downtime, and stuck in draining in order to get disruption approval automatically.

        You can get more info by running: `kubectl -n default get event --field-selector involvedObject.name={{ $labels.node }},reason=ScaleDown --sort-by='.metadata.creationTimestamp'`
      summary: |
        The {{ $labels.node }} Node is stuck in draining.
      severity: "6"
      markupFormat: markdown
    - name: NodeSUnreclaimBytesUsageHigh
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The {{ $labels.node }} Node has potential kernel memory leak. There is one known issue that can be reason for it.

        You should check cgroupDriver on the {{ $labels.node }} Node:
        - `cat /var/lib/kubelet/config.yaml | grep 'cgroupDriver: systemd'`

        If cgroupDriver is set to systemd then reboot is required to roll back to cgroupfs driver. Please, drain and reboot the node.

        You can check this [issue](https://github.com/deckhouse/deckhouse/issues/2152) for extra information.
      summary: |
        The {{ $labels.node }} Node has high kernel memory usage.
      severity: "4"
      markupFormat: markdown
    - name: NodeSystemExporterDoesNotExistsForNode
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Some of the Node system exporters don't work correctly for the {{ $labels.node }} Node.

        The recommended course of action:
        1. Find the Node exporter Pod for this Node: `kubectl -n d8-monitoring get pod -l app=node-exporter -o json | jq -r ".items[] | select(.spec.nodeName==\"{{$labels.node}}\") | .metadata.name"`;
        2. Describe the Node exporter Pod: `kubectl -n d8-monitoring describe pod <pod_name>`;
        3. Check that kubelet is running on the {{ $labels.node }} node.
      summary: ""
      severity: "4"
      markupFormat: markdown
    - name: NodeTimeOutOfSync
      sourceFile: modules/470-chrony/monitoring/prometheus-rules/chrony.yaml
      moduleUrl: 470-chrony
      module: chrony
      edition: ce
      description: |
        Node's {{$labels.node}} time is out of sync from ntp server by {{ $value }} seconds.
      summary: |
        Node's {{$labels.node}} clock is drifting.
      severity: "5"
      markupFormat: markdown
    - name: NodeUnschedulable
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The {{ $labels.node }} Node is cordon-protected; no new Pods can be scheduled onto it.

        This means that someone has executed one of the following commands on that Node:
        - `kubectl cordon {{ $labels.node }}`
        - `kubectl drain {{ $labels.node }}` that runs for more than 20 minutes

        Probably, this is due to the maintenance of this Node.
      summary: |
        The {{ $labels.node }} Node is cordon-protected; no new Pods can be scheduled onto it.
      severity: "8"
      markupFormat: markdown
    - name: NTPDaemonOnNodeDoesNotSynchronizeTime
      sourceFile: modules/470-chrony/monitoring/prometheus-rules/chrony.yaml
      moduleUrl: 470-chrony
      module: chrony
      edition: ce
      description: |
        1. check if Chrony pod is running on the node by executing the following command:
           * 'kubectl -n d8-chrony --field-selector spec.nodeName="{{$labels.node}}" get pods'
        2. check the Chrony daemon's status by executing the following command:
           * 'kubectl -n d8-chrony exec <POD_NAME> -- /opt/chrony-static/bin/chronyc sources'
        3. Correct the time synchronization problems:
           * correct network problems:
             - provide availability to upstream time synchronization servers defined in the module [configuration](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/470-chrony/configuration.html);
             - eliminate large packet loss and excessive latency to upstream time synchronization servers.
           * Modify NTP servers list defined in the module [configuration](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/470-chrony/configuration.html).
      summary: |
        NTP daemon on node {{$labels.node}} have not synchronized time for too long.
      severity: "5"
      markupFormat: markdown
    - name: OperationPolicyViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured OperationPolicy for the cluster.

        You can find existing objects violating policies by running `count by (violating_namespace, violating_kind, violating_name, violation_msg) (d8_gatekeeper_exporter_constraint_violations{violation_enforcement="deny",source_type="OperationPolicy"})`
        prometheus query or via the Admission policy engine Grafana dashboard.
      summary: |
        At least one object violates configured cluster Operation Policies.
      severity: "7"
      markupFormat: markdown
    - name: PersistentVolumeClaimBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-bytes-critical\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of volume storage capacity.
        Currently at: {{ .Value }}%

        PersistentVolumeClaim is used by the following pods: {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-bytes-critical&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of volume storage capacity.
      severity: "4"
      markupFormat: markdown
    - name: PersistentVolumeClaimBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-bytes-warning\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of volume storage capacity.
        Currently at: {{ .Value }}%

        PersistentVolumeClaim is used by the following pods: {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-bytes-warning&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of volume storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: PersistentVolumeClaimInodesUsed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-inodes-critical\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of volume inode capacity.
        Currently at: {{ .Value }}%

        PersistentVolumeClaim is used by the following pods: {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-inodes-critical&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of volume inode capacity.
      severity: "4"
      markupFormat: markdown
    - name: PersistentVolumeClaimInodesUsed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-inodes-warning\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of volume inode capacity.
        Currently at: {{ .Value }}%

        PersistentVolumeClaim is used by the following pods: {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-inodes-warning&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of volume inode capacity.
      severity: "5"
      markupFormat: markdown
    - name: PodSecurityStandardsViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured pod security standards (https://kubernetes.io/docs/concepts/security/pod-security-standards/).

        You can find already Running pods which are violate standards by running `count by (violating_namespace, violating_name, violation_msg) (d8_gatekeeper_exporter_constraint_violations{violation_enforcement="deny",violating_namespace=~".*",violating_kind="Pod",source_type="PSS"})`
        prometheus query or via the Admission policy engine grafana dashboard.
      summary: |
        At least one pod violates configured cluster pod security standards.
      severity: "7"
      markupFormat: markdown
    - name: PodStatusIsIncorrect
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/pod-status.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        There is a {{ $labels.namespace }}/{{ $labels.pod }} Pod in the cluster that runs on the {{ $labels.node }} and listed as NotReady while all the Pod's containers are Ready.

        This could be due to the [Kubernetes bug](https://github.com/kubernetes/kubernetes/issues/80968).

        The recommended course of action:
        1. Find all the Pods having this state: `kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | "\(.spec.nodeName)/\(.metadata.namespace)/\(.metadata.name)"'`;
        2. Find all the Nodes affected: `kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | .spec.nodeName' -r | sort | uniq -c`;
        3. Restart `kubelet` on each Node: `systemctl restart kubelet`.
      summary: |
        The state of the {{ $labels.namespace }}/{{ $labels.pod }} Pod running on the {{ $labels.node }} Node is incorrect. You need to restart kubelet.
      severity: undefined
      markupFormat: markdown
    - name: PrometheusDiskUsage
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        For more information, use the command:
        ```shell
        kubectl -n {{ $labels.namespace }} exec -ti {{ $labels.pod_name }} -c prometheus -- df -PBG /prometheus
        ```
        Consider increasing it https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/300-prometheus/faq.html#how-to-expand-disk-size
      summary: |
        Prometheus disk is over 95% used.
      severity: "4"
      markupFormat: markdown
    - name: PrometheusLongtermRotatingEarlierThanConfiguredRetentionDays
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        You need to increase the disk size, reduce the number of metrics or decrease `longtermRetentionDays` module parameter.
      summary: |
        Prometheus-longterm data is being rotated earlier than configured retention days
      severity: "4"
      markupFormat: markdown
    - name: PrometheusMainRotatingEarlierThanConfiguredRetentionDays
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        You need to increase the disk size, reduce the number of metrics or decrease `retentionDays` module parameter.
      summary: |
        Prometheus-main data is being rotated earlier than configured retention days
      severity: "4"
      markupFormat: markdown
    - name: PrometheusScapeConfigDeclarationDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        Old way for describing additional scrape config via secrets will be deprecated in prometheus-operator > v0.65.1. Please use CRD ScrapeConfig instead.
        ```https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/proposals/202212-scrape-config.md```
      summary: |
        AdditionalScrapeConfigs from secrets will be deprecated in soon
      severity: "8"
      markupFormat: markdown
    - name: PrometheusServiceMonitorDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Kubernetes cluster uses a more advanced network mechanism - EndpointSlice
        You service monitor `{{ $labels.namespace }}/{{ $labels.name }}` has relabeling with old Endpoint mechanism, starts with `__meta_kubernetes_endpoints_`.
        This relabeling rule support, based on the `_endpoint_` label, will be remove in the future (Deckhouse release 1.60).
        Please, migrate to EndpointSlice relabeling rules. To do this, you have modify ServiceMonitor with changing the following labels:
        ```shell
        __meta_kubernetes_endpoints_name -> __meta_kubernetes_endpointslice_name
        __meta_kubernetes_endpoints_label_XXX -> __meta_kubernetes_endpointslice_label_XXX
        __meta_kubernetes_endpoints_labelpresent_XXX -> __meta_kubernetes_endpointslice_labelpresent_XXX
        __meta_kubernetes_endpoints_annotation_XXX -> __meta_kubernetes_endpointslice_annotation_XXX
        __meta_kubernetes_endpoints_annotationpresent_XXX -> __meta_kubernetes_endpointslice_annotationpresent_XXX
        __meta_kubernetes_endpoint_node_name -> __meta_kubernetes_endpointslice_endpoint_topology_kubernetes_io_hostname
        __meta_kubernetes_endpoint_ready -> __meta_kubernetes_endpointslice_endpoint_conditions_ready
        __meta_kubernetes_endpoint_port_name -> __meta_kubernetes_endpointslice_port_name
        __meta_kubernetes_endpoint_port_protocol -> __meta_kubernetes_endpointslice_port_protocol
        __meta_kubernetes_endpoint_address_target_kind -> __meta_kubernetes_endpointslice_address_target_kind
        __meta_kubernetes_endpoint_address_target_name -> __meta_kubernetes_endpointslice_address_target_name
        ```
      summary: |
        Deprecated Prometheus ServiceMonitor has found.
      severity: "8"
      markupFormat: markdown
    - name: SecurityPolicyViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured SecurityPolicy for the cluster.

        You can find existing objects violating policies by running `count by (violating_namespace, violating_kind, violating_name, violation_msg) (d8_gatekeeper_exporter_constraint_violations{violation_enforcement="deny",source_type="SecurityPolicy"})`
        prometheus query or via the Admission policy engine Grafana dashboard.
      summary: |
        At least one object violates configured cluster Security Policies.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Unable to login to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image in the `{{ $labels.namespace }}` Namespace; in the StatefulSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to login to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Insufficient privileges to pull the `{{ $labels.image }}` image using the `imagePullSecrets` specified in the `{{ $labels.namespace }}` Namespace; in the StatefulSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the imagePullSecrets specified.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image name is spelled correctly: in the `{{ $labels.namespace }}` Namespace; in the StatefulSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image has incorrect name.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        You should check whether the `{{ $labels.image }}` image is available: in the `{{ $labels.namespace }}` Namespace; in the StatefulSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The container registry is not available for the `{{ $labels.image }}` image: in the `{{ $labels.namespace }}` Namespace; in the StatefulSet `{{ $labels.name }}` in the `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        An unknown error occurred for the  `{{ $labels.image }}` image
        in the `{{ $labels.namespace }}` Namespace;
        in the StatefulSet `{{ $labels.name }}`
        in the `{{ $labels.container }}` container in the registry.

        Refer to the exporter logs: `kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter`
      summary: |
        An unknown error occurred for the  {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StorageClassCloudManual
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/storage-class.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        StorageClass having a cloud-provider provisioner shouldn't be deployed manually.
        They are managed by the cloud-provider module, you only need to change the module configuration to fit your needs.


        [Find storage configuration documentation for your cloud-provider here](http://documentation.example.com/kubernetes.html).
      summary: |
        Manually deployed StorageClass {{ $labels.name }} found in the cluster
      severity: "6"
      markupFormat: markdown
    - name: StorageClassDefaultDuplicate
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/storage-class.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        More than one StorageClass in the cluster annotated as a default.
        Probably manually deployed StorageClass exists, that overlaps with cloud-provider module default Storage configuration.


        [Find storage configuration documentation for your cloud-provider here](http://documentation.example.com/kubernetes.html).
      summary: |
        Multiple default StorageClasses found in the cluster
      severity: "6"
      markupFormat: markdown
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: '{{ $labels.job }} target is down.'
      summary: |
        Target is down
      severity: "5"
      markupFormat: default
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: '{{ $labels.job }} target is down.'
      summary: |
        Target is down
      severity: "6"
      markupFormat: default
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: '{{ $labels.job }} target is down.'
      summary: |
        Target is down
      severity: "7"
      markupFormat: default
    - name: TargetSampleLimitExceeded
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Target are down because of a sample limit exceeded.
      summary: |
        Scrapes are exceeding sample limit
      severity: "6"
      markupFormat: markdown
    - name: TargetSampleLimitExceeded
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The target is close to exceeding the sampling limit. less than 10% left to the limit
      summary: |
        The sampling limit is close.
      severity: "7"
      markupFormat: markdown
    - name: UnsupportedContainerRuntimeVersion
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/cri-version.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Unsupported version {{$labels.container_runtime_version}} of CRI installed on {{$labels.node}} node.
        Supported version of CRI for kubernetes {{$labels.kubelet_version}} version:
        * Containerd 1.4.*
        * Containerd 1.5.*
        * Containerd 1.6.*
        * Containerd 1.7.*
      summary: |
        Unsupported version of CRI {{$labels.container_runtime_version}} installed for Kubernetes version: {{$labels.kubelet_version}}
      severity: undefined
      markupFormat: markdown
modules-having-alerts:
    - admission-policy-engine
    - cert-manager
    - chrony
    - cloud-provider-yandex
    - cni-cilium
    - control-plane-manager
    - documentation
    - extended-monitoring
    - flow-schema
    - ingress-nginx
    - istio
    - kube-dns
    - l2-load-balancer
    - log-shipper
    - metallb
    - monitoring-applications
    - monitoring-custom
    - monitoring-deckhouse
    - monitoring-kubernetes
    - monitoring-kubernetes-control-plane
    - monitoring-ping
    - node-manager
    - okmeter
    - operator-prometheus
    - prometheus
    - runtime-audit-engine
    - secret-copier
    - snapshot-controller
    - terraform-manager
    - upmeter
    - user-authn
