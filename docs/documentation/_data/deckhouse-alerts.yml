alerts:
    - name: CapsInstanceUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/caps-nodes.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        In MachineDeployment `{{ $labels.machine_deployment_name }}` number of unavailable instances is **{{ $value }}**. Take a look and check at the state of the instances in the cluster: `kubectl get instance -l node.deckhouse.io/group={{ $labels.machine_deployment_name }}`
      summary: |
        There are unavailable instances in the {{ $labels.machine_deployment_name }} MachineDeployment.
      severity: "8"
      markupFormat: markdown
    - name: CertificateSecretExpired
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificate.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        A certificate in Secret `{{$labels.secret_namespace}}/{{$labels.secret_name}}` has expired.

        Ways to resolve:

        - If the certificate is managed manually, upload a new certificate.
        - If the certificate is managed by the `cert-manager` module, inspect the certificate resource:
          1. Retrieve the certificate name from the Secret:

             ```bash
             cert=$(kubectl get secret -n {{$labels.secret_namespace}} {{$labels.secret_name}} -o 'jsonpath={.metadata.annotations.cert-manager\.io/certificate-name}')
             ```

          2. Check the certificate status and investigate why it hasn't been updated:

             ```bash
             kubectl describe cert -m {{$labels.secret_namespace}} "$cert"
             ```
      summary: |
        Certificate has expired.
      severity: "8"
      markupFormat: markdown
    - name: CertificateSecretExpiredSoon
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificate.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        A certificate in Secret `{{$labels.secret_namespace}}/{{$labels.secret_name}}` will expire in less than two weeks.

        Ways to resolve:

        - If the certificate is managed manually, upload a new certificate.
        - If the certificate is managed by the `cert-manager` module, inspect the certificate resource:
          1. Retrieve the certificate name from the Secret:

             ```bash
             cert=$(kubectl get secret -n {{$labels.secret_namespace}} {{$labels.secret_name}} -o 'jsonpath={.metadata.annotations.cert-manager\.io/certificate-name}')
             ```

          2. Check the certificate status and investigate why it hasn't been updated:

             ```bash
             kubectl describe cert -n {{$labels.secret_namespace}} "$cert"
             ```
      summary: |
        Certificate is expiring soon.
      severity: "8"
      markupFormat: markdown
    - name: CertmanagerCertificateExpired
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        Certificate is not provided.

        To check the certificate details, run the following command:

        ```shell
        kubectl -n {{$labels.exported_namespace}} describe certificate {{$labels.name}}
        ```
      summary: |
        Certificate {{$labels.exported_namespace}}/{{$labels.name}} is not provided.
      severity: "4"
      markupFormat: default
    - name: CertmanagerCertificateExpiredSoon
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        The certificate `{{$labels.exported_namespace}}/{{$labels.name}}` will expire in less than two weeks.

        To check the certificate details, run the following command:

        ```bash
        kubectl -n <NAMESPACE> describe certificate <CERTIFICATE-NAME>
        ```
      summary: |
        Certificate will expire soon.
      severity: "4"
      markupFormat: default
    - name: CertmanagerCertificateOrderErrors
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        Cert-manager received responses with the status code `{{ $labels.status }}` when requesting `{{ $labels.scheme }}://{{ $labels.host }}{{ $labels.path }}`.

        This can affect certificate ordering and prolongation in the future. For details, check the cert-manager logs using the following command:

        ```bash
        kubectl -n d8-cert-manager logs -l app=cert-manager -c cert-manager
        ```
      summary: |
        Cert-manager couldn't order a certificate.
      severity: "5"
      markupFormat: default
    - name: CiliumAgentEndpointsNotReady
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        For details, refer to the logs of the agent:

        ```bash
        kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}
        ```
      summary: |
        Over 50% of all known endpoints aren't ready in agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentMapPressureCritical
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        The eBPF map resource utilization limit has almost been reached.

        Check with the vendor for potential remediation steps.
      summary: |
        eBPF map {{ $labels.map_name }} exceeds 90% utilization in agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentMetricNotFound
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Check the logs of the agent:

           ```bash
           kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}
           ```

        1. Verify the agent's health status:

           ```bash
           kubectl -n {{ $labels.namespace }} exec -ti {{ $labels.pod }} cilium-health status
           ```

        1. Compare the metrics with those of a neighboring agent.

        > Note that the absence of metrics can indirectly indicate that new pods can't be created on the node due to connectivity issues with the agent.
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} isn't sending some metrics.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentPolicyImportErrors
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        For details, refer to the logs of the agent:

        ```bash
        kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}
        ```
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} fails to import policies.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentUnreachableHealthEndpoints
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        For details, refer to the logs of the agent:

        ```bash
        kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}
        ```
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} can't reach some of the node health endpoints.
      severity: "4"
      markupFormat: markdown
    - name: ClusterHasOrphanedDisks
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cloud data discoverer finds disks in the cloud for which there is no PersistentVolume in the cluster. You can manually delete these disks from your cloud:
          ID: {{ $labels.id }}, Name: {{ $labels.name }}
      summary: |
        Cloud data discoverer finds disks in the cloud for which there is no PersistentVolume in the cluster
      severity: "6"
      markupFormat: markdown
    - name: CniCiliumNonStandardVXLANPortFound
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/configmap.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        The Cilium configuration specifies a non-standard VXLAN port `{{$labels.port}}`. This port falls outside the recommended range:

        - `4298`: When the virtualization module is enabled.
        - `4299`: For a standard Deckhouse setup.

        To resolve this issue, update the `tunnel-port` parameter in the `cilium-configmap` ConfigMap located in the `d8-cni-cilium` namespace to match the recommended range.

        If you configured the non-standard port on purpose, ignore this alert.
      summary: |
        Cilium configuration uses a non-standard VXLAN port.
      severity: "4"
      markupFormat: markdown
    - name: CniCiliumOrphanEgressGatewayPolicyFound
      sourceFile: ee/se-plus/modules/021-cni-cilium/monitoring/prometheus-rules/egressgatewaypolicies.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: se-plus
      description: |
        The cluster contains an orphaned EgressGatewayPolicy named `{{$labels.name}}` with an irrelevant EgressGateway name.

        To resolve this issue, verify the EgressGateway name specified in the EgressGatewayPolicy resource `{{$labels.egressgateway}}` and update it as needed.
      summary: |
        Orphaned EgressGatewayPolicy with an irrelevant EgressGateway name has been found.
      severity: "4"
      markupFormat: markdown
    - name: CPUStealHigh
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The CPU steal has been excessively high on node `{{ $labels.node }}` over the past 30 minutes.

        Another component, such as a neighboring virtual machine, may be consuming the node's resources. This may be the result of "overselling" the hypervisor, meaning the hypervisor is hosting more virtual machines than it can handle.
      summary: |
        CPU steal on node {{ $labels.node }} is too high.
      severity: "4"
      markupFormat: default
    - name: CronJobAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CronJobAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: CronJobBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: CronJobFailed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that Job `{{$labels.namespace}}/{{$labels.job_name}}` failed in CronJob `{{$labels.namespace}}/{{$labels.owner_name}}`.

        Steps to resolve:

        1. Print the job details:

           ```bash
           kubectl -n {{$labels.namespace}} describe job {{$labels.job_name}}
           ```

        1. Check the job status:

           ```bash
           kubectl -n {{$labels.namespace}} get job {{$labels.job_name}}
           ```

        1. Check the status of pods created by the job:

           ```bash
           kubectl -n {{$labels.namespace}} get pods -l job-name={{$labels.job_name}}
           ```
      summary: |
        Job {{$labels.namespace}}/{{$labels.job_name}} failed in CronJob {{$labels.namespace}}/{{$labels.owner_name}}.
      severity: "5"
      markupFormat: default
    - name: CronJobImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: CronJobPodsNotCreated
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the pods set in CronJob `{{$labels.namespace}}/{{$labels.owner_name}}` still haven't been created.

        Steps to resolve:

        1. Print the job details:

           ```bash
           kubectl -n {{$labels.namespace}} describe job {{$labels.job_name}}
           ```

        1. Check the job status:

           ```bash
           kubectl -n {{$labels.namespace}} get job {{$labels.job_name}}
           ```

        1. Check the status of pods created by the job:

           ```bash
           kubectl -n {{$labels.namespace}} get pods -l job-name={{$labels.job_name}}
           ```
      summary: |
        Pods set in CronJob {{$labels.namespace}}/{{$labels.job_name}} haven't been created.
      severity: "5"
      markupFormat: markdown
    - name: CronJobRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CronJobSchedulingError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that CronJob `{{$labels.namespace}}/{{$labels.cronjob}}` failed to schedule on time.

        - Current schedule: `{{ printf "kube_cronjob_info{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | label "schedule" }}`
        - Last scheduled time: `{{ printf "kube_cronjob_status_last_schedule_time{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | value | humanizeTimestamp }}%`
        - Next projected schedule time: `{{ printf "kube_cronjob_next_schedule_time{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | value | humanizeTimestamp }}%`
      summary: |
        CronJob {{$labels.namespace}}/{{$labels.cronjob}} failed to schedule on time.
      severity: "6"
      markupFormat: markdown
    - name: CronJobUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CustomPodMonitorFoundInCluster
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are PodMonitors in the Deckhouse namespace that were not created by Deckhouse.

        To resolve the issue, move these PodMonitors to the `user-spec` namespace by removing the `heritage: deckhouse` label.

        To list all PodMonitors in the Deckhouse namespace, run the following command:

        ```bash
        kubectl get podmonitors --all-namespaces -l heritage!=deckhouse
        ```

        For more information on metric collection, refer to the [Prometheus module FAQ](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/prometheus/faq.html).
      summary: |
        Deckhouse namespace contains PodMonitors not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: CustomServiceMonitorFoundInD8Namespace
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are ServiceMonitors in the Deckhouse namespace that were not created by Deckhouse.

        To resolve the issue, move these ServiceMonitors to the `user-spec` namespace by removing the `heritage: deckhouse` label.

        To list all ServiceMonitors in the Deckhouse namespace, run the following command:

        ```bash
        kubectl get servicemonitors --all-namespaces -l heritage!=deckhouse
        ```

        For more information on metric collection, refer to the [Prometheus module FAQ](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/prometheus/faq.html).
      summary: |
        Deckhouse namespace contains ServiceMonitors not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: D8AdmissionPolicyEngineNotBootstrapped
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/bootstrap.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        The admission-policy-engine module couldn't bootstrap.

        Steps to troubleshoot:

        1. Verify that the module's components are up and running:

           ```bash
           kubectl get pods -n d8-admission-policy-engine
           ```

        2. Check logs for issues, such as missing constraint templates or incomplete CRD creation:

           ```bash
           kubectl logs -n d8-system -lapp=deckhouse --tail=1000 | grep admission-policy-engine
           ```
      summary: |
        Admission-policy-engine module hasn't been bootstrapped for 10 minutes.
      severity: "7"
      markupFormat: markdown
    - name: D8BashibleApiserverLocked
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Check bashible-apiserver pods are up-to-date and running `kubectl -n d8-cloud-instance-manager get pods -l app=bashible-apiserver`
      summary: |
        Bashible-apiserver is locked for too long
      severity: "6"
      markupFormat: markdown
    - name: D8CertExporterPodIsNotReady
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: "Steps to resolve:\n\n1. Retrieve the deployment details:\n   \n   ```bash\n   kubectl -n d8-monitoring describe deploy x509-certificate-exporter\n   ```\n\n2. Check the pod status and investigate why it's not ready:\n\n   ```bash\n   kubectl -n d8-monitoring describe pod -l app=x509-certificate-exporter\n   ```\n"
      summary: |
        The x509-certificate-exporter pod isn't ready.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterPodIsNotRunning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: "Steps to resolve:\n\n1. Retrieve the deployment details:\n   \n   ```bash\n   kubectl -n d8-monitoring describe deploy x509-certificate-exporter\n   ```\n\n2. Check the pod status and investigate why it's not running:\n\n   ```bash\n   kubectl -n d8-monitoring describe pod -l app=x509-certificate-exporter\n   ```\n"
      summary: |
        The x509-certificate-exporter pod isn't running.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterTargetAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Ways to resolve:

        - Check the pod status:

          ```bash
          kubectl -n d8-monitoring get pod -l app=x509-certificate-exporter
          ```

        - Check the pod logs:

          ```bash
          kubectl -n d8-monitoring logs -l app=x509-certificate-exporter -c x509-certificate-exporter
          ```
      summary: |
        There is no x509-certificate-exporter target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Ways to resolve:

        - Check the pod status:

          ```bash
          kubectl -n d8-monitoring get pod -l app=x509-certificate-exporter
          ```

        - Check the pod logs:

          ```bash
          kubectl -n d8-monitoring logs -l app=x509-certificate-exporter -c x509-certificate-exporter
          ```
      summary: |
        Prometheus can't scrape x509-certificate-exporter metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8CloudDataDiscovererCloudRequestError
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cloud data discoverer cannot get data from cloud. See cloud data discoverer logs for more information:
        `kubectl -n {{ $labels.namespace }} logs deploy/cloud-data-discoverer`
      summary: |
        Cloud data discoverer cannot get data from cloud
      severity: "6"
      markupFormat: markdown
    - name: D8CloudDataDiscovererSaveError
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cloud data discoverer cannot save data to k8s resource. See cloud data discoverer logs for more information:
        `kubectl -n {{ $labels.namespace }} logs deploy/cloud-data-discoverer`
      summary: |
        Cloud data discoverer cannot save data to k8s resource
      severity: "6"
      markupFormat: markdown
    - name: D8ClusterAutoscalerManagerPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerPodIsNotRunning
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The {{$labels.pod}} Pod is {{$labels.phase}}.

        Run the following command to check its status: `kubectl -n {{$labels.namespace}} get pods {{$labels.pod}} -o json | jq .status`.
      summary: |
        The cluster-autoscaler Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerPodIsRestartingTooOften
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive cluster-autoscaler restarts indicate that something is wrong. Normally, it should be up and running all the time.

        Please, refer to the corresponding logs: `kubectl -n d8-cloud-instance-manager logs -f -l app=cluster-autoscaler -c cluster-autoscaler`.
      summary: |
        Too many cluster-autoscaler restarts have been detected.
      severity: "9"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTargetAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        Cluster-autoscaler automatically scales Nodes in the cluster; its unavailability will result in the inability
        to add new Nodes if there is a lack of resources to schedule Pods. In addition, the unavailability of cluster-autoscaler
        may result in over-spending due to provisioned but inactive cloud instances.

        The recommended course of action:
        1. Check the availability and status of cluster-autoscaler Pods: `kubectl -n d8-cloud-instance-manager get pods -l app=cluster-autoscaler`
        2. Check whether the cluster-autoscaler deployment is present: `kubectl -n d8-cloud-instance-manager get deploy cluster-autoscaler`
        3. Check the status of the cluster-autoscaler deployment: `kubectl -n d8-cloud-instance-manager describe deploy cluster-autoscaler`
      summary: |
        There is no cluster-autoscaler target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTargetDown
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape cluster-autoscaler's metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTooManyErrors
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Cluster-autoscaler's scaling attempt resulted in an error from the cloud provider.

        Please, refer to the corresponding logs: `kubectl -n d8-cloud-instance-manager logs -f -l app=cluster-autoscaler -c cluster-autoscaler`.
      summary: |
        Cluster-autoscaler issues too many errors.
      severity: "8"
      markupFormat: markdown
    - name: D8CNIEnabledMoreThanOne
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/cni-checks.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse has detected that multiple CNIs are enabled in the cluster.
        For the cluster to work correctly, only one CNI must be enabled.

        To resolve this issue, disable any unnecessary CNI.
      summary: |
        More than one CNI is enabled in the cluster.
      severity: "2"
      markupFormat: markdown
    - name: D8CNIMisconfigured
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/cni-checks.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Find the desired settings in the ConfigMap `d8-system/desired-cni-moduleconfig` by running the following command:

           ```bash
           kubectl -n d8-system get configmap desired-cni-moduleconfig -o yaml
           ```

        1. Update the conflicting settings in the CNI `{{ $labels.cni }}` ModuleConfig to match the desired configuration.
      summary: |
        Settings in the secret d8-cni-configuration conflict with the ModuleConfig.
      severity: "5"
      markupFormat: markdown
    - name: D8CNIMisconfigured
      sourceFile: modules/035-cni-flannel/monitoring/prometheus-rules/cni-checks.yaml
      moduleUrl: 035-cni-flannel
      module: cni-flannel
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Find the desired settings in the ConfigMap `d8-system/desired-cni-moduleconfig` by running the following command:

           ```bash
           kubectl -n d8-system get configmap desired-cni-moduleconfig -o yaml
           ```

        1. Update the conflicting settings in the CNI `{{ $labels.cni }}` ModuleConfig to match the desired configuration.
      summary: |
        Settings in the secret d8-cni-configuration conflict with the ModuleConfig.
      severity: "5"
      markupFormat: markdown
    - name: D8CNIMisconfigured
      sourceFile: modules/035-cni-simple-bridge/monitoring/prometheus-rules/cni-checks.yaml
      moduleUrl: 035-cni-simple-bridge
      module: cni-simple-bridge
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Find the desired settings in the ConfigMap `d8-system/desired-cni-moduleconfig` by running the following command:

           ```bash
           kubectl -n d8-system get configmap desired-cni-moduleconfig -o yaml
           ```

        1. Update the conflicting settings in the CNI `{{ $labels.cni }}` ModuleConfig to match the desired configuration.
      summary: |
        Settings in the secret d8-cni-configuration conflict with the ModuleConfig.
      severity: "5"
      markupFormat: markdown
    - name: D8ControlPlaneManagerPodNotRunning
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        The `d8-control-plane-manager` pod is either failing or hasn't been scheduled on node `{{ $labels.node }}`.

        To resolve this issue, check the status of the `kube-system/d8-control-plane-manager` DaemonSet and its pods by running the following command:

        ```bash
        kubectl -n kube-system get daemonset,pod --selector=app=d8-control-plane-manager
        ```
      summary: |
        Controller pod isn't running on node {{ $labels.node }}.
      severity: "6"
      markupFormat: markdown
    - name: D8CustomPrometheusRuleFoundInCluster
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are PrometheusRules in the Deckhouse namespace that were not created by Deckhouse.

        To resolve the issue, replace these PrometheusRules with the CustomPrometheusRules object.

        To list all PrometheusRules in the Deckhouse namespace, run the following command:

        ```bash
        kubectl get prometheusrules --all-namespaces -l heritage!=deckhouse
        ```

        For details on adding alerts and recording rules, refer to the [Prometheus module FAQ](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/prometheus/faq.html#how-do-i-add-alerts-andor-recording-rules).
      summary: |
        Deckhouse namespace contains PrometheusRules not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseConfigInvalid
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "Deckhouse configuration contains errors.\n\nSteps to troubleshoot:\n\n1. Check Deckhouse logs by running the following command:\n\n   ```bash\n   kubectl -n d8-system logs -f -l app=deckhouse\n   ```\n\n1. Edit the Deckhouse global configuration:\n\n   ```bash\n   kubectl edit mc global\n   ```\n   \n   Or edit configuration of a specific module:\n\n   ```bash\n   kubectl edit mc <MODULE_NAME>\n   ```\n"
      summary: |
        Deckhouse configuration is invalid.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotDeleteModule
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to delete the {{ $labels.module }} module.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotDiscoverModules
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to discover modules.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunGlobalHook
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to run the {{ $labels.hook }} global hook.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunModule
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to start the {{ $labels.module }} module.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunModuleHook
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to run the {{ $labels.module }}/{{ $labels.hook }} module hook.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseCustomTargetDown
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape custom metrics generated by Deckhouse hooks.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseDeprecatedConfigmapManagedByArgoCD
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The Deckhouse ConfigMap is no longer used.

        To resolve this issue, remove the `d8-system/deckhouse` ConfigMap from Argo CD.
      summary: |
        Deprecated Deckhouse ConfigMap managed by Argo CD.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseGlobalHookFailsTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The `{{ $labels.hook }}` hook has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        The {{ $labels.hook }} Deckhouse global hook is crashing too frequently.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseHasNoAccessToRegistry
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse can't connect to the registry (typically `registry.deckhouse.io`) to check for a new Docker image. These checks are performed every 15 seconds. Without access to the registry, automatic updates are unavailable.

        This alert often indicates that the Deckhouse Pod is experiencing connectivity issues with the Internet.
      summary: |
        Deckhouse is unable to connect to the registry.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseIsHung
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse is probably down, since the `deckhouse_live_ticks` metric in Prometheus has stopped increasing.
        This metric is expected to increment every 10 seconds.
      summary: |
        Deckhouse is down.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseIsNotOnReleaseChannel
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse in this cluster isn't subscribed to any of the regular release channels: `Alpha`, `Beta`, `EarlyAccess`, `Stable`, or `RockSolid`.

        To resolve this issue, follow these steps:

        1. Check the current release channel used in the cluster:

           ```bash
           kubectl -n d8-system  get deploy deckhouse -o json | jq '.spec.template.spec.containers[0].image' -r
           ```

        1. Subscribe to one of the regular release channels by adjusting the [`deckhouse` module configuration](https://deckhouse.io/products/kubernetes-platform/documentation/latest/modules/deckhouse/configuration.html#parameters-releasechannel).
      summary: |
        Deckhouse isn't subscribed to any regular release channels.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseModuleHookFailsTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The `{{ $labels.hook }}` hook of the `{{ $labels.module }}` module has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        The {{ $labels.module }}/{{ $labels.hook }} Deckhouse hook is crashing too frequently.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseModuleUpdatePolicyNotFound
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The module update policy for {{ $labels.module_release }} is missing.

        To resolve this issue, remove the label from the module release using the following command:

        ```bash
        kubectl label mr {{ $labels.module_release }} modules.deckhouse.io/update-policy-
        ```

        A new suitable policy will be detected automatically.
      summary: |
        Module update policy not found for {{ $labels.module_release }}.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhousePodIsNotReady
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        The Deckhouse Pod is NOT Ready.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhousePodIsNotRunning
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        The Deckhouse Pod is NOT Running.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhousePodIsRestartingTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Number of restarts in the last hour: {{ $value }}.

        Excessive Deckhouse restarts indicate a potential issue. Normally, Deckhouse should be up and running continuously.

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Excessive Deckhouse restarts detected.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseQueueIsHung
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse cannot finish processing of the `{{ $labels.queue }}` queue, which currently has {{ $value }} pending task(s).

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        The {{ $labels.queue }} Deckhouse queue is stuck with {{ $value }} pending task(s).
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseSelfTargetAbsent
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        There is no Deckhouse target in Prometheus.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseSelfTargetDown
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Deckhouse metrics.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseWatchErrorOccurred
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "Deckhouse has detected an error in the client-go informer, possibly due to connection issues with the API server.\n\nSteps to investigate:\n\n1. Check Deckhouse logs for more information by running:\n\n   ```bash\n   kubectl -n d8-system logs deploy/deckhouse | grep error | grep -i watch\n   ```\n\n1. This alert attempts to detect a correlation between the faulty snapshot invalidation and API server connection errors, specifically for the `handle-node-template` hook in the `node-manager` module.\n   \n   To compare the snapshot with the actual node objects for this hook, run the following command:\n\n   ```bash\n   diff -u <(kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}'|sort) <(kubectl -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module snapshots node-manager -o json | jq '.\"040-node-manager/hooks/handle_node_templates.go\"' | jq '.nodes.snapshot[] | .filterResult.Name' -r | sort)\n   ```\n"
      summary: |
        Possible API server connection error in the client-go informer.
      severity: "5"
      markupFormat: markdown
    - name: D8DexAllTargetsDown
      sourceFile: modules/150-user-authn/monitoring/prometheus-rules/dex.yaml
      moduleUrl: 150-user-authn
      module: user-authn
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Dex metrics.
      severity: "6"
      markupFormat: markdown
    - name: D8EtcdDatabaseHighFragmentationRatio
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        The etcd database size in use on instance `{{ $labels.instance }}` is less than 50% of the allocated disk space, indicating potential fragmentation. Additionally, the total storage size exceeds 75% of the configured quota.

        To resolve this issue, defragment the etcd database by running the following command:

        ```bash
        kubectl -n kube-system exec -ti etcd-{{ $labels.node }} -- /usr/bin/etcdctl \
          --cacert /etc/kubernetes/pki/etcd/ca.crt \
          --cert /etc/kubernetes/pki/etcd/ca.crt \
          --key /etc/kubernetes/pki/etcd/ca.key \
          --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
        ```
      summary: |
        etcd database size in use is less than 50% of the allocated storage.
      severity: "7"
      markupFormat: markdown
    - name: D8EtcdExcessiveDatabaseGrowth
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        Based on the growth rate observed over the last six hours, Deckhouse predicts that the etcd database will run out of disk space within one day on instance `{{ $labels.instance }}`.

        To prevent disruptions, investigate the cause and take necessary action.
      summary: |
        etcd cluster database is growing rapidly.
      severity: "4"
      markupFormat: markdown
    - name: D8GrafanaDeploymentReplicasUnavailable
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The number of Grafana replicas is less than the specified number.

        The Deployment is in the MinimumReplicasUnavailable state.

        Run the following command to check the status of the Deployment: `kubectl -n d8-monitoring get deployment grafana-v10 -o json | jq .status`.

        Run the following command to check the status of the Pods: `kubectl -n d8-monitoring get pods -l app=grafana-v10 -o json | jq '.items[] | {(.metadata.name):.status}'`.
      summary: |
        One or more Grafana Pods are NOT Running.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaDeprecatedCustomDashboardDefinition
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The `grafana-dashboard-definitions-custom` ConfigMap was found in the `d8-monitoring` namespace. This means that the deprecated method of registering custom dashboards in Grafana is being used.

        **This method is no longer used!**
        Please, use the custom [GrafanaDashboardDefinition](https://github.com/deckhouse/deckhouse/blob/main/modules/300-prometheus/docs/internal/GRAFANA_DASHBOARD_DEVELOPMENT.md) resource instead.
      summary: |
        The deprecated ConfigMap for defining Grafana dashboards is detected.
      severity: "9"
      markupFormat: markdown
    - name: D8GrafanaPodIsNotReady
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: ""
      summary: |
        The Grafana Pod is NOT Ready.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaPodIsRestartingTooOften
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive Grafana restarts indicate that something is wrong. Normally, Grafana should be up and running all the time.

        Please, refer to the corresponding logs: `kubectl -n d8-monitoring logs -f -l app=grafana-v10 -c grafana`.
      summary: |
        Excessive Grafana restarts are detected.
      severity: "9"
      markupFormat: markdown
    - name: D8GrafanaTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        Grafana visualizes metrics collected by Prometheus. Grafana is critical for some tasks,
        such as monitoring the state of applications and the cluster as a whole. Additionally,
        Grafana unavailability can negatively impact users who actively use it in their work.

        The recommended course of action:
        1. Check the availability and status of Grafana Pods: `kubectl -n d8-monitoring get pods -l app=grafana-v10`;
        2. Check the availability of the Grafana Deployment: `kubectl -n d8-monitoring get deployment grafana-v10`;
        3. Examine the status of the Grafana Deployment: `kubectl -n d8-monitoring describe deployment grafana-v10`.
      summary: |
        There is no Grafana target in Prometheus.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaTargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Grafana metrics.
      severity: "6"
      markupFormat: markdown
    - name: D8HasModuleConfigAllowedToDisable
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The ModuleConfig is pending module disabling.

        It is recommended that you keep your module configurations clean by removing unnecessary approval annotations.

        If you ignore this alert and do not clear the annotation, the module may be accidentally removed from the cluster, potentially leading to irreversible consequences.

        To resolve this issue and stop the alert, run the following command:

        ```bash
        kubectl annotate moduleconfig {{ $labels.module }} modules.deckhouse.io/allow-disabling-
        ```
      summary: |
        The ModuleConfig annotation for allowing module disabling is set.
      severity: "4"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterMalfunctioning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The `image-availability-exporter` has failed to perform any image availability checks in the container registry for over 20 minutes.

        To investigate the issue, review the exporter's logs:

        ```bash
        kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        The image-availability-exporter has crashed.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterPodIsNotReady
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `image-availability-exporter` pod is not ready. As a result, the images listed in the `image` field aren't checked for availability in the container registry.

        Steps to resolve:

        1. Retrieve the deployment details:

           ```bash
           kubectl -n d8-monitoring describe deploy image-availability-exporter
           ```

        2. Check the pod status and investigate why it isn't `Ready`:

           ```bash
           kubectl -n d8-monitoring describe pod -l app=image-availability-exporter
           ```
      summary: |
        The image-availability-exporter pod is not ready.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterPodIsNotRunning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `image-availability-exporter` pod is not running. As a result, the images listed in the `image` field aren't checked for availability in the container registry.

        Steps to resolve:

        1. Retrieve the deployment details:

           ```bash
           kubectl -n d8-monitoring describe deploy image-availability-exporter
           ```

        2. Check the pod status and investigate why it isn't running:

           ```bash
           kubectl -n d8-monitoring describe pod -l app=image-availability-exporter
           ```
      summary: |
        The image-availability-exporter pod is not running.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterTargetAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `image-availability-exporter` target is missing from Prometheus.

        Steps to resolve:

        1. Check the pod status:

           ```bash
           kubectl -n d8-monitoring get pod -l app=image-availability-exporter
           ```

        1. Check the pod logs:

           ```bash
           kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
           ```
      summary: |
        The image-availability-exporter target is missing from Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that Prometheus is unable to scrape metrics of `image-availability-exporter`.

        Steps to resolve:

        1. Check the pod status:

           ```bash
           kubectl -n d8-monitoring get pod -l app=image-availability-exporter
           ```

        1. Check the pod logs:

           ```bash
           kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
           ```
      summary: |
        Prometheus can't scrape metrics of image-availability-exporter.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioActualDataPlaneVersionNotEqualDesired
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with Istio data plane version `{{$labels.version}}`, while the desired version is `{{$labels.desired_version}}`. As a result, the Istio version will be changed after the pod is restarted.

        To resolve the issue, use the following cheat sheet:

        ```text
        ### Namespace-wide configuration
        # `istio.io/rev=vXYZ`: Use a specific revision.
        # `istio-injection=enabled`: Use the global revision.
        kubectl get ns {{$labels.namespace}} --show-labels

        ### Pod-wide configuration
        kubectl -n {{$labels.namespace}} get pods -l istio.io/rev={{$labels.desired_revision}}
        ```
      summary: |
        There are pods with Istio data plane version {{$labels.version}}, but desired version is {{$labels.desired_version}}.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioActualVersionIsNotInstalled
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with injected sidecars of version `{{$labels.version}}` (revision `{{$labels.revision}}`), but the corresponding control plane version is not installed. As a result, these pods have lost synchronization with the state in Kubernetes.

        To resolve this issue, install the required control plane version. Alternatively, update the namespace or pod configuration to match an installed control plane version.

        To identify orphaned pods, run the following command:

        ```bash
        kubectl -n {{ $labels.namespace }} get pods -l 'service.istio.io/canonical-name' -o json | jq --arg revision {{ $labels.revision }} '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson | .revision == $revision) | .metadata.name'
        ```
      summary: |
        The control plane version for pods with injected sidecars isn't installed.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioAdditionalControlplaneDoesntWork
      sourceFile: modules/110-istio/monitoring/prometheus-rules/controlplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected that the additional Istio control plane `{{$labels.label_istio_io_rev}}` isn't working.

        As a result, sidecar injection for pods with `{{$labels.label_istio_io_rev}}` isn't working as well.

        To check the status of the control plane pods, run the following command:

        ```
        kubectl get pods -n d8-istio -l istio.io/rev={{$labels.label_istio_io_rev}}
        ```
      summary: |
        Additional control plane isn't working.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioDataPlaneVersionMismatch
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with Istio data plane version `{{$labels.full_version}}`, which is different from the control plane version `{{$labels.desired_full_version}}`.

        Steps to resolve the issue:

        1. Restart affected pods and use the following PromQL query to get a full list:

           ```promql
           max by (namespace, dataplane_pod) (d8_istio_dataplane_metadata{full_version="{{$labels.full_version}}"})
           ```

        1. Use the automatic Istio data plane upgrade described in the [guide](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/istio/examples.html#upgrading-istio).
      summary: |
        There are pods with data plane version different from the control plane version.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioDataPlaneWithoutIstioInjectionConfigured
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with Istio sidecars, but istio-injection isn't configured. As a result, these pods will lose their Istio sidecars after being recreated.

        To identify the affected pods, run the following command:

        ```bash
        kubectl -n {{$labels.namespace}} get pods -o json | jq -r --arg revision {{$labels.revision}} '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson | .revision == $revision) | .metadata.name'
        ```
      summary: |
        Detected pods with Istio sidecars but istio-injection isn't configured.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioDeprecatedIstioVersionInstalled
      sourceFile: modules/110-istio/monitoring/prometheus-rules/versions.tpl
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected that a deprecated Istio version `{{$labels.version}}` is installed.

        Support for this version will be removed in upcoming Deckhouse releases. The higher the alert severity, the greater the probability of support being discontinued.

        To learn how to upgrade Istio, refer to the [upgrade guide](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/istio/examples.html#upgrading-istio).
      summary: |
        The installed Istio version has been deprecated.
      severity: undefined
      markupFormat: markdown
    - name: D8IstioDesiredVersionIsNotInstalled
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There is a desired Istio control plane version `{{$labels.desired_version}}` (revision `{{$labels.revision}}`) configured for pods in the `{{$labels.namespace}}` namespace, but that version isn't installed. As a result, pods can't be recreated in the `{{$labels.namespace}}` namespace.

        To resolve this issue, install the desired control plane version. Alternatively, update the namespace or pod configuration to match an installed control plane version.

        Use the following cheat sheet:

        ```text
        ### Namespace-wide configuration
        # `istio.io/rev=vXYZ`: Use a specific revision.
        # `istio-injection=enabled`: Use the global revision.
        kubectl get ns {{$labels.namespace}} --show-labels

        ### Pod-wide configuration
        kubectl -n {{$labels.namespace}} get pods -l istio.io/rev={{$labels.revision}}
        ```
      summary: |
        Desired control plane version isn't installed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioFederationMetadataEndpointDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/federation.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The metadata endpoint `{{$labels.endpoint}}` for IstioFederation `{{$labels.federation_name}}` has failed to fetch via the Deckhouse hook.

        To reproduce the request to the public endpoint, run the following command:

        ```bash
        curl {{$labels.endpoint}}
        ```

        To reproduce the request to private endpoints (run from the Deckhouse pod), run the following:

        ```bash
        KEY="$(deckhouse-controller module values istio -o json | jq -r .internal.remoteAuthnKeypair.priv)"
        LOCAL_CLUSTER_UUID="$(deckhouse-controller module values -g istio -o json | jq -r .global.discovery.clusterUUID)"
        REMOTE_CLUSTER_UUID="$(kubectl get istiofederation {{$labels.federation_name}} -o json | jq -r .status.metadataCache.public.clusterUUID)"
        TOKEN="$(deckhouse-controller helper gen-jwt --private-key-path <(echo "$KEY") --claim iss=d8-istio --claim sub=$LOCAL_CLUSTER_UUID --claim aud=$REMOTE_CLUSTER_UUID --claim scope=private-federation --ttl 1h)"
        curl -H "Authorization: Bearer $TOKEN" {{$labels.endpoint}}
        ```
      summary: |
        Federation metadata endpoint failed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioGlobalControlplaneDoesntWork
      sourceFile: modules/110-istio/monitoring/prometheus-rules/controlplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected that the global Istio control plane `{{$labels.label_istio_io_rev}}` isn't working.

        As a result, sidecar injection for pods with global revision isn't working as well, and the validating webhook for Istio resources is absent.

        To check the status of the control plane pods, run the following command:

        ```bash
        kubectl get pods -n d8-istio -l istio.io/rev={{$labels.label_istio_io_rev}}
        ```
      summary: |
        Global control plane isn't working.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioMulticlusterMetadataEndpointDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The metadata endpoint `{{$labels.endpoint}}` for IstioMulticluster `{{$labels.multicluster_name}}` has failed to fetch via the Deckhouse hook.

        To reproduce the request to the public endpoint, run the following command:

        ```bash
        curl {{$labels.endpoint}}
        ```

        To reproduce the request to private endpoints (run from the `d8-system/deckhouse` pod), run the following:

        ```bash
        KEY="$(deckhouse-controller module values istio -o json | jq -r .internal.remoteAuthnKeypair.priv)"
        LOCAL_CLUSTER_UUID="$(deckhouse-controller module values -g istio -o json | jq -r .global.discovery.clusterUUID)"
        REMOTE_CLUSTER_UUID="$(kubectl get istiomulticluster {{$labels.multicluster_name}} -o json | jq -r .status.metadataCache.public.clusterUUID)"
        TOKEN="$(deckhouse-controller helper gen-jwt --private-key-path <(echo "$KEY") --claim iss=d8-istio --claim sub=$LOCAL_CLUSTER_UUID --claim aud=$REMOTE_CLUSTER_UUID --claim scope=private-multicluster --ttl 1h)"
        curl -H "Authorization: Bearer $TOKEN" {{$labels.endpoint}}
        ```
      summary: |
        Multicluster metadata endpoint failed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioMulticlusterRemoteAPIHostDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The remote API host `{{$labels.api_host}}` for IstioMulticluster `{{$labels.multicluster_name}}` has failed the health check performed by the Deckhouse monitoring hook.

        To reproduce the request (run from the `d8-system/deckhouse` pod), run the following:

        ```bash
        TOKEN="$(deckhouse-controller module values istio -o json | jq -r --arg ah {{$labels.api_host}} '.internal.multiclusters[]| select(.apiHost == $ah)| .apiJWT ')"
        curl -H "Authorization: Bearer $TOKEN" https://{{$labels.api_host}}/version
        ```
      summary: |
        Multicluster remote API host health check failed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioOperatorReconcileError
      sourceFile: modules/110-istio/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected an error in the `istio-operator` reconciliation loop.

        To investigate the issue, check the operator logs:

        ```bash
        kubectl -n d8-istio logs -l app=operator,revision={{$labels.revision}}
        ```
      summary: |
        The istio-operator is unable to reconcile Istio control plane setup.
      severity: "5"
      markupFormat: markdown
    - name: D8IstioPodsWithoutIstioSidecar
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There is a pod `{{$labels.dataplane_pod}}` in the `{{$labels.namespace}}` namespace without Istio sidecars, but with istio-injection configured.

        To identify the affected pods, run the following command:

        ```bash
        kubectl -n {{$labels.namespace}} get pods -l '!service.istio.io/canonical-name' -o json | jq -r '.items[] | select(.metadata.annotations."sidecar.istio.io/inject" != "false") | .metadata.name'
        ```
      summary: |
        Detected pods without Istio sidecars but with istio-injection configured.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioVersionIsIncompatibleWithK8sVersion
      sourceFile: modules/110-istio/monitoring/prometheus-rules/versions.tpl
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        The installed Istio version `{{$labels.istio_version}}` may not work properly with the current Kubernetes version `{{$labels.k8s_version}}` because it's not supported officially.

        To resolve the issue, upgrade Istio following the [guide](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/istio/examples.html#upgrading-istio).
      summary: |
        The installed Istio version is incompatible with the Kubernetes version.
      severity: "3"
      markupFormat: markdown
    - name: D8KubeEtcdDatabaseSizeCloseToTheLimit
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        The etcd database size on `{{ $labels.node }}` is nearing its size limit.
        This may be caused by a high number of events, such as pod evictions or the recent creation of numerous resources in the cluster.

        Possible solutions:

        - Defragment the etcd database by running the following command:

          ```bash
          kubectl -n kube-system exec -ti etcd-{{ $labels.node }} -- /usr/bin/etcdctl \
            --cacert /etc/kubernetes/pki/etcd/ca.crt \
            --cert /etc/kubernetes/pki/etcd/ca.crt \
            --key /etc/kubernetes/pki/etcd/ca.key \
            --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
          ```

        - Increase node memory. Starting from 24 GB, `quota-backend-bytes` will increase by 1 GB for every extra 8 GB of memory.

          Example:

          | Node memory | quota-backend-bytes |
          | ----------- | ------------------- |
          | 16 GB       | 2147483648 (2 GB)   |
          | 24 GB       | 3221225472 (3 GB)   |
          | 32 GB       | 4294967296 (4 GB)   |
          | 40 GB       | 5368709120 (5 GB)   |
          | 48 GB       | 6442450944 (6 GB)   |
          | 56 GB       | 7516192768 (7 GB)   |
          | 64 GB       | 8589934592 (8 GB)   |
          | 72 GB       | 8589934592 (8 GB)   |
          | ...         | ...                 |
      summary: |
        etcd database size is approaching the limit.
      severity: "3"
      markupFormat: markdown
    - name: D8KubernetesStaleTokensDetected
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        This issue may occur if an application reads the token only at startup and does not reload it periodically. As a result, an outdated token might be used, leading to security breach and authentication failures.

        **Recommended actions:**

        - Ensure your application is configured to periodically reload the token from the file system.
        - Verify that you are using an up-to-date client library that supports automatic token rotation.

        Note that currently these tokens are not blocked because the `--service-account-extend-token-expiration` flag is enabled by default (`Default: true`). With this flag enabled, admission-injected tokens are extended up to 1 year during token generation to facilitate a safe transition from legacy tokens to the bound service account token feature, ignoring the value of `service-account-max-token-expiration`.

        **For further investigation:**
        Log into the server with label `instance={{ $labels.instance }}` and inspect the audit log using the following command:

        ```bash
        jq 'select(.annotations["authentication.k8s.io/stale-token"]) | {auditID, stageTimestamp, requestURI, verb, user: .user.username, stale_token: .annotations["authentication.k8s.io/stale-token"]}' /var/log/kube-audit/audit.log
        ```
      summary: |
        Stale service account tokens detected.
      severity: "8"
      markupFormat: markdown
    - name: D8KubernetesVersionIsDeprecated
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        The current Kubernetes version `{{ $labels.k8s_version }}` has been deprecated, and support for it will be removed in upcoming releases.

        Please migrate to the next kubernetes version (at least 1.29)

        Check how to update the Kubernetes version in the cluster here - https://deckhouse.io/documentation/deckhouse-faq.html#how-do-i-upgrade-the-kubernetes-version-in-a-cluster

        Refer to the [Kubernetes upgrade guide](https://deckhouse.io/documentation/deckhouse-faq.html#how-do-i-upgrade-the-kubernetes-version-in-a-cluster) for instructions.
      summary: |
        Kubernetes version {{ $labels.k8s_version }} is deprecated.
      severity: "7"
      markupFormat: markdown
    - name: D8LogShipperAgentNotScheduledInCluster
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Deckhouse has detected that a number of `log-shipper-agent` pods are not scheduled.

        To resolve this issue, do the following:

        1. Check the state of the `d8-log-shipper/log-shipper-agent` DaemonSet:

           ```shell
           kubectl -n d8-log-shipper get daemonsets --selector=app=log-shipper
           ```

        1. Check the state of the `d8-log-shipper/log-shipper-agent` pods:

           ```shell
           kubectl -n d8-log-shipper get pods --selector=app=log-shipper-agent
           ```

        1. If you know where the DaemonSet should be scheduled, run the following command to identify the problematic nodes:

           ```shell
           kubectl -n d8-log-shipper get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="log-shipper-agent")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
           ```
      summary: |
        The log-shipper-agent pods can't be scheduled in the cluster.
      severity: "7"
      markupFormat: markdown
    - name: D8LogShipperClusterLogDestinationD8LokiAuthorizationRequired
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/warnings.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |-
        Deckhouse has detected the ClusterLogDestination resource `{{$labels.resource_name}}` without authorization parameters.

        Add the authorization parameters to the ClusterLogDestination resource following the [instructions](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/log-shipper/faq.html#how-to-add-authorization-to-the-clusterlogdestination-resource).
      summary: |
        Authorization parameters required for the ClusterLogDestination resource.
      severity: "9"
      markupFormat: markdown
    - name: D8LogShipperCollectLogErrors
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Deckhouse has detected that the {{ $labels.host }} `log-shipper-agent` on the `{{ $labels.node }}` node has failed to collect metrics for more than 10 minutes.

        This is caused by the `{{ $labels.error_type }}` errors occurred during the `{{ $labels.stage }}` stage while reading `{{ $labels.component_type }}`.

        To resolve this, check the pod logs or follow advanced instructions:

        ```bash
        kubectl -n d8-log-shipper logs {{ $labels.host }}` -c vector
        ```
      summary: |
        The log-shipper-agent pods can't collect logs to {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8LogShipperDestinationErrors
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Deckhouse has detected that the {{ $labels.host }} `log-shipper-agent` on the {{ $labels.node }} node has failed to send a log for more than 10 minutes.

        This is caused by the `{{ $labels.error_type }}` errors occurred during the `{{ $labels.stage }}` stage while sending logs to `{{ $labels.component_type }}`.

        To resolve this, check the pod logs or follow advanced instructions:

        ```bash
        kubectl -n d8-log-shipper logs {{ $labels.host }}` -c vector
        ```
      summary: |
        The log-shipper-agent pods can't send logs to {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8LogShipperLogsDroppedByRateLimit
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Rate-limiting rules have been applied, and the `log-shipper-agent` on the `{{ $labels.node }}` node has been dropping logs for more than 10 minutes.

        To resolve this, check the pod logs or follow advanced instructions:

        ```bash
        kubectl -n d8-log-shipper get pods -o wide | grep {{ $labels.node }}
        ```
      summary: |
        The log-shipper-agent pods are dropping logs to {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsNotRunning
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The {{$labels.pod}} Pod is {{$labels.phase}}.

        Run the following command to check the status of the Pod: `kubectl -n {{$labels.namespace}} get pods {{$labels.pod}} -o json | jq .status`.
      summary: |
        The machine-controller-manager Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsRestartingTooOften
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of restarts in the last hour: {{ $value }}.

        Excessive machine-controller-manager restarts indicate that something is wrong. Normally, it should be up and running all the time.

        Please, refer to the logs: `kubectl -n d8-cloud-instance-manager logs -f -l app=machine-controller-manager -c controller`.
      summary: |
        The machine-controller-manager module restarts too often.
      severity: "9"
      markupFormat: markdown
    - name: D8MachineControllerManagerTargetAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        Machine controller manager manages ephemeral Nodes in the cluster. Its unavailability will result in the inability to add/delete Nodes.

        The recommended course of action:
        1. Check the availability and status of `machine-controller-manager` Pods: `kubectl -n d8-cloud-instance-manager get pods -l app=machine-controller-manager`;
        2. Check the availability of the `machine-controller-manager` Deployment: `kubectl -n d8-cloud-instance-manager get deploy machine-controller-manager`;
        3. Check the status of the `machine-controller-manager` Deployment: `kubectl -n d8-cloud-instance-manager describe deploy machine-controller-manager`.
      summary: |
        There is no machine-controller-manager target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerTargetDown
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape machine-controller-manager's metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8MetallbBGPSessionDown
      sourceFile: ee/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: ee
      description: |
        {{ $labels.job }}, MetalLB {{ $labels.container }} on {{ $labels.pod}} has BGP session {{ $labels.peer }} down.

        Check the logs for details:

        ```bash
        kubectl -n d8-metallb logs daemonset/speaker -c speaker
        ```
      summary: |
        MetalLB BGP session is down.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbConfigNotLoaded
      sourceFile: ee/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: ee
      description: |
        {{ $labels.job }}, MetalLB {{ $labels.container }} on {{ $labels.pod}} hasn't been loaded.

        To find the cause of the issue, review the controller logs:

        ```bash
        kubectl -n d8-metallb logs deploy/controller -c controller
        ```
      summary: |
        The MetalLB configuration hasn't been loaded.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbConfigStale
      sourceFile: ee/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: ee
      description: |
        {{ $labels.job }}, MetalLB {{ $labels.container }} on {{ $labels.pod}} is running on a stale configuration because the latest configuration failed to load.

        To find the cause of the issue, review the controller logs:

        ```bash
        kubectl -n d8-metallb logs deploy/controller -c controller
        ```
      summary: |
        MetalLB is running on a stale configuration.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbNotSupportedServiceAnnotationsDetected
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        The annotation '{{$labels.annotation}}' has been deprecated for the service '{{$labels.name}}' in the '{{$labels.namespace}}' namespace.

        The following service annotations are no longer effective:
        - `metallb.universe.tf/ip-allocated-from-pool`: Remove this annotation.
        - `metallb.universe.tf/address-pool`: Replace it with the `.spec.loadBalancerClass` parameter or use the `network.deckhouse.io/metal-load-balancer-class` annotation, referencing the appropriate MetalLoadBalancerClass.
        - `metallb.universe.tf/loadBalancerIPs`: Replace it with `network.deckhouse.io/load-balancer-ips: <IP>`.
        - `metallb.universe.tf/allow-shared-ip`: Replace it with `network.deckhouse.io/load-balancer-shared-ip-key`.

        **Please note.** Existing LoadBalancer services of Deckhouse have been migrated automatically, but the new ones will not be.
      summary: |
        The annotation '{{$labels.annotation}}' has been deprecated for the service '{{$labels.name}}' in the '{{$labels.namespace}}' namespace.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbObsoleteLayer2PoolsAreUsed
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        In ModuleConfig version 2, addressPool '{{$labels.name}}' of type “layer2” are ignored. They should be removed from the configuration.
      summary: |
        The metallb module has obsolete layer2 pools configured.
      severity: "7"
      markupFormat: markdown
    - name: D8MetallbUpdateMCVersionRequired
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        D8 MetalLB settings are outdated.

        To resolve this issue, increase version in the ModuleConfig `metallb`.
      summary: |
        The metallb ModuleConfig settings are outdated.
      severity: "5"
      markupFormat: markdown
    - name: D8NeedDecreaseEtcdQuotaBackendBytes
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        When the control plane node memory is reduced, Deckhouse may suggest reducing `quota-backend-bytes`.
        While Deckhouse is capable of automatically increasing this value, reducing it must be done manually.

        To modify `quota-backend-bytes`, set the `controlPlaneManager.etcd.maxDbSize` parameter. Before setting a new value, check the current database usage on every control plane node by running:

        ```
        for pod in $(kubectl get pod -n kube-system -l component=etcd,tier=control-plane -o name); do
          kubectl -n kube-system exec -ti "$pod" -- /usr/bin/etcdctl \
            --cacert /etc/kubernetes/pki/etcd/ca.crt \
            --cert /etc/kubernetes/pki/etcd/ca.crt \
            --key /etc/kubernetes/pki/etcd/ca.key \
            endpoint status -w json | jq --arg a "$pod" -r \
            '.[0].Status.dbSize / 1024 / 1024 | tostring | $a + ": " + . + " MB"';
        done
        ```

        Things to note:

        - The maximum value for `controlPlaneManager.etcd.maxDbSize` is 8 GB.
        - If control plane nodes have less than 24 GB, set `controlPlaneManager.etcd.maxDbSize` to 2 GB.
        - Starting from 24 GB, `quota-backend-bytes` will increase by 1 GB for every extra 8 GB of memory.

          Example:

          | Node memory | quota-backend-bytes |
          | ----------- | ------------------- |
          | 16 GB       | 2147483648 (2 GB)   |
          | 24 GB       | 3221225472 (3 GB)   |
          | 32 GB       | 4294967296 (4 GB)   |
          | 40 GB       | 5368709120 (5 GB)   |
          | 48 GB       | 6442450944 (6 GB)   |
          | 56 GB       | 7516192768 (7 GB)   |
          | 64 GB       | 8589934592 (8 GB)   |
          | 72 GB       | 8589934592 (8 GB)   |
          | ...         | ...                 |
      summary: |
        Deckhouse suggests reducing quota-backend-bytes.
      severity: "6"
      markupFormat: markdown
    - name: D8NginxIngressKruiseControllerPodIsRestartingTooOften
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        {{ $value }} Kruise controller restarts detected in the last hour.

        Excessive Kruise controller restarts indicate that something is wrong. Normally, it should be up and running all the time.

        Steps to resolve:

        1. Check events associated with `kruise-controller-manager` in the `d8-ingress-nginx` namespace. Look for issues related to node failures or memory shortages (OOM events):

           ```bash
           kubectl -n d8-ingress-nginx get events | grep kruise-controller-manager
           ```

        2. Analyze the controller's pod descriptions to identify restarted containers and possible causes. Pay attention to exit codes and other details:

           ```bash
           kubectl -n d8-ingress-nginx describe pod -lapp=kruise,control-plane=controller-manager
           ```

        3. In case the `kruise` container has restarted, get a list of relevant container logs to identify any meaningful errors:

           ```bash
           kubectl -n d8-ingress-nginx logs -lapp=kruise,control-plane=controller-manager -c kruise
           ```
      summary: |
        Too many Kruise controller restarts detected.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeGroupIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for Nodes of the {{ $labels.node_group }} group; Nodes have learned about the update. However, no Node can get approval to start updating.

        Most likely, there is a problem with the `update_approval` hook of the `node-manager` module.
      summary: |
        The {{ $labels.node_group }} node group is not handling the update correctly.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeHasDeprecatedOSVersion
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/node-os-requirements.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Deckhouse has detected nodes running deprecated OS versions.

        Steps to troubleshoot:

        1. Get a list of affected nodes by running the following Prometheus query:

           ```promql
           kube_node_info{os_image=~"Ubuntu 18.04.*|Debian GNU/Linux 10.*|CentOS Linux 7.*"}
           ```

        1. Update the affected nodes to a supported OS version.
      summary: |
        Nodes with deprecated OS versions detected.
      severity: "4"
      markupFormat: markdown
    - name: D8NodeHasUnmetKernelRequirements
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/kernel-requirements.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Deckhouse has detected that some nodes don't meet the required kernel constraints.
        As a result, certain modules can't run on these nodes.

        Kernel requirements for each module:

        - **Cilium module**: Kernel version must be **>= 4.9.17**.
        - **Cilium with Istio**: Kernel version must be **>= 5.7**.
        - **Cilium with OpenVPN**: Kernel version must be **>= 5.7**.
        - **Cilium with Node-local-dns**: Kernel version must be **>= 5.7**.

        To list all affected nodes, use the `d8_node_kernel_does_not_satisfy_requirements == 1` expression in Prometheus.
      summary: |
        Nodes have unmet kernel requirements.
      severity: "4"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group; the Node has learned about the update, requested and received approval, started the update, ran into a step that causes possible downtime. The update manager (the update_approval hook of node-group module) performed the update, and the Node received downtime approval. However, there is no success message about the update.

        Here is how you can view Bashible logs on the Node:
        ```shell
        journalctl -fu bashible
        ```
      summary: |
        The {{ $labels.node }} Node cannot complete the update.
      severity: "7"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group}; the Node has learned about the update, requested and received approval, but cannot complete the update.

        Here is how you can view Bashible logs on the Node:
        ```shell
        journalctl -fu bashible
        ```
      summary: |
        The {{ $labels.node }} Node cannot complete the update.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group but it has not received the update nor trying to.

        Most likely Bashible for some reason is not handling the update correctly. At this point, it must add the `update.node.deckhouse.io/waiting-for-approval` annotation to the Node so that it can be approved.

        You can find out the most current version of the update using this command:
        ```shell
        kubectl -n d8-cloud-instance-manager get secret configuration-checksums -o jsonpath={.data.{{ $labels.node_group }}} | base64 -d
        ```

        Use the following command to find out the version on the Node:
        ```shell
        kubectl get node {{ $labels.node }} -o jsonpath='{.metadata.annotations.node\.deckhouse\.io/configuration-checksum}'
        ```

        Here is how you can view Bashible logs on the Node:
        ```shell
        journalctl -fu bashible
        ```
      summary: |
        The {{ $labels.node }} Node does not update.
      severity: "9"
      markupFormat: markdown
    - name: D8NodeIsUnmanaged
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-unmanaged.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The {{ $labels.node }} Node is not managed by the [node-manager](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/node-manager/) module.

        The recommended actions are as follows:
        - Follow these instructions to clean up the node and add it to the cluster: https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/node-manager/faq.html#how-to-clean-up-a-node-for-adding-to-the-cluster
      summary: |
        The {{ $labels.node }} Node is not managed by the node-manager module.
      severity: "9"
      markupFormat: markdown
    - name: D8NodeUpdateStuckWaitingForDisruptionApproval
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} group; the Node has learned about the update, requested and received approval, started the update, and ran into a stage that causes possible downtime. For some reason, the Node cannot get that approval (it is issued fully automatically by the `update_approval` hook of the `node-manager`).
      summary: |
        The {{ $labels.node }} Node cannot get disruption approval.
      severity: "8"
      markupFormat: markdown
    - name: D8OkmeterAgentPodIsNotReady
      sourceFile: modules/500-okmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-okmeter
      module: okmeter
      edition: ce
      description: ""
      summary: |
        Okmeter agent is not Ready
      severity: "6"
      markupFormat: markdown
    - name: D8OldPrometheusCustomTargetFormat
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        Deckhouse has detected that services with the `prometheus-custom-target` label are being used to collect metrics in the cluster.

        The label format has been changed. To resolve the issue, replace the `prometheus-custom-target` label with `prometheus.deckhouse.io/custom-target`.

        To list all services labeled with `prometheus-custom-target`, run the following command:

        ```bash
        kubectl get service --all-namespaces --show-labels | grep prometheus-custom-target
        ```

        For more information on metric collection, refer to the [Prometheus module FAQ](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/prometheus/faq.html).
      summary: |
        Services with the prometheus-custom-target label are being used for metric collection.
      severity: "9"
      markupFormat: markdown
    - name: D8OldPrometheusTargetFormat
      sourceFile: ee/fe/modules/340-monitoring-applications/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-applications
      module: monitoring-applications
      edition: fe
      description: |-
        Deckhouse has detected that services with the `prometheus-target` label are being used to collect metrics in the cluster.

        The label format has been changed. To resolve the issue, replace the `prometheus-target` label with `prometheus.deckhouse.io/target`.

        To list all services labeled with `prometheus-target`, run the following command:

        ```bash
        kubectl get service --all-namespaces --show-labels | grep prometheus-target
        ```
      summary: |
        Services with the prometheus-target label are being used for metric collection.
      severity: "6"
      markupFormat: markdown
    - name: D8ProblematicNodeGroupConfiguration
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for Nodes of the {{ $labels.node_group }} group; Nodes have learned about the update. However, {{ $labels.node }} Node cannot be updated.

        Node {{ $labels.node }} has no `node.deckhouse.io/configuration-checksum` annotation.
        Perhaps the bootstrap process of the Node did not complete correctly. Check the `cloud-init` logs (/var/log/cloud-init-output.log) of the Node.
        There is probably a problematic NodeGroupConfiguration resource for {{ $labels.node_group }} NodeGroup.
      summary: |
        The {{ $labels.node }} Node cannot begin the update.
      severity: "8"
      markupFormat: markdown
    - name: D8PrometheusLongtermFederationTargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/longterm-target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: prometheus-longterm cannot scrape "/federate" endpoint from Prometheus. Check error cause in prometheus-longterm WebUI or logs.
      summary: |
        prometheus-longterm cannot scrape prometheus.
      severity: "5"
      markupFormat: default
    - name: D8PrometheusLongtermTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        This Prometheus component is only used to display historical data and is not crucial. However, if its unavailability will last long enough, you will not be able to view the statistics.

        Usually, Pods of this type have problems because of disk unavailability (e.g., the disk cannot be mounted to a Node for some reason).

        The recommended course of action:
        1. Take a look at the StatefulSet data: `kubectl -n d8-monitoring describe statefulset prometheus-longterm`;
        2. Explore its PVC (if used): `kubectl -n d8-monitoring describe pvc prometheus-longterm-db-prometheus-longterm-0`;
        3. Explore the Pod's state: `kubectl -n d8-monitoring describe pod prometheus-longterm-0`.
      summary: |
        There is no prometheus-longterm target in Prometheus.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorPodIsNotReady
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action:
        1. Analyze the Deployment info: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`;
        2. Examine the status of the Pod and try to figure out why it is not running: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`.
      summary: |
        The prometheus-operator Pod is NOT Ready.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorPodIsNotRunning
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action:
        1. Analyze the Deployment info: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`;
        2. Examine the status of the Pod and try to figure out why it is not running: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`.
      summary: |
        The prometheus-operator Pod is NOT Running.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorTargetAbsent
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action is to analyze the deployment information: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`.
      summary: |
        There is no prometheus-operator target in Prometheus.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorTargetDown
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |-
        The `prometheus-operator` Pod is not available.

        The new `Prometheus`, `PrometheusRules`, `ServiceMonitor` settings cannot be applied in the cluster; however, all existing and configured components continue to operate correctly.
        This problem will not affect alerting or monitoring in the short term (a few days).

        The recommended course of action:
        1. Analyze the Deployment info: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`;
        2. Examine the status of the Pod and try to figure out why it is not running: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`.
      summary: |
        Prometheus is unable to scrape prometheus-operator metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8ReservedNodeLabelOrTaintFound
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/reserved-domain.tpl
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        Deckhouse has detected that node {{ $labels.name }} is using one of the following:
        - A reserved `metadata.labels` object `node-role.deckhouse.io/`, which doesn't end with `(system|frontend|monitoring|_deckhouse_module_name_)`.
        - A reserved `spec.taints` object `dedicated.deckhouse.io`, with a value other than `(system|frontend|monitoring|_deckhouse_module_name_)`.

        For instructions on how to resolve this issue, refer to the [node allocation guide](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/node-manager/faq.html#how-do-i-allocate-nodes-to-specific-loads).
      summary: |
        Node {{ $labels.name }} is using a reserved label or taint.
      severity: "6"
      markupFormat: markdown
    - name: D8RuntimeAuditEngineNotScheduledInCluster
      sourceFile: ee/modules/650-runtime-audit-engine/monitoring/prometheus-rules/runtime-audit-engine.yaml
      moduleUrl: 650-runtime-audit-engine
      module: runtime-audit-engine
      edition: ee
      description: |
        A number of runtime-audit-engine pods are not scheduled.
        Security audit is not fully operational.

        Consider checking state of the d8-runtime-audit-engine/runtime-audit-engine DaemonSet.
        `kubectl -n d8-runtime-audit-engine get daemonset,pod --selector=app=runtime-audit-engine`
        Get a list of nodes that have pods in an not Ready state.
        ```
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Pods of runtime-audit-engine cannot be scheduled in the cluster.
      severity: "4"
      markupFormat: markdown
    - name: D8SecretCopierDeprecatedLabels
      sourceFile: modules/600-secret-copier/monitoring/prometheus-rules/deprecated-label.yaml
      moduleUrl: 600-secret-copier
      module: secret-copier
      edition: ce
      description: |-
        The [secrets copier module](https://github.com/deckhouse/deckhouse/tree/main/modules/600-secret-copier/) has changed the service label for the original secrets in the `default` namespace.

        Soon we will abandon the old `antiopa-secret-copier: "yes"` label.

        You have to replace the `antiopa-secret-copier: "yes"` label with  `secret-copier.deckhouse.io/enabled: ""` for all secrets that the `secret-copier` module uses in the `default` namespace.
      summary: |
        Obsolete antiopa_secret_copier=yes label has been found.
      severity: "9"
      markupFormat: markdown
    - name: D8SmokeMiniNotBoundPersistentVolumeClaims
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        {{ $labels.persistentvolumeclaim }} persistent volume claim status is {{ $labels.phase }}.

        There is a problem with pv provisioning. Check the status of the pvc o find the problem:
        `kubectl -n d8-upmeter get pvc {{ $labels.persistentvolumeclaim }}`

        If you have no disk provisioning system in the cluster,
        you can disable ordering volumes for the some-mini through the module settings.
      summary: |
        Smoke-mini has unbound or lost persistent volume claims.
      severity: "9"
      markupFormat: markdown
    - name: D8SnapshotControllerPodIsNotReady
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-controller`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-controller`
      summary: |
        The snapshot-controller Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotControllerPodIsNotRunning
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-controller`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-controller`
      summary: |
        The snapshot-controller Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotControllerTargetAbsent
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Check the Pod status: `kubectl -n d8-snapshot-controller get pod -l app=snapshot-controller`
        2. Or check the Pod logs: `kubectl -n d8-snapshot-controller logs -l app=snapshot-controller -c snapshot-controller`
      summary: |
        There is no snapshot-controller target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotControllerTargetDown
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-controller.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Check the Pod status: `kubectl -n d8-snapshot-controller get pod -l app=snapshot-controller`
        2. Or check the Pod logs: `kubectl -n d8-snapshot-controller logs -l app=snapshot-controller -c snapshot-controller`
      summary: |
        Prometheus cannot scrape the snapshot-controller metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotValidationWebhookPodIsNotReady
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-validation-webhook.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-validation-webhook`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-validation-webhook`
      summary: |
        The snapshot-validation-webhook Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8SnapshotValidationWebhookPodIsNotRunning
      sourceFile: modules/045-snapshot-controller/monitoring/prometheus-rules/snapshot-validation-webhook.yaml
      moduleUrl: 045-snapshot-controller
      module: snapshot-controller
      edition: ce
      description: |
        The recommended course of action:
        1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-validation-webhook`
        2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-validation-webhook`
      summary: |
        The snapshot-validation-webhook Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterClusterStateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Real Kubernetes cluster state is `{{ $labels.status }}` comparing to Terraform state.

        It's important to make them equal.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter cluster state changed
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterClusterStateError
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter can't check difference between Kubernetes cluster state and Terraform state.

        Probably, it occurred because Terraform-state-exporter had failed to run terraform with current state and config.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter cluster state error
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterHasErrors
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Errors occurred while terraform-state-exporter working.

        Check pods logs to get more details: `kubectl -n d8-system logs -l app=terraform-state-exporter -c exporter`
      summary: |
        Terraform-state-exporter has errors
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeStateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Real Node `{{ $labels.node_group }}/{{ $labels.name }}` state is `{{ $labels.status }}` comparing to Terraform state.

        It's important to make them equal.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter node state changed
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeStateError
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter can't check difference between Node `{{ $labels.node_group }}/{{ $labels.name }}` state and Terraform state.

        Probably, it occurred because Terraform-manager had failed to run terraform with current state and config.
        First, run the `dhctl terraform check` command to check what will change.
        To converge state of Kubernetes cluster, use `dhctl converge` command.
      summary: |
        Terraform-state-exporter node state error
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeTemplateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter found difference between node template from cluster provider configuration and from NodeGroup `{{ $labels.name }}`.
        Node template is `{{ $labels.status }}`.

        First, run the `dhctl terraform check` command to check what will change.
        Use `dhctl converge` command or manually adjust NodeGroup settings to fix the issue.
      summary: |
        Terraform-state-exporter node template changed
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterPodIsNotReady
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter doesn't check the difference between real Kubernetes cluster state and Terraform state.

        Pease, check:
        1. Deployment description: `kubectl -n d8-system describe deploy terraform-state-exporter`
        2. Pod status: `kubectl -n d8-system describe pod -l app=terraform-state-exporter`
      summary: |
        Pod terraform-state-exporter is not Ready
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterPodIsNotRunning
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Terraform-state-exporter doesn't check the difference between real Kubernetes cluster state and Terraform state.

        Pease, check:
        1. Deployment description: `kubectl -n d8-system describe deploy terraform-state-exporter`
        2. Pod status: `kubectl -n d8-system describe pod -l app=terraform-state-exporter`
      summary: |
        Pod terraform-state-exporter is not Running
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterTargetAbsent
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        To get more details:
        Check pods state: `kubectl -n d8-system get pod -l app=terraform-state-exporter` or logs: `kubectl -n d8-system logs -l app=terraform-state-exporter -c exporter`
      summary: |
        Prometheus has no terraform-state-exporter target
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterTargetDown
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        To get more details:
        Check pods state: `kubectl -n d8-system get pod -l app=terraform-state-exporter` or logs: `kubectl -n d8-system logs -l app=terraform-state-exporter -c exporter`
      summary: |
        Prometheus can't scrape terraform-state-exporter
      severity: "8"
      markupFormat: markdown
    - name: D8TricksterTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The following modules use this component:
        * `prometheus-metrics-adapter` — the unavailability of the component means that HPA (auto scaling) is not running and you cannot view resource consumption using `kubectl`;
        * `vertical-pod-autoscaler` — this module is quite capable of surviving a short-term unavailability, as VPA looks at the consumption history for 8 days;
        * `grafana` — by default, all dashboards use Trickster for caching requests to Prometheus. You can retrieve data directly from Prometheus (bypassing the Trickster). However, this may lead to high memory usage by Prometheus and, hence, to its unavailability.

        The recommended course of action:
        1. Analyze the Deployment information: `kubectl -n d8-monitoring describe deployment trickster`;
        2. Analyze the Pod information: `kubectl -n d8-monitoring describe pod -l app=trickster`;
        3. Usually, Trickster is unavailable due to Prometheus-related issues because the Trickster's readinessProbe checks the Prometheus availability. Thus, make sure that Prometheus is running: `kubectl -n d8-monitoring describe pod -l app.kubernetes.io/name=prometheus,prometheus=main`.
      summary: |
        There is no Trickster target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: D8TricksterTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The following modules use this component:
        * `prometheus-metrics-adapter` — the unavailability of the component means that HPA (auto scaling) is not running and you cannot view resource consumption using `kubectl`;
        * `vertical-pod-autoscaler` — this module is quite capable of surviving a short-term unavailability, as VPA looks at the consumption history for 8 days;
        * `grafana` — by default, all dashboards use Trickster for caching requests to Prometheus. You can retrieve data directly from Prometheus (bypassing the Trickster). However, this may lead to high memory usage by Prometheus and, hence, to unavailability.

        The recommended course of action:
        1. Analyze the Deployment stats: `kubectl -n d8-monitoring describe deployment trickster`;
        2. Analyze the Pod stats: `kubectl -n d8-monitoring describe pod -l app=trickster`;
        3. Usually, Trickster is unavailable due to Prometheus-related issues because the Trickster's readinessProbe checks the Prometheus availability. Thus, make sure that Prometheus is running: `kubectl -n d8-monitoring describe pod -l app.kubernetes.io/name=prometheus,prometheus=main`.
      summary: |
        There is no Trickster target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: D8UpmeterAgentPodIsNotReady
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: ""
      summary: |
        Upmeter agent is not Ready
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterAgentReplicasUnavailable
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |-
        Check DaemonSet status:
        `kubectl -n d8-upmeter get daemonset upmeter-agent -o json | jq .status`

        Check the status of its pod:
        `kubectl -n d8-upmeter get pods -l app=upmeter-agent -o json | jq '.items[] | {(.metadata.name):.status}'`
      summary: |
        One or more Upmeter agent pods is NOT Running
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterDiskUsage
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The only way to resolve is to recreate the PVC using the following steps:

        1. Save the PVC data if you need it.

        1. Delete the PVC and restart `upmeter`:

           ```shell
           kubectl -n d8-upmeter delete persistentvolumeclaim/data-upmeter-0 pod/upmeter-0
           ```

        1. Check the status of the created PVC:

           ```shell
           kubectl -n d8-upmeter get pvc
           ```
      summary: |
        Upmeter disk usage is over 80%.
      severity: "5"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageConfigmap
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Probe configmaps found.

        Upmeter agents should clean ConfigMaps produced by control-plane/basic probe. There should not be more
        configmaps than master nodes (upmeter-agent is a DaemonSet with master nodeSelector). Also, they should be
        deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the configmaps were left by old
        upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "basic-functionality") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional.

        3. Delete configmaps manually:

        `kubectl -n d8-upmeter delete cm -l heritage=upmeter`
      summary: |
        Garbage produced by basic probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageDeployment
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe deployments count per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean Deployments produced by control-plane/controller-manager probe. There should not
        be more deployments than master nodes (upmeter-agent is a DaemonSet with master nodeSelector).
        Also, they should be deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the deployments were left by old
        upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "controller-manager") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional, kube-controller-manager in particular.

        3. Delete deployments manually:

        `kubectl -n d8-upmeter delete deploy -l heritage=upmeter`
      summary: |
        Garbage produced by controller-manager probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageNamespaces
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe namespace per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean namespaces produced by control-plane/namespace probe. There should not be more
        of these namespaces than master nodes (upmeter-agent is a DaemonSet with master nodeSelector).
        Also, they should be deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the namespaces were left
        by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "namespace") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional.

        3. Delete namespaces manually: `kubectl -n d8-upmeter delete ns -l heritage=upmeter`
      summary: |
        Garbage produced by namespace probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbagePods
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe pods count per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean Pods produced by control-plane/scheduler probe. There should not be more
        of these pods than master nodes (upmeter-agent is a DaemonSet with master nodeSelector). Also, they should be
        deleted within seconds.

        This might be an indication of a problem with kube-apiserver. Or, possibly, the pods were left
        by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "scheduler") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional.

        3. Delete pods manually:

        `kubectl -n d8-upmeter delete po -l upmeter-probe=scheduler`
      summary: |
        Garbage produced by scheduler probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbagePodsFromDeployments
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average probe pods count per upmeter-agent pod: {{ $value }}.

        Upmeter agents should clean Deployments produced by control-plane/controller-manager probe,
        and hence kube-controller-manager should clean their pods. There should not be more of these pods than
        master nodes (upmeter-agent is a DaemonSet with master nodeSelector). Also, they should be
        deleted within seconds.

        This might be an indication of a problem with kube-apiserver or kube-controller-manager. Or, probably,
        the pods were left by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "controller-manager") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane is functional, kube-controller-manager in particular.

        3. Delete pods manually:

        `kubectl -n d8-upmeter delete po -l upmeter-probe=controller-manager`
      summary: |
        Garbage produced by controller-manager probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageSecretsByCertManager
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Probe secrets found.

        Upmeter agents should clean certificates, and thus secrets produced by cert-manager should clean, too.
        There should not be more secrets than master nodes (upmeter-agent is a DaemonSet with master nodeSelector).
        Also, they should be deleted within seconds.

        This might be an indication of a problem with kube-apiserver, or cert-manager, or upmeter itself.
        It is also possible, that the secrets were left by old upmeter-agent pods due to Upmeter update.

        1. Check upmeter-agent logs

        `kubectl -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "cert-manager") | [.time, .level, .msg] | @tsv'`

        2. Check that control plane and cert-manager are functional.

        3. Delete certificates manually, and secrets, if needed:

        ```
        kubectl -n d8-upmeter delete certificate -l upmeter-probe=cert-manager
        kubectl -n d8-upmeter get secret -ojson | jq -r '.items[] | .metadata.name' | grep upmeter-cm-probe | xargs -n 1 -- kubectl -n d8-upmeter delete secret
        ```
      summary: |
        Garbage produced by cert-manager probe is not being cleaned.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterServerPodIsNotReady
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: ""
      summary: |
        Upmeter server is not Ready
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterServerPodIsRestartingTooOften
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Restarts for the last hour: {{ $value }}.

        Upmeter server should not restart too often. It should always be running and collecting episodes.
        Check its logs to find the problem:
        `kubectl -n d8-upmeter logs -f upmeter-0 upmeter`
      summary: |
        Upmeter server is restarting too often.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterServerReplicasUnavailable
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |-
        Check StatefulSet status:
        `kubectl -n d8-upmeter get statefulset upmeter -o json | jq .status`

        Check the status of its pod:
        `kubectl -n d8-upmeter get pods upmeter-0 -o json | jq '.items[] | {(.metadata.name):.status}'`
      summary: |
        One or more Upmeter server pods is NOT Running
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterSmokeMiniMoreThanOnePVxPVC
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The number of unnecessary smoke-mini PVs: {{ $value }}.

        Smoke-mini PVs should be deleted when released. Probably smoke-mini storage class has Retain policy by default,
        or there is CSI/cloud issue.

        These PVs have no valuable data on them an should be deleted.

        The list of PVs: `kubectl get pv | grep disk-smoke-mini`.
      summary: |
        Unnecessary smoke-mini volumes in cluster
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterTooManyHookProbeObjects
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Average UpmeterHookProbe count per upmeter-agent pod is {{ $value }}, but should be strictly 1.

        Some of the objects were left by old upmeter-agent pods due to Upmeter update or downscale.

        Leave only newest objects corresponding to upmeter-agent pods, when the reason it investigated.

        See `kubectl get upmeterhookprobes.deckhouse.io`.
      summary: |
        Too many UpmeterHookProbe objects in cluster
      severity: "9"
      markupFormat: markdown
    - name: D8YandexNatInstanceConnectionsQuotaUtilization
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/nat-instance.tpl
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: "The connection quota for the Yandex NAT instance has exceeded 85% utilization over the past 5 minutes. \n\nTo prevent potential issues, contact Yandex technical support and request an increase in the connection quota.\n"
      summary: |
        Connection quota utilization of the Yandex NAT instance exceeds 85% over the last 5 minutes.
      severity: "4"
      markupFormat: markdown
    - name: DaemonSetAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeadMansSwitch
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: This is a dead man's switch meant to ensure that the entire Alerting pipeline is functional.
      summary: |
        Alerting dead man's switch.
      severity: "4"
      markupFormat: default
    - name: DeckhouseModuleUseEmptyDir
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/emptydir.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Deckhouse module {{ $labels.module_name }} use emptydir as storage.
      summary: |
        Deckhouse module {{ $labels.module_name }} use emptydir as storage.
      severity: "9"
      markupFormat: markdown
    - name: DeckhouseReleaseDisruptionApprovalRequired
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The new Deckhouse release includes a disruptive update that requires manual approval.

        To check the details, run the following command:

        ```bash
        kubectl describe DeckhouseRelease {{ $labels.name }}
        ```

        To approve the disruptive update, run the following command:

        ```bash
        kubectl annotate DeckhouseRelease {{ $labels.name }} release.deckhouse.io/disruption-approved=true
        ```
      summary: |
        Deckhouse release disruption approval required.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseReleaseIsBlocked
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The requirements for the Deckhouse release haven't been met.

        To check the details, run the following command:

        ```bash
        kubectl describe DeckhouseRelease {{ $labels.name }}
        ```
      summary: |
        Deckhouse release requirements haven't been met.
      severity: "5"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new Deckhouse release is available but requires manual approval before it can be applied.

        To approve the release, run the following command:

        ```bash
        kubectl patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'
        ```
      summary: |
        A new Deckhouse release is awaiting manual approval.
      severity: "3"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "A new Deckhouse release is available but requires manual approval before it can be applied.\n\nTo approve the release, run the following command: \n\n```bash\nkubectl patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{\"approved\": true}'\n```\n"
      summary: |
        A new Deckhouse release is awaiting manual approval.
      severity: "6"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "A new Deckhouse release is available but requires manual approval before it can be applied.\n\nTo approve the release, run the following command: \n\n```bash\nkubectl patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{\"approved\": true}'\n```\n"
      summary: |
        A new Deckhouse release is awaiting manual approval.
      severity: "9"
      markupFormat: markdown
    - name: DeckhouseReleaseNotificationNotSent
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The Deckhouse release notification webhook failed to send.

        To check the notification webhook address, run the following command:

        ```bash
        kubectl get mc deckhouse -o yaml
        ```
      summary: |
        Deckhouse release notification webhook hasn't been sent.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseUpdating
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Deckhouse is being updated.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseUpdatingFailed
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The Deckhouse update has failed.

        Possible reasons:

        - The next minor or patch version of the Deckhouse image is not available in the registry.
        - The Deckhouse image is corrupted.

        Current version: {{ $labels.version }}.

        To resolve this issue, ensure that the next version of the Deckhouse image is available in the registry.
      summary: |
        Deckhouse update has failed.
      severity: "4"
      markupFormat: markdown
    - name: DeploymentAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentGenerationMismatch
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kube-state-metrics.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The observed deployment generation doesn't match the expected one for Deployment `{{$labels.namespace}}/{{$labels.deployment}}`.
      summary: |
        Deployment is outdated.
      severity: "4"
      markupFormat: default
    - name: DeploymentImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeprecatedGeoIPVersion
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/deprecated-geoip-version.tpl
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        An IngressNginxController and/or Ingress object in the cluster is using variables from the deprecated NGINX GeoIPv1 module. Support for this module has been discontinued in Ingress NGINX Controller version 1.10 and higher.

        It's recommended that you update your configuration to use the [GeoIPv2 module](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-geoip2).

        To get a list of the IngressNginxControllers using GeoIPv1 variables, run the following command:

        ```shell
        kubectl get ingressnginxcontrollers.deckhouse.io -o json | jq '.items[] | select(..|strings | test("\\$geoip_(country_(code3|code|name)|area_code|city_continent_code|city_country_(code3|code|name)|dma_code|latitude|longitude|region|region_name|city|postal_code|org)([^_a-zA-Z0-9]|$)+")) | .metadata.name'
        ```

        To get a list of the Ingress objects using GeoIPv1 variables, run the following command:

        ```shell
        kubectl get ingress -A -o json | jq '.items[] | select(..|strings | test("\\$geoip_(country_(code3|code|name)|area_code|city_continent_code|city_country_(code3|code|name)|dma_code|latitude|longitude|region|region_name|city|postal_code|org)([^_a-zA-Z0-9]|$)+")) | "\(.metadata.namespace)/\(.metadata.name)"' | sort | uniq
        ```
      summary: |
        Deprecated GeoIP version 1 is used in the cluster.
      severity: "9"
      markupFormat: markdown
    - name: EarlyOOMPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/early-oom.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod has detected unavailable PSI subsystem. Check logs for additional information: kubectl -n d8-cloud-instance-manager logs {{$labels.pod}} Possible actions to resolve the problem: * Upgrade kernel to version 4.20 or higher. * Enable Pressure Stall Information. * Disable early oom.
      severity: "8"
      markupFormat: markdown
    - name: EbpfExporterKernelNotSupported
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/ebpf-exporter.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Possible options to resolve the issue:

        * Build the kernel with [BTF type information](https://github.com/libbpf/libbpf?tab=readme-ov-file#bpf-co-re-compile-once--run-everywhere).
        * [Disable ebpf_exporter](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/monitoring-kubernetes/configuration.html#parameters-ebpfexporterenabled).
      summary: |
        The BTF module required for ebpf_exporter is missing from the kernel.
      severity: "8"
      markupFormat: markdown
    - name: ExtendedMonitoringDeprecatatedAnnotation
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/deprecated-annotation.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the deprecated annotation `extended-monitoring.flant.com/enabled` is used in the cluster.

        Steps to resolve:

        1. Check the `d8_deprecated_legacy_annotation` metric in Prometheus for a list of all detected usages.
        1. Migrate to the `extended-monitoring.deckhouse.io/enabled` label.
      summary: |
        Deprecated annotation is used in the cluster.
      severity: "4"
      markupFormat: markdown
    - name: ExtendedMonitoringTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/self.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        The pod running `extended-monitoring-exporter` is currently unavailable.

        As a result, the following alerts will not be triggered:

        * Low disk space and inode usage on volumes.
        * CPU overloads and container throttling.
        * `500` errors on Ingress.
        * Insufficient replicas of Deployments, StatefulSets, and DaemonSets.
        * [Other alerts](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/extended-monitoring/) associated with this exporter.

        To resolve this issue, investigate its possible causes:

        1. Print detailed information about the `extended-monitoring-exporter` deployment:

           ```bash
           kubectl -n d8-monitoring describe deploy extended-monitoring-exporter
           ```

        2. Print detailed information about the pods associated with the `extended-monitoring-exporter`:

           ```bash
           kubectl -n d8-monitoring describe pod -l app=extended-monitoring-exporter
           ```
      summary: |
        Extended monitoring is unavailable.
      severity: "5"
      markupFormat: markdown
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.instance }}` is expected to exhaust its available file/socket descriptors within the next hour.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.instance }} are almost exhausted.
      severity: "3"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }}` is expected to exhaust its available file/socket descriptors within the next hour.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} are almost exhausted.
      severity: "3"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.instance }}` is expected to exhaust its available file/socket descriptors within the next 4 hours.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.instance }} are exhausting soon.
      severity: "4"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }}` is expected to exhaust its available file/socket descriptors within the next 4 hours.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} are exhausting soon.
      severity: "4"
      markupFormat: default
    - name: GrafanaDashboardAlertRulesDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before updating to Grafana 10, it's required to migrate an outdated alerts from Grafana to the external alertmanager (or exporter-alertmanager stack)
        To list all deprecated alert rules use the expr `sum by (dashboard, panel, alert_rule) (d8_grafana_dashboards_deprecated_alert_rule) > 0`

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        Deprecated Grafana alerts have been found.
      severity: "8"
      markupFormat: markdown
    - name: GrafanaDashboardPanelIntervalDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before updating to Grafana 10, it's required to rewrite an outdated expressions that uses `$interval_rv`, `interval_sx3` or `interval_sx4` to `$__rate_interval`
        To list all deprecated panel intervals use the expr `sum by (dashboard, panel, interval) (d8_grafana_dashboards_deprecated_interval) > 0`

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        Deprecated Grafana panel intervals have been found.
      severity: "8"
      markupFormat: markdown
    - name: GrafanaDashboardPluginsDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before updating to Grafana 10, it's required to check if currently installed plugins will work correctly with Grafana 10
        To list all potentially outdated plugins use the expr `sum by (dashboard, panel, plugin) (d8_grafana_dashboards_deprecated_plugin) > 0`

        Plugin "flant-statusmap-panel" is being deprecated and won't be supported in the near future
        We recommend you to migrate to the State Timeline plugin: https://grafana.com/docs/grafana/latest/panels-visualizations/visualizations/state-timeline/

        Attention: The check runs once per hour, so this alert should go out within an hour after deprecated resources migration.
      summary: |
        Deprecated Grafana plugins have been found.
      severity: "8"
      markupFormat: markdown
    - name: HelmReleasesHasResourcesWithDeprecatedVersions
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/helm/deprecated-versions.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        To list all affected resources, run the following Prometheus query:

        ```promql
        max by (helm_release_namespace, helm_release_name, helm_version, resource_namespace, resource_name, api_version, kind, k8s_version) (resource_versions_compatibility) == 1
        ```

        For more details on upgrading deprecated resources, refer to the Kubernetes deprecation guide available at `https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v{{ $labels.k8s_version | reReplaceAll "\\." "-" }}`.

        Note that the check runs once per hour, so this alert should resolve within an hour after migrating deprecated resources.
      summary: |
        At least one Helm release contains resources with deprecated apiVersion, which will be removed in Kubernetes version {{ $labels.k8s_version }}.
      severity: "5"
      markupFormat: markdown
    - name: HelmReleasesHasResourcesWithUnsupportedVersions
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/helm/deprecated-versions.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        To list all affected resources, run the following Prometheus query:

        ```promql
        max by (helm_release_namespace, helm_release_name, helm_version, resource_namespace, resource_name, api_version, kind, k8s_version) (resource_versions_compatibility) == 2
        ```

        For more details on migrating deprecated resources, refer to the Kubernetes deprecation guide available at `https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v{{ $labels.k8s_version | reReplaceAll "\\." "-" }}`.

        Note that the check runs once per hour, so this alert should resolve within an hour after migrating deprecated resources.
      summary: |
        At least one Helm release contains resources with unsupported apiVersion for Kubernetes version {{ $labels.k8s_version }}.
      severity: "4"
      markupFormat: markdown
    - name: IngressResponses5xx
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/ingress.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that URL {{$labels.vhost}}{{$labels.location}} on Ingress `{{$labels.ingress}}`, using service `{{$labels.service}}` on port {{$labels.service_port}} has more than {{ printf "extended_monitoring_ingress_threshold{threshold=\"5xx-critical\", namespace=\"%s\", ingress=\"%s\"}" $labels.namespace $labels.ingress | query | first | value }}% of `5xx` responses from the backend.

        Current rate of `5xx` responses: {{ .Value }}%
      summary: |
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} has more than {{ printf &quot;extended_monitoring_ingress_threshold{threshold=&quot;5xx-critical&quot;, namespace=&quot;%s&quot;, ingress=&quot;%s&quot;}&quot; $labels.namespace $labels.ingress | query | first | value }}% of 5xx responses from the backend.
      severity: "4"
      markupFormat: default
    - name: IngressResponses5xx
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/ingress.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that URL {{$labels.vhost}}{{$labels.location}} on Ingress `{{$labels.ingress}}`, using service `{{$labels.service}}` on port {{$labels.service_port}}, has more than {{ printf "extended_monitoring_ingress_threshold{threshold=\"5xx-warning\", namespace=\"%s\", ingress=\"%s\"}" $labels.namespace $labels.ingress | query | first | value }}% of `5xx` responses from the backend.

        Current rate of `5xx` responses: {{ .Value }}%
      summary: |
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} has more than {{ printf &quot;extended_monitoring_ingress_threshold{threshold=&quot;5xx-warning&quot;, namespace=&quot;%s&quot;, ingress=&quot;%s&quot;}&quot; $labels.namespace $labels.ingress | query | first | value }}% of 5xx responses from the backend.
      severity: "5"
      markupFormat: default
    - name: IstioIrrelevantExternalServiceFound
      sourceFile: modules/110-istio/monitoring/prometheus-rules/services.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        A service `{{$labels.name}}` in the `{{$labels.namespace}}` namespace has an irrelevant port specification.

        The `.spec.ports[]` field isn't applicable for services of the `ExternalName` type.
        However, Istio renders port listeners for external services as `0.0.0.0:port`, which captures all traffic to the specified port. This can cause problems for services that aren't registered in the Istio registry.

        To resolve the issue, remove the `.spec.ports` section from the service configuration. It is safe.
      summary: |
        External service found with irrelevant ports specifications.
      severity: "5"
      markupFormat: markdown
    - name: K8SApiserverDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: No API servers are reachable, or they have all disappeared from service discovery.
      summary: |
        API servers can't be reached.
      severity: "3"
      markupFormat: default
    - name: K8sCertificateExpiration
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: "Some clients are connecting to {{$labels.component}} with certificates that will expire in less than a day on node `{{$labels.component}}`.\n\nTo check control plane certificates, use kubeadm:\n\n1. Install kubeadm using the following command:\n   \n   ```bash\n   apt install kubeadm=1.24.*\n   ```\n\n2. Check certificates:\n\n   ```bash\n   kubeadm alpha certs check-expiration\n   ```\n\nTo check kubelet certificates, do the following on each node:\n\n1. Check kubelet configuration:\n\n   ```bash\n   ps aux \\\n     | grep \"/usr/bin/kubelet\" \\\n     | grep -o -e \"--kubeconfig=\\S*\" \\\n     | cut -f2 -d\"=\" \\\n     | xargs cat\n   ```\n\n2. Locate the `client-certificate` or `client-certificate-data` field.\n3. Check certificate expiration using OpenSSL.\n\nNote that there are no tools to find other stale kubeconfig files.\nConsider enabling the `control-plane-manager` module for advanced debugging.\n"
      summary: |
        Kubernetes has API clients with soon-to-expire certificates.
      severity: "5"
      markupFormat: markdown
    - name: K8sCertificateExpiration
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: "Some clients are connecting to {{$labels.component}} with certificates that will expire in less than 7 days on node `{{$labels.node}}`.\n\nTo check control plane certificates, use kubeadm:\n\n1. Install kubeadm using the following command:\n   \n   ```bash\n   apt install kubeadm=1.24.*\n   ```\n\n2. Check certificates:\n\n   ```bash\n   kubeadm alpha certs check-expiration\n   ```\n\nTo check kubelet certificates, do the following on each node:\n\n1. Check kubelet configuration:\n\n   ```bash\n   ps aux \\\n     | grep \"/usr/bin/kubelet\" \\\n     | grep -o -e \"--kubeconfig=\\S*\" \\\n     | cut -f2 -d\"=\" \\\n     | xargs cat\n   ```\n\n2. Locate the `client-certificate` or `client-certificate-data` field.\n3. Check certificate expiration using OpenSSL.\n\nNote that there are no tools to find other stale kubeconfig files.\nConsider enabling the `control-plane-manager` module for advanced debugging.\n"
      summary: |
        Kubernetes has API clients with soon-to-expire certificates.
      severity: "6"
      markupFormat: markdown
    - name: K8SControllerManagerTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-controller-manager.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: There is no running kube-controller-manager. As a result, deployments and replication controllers are not progressing.
      summary: |
        Controller manager is down.
      severity: "3"
      markupFormat: default
    - name: K8SKubeletDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus failed to scrape {{ $value }}% of kubelets.
      summary: |
        Multiple kubelets couldn't be scraped.
      severity: "3"
      markupFormat: default
    - name: K8SKubeletDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus failed to scrape {{ $value }}% of kubelets.
      summary: |
        Several kubelets couldn't be scraped.
      severity: "4"
      markupFormat: default
    - name: K8SKubeletTooManyPods
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: The kubelet on node {{ $labels.node }} is running {{ $value }} pods, which is close to the limit of {{ printf "kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\",unit=\"integer\",node=\"%s\"}" $labels.node | query | first | value }}.
      summary: |
        The kubelet on node {{ $labels.node }} is approaching the pod limit.
      severity: "7"
      markupFormat: default
    - name: K8SManyNodesNotReady
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $value }}% of Kubernetes nodes are not ready.'
      summary: |
        Too many nodes are not ready.
      severity: "3"
      markupFormat: default
    - name: K8SNodeNotReady
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The kubelet on node {{ $labels.node }} has either failed to check in with the API server or has set itself to `NotReady` for more than 10 minutes.
      summary: |
        The status of node {{ $labels.node }} is NotReady.
      severity: "3"
      markupFormat: default
    - name: K8SSchedulerTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-scheduler.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        The Kubernetes scheduler is not running.
        As a result, new pods are not being assigned to nodes.
      summary: |
        Scheduler is down.
      severity: "3"
      markupFormat: default
    - name: K8STooManyNodes
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/nodes.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: Cluster is running {{ $value }} nodes, close to the maximum amount of {{ print "d8_max_nodes_amount{}" | query | first | value }} nodes.
      summary: |
        Nodes amount is close to the maximum allowed amount.
      severity: "7"
      markupFormat: default
    - name: KubeEtcdHighFsyncDurations
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        In the last 15 minutes, the 99th percentile of the fsync duration for WAL files exceeded 0.5 seconds: {{ $value }}.

        Possible causes:

        - High disk latency where etcd data is stored.
        - High CPU usage on the node.
      summary: |
        Syncing (fsync) WAL files to disk is slow.
      severity: "7"
      markupFormat: markdown
    - name: KubeEtcdHighNumberOfLeaderChanges
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        There have been {{ $value }} leader re-elections for the etcd cluster member running on node `{{ $labels.node }}` in the last 10 minutes.

        Possible causes:

        - High disk latency where etcd data is stored.
        - High CPU usage on the node.
        - Degradation of network connectivity between cluster members in the multi-master mode.
      summary: |
        The etcd cluster is re-electing the leader too frequently.
      severity: "5"
      markupFormat: markdown
    - name: KubeEtcdInsufficientMembers
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        The etcd cluster has too few members, increasing the risk of failure if another member becomes unavailable.
        To resolve this issue, check the status of etcd Pods:
        ```bash kubectl -n kube-system get pod -l component=etcd ```
      summary: |
        Insufficient members in the etcd cluster.
      severity: "4"
      markupFormat: markdown
    - name: KubeEtcdNoLeader
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        To resolve this issue, check the status of the etcd Pods:
        ```bash kubectl -n kube-system get pod -l component=etcd | grep {{ $labels.node }} ```
      summary: |
        The etcd cluster member running on node {{ $labels.node }} has lost the leader.
      severity: "4"
      markupFormat: markdown
    - name: KubeEtcdTargetAbsent
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Steps to troubleshoot:
        1. Check the status of the etcd Pods:

           ```bash
           kubectl -n kube-system get pod -l component=etcd
           ```

        1. Review Prometheus logs:

           ```bash
           kubectl -n d8-monitoring logs -l app.kubernetes.io/name=prometheus -c prometheus
           ```
      summary: |
        There is no etcd target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: KubeEtcdTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Steps to troubleshoot:
        1. Check the status of the etcd Pods:

           ```bash
           kubectl -n kube-system get pod -l component=etcd
           ```

        1. Review Prometheus logs:

           ```bash
           kubectl -n d8-monitoring logs -l app.kubernetes.io/name=prometheus -c prometheus
           ```
      summary: |
        Prometheus is unable to scrape etcd metrics.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free space remaining on imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The imagefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_imagefs_bytes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free inodes remaining on imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The imagefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_imagefs_inodes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free space remaining on nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The nodefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_nodefs_bytes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free inodes remaining on nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The nodefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_nodefs_inodes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesCoreDNSHasCriticalErrors
      sourceFile: modules/042-kube-dns/monitoring/prometheus-rules/kubernetes/dns.yaml
      moduleUrl: 042-kube-dns
      module: kube-dns
      edition: ce
      description: |-
        Deckhouse has detected at least one critical error in the CoreDNS pod {{$labels.pod}}.

        To resolve the issue, review the container logs:

        ```bash
        kubectl -n kube-system logs {{$labels.pod}}
        ```
      summary: |
        Critical errors found in CoreDNS.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDaemonSetNotUpToDate
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected {{ .Value }} outdated pods in DaemonSet `{{ $labels.namespace }}/{{ $labels.daemonset }}` over the last 15 minutes.

        Steps to resolve:

        1. Check the DaemonSet's status:

           ```bash
           kubectl -n {{ $labels.namespace }} get ds {{ $labels.daemonset }}
           ```

        2. Analyze the DaemonSet's description:

           ```bash
           kubectl -n {{ $labels.namespace }} describe ds {{ $labels.daemonset }}
           ```

        3. If the parameter `Number of Nodes Scheduled with Up-to-date Pods` does not match `Current Number of Nodes Scheduled`, check the DaemonSet's `updateStrategy`:

           ```bash
           kubectl -n {{ $labels.namespace }} get ds {{ $labels.daemonset }} -o json | jq '.spec.updateStrategy'
           ```

           If `updateStrategy` is set to `OnDelete`, the DaemonSet is updated only when pods are deleted.
      summary: |
        There were {{ .Value }} outdated pods in DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} over the last 15 minutes.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesDaemonSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas remaining in DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        No available replicas remaining in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}}.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDaemonSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that the number of unavailable replicas in DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}` exceeds the threshold.

        - Current number: `{{ .Value }}` unavailable replica(s).
        - Threshold number: `{{ printf "extended_monitoring_daemonset_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", daemonset=\"%s\"}" $labels.namespace $labels.daemonset | query | first | value }}` unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        The number of unavailable replicas in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} exceeds the threshold.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesDeploymentReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas remaining in deployment `{{$labels.namespace}}/{{$labels.deployment}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"Deployment\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        No available replicas remaining in deployment {{$labels.namespace}}/{{$labels.deployment}}.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDeploymentReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that the number of unavailable replicas in deployment `{{$labels.namespace}}/{{$labels.deployment}}` exceeds the value set in `spec.strategy.rollingupdate.maxunavailable`.

        - Current number: `{{ .Value }}` unavailable replica(s).
        - Threshold number: `{{ printf "extended_monitoring_deployment_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", deployment=\"%s\"}" $labels.namespace $labels.deployment | query | first | value }}` unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"Deployment\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        The number of unavailable replicas in deployment {{$labels.namespace}}/{{$labels.deployment}} exceeds spec.strategy.rollingupdate.maxunavailable.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesDnsTargetDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/kube-dns.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Prometheus is unable to collect metrics from `kube-dns`, which makes its status unknown.

        Steps to troubleshoot:

        1. Check the deployment details:

           ```bash
           kubectl -n kube-system describe deployment -l k8s-app=kube-dns
           ```

        2. Check the pod details:

           ```bash
           kubectl -n kube-system describe pod -l k8s-app=kube-dns
           ```
      summary: |
        Kube-dns or CoreDNS are not being monitored.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesStatefulSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that there are no ready replicas remaining in StatefulSet `{{$labels.namespace}}/{{$labels.statefulset}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"StatefulSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        No ready replicas remaining in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}}.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesStatefulSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that the number of unavailable replicas in StatefulSet `{{$labels.namespace}}/{{$labels.statefulset}}` exceeds the threshold.

        - Current number: `{{ .Value }}` unavailable replica(s).
        - Threshold number: `{{ printf "extended_monitoring_statefulset_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", statefulset=\"%s\"}" $labels.namespace $labels.statefulset | query | first | value }}` unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"StatefulSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        The number of unavailable replicas in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}} exceeds the threshold.
      severity: "6"
      markupFormat: markdown
    - name: KubeStateMetricsDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kube-state-metrics.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Deckhouse has detected that no metrics about cluster resources have been available for the past 5 minutes.
        As a result, most alerts and monitoring panels aren't working.

        Steps to troubleshoot:

        1. Check the `kube-state-metrics` pods:

           ```bash
           kubectl -n d8-monitoring describe pod -l app=kube-state-metrics
           ```

        2. Check the deployment logs:

           ```bash
           kubectl -n d8-monitoring describe deploy kube-state-metrics
           ```
      summary: |
        Kube-state-metrics isn't working in the cluster.
      severity: "3"
      markupFormat: markdown
    - name: L2LoadBalancerOrphanServiceFound
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        The cluster contains an orphaned service `{{$labels.name}}` in the `{{$labels.namespace}}` namespace  with an irrelevant L2LoadBalancer name.

        To resolve this issue, verify the L2LoadBalancer name specified in the annotation `network.deckhouse.io/l2-load-balancer-name`.
      summary: |
        Orphaned service with an irrelevant L2LoadBalancer name has been found.
      severity: "4"
      markupFormat: markdown
    - name: LoadAverageHigh
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Over the last 5 minutes, the average load on node `{{ $labels.node }}` has been higher than {{ printf "extended_monitoring_node_threshold{threshold=\"load-average-per-core-critical\", node=\"%s\"}" $labels.node | query | first | value }} per core.

        There are more processes in the queue than the CPU can handle.

        Possible causes:

        - A process has created too many threads or child processes.
        - The CPU is overloaded.
      summary: |
        Average load on node {{ $labels.node }} is too high.
      severity: "4"
      markupFormat: markdown
    - name: LoadAverageHigh
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Over the last 30 minutes, the average load on node `{{ $labels.node }}` has been higher than or equal to {{ printf "extended_monitoring_node_threshold{threshold=\"load-average-per-core-warning\", node=\"%s\"}" $labels.node | query | first | value }} per core.

        There are more processes in the queue than the CPU can handle.

        Possible causes:

        - A process has created too many threads or child processes.
        - The CPU is overloaded.
      summary: |
        Average load on node {{ $labels.node }} is too high.
      severity: "5"
      markupFormat: markdown
    - name: LoadBalancerServiceWithoutExternalIP
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/loadbalancer.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        One or more services of the `LoadBalancer` type have not received an external address.

        To list affected services, run the following command:

        ```bash
        kubectl get svc -Ao json | jq -r '.items[] | select(.spec.type == "LoadBalancer") | select(.status.loadBalancer.ingress[0].ip == null) | "namespace: \(.metadata.namespace), name: \(.metadata.name), ip: \(.status.loadBalancer.ingress[0].ip)"'
        ```

        Steps to troubleshoot:

        - Check the `cloud-controller-manager` logs in the `d8-cloud-provider-*` namespace.
        - If you are using a bare-metal cluster with the `metallb` module enabled, ensure the address pool has not been exhausted.
      summary: |
        A LoadBalancer has not been created.
      severity: "4"
      markupFormat: default
    - name: LokiRetentionPerionViolation
      sourceFile: modules/462-loki/monitoring/prometheus-rules/alerts.tpl
      moduleUrl: 462-loki
      module: loki
      edition: ce
      description: |-
        Not enough disk space to retain logs for 168 hours. Current effective retention period is {{ $value }} hours.

        You need either decrease expected `retentionPeriodHours` or increase resize Loki PersistentVolumeClaim
      summary: |
        Not enough disk space to retain logs for 168 hours
      severity: "4"
      markupFormat: markdown
    - name: MigrationRequiredFromRBDInTreeProvisionerToCSIDriver
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/migration-alerts.tpl
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To resolve this issue, migrate volumes to the Ceph CSI driver using the `rbd-in-tree-to-ceph-csi-migration-helper.sh` script available at `https://github.com/deckhouse/deckhouse/blob//modules/031-ceph-csi/tools/rbd-in-tree-to-ceph-csi-migration-helper.sh`.

        For details on volume migration, refer to the migration guide available at `https://github.com/deckhouse/deckhouse/blob//modules/031-ceph-csi/docs/internal/INTREE_MIGRATION.md`.
      summary: |
        StorageClass {{ $labels.storageclass }} is using a deprecated RBD provisioner.
      severity: "9"
      markupFormat: markdown
    - name: ModuleAtConflict
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse has detected conflicting sources for the {{ $labels.moduleName }} module.

        To resolve this issue, specify the correct source in the module configuration.
      summary: |
        Conflict detected for module {{ $labels.moduleName }}.
      severity: "4"
      markupFormat: markdown
    - name: ModuleConfigDeprecated
      sourceFile: modules/810-documentation/monitoring/prometheus-rules/deprecated-mc.yaml
      moduleUrl: 810-documentation
      module: documentation
      edition: ce
      description: |-
        The `deckhouse-web` module has been renamed to `documentation`, and a new `documentation` ModuleConfig is generated automatically.

        Steps to troubleshoot:

        1. Remove the deprecated `deckhouse-web` ModuleConfig from the CI deployment process.
        1. Delete it using the following command:

           ```bash
           kubectl delete mc deckhouse-web
           ```
      summary: |
        Deprecated deckhouse-web ModuleConfig detected.
      severity: "9"
      markupFormat: markdown
    - name: ModuleConfigObsoleteVersion
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse has detected that ModuleConfig `{{ $labels.name }}` is outdated.

        To resolve this issue, update ModuleConfig `{{ $labels.name }}` to the latest version.
      summary: |
        ModuleConfig {{ $labels.name }} is outdated.
      severity: "4"
      markupFormat: markdown
    - name: ModuleHasDeprecatedUpdatePolicy
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Deckhouse has detected that the module `{{ $labels.moduleName }}` is using the deprecated update policy `{{ $labels.updatePolicy }}`. The `v1alpha1` policy has a selector that no longer functions.

        To specify the update policy in the module configuration, run the following command:

        ```bash
        kubectl patch moduleconfig {{ $labels.moduleName }} --type='json' -p='[{"op": "add", "path": "/spec/updatePolicy", "value": "{{ $labels.updatePolicy }}"}]'
        ```

        After resolving all alerts related to the update policy `{{ $labels.updatePolicy }}`, clear the selector by running the following command:

        ```bash
        kubectl patch moduleupdatepolicies.v1alpha1.deckhouse.io {{ $labels.updatePolicy }} --type='json' -p='[{"op": "replace", "path": "/spec/moduleReleaseSelector/labelSelector/matchLabels", "value": {"": ""}}]'
        ```
      summary: |
        Module {{ $labels.moduleName }} is matched by the deprecated update policy {{ $labels.updatePolicy }}.
      severity: "4"
      markupFormat: markdown
    - name: ModuleReleaseIsBlockedByRequirements
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new release for module `{{ $labels.moduleName }}` has been blocked because it doesn't meet the required conditions.

        To check the requirements, run the following command:

        ```bash
        kubectl  get mr {{ $labels.name }} -o json | jq .spec.requirements
        ```
      summary: |
        A new release for module {{ $labels.moduleName }} has been blocked due to unmet requirements.
      severity: "6"
      markupFormat: markdown
    - name: ModuleReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new release for module `{{ $labels.moduleName }}` is available but requires manual approval before it can be applied.

        To approve the module release, run the following command:

        ```bash
        kubectl annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"
        ```
      summary: |
        A new release for module {{ $labels.moduleName }} is awaiting manual approval.
      severity: "6"
      markupFormat: markdown
    - name: NATInstanceWithDeprecatedAvailabilityZone
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/deprecated_availability_zone.yaml
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: |
        The NAT instance `{{ $labels.name }}` is located in the availability zone `ru-central1-c`, which has been deprecated by Yandex Cloud. To resolve this issue, migrate the NAT instance to either `ru-central1-a` or `ru-central1-b` by following the instructions below.

        > The migration process involves irreversible changes and may result in a significant downtime. The duration typically depends on Yandex Cloud’s response time and can last several tens of minutes.

        1. Migrate the NAT instance. To get `providerClusterConfiguration.withNATInstance`, run the following command:

           ```bash
           kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.cloudProviderYandex.internal.providerClusterConfiguration.withNATInstance'
           ```

           - If you specified `withNATInstance.natInstanceInternalAddress` and/or `withNATInstance.internalSubnetID` in `providerClusterConfiguration`, remove them using the following command:

             ```bash
             kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller edit provider-cluster-configuration
             ```

           - If you specified `withNATInstance.externalSubnetID` and/or `withNATInstance.natInstanceExternalAddress` in `providerClusterConfiguration`, change them to the appropriate values.

             To get the address and subnet ID, use the Yandex Cloud Console or CLI.

             To change `withNATInstance.externalSubnetID` and `withNATInstance.natInstanceExternalAddress`, run the following command:

             ```bash
             kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller edit provider-cluster-configuration
             ```

        1. Run the appropriate edition and version of the Deckhouse installer container on the **local** machine. You may have to change the container registry address to do that. After that, perform the converge.

           - Get the appropriate edition and version of Deckhouse:

             ```bash
             DH_VERSION=$(kubectl -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}')
             DH_EDITION=$(kubectl -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]')
             echo "DH_VERSION=$DH_VERSION DH_EDITION=$DH_EDITION"
             ```

           - Run the installer:

             ```bash
             docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
             ```

           - Perform the converge:

             ```bash
             dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
             ```

        1. Update the route table.

           - Get the route table name:

             ```bash
             kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.global.clusterConfiguration.cloud.prefix'
             ```

           - Get the NAT instance name:

             ```bash
             kubectl -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.cloudProviderYandex.internal.providerDiscoveryData.natInstanceName'
             ```

           - Get the NAT instance internal IP address:

             ```bash
             yc compute instance list | grep -e "INTERNAL IP" -e <NAT_INSTANCE_NAME_FROM_PREVIOUS_STEP>
             ```

           - Update the route:

             ```bash
             yc vpc route-table update --name <ROUTE_TABLE_NAME_FROM_PREVIOUS_STEP> --route "destination=0.0.0.0/0,next-hop=<NAT_INSTANCE_INTERNAL_IP_FROM_PREVIOUS_STEP>"
             ```
      summary: |
        NAT instance {{ $labels.name }} is in a deprecated availability zone.
      severity: "9"
      markupFormat: markdown
    - name: NginxIngressConfigTestFailed
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The configuration test (`nginx -t`) for the `{{ $labels.controller }}` Ingress controller in the `{{ $labels.controller_namespace }}` namespace has failed.

        Steps to resolve:

        1. Check the controller logs:

           ```bash
           kubectl -n {{ $labels.controller_namespace }} logs {{ $labels.controller_pod }} -c controller
           ```

        2. Find the most recently created Ingress in the cluster:

           ```bash
           kubectl get ingress --all-namespaces --sort-by="metadata.creationTimestamp"
           ```

        3. Check for errors in the `configuration-snippet` or `server-snippet` annotations.
      summary: |
        Configuration test failed on NGINX Ingress {{ $labels.controller_namespace }}/{{ $labels.controller }}.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressDaemonSetNotUpToDate
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected {{ .Value }} outdated pods in NGINX Ingress DaemonSet `{{ $labels.namespace }}/{{ $labels.daemonset }}` over the last 20 minutes.

        Steps to resolve:

        1. Check the DaemonSet's status:

           ```bash
           kubectl -n {{ $labels.namespace }} get ads {{ $labels.daemonset }}
           ```

        2. Analyze the DaemonSet's description:

           ```bash
           kubectl -n {{ $labels.namespace }} describe ads {{ $labels.daemonset }}
           ```

        3. If the parameter `Number of Nodes Scheduled with Up-to-date Pods` does not match
        `Current Number of Nodes Scheduled`, check the 'nodeSelector' and 'toleration' settings of the corresponding NGINX Ingress Controller and compare them to the 'labels' and 'taints' settings of the relevant nodes.
      summary: |
        There were {{ .Value }} outdated pods in NGINX Ingress DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} over the last 20 minutes.
      severity: "9"
      markupFormat: markdown
    - name: NginxIngressDaemonSetReplicasUnavailable
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas remaining in NGINX Ingress DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        No available replicas remaining in NGINX Ingress DaemonSet {{$labels.namespace}}/{{$labels.daemonset}}.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressDaemonSetReplicasUnavailable
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected that some replicas of NGINX Ingress DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}` are unavailable.

        Current number: {{ .Value }} unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        kubectl -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Some replicas of NGINX Ingress DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} are unavailable.
      severity: "6"
      markupFormat: markdown
    - name: NginxIngressPodIsRestartingTooOften
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        {{ $value }} NGINX Ingress controller restarts detected in the last hour.

        Excessive NGINX Ingress restarts indicate that something is wrong. Normally, it should be up and running all the time.
      summary: |
        Too many NGINX Ingress restarts detected.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressProtobufExporterHasErrors
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected that the Ingress NGINX sidecar container with `protobuf_exporter` has {{ $labels.type }} errors.

        To resolve the issue, check the Ingress controller's logs:

        ```bash
        kubectl -n d8-ingress-nginx logs $(kubectl -n d8-ingress-nginx get pods -l app=controller,name={{ $labels.controller }} -o wide | grep {{ $labels.node }} | awk '{print $1}') -c protobuf-exporter
        ```
      summary: |
        The Ingress NGINX sidecar container with protobuf_exporter has {{ $labels.type }} errors.
      severity: "8"
      markupFormat: markdown
    - name: NginxIngressSslExpired
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The SSL certificate for {{ $labels.host }} in the `{{ $labels.namespace }}` namespace has expired.

        To verify the certificate, run the following command:

        ```bash
        kubectl -n {{ $labels.namespace }} get secret {{ $labels.secret_name }} -o json | jq -r '.data."tls.crt" | @base64d' | openssl x509 -noout -alias -subject -issuer -dates
        ```

        The site at `https://{{ $labels.host }}` is not accessible.
      summary: |
        Certificate has expired.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressSslWillExpire
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The SSL certificate for {{ $labels.host }} in the `{{ $labels.namespace }}` namespace will expire in less than two weeks.

        To verify the certificate, run the following command:

        ```bash
        kubectl -n {{ $labels.namespace }} get secret {{ $labels.secret_name }} -o json | jq -r '.data."tls.crt" | @base64d' | openssl x509 -noout -alias -subject -issuer -dates
        ```
      summary: |
        Certificate is expiring soon.
      severity: "5"
      markupFormat: markdown
    - name: NodeConntrackTableFull
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        As a result, no new connections created or accepted on the node. Keeping the `conntrack` table at full capacity may lead to erratic software behavior.

        To identify the source of excessive `conntrack` entries, use Okmeter or Grafana dashboards.
      summary: |
        The conntrack table on node {{ $labels.node }} is full.
      severity: "3"
      markupFormat: markdown
    - name: NodeConntrackTableFull
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The `conntrack` table on node `{{ $labels.node }}` is currently at {{ $value }}% of its maximum capacity.

        This is acceptable as long as the `conntrack` table remains 70-80% full. However, if it reaches full capacity, new connections may fail, causing network disruptions and erratic software behavior.

        To identify the source of excessive `conntrack` entries, use Okmeter or Grafana dashboards.
      summary: |
        The conntrack table on node {{ $labels.node }} is approaching the size limit.
      severity: "4"
      markupFormat: markdown
    - name: NodeDiskBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-bytes-critical\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%

        Steps to resolve:

        1. Retrieve disk usage information on the node:

           ```bash
           ncdu -x {{$labels.mountpoint}}
           ```

        1. If the output shows high disk usage in the `/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/` directory, identify the pods with the highest usage:

           ```bash
           crictl stats -o json | jq '.stats[] | select((.writableLayer.usedBytes.value | tonumber) > 1073741824) | { meta: .attributes.labels, diskUsage: ((.writableLayer.usedBytes.value | tonumber) / 1073741824 * 100 | round / 100 | tostring + " GiB")}'
           ```
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-bytes-critical&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: NodeDiskBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-bytes-warning\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%

        Steps to resolve:

        1. Retrieve disk usage information on the node:

           ```bash
           ncdu -x {{$labels.mountpoint}}
           ```

        1. If the output shows high disk usage in the `/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/` directory, identify the pods with the highest usage:

           ```bash
           crictl stats -o json | jq '.stats[] | select((.writableLayer.usedBytes.value | tonumber) > 1073741824) | { meta: .attributes.labels, diskUsage: ((.writableLayer.usedBytes.value | tonumber) / 1073741824 * 100 | round / 100 | tostring + " GiB")}'
           ```
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-bytes-warning&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "6"
      markupFormat: markdown
    - name: NodeDiskInodesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-inodes-critical\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-inodes-critical&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: NodeDiskInodesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-inodes-warning\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-inodes-warning&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "6"
      markupFormat: markdown
    - name: NodeExporterDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus has been unable to scrape a `node-exporter` for more than 10 minutes, or `node-exporters` have disappeared from service discovery.
      summary: |
        Prometheus couldn't scrape a node-exporter.
      severity: "3"
      markupFormat: default
    - name: NodeFilesystemIsRO
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-ro-fs.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The file system on the node has been switched to read-only mode.

        To investigate the cause, check the node logs.
      summary: |
        Node file system is read-only.
      severity: "4"
      markupFormat: default
    - name: NodeGroupHasStaticInternalNetworkCIDRsField
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group-deprecate.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Internal network CIDRs setting now located in the static cluster configuration.
        Delete this field from NodeGroup {{ $labels.name }} to fix this alert.
        Do not worry, it has been already migrated to another place.
      summary: |
        NodeGroup {{ $labels.name }} has deprecated filed spec.static.internalNetworkCIDRs
      severity: "9"
      markupFormat: markdown
    - name: NodeGroupMasterTaintIsAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        `master` node group has no `node-role.kubernetes.io/control-plane` taint. Probably control-plane nodes are misconfigured
        and are able to run not only control-plane Pods. Please, add:
        ```yaml
          nodeTemplate:
            taints:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
        ```
        to the `master` node group spec.
        `key: node-role.kubernetes.io/master` taint was deprecated and will have no effect in Kubernetes 1.24+.
      summary: |
        The 'master' node group does not contain desired taint.
      severity: "4"
      markupFormat: markdown
    - name: NodeGroupNodeWithDeprecatedAvailabilityZone
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/deprecated_availability_zone.yaml
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: |
        Certain nodes in the node group `{{ $labels.node_group }}` are located in the availability zone `ru-central1-c`, which has been deprecated by Yandex Cloud.

        Steps to troubleshoot:

        1. Identify the nodes that need to be migrated by running the following command:

           ```bash
           kubectl get node -l "topology.kubernetes.io/zone=ru-central1-c"
           ```

        2. Migrate your nodes, disks, and load balancers to one of the supported zones: `ru-central1-a`, `ru-central1-b`, or `ru-central1-d`. Refer to the [Yandex migration guide](https://cloud.yandex.com/en/docs/overview/concepts/zone-migration) for detailed instructions.

           > You can't migrate public IP addresses between zones. For details, refer to the migration guide.
      summary: |
        NodeGroup {{ $labels.node_group }} contains nodes in a deprecated availability zone.
      severity: "9"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Probably, machine-controller-manager is unable to create a machine using the cloud provider module. Possible causes:
          1. Cloud provider limits on available resources;
          2. No access to the cloud provider API;
          3. Cloud provider or instance class misconfiguration;
          4. Problems with bootstrapping the Machine.

        The recommended course of action:
          1. Run `kubectl get ng {{ $labels.node_group }} -o yaml`. In the `.status.lastMachineFailures` field you can find all errors related to the creation of Machines;
          2. The absence of Machines in the list that have been in Pending status for more than a couple of minutes means that Machines are continuously being created and deleted because of some error:
          `kubectl -n d8-cloud-instance-manager get machine`;
          3. Refer to the Machine description if the logs do not include error messages and the Machine continues to be Pending:
          `kubectl -n d8-cloud-instance-manager get machine <MACHINE_NAME> -o json | jq .status.bootstrapStatus`;
          4. The output similar to the one below means that you have to use nc to examine the bootstrap logs:
             ```json
             {
               "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
               "tcpEndpoint": "192.168.199.158"
             }
             ```
          5. The absence of information about the endpoint for getting logs means that `cloudInit` is not working correctly. This may be due to the incorrect configuration of the instance class for the cloud provider.
      summary: |
        There are no available instances in the {{ $labels.node_group }} node group.
      severity: "7"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Possibly, autoscaler has provisioned too many Nodes. Take a look at the state of the Machine in the cluster.
        Probably, machine-controller-manager is unable to create a machine using the cloud provider module. Possible causes:
          1. Cloud provider limits on available resources;
          2. No access to the cloud provider API;
          3. Cloud provider or instance class misconfiguration;
          4. Problems with bootstrapping the Machine.

        The recommended course of action:
          1. Run `kubectl get ng {{ $labels.node_group }} -o yaml`. In the `.status.lastMachineFailures` field you can find all errors related to the creation of Machines;
          2. The absence of Machines in the list that have been in Pending status for more than a couple of minutes means that Machines are continuously being created and deleted because of some error:
          `kubectl -n d8-cloud-instance-manager get machine`;
          3. Refer to the Machine description if the logs do not include error messages and the Machine continues to be Pending:
          `kubectl -n d8-cloud-instance-manager get machine <MACHINE_NAME> -o json | jq .status.bootstrapStatus`;
          4. The output similar to the one below means that you have to use nc to examine the bootstrap logs:
             ```json
             {
               "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
               "tcpEndpoint": "192.168.199.158"
             }
             ```
          5. The absence of information about the endpoint for getting logs means that `cloudInit` is not working correctly. This may be due to the incorrect configuration of the instance class for the cloud provider.
      summary: |
        The number of simultaneously unavailable instances in the {{ $labels.node_group }} node group exceeds the allowed value.
      severity: "8"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of unavailable instances is {{ $value }}. See the relevant alerts for more information.
        Probably, machine-controller-manager is unable to create a machine using the cloud provider module. Possible causes:
          1. Cloud provider limits on available resources;
          2. No access to the cloud provider API;
          3. Cloud provider or instance class misconfiguration;
          4. Problems with bootstrapping the Machine.

        The recommended course of action:
          1. Run `kubectl get ng {{ $labels.node_group }} -o yaml`. In the `.status.lastMachineFailures` field you can find all errors related to the creation of Machines;
          2. The absence of Machines in the list that have been in Pending status for more than a couple of minutes means that Machines are continuously being created and deleted because of some error:
          `kubectl -n d8-cloud-instance-manager get machine`;
          3. Refer to the Machine description if the logs do not include error messages and the Machine continues to be Pending:
          `kubectl -n d8-cloud-instance-manager get machine <MACHINE_NAME> -o json | jq .status.bootstrapStatus`;
          4. The output similar to the one below means that you have to use nc to examine the bootstrap logs:
             ```json
             {
               "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
               "tcpEndpoint": "192.168.199.158"
             }
             ```
          5. The absence of information about the endpoint for getting logs means that `cloudInit` is not working correctly. This may be due to the incorrect configuration of the instance class for the cloud provider.
      summary: |
        There are unavailable instances in the {{ $labels.node_group }} node group.
      severity: "8"
      markupFormat: markdown
    - name: NodePingPacketLoss
      sourceFile: modules/340-monitoring-ping/monitoring/prometheus-rules/node-ping.yaml
      moduleUrl: 340-monitoring-ping
      module: monitoring-ping
      edition: ce
      description: ICMP packet loss to node `{{$labels.destination_node}}` has exceeded 5%.
      summary: |
        Ping loss exceeds 5%.
      severity: "4"
      markupFormat: default
    - name: NodeRequiresDisruptionApprovalForUpdate
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for Nodes and the {{ $labels.node }} Node of the {{ $labels.node_group }} group has learned about the update, requested and received approval, started the update, and ran into a step that causes possible downtime.

        You have to manually approve the disruption since the `Manual` mode is active in the group settings (`disruptions.approvalMode`).

        Grant approval to the Node using the `update.node.deckhouse.io/disruption-approved=` annotation if it is ready for unsafe updates (e.g., drained).

        **Caution!!!** The Node will not be drained automatically since the manual mode is enabled (`disruptions.approvalMode: Manual`).

        **Caution!!!** No need to drain the master node.

        * Use the following commands to drain the Node and grant it update approval:
          ```shell
          kubectl drain {{ $labels.node }} --delete-local-data=true --ignore-daemonsets=true --force=true &&
            kubectl annotate node {{ $labels.node }} update.node.deckhouse.io/disruption-approved=
          ```
        * Note that you need to **uncordon the node** after the update is complete (i.e., after removing the `update.node.deckhouse.io/approved` annotation).
          ```
          while kubectl get node {{ $labels.node }} -o json | jq -e '.metadata.annotations | has("update.node.deckhouse.io/approved")' > /dev/null; do sleep 1; done
          kubectl uncordon {{ $labels.node }}
          ```

        Note that if there are several Nodes in a NodeGroup, you will need to repeat this operation for each Node since only one Node can be updated at a time. Perhaps it makes sense to temporarily enable the automatic disruption approval mode (`disruptions.approvalMode: Automatic`).
      summary: |
        The {{ $labels.node }} Node requires disruption approval to proceed with the update
      severity: "8"
      markupFormat: markdown
    - name: NodeStuckInDraining
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The {{ $labels.node }} Node of the {{ $labels.node_group }} NodeGroup stuck in draining.

        You can get more info by running: `kubectl -n default get event --field-selector involvedObject.name={{ $labels.node }},reason=DrainFailed --sort-by='.metadata.creationTimestamp'`

        The error message is: {{ $labels.message }}
      summary: |
        The {{ $labels.node }} Node is stuck in draining.
      severity: "6"
      markupFormat: markdown
    - name: NodeStuckInDrainingForDisruptionDuringUpdate
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update for the {{ $labels.node }} Node of the {{ $labels.node_group }} NodeGroup. The Node has learned about the update, requested and received approval, started the update, ran into a step that causes possible downtime, and stuck in draining in order to get disruption approval automatically.

        You can get more info by running: `kubectl -n default get event --field-selector involvedObject.name={{ $labels.node }},reason=ScaleDown --sort-by='.metadata.creationTimestamp'`
      summary: |
        The {{ $labels.node }} Node is stuck in draining.
      severity: "6"
      markupFormat: markdown
    - name: NodeSUnreclaimBytesUsageHigh
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The node `{{ $labels.node }}` has a potential kernel memory leak. One known issue could be causing this problem.

        Steps to troubleshoot:

        1. Check the `cgroupDriver` setting on node `{{ $labels.node }}`:

           ```bash
           cat /var/lib/kubelet/config.yaml | grep 'cgroupDriver: systemd'
           ```

        1. If `cgroupDriver` is set to `systemd`, a reboot is required to switch back to the `cgroupfs` driver.
           In this case, drain and reboot the node.

        For further details, refer to the [issue](https://github.com/deckhouse/deckhouse/issues/2152).
      summary: |
        Node {{ $labels.node }} has high kernel memory usage.
      severity: "4"
      markupFormat: markdown
    - name: NodeSystemExporterDoesNotExistsForNode
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        To resolve the issue, follow these steps:

        1. Find the `node-exporter` Pod running on the affected node:

           ```bash
           kubectl -n d8-monitoring get pod -l app=node-exporter -o json | jq -r ".items[] | select(.spec.nodeName==\"{{$labels.node}}\") | .metadata.name"
           ```

        2. Describe the `node-exporter` Pod:

           ```bash
           kubectl -n d8-monitoring describe pod <POD_NAME>
           ```

        3. Verify that the kubelet is running on node `{{ $labels.node }}`.
      summary: |
        Some system exporters aren't functioning correctly on node {{ $labels.node }}.
      severity: "4"
      markupFormat: markdown
    - name: NodeTCPMemoryExhaust
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The TCP stack on node `{{ $labels.node }}` is experiencing high memory pressure.
        This may be caused by applications with intensive TCP networking usage.

        Steps to troubleshoot:

        1. Identify applications consuming excessive TCP memory.
        1. Adjust TCP memory configuration if needed.
        1. Investigate network traffic sources.
      summary: |
        Node {{ $labels.node }} has high TCP stack memory usage.
      severity: "6"
      markupFormat: markdown
    - name: NodeTimeOutOfSync
      sourceFile: modules/470-chrony/monitoring/prometheus-rules/chrony.yaml
      moduleUrl: 470-chrony
      module: chrony
      edition: ce
      description: |
        Time on the node `{{$labels.node}}` is out of sync and drifts apart from the NTP server clock by {{ $value }} seconds.

        To resolve the time synchronization issues:

        - Fix network errors:
          - Ensure the upstream time synchronization servers defined in the [chrony configuration](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/chrony/configuration.html) are available.
          - Eliminate large packet loss and excessive latency to upstream time synchronization servers.
        - Modify the NTP servers list defined in the [chrony configuration](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/chrony/configuration.html).
      summary: |
        Clock on the node {{$labels.node}} is drifting.
      severity: "5"
      markupFormat: markdown
    - name: NodeUnschedulable
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The node `{{ $labels.node }}` is in a cordon-protected state, meaning no new pods can be scheduled onto it.

        Someone may have executed one of the following commands on this node:

        - To cordon the node:

          ```bash
          kubectl cordon {{ $labels.node }}
          ```

        - To drain the node (if draining has been running for more than 20 minutes):

          ```bash
          kubectl drain {{ $labels.node }}
          ```

        The was likely caused by the scheduled maintenance of that node.
      summary: |
        Node {{ $labels.node }} is cordon-protected, preventing new pods from being scheduled.
      severity: "8"
      markupFormat: markdown
    - name: NTPDaemonOnNodeDoesNotSynchronizeTime
      sourceFile: modules/470-chrony/monitoring/prometheus-rules/chrony.yaml
      moduleUrl: 470-chrony
      module: chrony
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Check if the chrony pod is running on the node by executing the following command:

           ```bash
           kubectl -n d8-chrony --field-selector spec.nodeName="{{$labels.node}}" get pods
           ```

        2. Verify the chrony daemon's status by executing the following command:

           ```bash
           kubectl -n d8-chrony exec <POD_NAME> -- /opt/chrony-static/bin/chronyc sources
           ```

        3. Resolve the time synchronization issues:
           - Fix network errors:
             - Ensure the upstream time synchronization servers defined in the [chrony configuration](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/chrony/configuration.html) are available.
             - Eliminate large packet loss and excessive latency to upstream time synchronization servers.
           - Modify the NTP servers list defined in the [chrony configuration](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/chrony/configuration.html).
      summary: |
        NTP daemon on the node {{$labels.node}} haven't synchronized time for too long.
      severity: "5"
      markupFormat: markdown
    - name: OperationPolicyViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured operation policies for the cluster, and one or more existing objects are violating these policies.

        To identify violating objects:

        - Run the following Prometheus query:

          ```promql
          count by (violating_namespace, violating_kind, violating_name, violation_msg) (
            d8_gatekeeper_exporter_constraint_violations{
              violation_enforcement="deny",
              source_type="OperationPolicy"
            }
          )
          ```

        - Alternatively, check the admission-policy-engine Grafana dashboard.
      summary: |
        At least one object violates the configured cluster operation policies.
      severity: "7"
      markupFormat: markdown
    - name: PersistentVolumeClaimBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-bytes-critical\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.

        Current volume storage usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-bytes-critical&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.
      severity: "4"
      markupFormat: markdown
    - name: PersistentVolumeClaimBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-bytes-warning\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.

        Currently volume storage usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-bytes-warning&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: PersistentVolumeClaimInodesUsed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-inodes-critical\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.

        Current volume inode usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-inodes-critical&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.
      severity: "4"
      markupFormat: markdown
    - name: PersistentVolumeClaimInodesUsed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-inodes-warning\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.

        Current volume inode usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-inodes-warning&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.
      severity: "5"
      markupFormat: markdown
    - name: PodSecurityStandardsViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/), and one or more running pods are violating these standards.

        To identify violating pods:

        - Run the following Prometheus query:

          ```promql
          count by (violating_namespace, violating_name, violation_msg) (
            d8_gatekeeper_exporter_constraint_violations{
              violation_enforcement="deny",
              violating_namespace=~".*",
              violating_kind="Pod",
              source_type="PSS"
            }
          )
          ```

        - Alternatively, check the admission-policy-engine Grafana dashboard.
      summary: |
        At least one pod violates the configured cluster pod security standards.
      severity: "7"
      markupFormat: markdown
    - name: PodStatusIsIncorrect
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/pod-status.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The Pod `{{ $labels.namespace }}/{{ $labels.pod }}` running on node `{{ $labels.node }}` is listed as `NotReady` while all the Pod's containers are `Ready`.

        This could have been caused by the [known Kubernetes bug](https://github.com/kubernetes/kubernetes/issues/80968).

        Steps to troubleshoot:

        1. Identify all pods with this state:

           ```bash
           kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | "\(.spec.nodeName)/\(.metadata.namespace)/\(.metadata.name)"'
           ```

        2. Identify all affected nodes:

           ```bash
           kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | .spec.nodeName' -r | sort | uniq -c
           ```

        3. Restart kubelet on each node:

           ```bash
           systemctl restart kubelet
           ```
      summary: |
        Incorrect state of Pod {{ $labels.namespace }}/{{ $labels.pod }} running on node {{ $labels.node }}.
      severity: undefined
      markupFormat: markdown
    - name: PrometheusDiskUsage
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        For more information, use the command:
        ```shell
        kubectl -n {{ $labels.namespace }} exec -ti {{ $labels.pod_name }} -c prometheus -- df -PBG /prometheus
        ```
        Consider increasing it https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/prometheus/faq.html#how-to-expand-disk-size
      summary: |
        Prometheus disk is over 95% used.
      severity: "4"
      markupFormat: markdown
    - name: PrometheusLongtermRotatingEarlierThanConfiguredRetentionDays
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        You need to increase the disk size, reduce the number of metrics or decrease `longtermRetentionDays` module parameter.
      summary: |
        Prometheus-longterm data is being rotated earlier than configured retention days
      severity: "4"
      markupFormat: markdown
    - name: PrometheusMainRotatingEarlierThanConfiguredRetentionDays
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        You need to increase the disk size, reduce the number of metrics or decrease `retentionDays` module parameter.
      summary: |
        Prometheus-main data is being rotated earlier than configured retention days
      severity: "4"
      markupFormat: markdown
    - name: PrometheusScapeConfigDeclarationDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        Old way for describing additional scrape config via secrets will be deprecated in prometheus-operator > v0.65.1. Please use CRD ScrapeConfig instead.
        ```https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/proposals/202212-scrape-config.md```
      summary: |
        AdditionalScrapeConfigs from secrets will be deprecated in soon
      severity: "8"
      markupFormat: markdown
    - name: PrometheusServiceMonitorDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Kubernetes cluster uses a more advanced network mechanism - EndpointSlice
        You service monitor `{{ $labels.namespace }}/{{ $labels.name }}` has relabeling with old Endpoint mechanism, starts with `__meta_kubernetes_endpoints_`.
        This relabeling rule support, based on the `_endpoint_` label, will be remove in the future (Deckhouse release 1.60).
        Please, migrate to EndpointSlice relabeling rules. To do this, you have modify ServiceMonitor with changing the following labels:
        ```shell
        __meta_kubernetes_endpoints_name -> __meta_kubernetes_endpointslice_name
        __meta_kubernetes_endpoints_label_XXX -> __meta_kubernetes_endpointslice_label_XXX
        __meta_kubernetes_endpoints_labelpresent_XXX -> __meta_kubernetes_endpointslice_labelpresent_XXX
        __meta_kubernetes_endpoints_annotation_XXX -> __meta_kubernetes_endpointslice_annotation_XXX
        __meta_kubernetes_endpoints_annotationpresent_XXX -> __meta_kubernetes_endpointslice_annotationpresent_XXX
        __meta_kubernetes_endpoint_node_name -> __meta_kubernetes_endpointslice_endpoint_topology_kubernetes_io_hostname
        __meta_kubernetes_endpoint_ready -> __meta_kubernetes_endpointslice_endpoint_conditions_ready
        __meta_kubernetes_endpoint_port_name -> __meta_kubernetes_endpointslice_port_name
        __meta_kubernetes_endpoint_port_protocol -> __meta_kubernetes_endpointslice_port_protocol
        __meta_kubernetes_endpoint_address_target_kind -> __meta_kubernetes_endpointslice_address_target_kind
        __meta_kubernetes_endpoint_address_target_name -> __meta_kubernetes_endpointslice_address_target_name
        ```
      summary: |
        Deprecated Prometheus ServiceMonitor has found.
      severity: "8"
      markupFormat: markdown
    - name: SecurityPolicyViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured security policies for the cluster, and one or more existing objects are violating these policies.

        To identify violating objects:

        - Run the following Prometheus query:

          ```promql
          count by (violating_namespace, violating_kind, violating_name, violation_msg) (
            d8_gatekeeper_exporter_constraint_violations{
              violation_enforcement="deny",
              source_type="SecurityPolicy"
            }
          )
          ```

        - Alternatively, check the admission-policy-engine Grafana dashboard.
      summary: |
        At least one object violates the configured cluster security policies.
      severity: "7"
      markupFormat: markdown
    - name: StandbyHolderDeploymentReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/standby-deployment-unavailable.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: Deckhouse has detected that there are no available replicas remaining in Deployment `standby-holder-{{$labels.node_group_name}}` in namespace `d8-cloud-instance-manager`.
      summary: |
        No available replicas remaining in Deployment standby-holder-{{$labels.node_group_name}} in namespace d8-cloud-instance-manager.
      severity: "5"
      markupFormat: markdown
    - name: StatefulSetAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        kubectl -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StorageClassCloudManual
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/storage-class.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        A StorageClass using a `cloud-provider` provisioner shouldn't be deployed manually.
        Such StorageClasses are managed by the `cloud-provider` module.

        Instead of the manual deployment, modify the `cloud-provider` module configuration as needed.
      summary: |
        Manually deployed StorageClass {{ $labels.name }} found in the cluster.
      severity: "6"
      markupFormat: markdown
    - name: StorageClassDefaultDuplicate
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/storage-class.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Deckhouse has detected that more than one StorageClass in the cluster is annotated as default.

        This may have been caused by a manually deployed StorageClass that is overlapping with the default storage configuration provided by the `cloud-provider` module.
      summary: |
        Multiple default StorageClasses found in the cluster.
      severity: "6"
      markupFormat: markdown
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: '{{ $labels.job }} target is down.'
      summary: |
        Target is down
      severity: "5"
      markupFormat: default
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: '{{ $labels.job }} target is down.'
      summary: |
        Target is down
      severity: "6"
      markupFormat: default
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: '{{ $labels.job }} target is down.'
      summary: |
        Target is down
      severity: "7"
      markupFormat: default
    - name: TargetSampleLimitExceeded
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Target are down because of a sample limit exceeded.
      summary: |
        Scrapes are exceeding sample limit
      severity: "6"
      markupFormat: markdown
    - name: TargetSampleLimitExceeded
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The target is close to exceeding the sampling limit. less than 10% left to the limit
      summary: |
        The sampling limit is close.
      severity: "7"
      markupFormat: markdown
    - name: UnsupportedContainerRuntimeVersion
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/cri-version.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Deckhouse has detected that the node {{$labels.node}} is running an unsupported version of CRI {{$labels.container_runtime_version}}.

        Supported CRI versions for Kubernetes {{$labels.kubelet_version}}:

        * Containerd 1.4.
        * Containerd 1.5.
        * Containerd 1.6.
        * Containerd 1.7.
      summary: |
        Unsupported version of CRI {{$labels.container_runtime_version}} installed for Kubernetes version {{$labels.kubelet_version}}.
      severity: undefined
      markupFormat: markdown
modules-having-alerts:
    - admission-policy-engine
    - cert-manager
    - chrony
    - cloud-provider-yandex
    - cni-cilium
    - cni-flannel
    - cni-simple-bridge
    - control-plane-manager
    - documentation
    - extended-monitoring
    - ingress-nginx
    - istio
    - kube-dns
    - log-shipper
    - loki
    - metallb
    - monitoring-applications
    - monitoring-custom
    - monitoring-deckhouse
    - monitoring-kubernetes
    - monitoring-kubernetes-control-plane
    - monitoring-ping
    - node-manager
    - okmeter
    - operator-prometheus
    - prometheus
    - runtime-audit-engine
    - secret-copier
    - snapshot-controller
    - terraform-manager
    - upmeter
    - user-authn
