alerts:
    - name: CapsInstanceUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/caps-nodes.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The MachineDeployment `{{ $labels.machine_deployment_name }}` has **{{ $value }}** unavailable instances.

        Check the status of the instances in the cluster with the following command:

        ```shell
        d8 k get instance -l node.deckhouse.io/group={{ $labels.machine_deployment_name }}
        ```
      summary: |
        There are unavailable instances in the {{ $labels.machine_deployment_name }} MachineDeployment.
      severity: "8"
      markupFormat: markdown
    - name: CertificateSecretExpired
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificate.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        A certificate in Secret `{{$labels.secret_namespace}}/{{$labels.secret_name}}` has expired.

        Ways to resolve:

        - If the certificate is managed manually, upload a new certificate.
        - If the certificate is managed by the `cert-manager` module, inspect the certificate resource:
          1. Retrieve the certificate name from the Secret:

             ```bash
             cert=$(d8 k get secret -n {{$labels.secret_namespace}} {{$labels.secret_name}} -o 'jsonpath={.metadata.annotations.cert-manager\.io/certificate-name}')
             ```

          2. Check the certificate status and investigate why it hasn't been updated:

             ```bash
             d8 k describe cert -m {{$labels.secret_namespace}} "$cert"
             ```
      summary: |
        Certificate has expired.
      severity: "8"
      markupFormat: markdown
    - name: CertificateSecretExpiredSoon
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificate.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        A certificate in Secret `{{$labels.secret_namespace}}/{{$labels.secret_name}}` will expire in less than two weeks.

        Ways to resolve:

        - If the certificate is managed manually, upload a new certificate.
        - If the certificate is managed by the `cert-manager` module, inspect the certificate resource:
          1. Retrieve the certificate name from the Secret:

             ```bash
             cert=$(d8 k get secret -n {{$labels.secret_namespace}} {{$labels.secret_name}} -o 'jsonpath={.metadata.annotations.cert-manager\.io/certificate-name}')
             ```

          2. Check the certificate status and investigate why it hasn't been updated:

             ```bash
             d8 k describe cert -n {{$labels.secret_namespace}} "$cert"
             ```
      summary: |
        Certificate is expiring soon.
      severity: "8"
      markupFormat: markdown
    - name: CertmanagerCertificateExpired
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        Certificate is not provided.

        To check the certificate details, run the following command:

        ```shell
        d8 k -n {{$labels.exported_namespace}} describe certificate {{$labels.name}}
        ```
      summary: |
        Certificate {{$labels.exported_namespace}}/{{$labels.name}} is not provided.
      severity: "4"
      markupFormat: default
    - name: CertmanagerCertificateExpiredSoon
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        The certificate `{{$labels.exported_namespace}}/{{$labels.name}}` will expire in less than two weeks.

        To check the certificate details, run the following command:

        ```bash
        d8 k -n <NAMESPACE> describe certificate <CERTIFICATE-NAME>
        ```
      summary: |
        Certificate will expire soon.
      severity: "4"
      markupFormat: default
    - name: CertmanagerCertificateOrderErrors
      sourceFile: modules/101-cert-manager/monitoring/prometheus-rules/certificate.yaml
      moduleUrl: 101-cert-manager
      module: cert-manager
      edition: ce
      description: |
        Cert-manager received responses with the status code `{{ $labels.status }}` when requesting `{{ $labels.scheme }}://{{ $labels.host }}{{ $labels.path }}`.

        This can affect certificate ordering and prolongation in the future. For details, check the cert-manager logs using the following command:

        ```bash
        d8 k -n d8-cert-manager logs -l app=cert-manager -c cert-manager
        ```
      summary: |
        Cert-manager couldn't order a certificate.
      severity: "5"
      markupFormat: default
    - name: CiliumAgentEndpointsNotReady
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        For details, refer to the logs of the agent:

        ```bash
        d8 k -n {{ $labels.namespace }} logs {{ $labels.pod }}
        ```
      summary: |
        Over 50% of all known endpoints aren't ready in agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentMapPressureCritical
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        The eBPF map resource utilization limit has almost been reached.

        Check with the vendor for potential remediation steps.
      summary: |
        eBPF map {{ $labels.map_name }} exceeds 90% utilization in agent {{ $labels.namespace }}/{{ $labels.pod }}.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentMetricNotFound
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Check the logs of the agent:

           ```bash
           d8 k -n {{ $labels.namespace }} logs {{ $labels.pod }}
           ```

        1. Verify the agent's health status:

           ```bash
           d8 k -n {{ $labels.namespace }} exec -ti {{ $labels.pod }} cilium-health status
           ```

        1. Compare the metrics with those of a neighboring agent.

        > Note that the absence of metrics can indirectly indicate that new pods can't be created on the node due to connectivity issues with the agent.
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} isn't sending some metrics.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentPolicyImportErrors
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        For details, refer to the logs of the agent:

        ```bash
        d8 k -n {{ $labels.namespace }} logs {{ $labels.pod }}
        ```
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} fails to import policies.
      severity: "4"
      markupFormat: markdown
    - name: CiliumAgentUnreachableHealthEndpoints
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/agent.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        For details, refer to the logs of the agent:

        ```bash
        d8 k -n {{ $labels.namespace }} logs {{ $labels.pod }}
        ```
      summary: |
        Agent {{ $labels.namespace }}/{{ $labels.pod }} can't reach some of the node health endpoints.
      severity: "4"
      markupFormat: markdown
    - name: ClusterHasOrphanedDisks
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The cloud data discoverer has found disks in the cloud that do not have a corresponding PersistentVolume in the cluster.

        You can safely delete these disks manually from your cloud provider:

        ID: {{ $labels.id }}, Name: {{ $labels.name }}
      summary: |
        Cloud data discoverer found orphaned disks in the cloud.
      severity: "6"
      markupFormat: markdown
    - name: CniCiliumNonStandardVXLANPortFound
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/configmap.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |
        The Cilium configuration specifies a non-standard VXLAN port `{{ $labels.current_port }}`. The recommended port number is `{{ $labels.recommended_port }}`.

        To resolve this issue, update the `tunnel-port` parameter in the `cilium-configmap` ConfigMap located in the `d8-cni-cilium` namespace to match the recommended port.

        If you configured the non-standard port on purpose, ignore this alert.
      summary: |
        Cilium configuration uses a non-standard VXLAN port.
      severity: "4"
      markupFormat: markdown
    - name: CniCiliumOrphanEgressGatewayPolicyFound
      sourceFile: ee/se-plus/modules/021-cni-cilium/monitoring/prometheus-rules/egressgatewaypolicies.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: se-plus
      description: |
        The cluster contains an orphaned EgressGatewayPolicy named `{{$labels.name}}` with an irrelevant EgressGateway name.

        To resolve this issue, verify the EgressGateway name specified in the EgressGatewayPolicy resource `{{$labels.egressgateway}}` and update it as needed.
      summary: |
        Orphaned EgressGatewayPolicy with an irrelevant EgressGateway name has been found.
      severity: "4"
      markupFormat: markdown
    - name: CPUStealHigh
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The CPU steal has been excessively high on node `{{ $labels.node }}` over the past 30 minutes.

        Another component, such as a neighboring virtual machine, may be consuming the node's resources. This may be the result of "overselling" the hypervisor, meaning the hypervisor is hosting more virtual machines than it can handle.
      summary: |
        CPU steal on node {{ $labels.node }} is too high.
      severity: "4"
      markupFormat: default
    - name: CronJobAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CronJobAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: CronJobBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: CronJobFailed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that Job `{{$labels.namespace}}/{{$labels.job_name}}` failed in CronJob `{{$labels.namespace}}/{{$labels.owner_name}}`.

        Steps to resolve:

        1. Print the job details:

           ```bash
           d8 k -n {{$labels.namespace}} describe job {{$labels.job_name}}
           ```

        1. Check the job status:

           ```bash
           d8 k -n {{$labels.namespace}} get job {{$labels.job_name}}
           ```

        1. Check the status of pods created by the job:

           ```bash
           d8 k -n {{$labels.namespace}} get pods -l job-name={{$labels.job_name}}
           ```
      summary: |
        Job {{$labels.namespace}}/{{$labels.job_name}} failed in CronJob {{$labels.namespace}}/{{$labels.owner_name}}.
      severity: "5"
      markupFormat: default
    - name: CronJobImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: CronJobPodsNotCreated
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the pods set in CronJob `{{$labels.namespace}}/{{$labels.owner_name}}` still haven't been created.

        Steps to resolve:

        1. Print the job details:

           ```bash
           d8 k -n {{$labels.namespace}} describe job {{$labels.job_name}}
           ```

        1. Check the job status:

           ```bash
           d8 k -n {{$labels.namespace}} get job {{$labels.job_name}}
           ```

        1. Check the status of pods created by the job:

           ```bash
           d8 k -n {{$labels.namespace}} get pods -l job-name={{$labels.job_name}}
           ```
      summary: |
        Pods set in CronJob {{$labels.namespace}}/{{$labels.job_name}} haven't been created.
      severity: "5"
      markupFormat: markdown
    - name: CronJobRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CronJobSchedulingError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/cronjob.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that CronJob `{{$labels.namespace}}/{{$labels.cronjob}}` failed to schedule on time.

        - Current schedule: `{{ printf "kube_cronjob_info{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | label "schedule" }}`
        - Last scheduled time: `{{ printf "kube_cronjob_status_last_schedule_time{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | value | humanizeTimestamp }}%`
        - Next projected schedule time: `{{ printf "kube_cronjob_next_schedule_time{namespace=\"%s\", cronjob=\"%s\"}" $labels.namespace $labels.cronjob | query | first | value | humanizeTimestamp }}%`
      summary: |
        CronJob {{$labels.namespace}}/{{$labels.cronjob}} failed to schedule on time.
      severity: "6"
      markupFormat: markdown
    - name: CronJobUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The CronJob `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: CustomPodMonitorFoundInCluster
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are PodMonitors in the Deckhouse namespace that were not created by Deckhouse.

        To resolve the issue, move these PodMonitors to the `user-spec` namespace by removing the `heritage: deckhouse` label.

        To list all PodMonitors in the Deckhouse namespace, run the following command:

        ```bash
        d8 k get podmonitors --all-namespaces -l heritage!=deckhouse
        ```

        For more information on metric collection, refer to the [Prometheus module FAQ](https://deckhouse.io/modules/prometheus/faq.html).
      summary: |
        Deckhouse namespace contains PodMonitors not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: CustomServiceMonitorFoundInD8Namespace
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are ServiceMonitors in the Deckhouse namespace that were not created by Deckhouse.

        To resolve the issue, move these ServiceMonitors to the `user-spec` namespace by removing the `heritage: deckhouse` label.

        To list all ServiceMonitors in the Deckhouse namespace, run the following command:

        ```bash
        d8 k get servicemonitors --all-namespaces -l heritage!=deckhouse
        ```

        For more information on metric collection, refer to the [Prometheus module FAQ](https://deckhouse.io/modules/prometheus/faq.html).
      summary: |
        Deckhouse namespace contains ServiceMonitors not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: D8AdmissionPolicyEngineNotBootstrapped
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/bootstrap.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        The admission-policy-engine module couldn't bootstrap.

        Steps to troubleshoot:

        1. Verify that the module's components are up and running:

           ```bash
           d8 k get pods -n d8-admission-policy-engine
           ```

        2. Check logs for issues, such as missing constraint templates or incomplete CRD creation:

           ```bash
           d8 k logs -n d8-system -lapp=deckhouse --tail=1000 | grep admission-policy-engine
           ```
      summary: |
        Admission-policy-engine module hasn't been bootstrapped for 10 minutes.
      severity: "7"
      markupFormat: markdown
    - name: D8BashibleApiserverLocked
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        `Bashible-apiserver` has been locked for an extended period.

        To resolve the issue, check if the `bashible-apiserver` Pods are up-to-date and running:

        ```shell
        d8 k -n d8-cloud-instance-manager get pods -l app=bashible-apiserver
        ```
      summary: |
        Bashible-apiserver has been locked for too long.
      severity: "6"
      markupFormat: markdown
    - name: D8CertExporterPodIsNotReady
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: "Steps to resolve:\n\n1. Retrieve the deployment details:\n   \n   ```bash\n   d8 k -n d8-monitoring describe deploy x509-certificate-exporter\n   ```\n\n2. Check the pod status and investigate why it's not ready:\n\n   ```bash\n   d8 k -n d8-monitoring describe pod -l app=x509-certificate-exporter\n   ```\n"
      summary: |
        The x509-certificate-exporter pod isn't ready.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterPodIsNotRunning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: "Steps to resolve:\n\n1. Retrieve the deployment details:\n   \n   ```bash\n   d8 k -n d8-monitoring describe deploy x509-certificate-exporter\n   ```\n\n2. Check the pod status and investigate why it's not running:\n\n   ```bash\n   d8 k -n d8-monitoring describe pod -l app=x509-certificate-exporter\n   ```\n"
      summary: |
        The x509-certificate-exporter pod isn't running.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterTargetAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Ways to resolve:

        - Check the pod status:

          ```bash
          d8 k -n d8-monitoring get pod -l app=x509-certificate-exporter
          ```

        - Check the pod logs:

          ```bash
          d8 k -n d8-monitoring logs -l app=x509-certificate-exporter -c x509-certificate-exporter
          ```
      summary: |
        There is no x509-certificate-exporter target in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8CertExporterTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/certificates/certificates-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Ways to resolve:

        - Check the pod status:

          ```bash
          d8 k -n d8-monitoring get pod -l app=x509-certificate-exporter
          ```

        - Check the pod logs:

          ```bash
          d8 k -n d8-monitoring logs -l app=x509-certificate-exporter -c x509-certificate-exporter
          ```
      summary: |
        Prometheus can't scrape x509-certificate-exporter metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8CloudDataDiscovererCloudRequestError
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Refer to the cloud data discoverer's logs for details:

        ```shell
        d8 k -n {{ $labels.namespace }} logs deploy/cloud-data-discoverer
        ```
      summary: |
        Cloud data discoverer cannot get data from the cloud.
      severity: "6"
      markupFormat: markdown
    - name: D8CloudDataDiscovererSaveError
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Refer to the cloud data discoverer's logs for details:

        ```shell
        d8 k -n {{ $labels.namespace }} logs deploy/cloud-data-discoverer
        ```
      summary: |
        Cloud data discoverer cannot save data to a Kubernetes resource.
      severity: "6"
      markupFormat: markdown
    - name: D8ClusterAutoscalerManagerPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerPodIsNotRunning
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The {{$labels.pod}} Pod is {{$labels.phase}}.

        To check the Pod's status, run the following command:

        ```shell
        d8 k -n {{$labels.namespace}} get pods {{$labels.pod}} -o json | jq .status
        ```
      summary: |
        The cluster-autoscaler Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerPodIsRestartingTooOften
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The cluster-autoscaler has restarted {{ $value }} times in the past hour.

        Frequent restarts may indicate a problem.
        The cluster-autoscaler is expected to run continuously without interruption.

        Check the logs for details:

        ```shell
        d8 k -n d8-cloud-instance-manager logs -f -l app=cluster-autoscaler -c cluster-autoscaler
        ```
      summary: |
        Too many cluster-autoscaler restarts detected.
      severity: "9"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTargetAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The cluster-autoscaler automatically scales nodes in the cluster.
        If it's unavailable, it will be impossible to add new nodes when there's not enough resources for scheduling Pods.
        It may also lead to unnecessary cloud costs due to unused but still provisioned cloud instances.

        To resolve the issue, follow these steps:

        1. Check availability and status of cluster-autoscaler Pods:

           ```shell
           d8 k -n d8-cloud-instance-manager get pods -l app=cluster-autoscaler
           ```

        2. Verify that the cluster-autoscaler Deployment exists:

           ```shell
           d8 k -n d8-cloud-instance-manager get deploy cluster-autoscaler
           ```

        3. Check the Deployment's status:

           ```bash
           d8 k -n d8-cloud-instance-manager describe deploy cluster-autoscaler
           ```
      summary: |
        Cluster-autoscaler target is missing in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTargetDown
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape cluster-autoscaler's metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8ClusterAutoscalerTooManyErrors
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cluster-autoscaler.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The cluster-autoscaler encountered multiple errors from the cloud provider when attempting to scale the cluster.

        Check the logs for details:

        ```shell
        d8 k -n d8-cloud-instance-manager logs -f -l app=cluster-autoscaler -c cluster-autoscaler
        ```
      summary: |
        Cluster-autoscaler is issuing too many errors.
      severity: "8"
      markupFormat: markdown
    - name: D8CNIEnabledMoreThanOne
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/cni-checks.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse has detected that multiple CNIs are enabled in the cluster.
        For the cluster to work correctly, only one CNI must be enabled.

        To resolve this issue, disable any unnecessary CNI.
      summary: |
        More than one CNI is enabled in the cluster.
      severity: "2"
      markupFormat: markdown
    - name: D8CNIMisconfigured
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/cni-checks.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        This happened because there were several sources for configuring CNI parameters in the cluster, and ModuleConfig did not have the highest priority previously.

        To resolve this issue, the following steps should be taken:

        1. Find the ConfigMap `d8-system/desired-cni-moduleconfig` in the cluster, which contains the actual ModuleConfig settings.

           ```bash
           d8 k -n d8-system get configmap desired-cni-moduleconfig -o yaml
           ```

        2. Apply this prepared ModuleConfig to the cluster. This will not cause any actual reconfigurations in the cluster and is completely safe.

        3. Once the parameters in "ModuleConfig" match those used in the cluster:
             - this alert will be resolved,
             - and the priority of the sources for configuring CNI will change, and ModuleConfig will become the main source of truth.

        4. After that, you can add the desired parameters to the ModuleConfig CNI and they will be applied immediately.
      summary: |
        The parameters specified in ModuleConfig of {{ $labels.cni }} do not match the ones that are actually being used in the cluster.
      severity: "5"
      markupFormat: markdown
    - name: D8ControlPlaneManagerPodNotRunning
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        The `d8-control-plane-manager` pod is either failing or hasn't been scheduled on node `{{ $labels.node }}`.

        To resolve this issue, check the status of the `kube-system/d8-control-plane-manager` DaemonSet and its pods by running the following command:

        ```bash
        d8 k -n kube-system get daemonset,pod --selector=app=d8-control-plane-manager
        ```
      summary: |
        Controller pod isn't running on node {{ $labels.node }}.
      severity: "6"
      markupFormat: markdown
    - name: D8CustomPrometheusRuleFoundInCluster
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        There are PrometheusRules in the Deckhouse namespace that were not created by Deckhouse.

        To resolve the issue, replace these PrometheusRules with the CustomPrometheusRules object.

        To list all PrometheusRules in the Deckhouse namespace, run the following command:

        ```bash
        d8 k get prometheusrules --all-namespaces -l heritage!=deckhouse
        ```

        For details on adding alerts and recording rules, refer to the [Prometheus module FAQ](https://deckhouse.io/modules/prometheus/faq.html#how-do-i-add-alerts-andor-recording-rules).
      summary: |
        Deckhouse namespace contains PrometheusRules not created by Deckhouse.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseConfigInvalid
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "Deckhouse configuration contains errors.\n\nSteps to troubleshoot:\n\n1. Check Deckhouse logs by running the following command:\n\n   ```bash\n   d8 k -n d8-system logs -f -l app=deckhouse\n   ```\n\n1. Edit the Deckhouse global configuration:\n\n   ```bash\n   d8 k edit mc global\n   ```\n   \n   Or edit configuration of a specific module:\n\n   ```bash\n   d8 k edit mc <MODULE_NAME>\n   ```\n"
      summary: |
        Deckhouse configuration is invalid.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotDeleteModule
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to delete the {{ $labels.module }} module.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotDiscoverModules
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to discover modules.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunGlobalHook
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to run the {{ $labels.hook }} global hook.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunModule
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to start the {{ $labels.module }} module.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseCouldNotRunModuleHook
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Deckhouse is unable to run the {{ $labels.module }}/{{ $labels.hook }} module hook.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseCustomTargetDown
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape custom metrics generated by Deckhouse hooks.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseDeprecatedConfigmapManagedByArgoCD
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The Deckhouse ConfigMap is no longer used.

        To resolve this issue, remove the `d8-system/deckhouse` ConfigMap from Argo CD.
      summary: |
        Deprecated Deckhouse ConfigMap managed by Argo CD.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseGlobalHookFailsTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The `{{ $labels.hook }}` hook has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        The {{ $labels.hook }} Deckhouse global hook is crashing too frequently.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseHasNoAccessToRegistry
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse can't connect to the registry (typically `registry.deckhouse.io`) to check for a new Docker image. These checks are performed every 15 seconds. Without access to the registry, automatic updates are unavailable.

        This alert often indicates that the Deckhouse Pod is experiencing connectivity issues with the Internet.
      summary: |
        Deckhouse is unable to connect to the registry.
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseIsHung
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse is probably down, since the `deckhouse_live_ticks` metric in Prometheus has stopped increasing.
        This metric is expected to increment every 10 seconds.
      summary: |
        Deckhouse is down.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseIsNotOnReleaseChannel
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse in this cluster isn't subscribed to any of the regular release channels: `Alpha`, `Beta`, `EarlyAccess`, `Stable`, or `RockSolid`.

        To resolve this issue, follow these steps:

        1. Check the current release channel used in the cluster:

           ```bash
           d8 k -n d8-system  get deploy deckhouse -o json | jq '.spec.template.spec.containers[0].image' -r
           ```

        1. Subscribe to one of the regular release channels by adjusting the [`deckhouse` module configuration](https://deckhouse.io/modules/deckhouse/configuration.html#parameters-releasechannel).
      summary: |
        Deckhouse isn't subscribed to any regular release channels.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseModuleHookFailsTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The `{{ $labels.hook }}` hook of the `{{ $labels.module }}` module has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        The {{ $labels.module }}/{{ $labels.hook }} Deckhouse hook is crashing too frequently.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseModuleUpdatePolicyNotFound
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The module update policy for {{ $labels.module_release }} is missing.

        To resolve this issue, remove the label from the module release using the following command:

        ```bash
        d8 k label mr {{ $labels.module_release }} modules.deckhouse.io/update-policy-
        ```

        A new suitable policy will be detected automatically.
      summary: |
        Module update policy not found for {{ $labels.module_release }}.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhouseModuleValidationError
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "Initial config for module {{ $labels.module }} is not valid. \n\nYou can get more details via\n\n```bash\nd8 k get mr -l module={{ $labels.module }}\n```\n\nProvided error: {{ $labels.error }}\n"
      summary: |
        Module configuration failed for module {{ $labels.module }}.
      severity: "5"
      markupFormat: markdown
    - name: D8DeckhousePodIsNotReady
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        The Deckhouse Pod is NOT Ready.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhousePodIsNotRunning
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        The Deckhouse Pod is NOT Running.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhousePodIsRestartingTooOften
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Number of restarts in the last hour: {{ $value }}.

        Excessive Deckhouse restarts indicate a potential issue. Normally, Deckhouse should be up and running continuously.

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Excessive Deckhouse restarts detected.
      severity: "9"
      markupFormat: markdown
    - name: D8DeckhouseQueueIsHung
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse cannot finish processing of the `{{ $labels.queue }}` queue, which currently has {{ $value }} pending task(s).

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        The {{ $labels.queue }} Deckhouse queue is stuck with {{ $value }} pending task(s).
      severity: "7"
      markupFormat: markdown
    - name: D8DeckhouseSelfTargetAbsent
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        There is no Deckhouse target in Prometheus.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseSelfTargetDown
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Deckhouse metrics.
      severity: "4"
      markupFormat: markdown
    - name: D8DeckhouseWatchErrorOccurred
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: "Deckhouse has detected an error in the client-go informer, possibly due to connection issues with the API server.\n\nSteps to investigate:\n\n1. Check Deckhouse logs for more information by running:\n\n   ```bash\n   d8 k -n d8-system logs deploy/deckhouse | grep error | grep -i watch\n   ```\n\n1. This alert attempts to detect a correlation between the faulty snapshot invalidation and API server connection errors, specifically for the `handle-node-template` hook in the `node-manager` module.\n   \n   To compare the snapshot with the actual node objects for this hook, run the following command:\n\n   ```bash\n   diff -u <(d8 k get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}'|sort) <(d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module snapshots node-manager -o json | jq '.\"040-node-manager/hooks/handle_node_templates.go\"' | jq '.nodes.snapshot[] | .filterResult.Name' -r | sort)\n   ```\n"
      summary: |
        Possible API server connection error in the client-go informer.
      severity: "5"
      markupFormat: markdown
    - name: D8DexAllTargetsDown
      sourceFile: modules/150-user-authn/monitoring/prometheus-rules/dex.yaml
      moduleUrl: 150-user-authn
      module: user-authn
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Dex metrics.
      severity: "6"
      markupFormat: markdown
    - name: D8EtcdDatabaseHighFragmentationRatio
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        The etcd database size in use on instance `{{ $labels.instance }}` is less than 50% of the allocated disk space, indicating potential fragmentation. Additionally, the total storage size exceeds 75% of the configured quota.

        To resolve this issue, defragment the etcd database by running the following command:

        ```bash
        d8 k -n kube-system exec -ti etcd-{{ $labels.node }} -- /usr/bin/etcdctl \
          --cacert /etc/kubernetes/pki/etcd/ca.crt \
          --cert /etc/kubernetes/pki/etcd/ca.crt \
          --key /etc/kubernetes/pki/etcd/ca.key \
          --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
        ```
      summary: |
        etcd database size in use is less than 50% of the allocated storage.
      severity: "7"
      markupFormat: markdown
    - name: D8EtcdExcessiveDatabaseGrowth
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        Based on the growth rate observed over the last six hours, Deckhouse predicts that the etcd database will run out of disk space within one day on instance `{{ $labels.instance }}`.

        To prevent disruptions, investigate the cause and take necessary action.
      summary: |
        etcd cluster database is growing rapidly.
      severity: "4"
      markupFormat: markdown
    - name: D8GrafanaDeploymentReplicasUnavailable
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The number of Grafana replicas is lower than the specified number.

        The Deployment is in the `MinimumReplicasUnavailable` state.

        Troubleshooting options:

        - To check the Deployment's status:

          ```shell
          d8 k -n d8-monitoring get deployment grafana-v10 -o json | jq .status
          ```

        - To check a Pod's status:

          ```shell
          d8 k -n d8-monitoring get pods -l app=grafana-v10 -o json | jq '.items[] | {(.metadata.name):.status}'
          ```
      summary: |
        One or more Grafana Pods are NOT Running.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaDeprecatedCustomDashboardDefinition
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The ConfigMap `grafana-dashboard-definitions-custom` has been found in the `d8-monitoring` namespace.
        This indicates that a deprecated method for registering custom dashboards in Grafana is used.

        **This method is no longer supported**.

        Migrate to using the custom [GrafanaDashboardDefinition resource](https://github.com/deckhouse/deckhouse/blob/main/modules/300-prometheus/docs/internal/GRAFANA_DASHBOARD_DEVELOPMENT.md) instead.
      summary: |
        Deprecated ConfigMap for Grafana dashboards detected.
      severity: "9"
      markupFormat: markdown
    - name: D8GrafanaPodIsNotReady
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: ""
      summary: |
        The Grafana Pod is NOT Ready.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaPodIsRestartingTooOften
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Grafana has restarted {{ $value }} times in the last hour.

        Frequent restarts indicate a problem. Grafana is expected to run continuously without interruption.

        To investigate the issue, check the logs:

        ```shell
        d8 k -n d8-monitoring logs -f -l app=grafana-v10 -c grafana
        ```
      summary: |
        Excessive Grafana restarts detected.
      severity: "9"
      markupFormat: markdown
    - name: D8GrafanaTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        Grafana visualizes metrics collected by Prometheus. Grafana is critical for some tasks,
        such as monitoring the state of applications and the cluster as a whole. Additionally,
        Grafana's unavailability can negatively impact users who actively use it in their work.

        The recommended course of action:

        1. Check the availability and status of Grafana Pods:

           ```shell
           d8 k -n d8-monitoring get pods -l app=grafana-v10
           ```

        2. Check the availability of the Grafana Deployment:

           ```shell
           d8 k -n d8-monitoring get deployment grafana-v10
           ```

        3. Examine the status of the Grafana Deployment:

           ```shell
           d8 k -n d8-monitoring describe deployment grafana-v10
           ```
      summary: |
        Grafana target is missing in Prometheus.
      severity: "6"
      markupFormat: markdown
    - name: D8GrafanaTargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/grafana.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape Grafana metrics.
      severity: "6"
      markupFormat: markdown
    - name: D8HasModuleConfigAllowedToDisable
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The ModuleConfig is pending module disabling.

        It is recommended that you keep your module configurations clean by removing unnecessary approval annotations.

        If you ignore this alert and do not clear the annotation, the module may be accidentally removed from the cluster, potentially leading to irreversible consequences.

        To resolve this issue and stop the alert, run the following command:

        ```bash
        d8 k annotate moduleconfig {{ $labels.module }} modules.deckhouse.io/allow-disabling-
        ```
      summary: |
        The ModuleConfig annotation for allowing module disabling is set.
      severity: "4"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterMalfunctioning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        The `image-availability-exporter` has failed to perform any image availability checks in the container registry for over 20 minutes.

        To investigate the issue, review the exporter's logs:

        ```bash
        d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        The image-availability-exporter has crashed.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterPodIsNotReady
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `image-availability-exporter` pod is not ready. As a result, the images listed in the `image` field aren't checked for availability in the container registry.

        Steps to resolve:

        1. Retrieve the deployment details:

           ```bash
           d8 k -n d8-monitoring describe deploy image-availability-exporter
           ```

        2. Check the pod status and investigate why it isn't `Ready`:

           ```bash
           d8 k -n d8-monitoring describe pod -l app=image-availability-exporter
           ```
      summary: |
        The image-availability-exporter pod is not ready.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterPodIsNotRunning
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `image-availability-exporter` pod is not running. As a result, the images listed in the `image` field aren't checked for availability in the container registry.

        Steps to resolve:

        1. Retrieve the deployment details:

           ```bash
           d8 k -n d8-monitoring describe deploy image-availability-exporter
           ```

        2. Check the pod status and investigate why it isn't running:

           ```bash
           d8 k -n d8-monitoring describe pod -l app=image-availability-exporter
           ```
      summary: |
        The image-availability-exporter pod is not running.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterTargetAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `image-availability-exporter` target is missing from Prometheus.

        Steps to resolve:

        1. Check the pod status:

           ```bash
           d8 k -n d8-monitoring get pod -l app=image-availability-exporter
           ```

        1. Check the pod logs:

           ```bash
           d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
           ```
      summary: |
        The image-availability-exporter target is missing from Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8ImageAvailabilityExporterTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-availability-exporter-health.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that Prometheus is unable to scrape metrics of `image-availability-exporter`.

        Steps to resolve:

        1. Check the pod status:

           ```bash
           d8 k -n d8-monitoring get pod -l app=image-availability-exporter
           ```

        1. Check the pod logs:

           ```bash
           d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
           ```
      summary: |
        Prometheus can't scrape metrics of image-availability-exporter.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioActualDataPlaneVersionNotEqualDesired
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with Istio data plane version `{{$labels.version}}`, while the desired version is `{{$labels.desired_version}}`. As a result, the Istio version will be changed after the pod is restarted.

        To resolve the issue, use the following cheat sheet:

        ```text
        ### Namespace-wide configuration
        # `istio.io/rev=vXYZ`: Use a specific revision.
        # `istio-injection=enabled`: Use the global revision.
        d8 k get ns {{$labels.namespace}} --show-labels

        ### Pod-wide configuration
        d8 k -n {{$labels.namespace}} get pods -l istio.io/rev={{$labels.desired_revision}}
        ```
      summary: |
        There are pods with Istio data plane version {{$labels.version}}, but desired version is {{$labels.desired_version}}.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioActualVersionIsNotInstalled
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with injected sidecars of version `{{$labels.version}}` (revision `{{$labels.revision}}`), but the corresponding control plane version is not installed. As a result, these pods have lost synchronization with the state in Kubernetes.

        To resolve this issue, install the required control plane version. Alternatively, update the namespace or pod configuration to match an installed control plane version.

        To identify orphaned pods, run the following command:

        ```bash
        d8 k -n {{ $labels.namespace }} get pods -l 'service.istio.io/canonical-name' -o json | jq --arg revision {{ $labels.revision }} '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson | .revision == $revision) | .metadata.name'
        ```
      summary: |
        The control plane version for pods with injected sidecars isn't installed.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioAdditionalControlplaneDoesntWork
      sourceFile: modules/110-istio/monitoring/prometheus-rules/controlplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected that the additional Istio control plane `{{$labels.label_istio_io_rev}}` isn't working.

        As a result, sidecar injection for pods with `{{$labels.label_istio_io_rev}}` isn't working as well.

        To check the status of the control plane pods, run the following command:

        ```
        d8 k get pods -n d8-istio -l istio.io/rev={{$labels.label_istio_io_rev}}
        ```
      summary: |
        Additional control plane isn't working.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioDataPlaneVersionMismatch
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with Istio data plane version `{{$labels.full_version}}`, which is different from the control plane version `{{$labels.desired_full_version}}`.

        Steps to resolve the issue:

        1. Restart affected pods and use the following PromQL query to get a full list:

           ```prometheus
           max by (namespace, dataplane_pod) (d8_istio_dataplane_metadata{full_version="{{$labels.full_version}}"})
           ```

        1. Use the automatic Istio data plane upgrade described in the [guide](https://deckhouse.io/modules/istio/examples.html#upgrading-istio).
      summary: |
        There are pods with data plane version different from the control plane version.
      severity: "8"
      markupFormat: markdown
    - name: D8IstioDataPlaneWithoutIstioInjectionConfigured
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There are pods in the `{{$labels.namespace}}` namespace with Istio sidecars, but istio-injection isn't configured. As a result, these pods will lose their Istio sidecars after being recreated.

        To identify the affected pods, run the following command:

        ```bash
        d8 k -n {{$labels.namespace}} get pods -o json | jq -r --arg revision {{$labels.revision}} '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson | .revision == $revision) | .metadata.name'
        ```
      summary: |
        Detected pods with Istio sidecars but istio-injection isn't configured.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioDeprecatedIstioVersionInstalled
      sourceFile: modules/110-istio/monitoring/prometheus-rules/versions.tpl
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected that a deprecated Istio version `{{$labels.version}}` is installed.

        Support for this version will be removed in upcoming Deckhouse releases. The higher the alert severity, the greater the probability of support being discontinued.

        To learn how to upgrade Istio, refer to the [upgrade guide](https://deckhouse.io/modules/istio/examples.html#upgrading-istio).
      summary: |
        The installed Istio version has been deprecated.
      severity: undefined
      markupFormat: markdown
    - name: D8IstioDesiredVersionIsNotInstalled
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There is a desired Istio control plane version `{{$labels.desired_version}}` (revision `{{$labels.revision}}`) configured for pods in the `{{$labels.namespace}}` namespace, but that version isn't installed. As a result, pods can't be recreated in the `{{$labels.namespace}}` namespace.

        To resolve this issue, install the desired control plane version. Alternatively, update the namespace or pod configuration to match an installed control plane version.

        Use the following cheat sheet:

        ```text
        ### Namespace-wide configuration
        # `istio.io/rev=vXYZ`: Use a specific revision.
        # `istio-injection=enabled`: Use the global revision.
        d8 k get ns {{$labels.namespace}} --show-labels

        ### Pod-wide configuration
        d8 k -n {{$labels.namespace}} get pods -l istio.io/rev={{$labels.revision}}
        ```
      summary: |
        Desired control plane version isn't installed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioFederationMetadataEndpointDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/federation.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The metadata endpoint `{{$labels.endpoint}}` for IstioFederation `{{$labels.federation_name}}` has failed to fetch via the Deckhouse hook.

        To reproduce the request to the public endpoint, run the following command:

        ```bash
        curl {{$labels.endpoint}}
        ```

        To reproduce the request to private endpoints (run from the Deckhouse pod), run the following:

        ```bash
        KEY="$(deckhouse-controller module values istio -o json | jq -r .internal.remoteAuthnKeypair.priv)"
        LOCAL_CLUSTER_UUID="$(deckhouse-controller module values -g istio -o json | jq -r .global.discovery.clusterUUID)"
        REMOTE_CLUSTER_UUID="$(d8 k get istiofederation {{$labels.federation_name}} -o json | jq -r .status.metadataCache.public.clusterUUID)"
        TOKEN="$(deckhouse-controller helper gen-jwt --private-key-path <(echo "$KEY") --claim iss=d8-istio --claim sub=$LOCAL_CLUSTER_UUID --claim aud=$REMOTE_CLUSTER_UUID --claim scope=private-federation --ttl 1h)"
        curl -H "Authorization: Bearer $TOKEN" {{$labels.endpoint}}
        ```
      summary: |
        Federation metadata endpoint failed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioGlobalControlplaneDoesntWork
      sourceFile: modules/110-istio/monitoring/prometheus-rules/controlplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected that the global Istio control plane `{{$labels.label_istio_io_rev}}` isn't working.

        As a result, sidecar injection for pods with global revision isn't working as well, and the validating webhook for Istio resources is absent.

        To check the status of the control plane pods, run the following command:

        ```bash
        d8 k get pods -n d8-istio -l istio.io/rev={{$labels.label_istio_io_rev}}
        ```
      summary: |
        Global control plane isn't working.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioMulticlusterMetadataEndpointDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The metadata endpoint `{{$labels.endpoint}}` for IstioMulticluster `{{$labels.multicluster_name}}` has failed to fetch via the Deckhouse hook.

        To reproduce the request to the public endpoint, run the following command:

        ```bash
        curl {{$labels.endpoint}}
        ```

        To reproduce the request to private endpoints (run from the `d8-system/deckhouse` pod), run the following:

        ```bash
        KEY="$(deckhouse-controller module values istio -o json | jq -r .internal.remoteAuthnKeypair.priv)"
        LOCAL_CLUSTER_UUID="$(deckhouse-controller module values -g istio -o json | jq -r .global.discovery.clusterUUID)"
        REMOTE_CLUSTER_UUID="$(d8 k get istiomulticluster {{$labels.multicluster_name}} -o json | jq -r .status.metadataCache.public.clusterUUID)"
        TOKEN="$(deckhouse-controller helper gen-jwt --private-key-path <(echo "$KEY") --claim iss=d8-istio --claim sub=$LOCAL_CLUSTER_UUID --claim aud=$REMOTE_CLUSTER_UUID --claim scope=private-multicluster --ttl 1h)"
        curl -H "Authorization: Bearer $TOKEN" {{$labels.endpoint}}
        ```
      summary: |
        Multicluster metadata endpoint failed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioMulticlusterRemoteAPIHostDoesntWork
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The remote API host `{{$labels.api_host}}` for IstioMulticluster `{{$labels.multicluster_name}}` has failed the health check performed by the Deckhouse monitoring hook.

        To reproduce the request (run from the `d8-system/deckhouse` pod), run the following:

        ```bash
        TOKEN="$(deckhouse-controller module values istio -o json | jq -r --arg ah {{$labels.api_host}} '.internal.multiclusters[]| select(.apiHost == $ah)| .apiJWT ')"
        curl -H "Authorization: Bearer $TOKEN" https://{{$labels.api_host}}/version
        ```
      summary: |
        Multicluster remote API host health check failed.
      severity: "6"
      markupFormat: markdown
    - name: D8IstioOperatorReconcileError
      sourceFile: modules/110-istio/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        Deckhouse has detected an error in the `istio-operator` reconciliation loop.

        To investigate the issue, check the operator logs:

        ```bash
        d8 k -n d8-istio logs -l app=operator,revision={{$labels.revision}}
        ```
      summary: |
        The istio-operator is unable to reconcile Istio control plane setup.
      severity: "5"
      markupFormat: markdown
    - name: D8IstioPodsWithoutIstioSidecar
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/dataplane.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        There is a pod `{{$labels.dataplane_pod}}` in the `{{$labels.namespace}}` namespace without Istio sidecars, but with istio-injection configured.

        To identify the affected pods, run the following command:

        ```bash
        d8 k -n {{$labels.namespace}} get pods -l '!service.istio.io/canonical-name' -o json | jq -r '.items[] | select(.metadata.annotations."sidecar.istio.io/inject" != "false") | .metadata.name'
        ```
      summary: |
        Detected pods without Istio sidecars but with istio-injection configured.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioRemoteClusterNotSynced
      sourceFile: ee/modules/110-istio/monitoring/prometheus-rules/multicluster.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ee
      description: |
        The Istio control plane instance `{{$labels.istiod}}` cannot synchronize with the remote cluster `{{$labels.cluster_id}}`.

        Possible causes:

        - The remote cluster is offline.
        - The remote API endpoint is not reachable.
        - The remote ServiceAccount token is invalid or expired.
        - There is a TLS or certificate issue between the clusters.
      summary: |
        Istio remote cluster {{$labels.cluster_id}} is not synchronized.
      severity: "4"
      markupFormat: markdown
    - name: D8IstioVersionIsIncompatibleWithK8sVersion
      sourceFile: modules/110-istio/monitoring/prometheus-rules/versions.tpl
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        The installed Istio version `{{$labels.istio_version}}` may not work properly with the current Kubernetes version `{{$labels.k8s_version}}` because it's not supported officially.

        To resolve the issue, upgrade Istio following the [guide](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/istio/examples.html#upgrading-istio).
      summary: |
        The installed Istio version is incompatible with the Kubernetes version.
      severity: "3"
      markupFormat: markdown
    - name: D8KubeEtcdDatabaseSizeCloseToTheLimit
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        The etcd database size on `{{ $labels.node }}` is nearing its size limit.
        This may be caused by a high number of events, such as pod evictions or the recent creation of numerous resources in the cluster.

        Possible solutions:

        - Defragment the etcd database by running the following command:

          ```bash
          d8 k -n kube-system exec -ti etcd-{{ $labels.node }} -- /usr/bin/etcdctl \
            --cacert /etc/kubernetes/pki/etcd/ca.crt \
            --cert /etc/kubernetes/pki/etcd/ca.crt \
            --key /etc/kubernetes/pki/etcd/ca.key \
            --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
          ```

        - Increase node memory. Starting from 24 GB, `quota-backend-bytes` will increase by 1 GB for every extra 8 GB of memory.

          Example:

          | Node memory | quota-backend-bytes |
          | ----------- | ------------------- |
          | 16 GB       | 2147483648 (2 GB)   |
          | 24 GB       | 3221225472 (3 GB)   |
          | 32 GB       | 4294967296 (4 GB)   |
          | 40 GB       | 5368709120 (5 GB)   |
          | 48 GB       | 6442450944 (6 GB)   |
          | 56 GB       | 7516192768 (7 GB)   |
          | 64 GB       | 8589934592 (8 GB)   |
          | 72 GB       | 8589934592 (8 GB)   |
          | ...         | ...                 |
      summary: |
        etcd database size is approaching the limit.
      severity: "3"
      markupFormat: markdown
    - name: D8KubernetesStaleTokensDetected
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        This issue may occur if an application reads the token only at startup and does not reload it periodically. As a result, an outdated token might be used, leading to security breach and authentication failures.

        **Recommended actions:**

        - Ensure your application is configured to periodically reload the token from the file system.
        - Verify that you are using an up-to-date client library that supports automatic token rotation.

        Note that currently these tokens are not blocked because the `--service-account-extend-token-expiration` flag is enabled by default (`Default: true`). With this flag enabled, admission-injected tokens are extended up to 1 year during token generation to facilitate a safe transition from legacy tokens to the bound service account token feature, ignoring the value of `service-account-max-token-expiration`.

        **For further investigation:**
        Log into the server with label `instance={{ $labels.instance }}` and inspect the audit log using the following command:

        ```bash
        jq 'select(.annotations["authentication.k8s.io/stale-token"]) | {auditID, stageTimestamp, requestURI, verb, user: .user.username, stale_token: .annotations["authentication.k8s.io/stale-token"]}' /var/log/kube-audit/audit.log
        ```

        If you do not see the necessary logs, set `settings.apiserver.auditPolicyEnabled` in control-plane-manager ModuleConfig [according to the documentation](https://deckhouse.io/modules/control-plane-manager/faq.html#how-do-i-configure-additional-audit-policies) and add an additional audit policy to log actions of all service accounts.

        ```yaml
        - level: Metadata
          omitStages:
          - RequestReceived
          userGroups:
          - system:serviceaccounts
        ```

        Example of applying an additional audit policy:

        ```bash
        d8 k apply -f - <<EOF
        apiVersion: v1
        kind: Secret
        metadata:
          name: audit-policy
          namespace: kube-system
        data:
          audit-policy.yaml: YXBpVmVyc2lvbjogYXVkaXQuazhzLmlvL3YxCmtpbmQ6IFBvbGljeQpydWxlczoKLSBsZXZlbDogTWV0YWRhdGEKICBvbWl0U3RhZ2VzOgogIC0gUmVxdWVzdFJlY2VpdmVkCiAgdXNlckdyb3VwczoKICAtIHN5c3RlbTpzZXJ2aWNlYWNjb3VudHM=
        EOF
        ```
      summary: |
        Stale service account tokens detected.
      severity: "8"
      markupFormat: markdown
    - name: D8KubernetesVersionIsDeprecated
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/control-plane-manager.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |-
        The current Kubernetes version `{{ $labels.k8s_version }}` has been deprecated, and support for it will be removed in upcoming releases.

        Please migrate to the next kubernetes version (at least 1.30)

        Read [documentation](/products/kubernetes-platform/documentation/v1/admin/configuration/platform-scaling/control-plane/updating-and-versioning.html) about how to update the Kubernetes version in the cluster.
      summary: |
        Kubernetes version {{ $labels.k8s_version }} is deprecated.
      severity: "7"
      markupFormat: markdown
    - name: D8LogShipperAgentNotScheduledInCluster
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Deckhouse has detected that a number of `log-shipper-agent` pods are not scheduled.

        To resolve this issue, do the following:

        1. Check the state of the `d8-log-shipper/log-shipper-agent` DaemonSet:

           ```shell
           d8 k -n d8-log-shipper get daemonsets --selector=app=log-shipper
           ```

        1. Check the state of the `d8-log-shipper/log-shipper-agent` pods:

           ```shell
           d8 k -n d8-log-shipper get pods --selector=app=log-shipper-agent
           ```

        1. If you know where the DaemonSet should be scheduled, run the following command to identify the problematic nodes:

           ```shell
           d8 k -n d8-log-shipper get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="log-shipper-agent")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
           ```
      summary: |
        The log-shipper-agent pods can't be scheduled in the cluster.
      severity: "7"
      markupFormat: markdown
    - name: D8LogShipperClusterLogDestinationD8LokiAuthorizationRequired
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/warnings.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |-
        Deckhouse has detected the ClusterLogDestination resource `{{$labels.resource_name}}` without authorization parameters.

        Add the authorization parameters to the ClusterLogDestination resource following the [instructions](https://deckhouse.io/modules/log-shipper/faq.html#how-to-add-authorization-to-the-clusterlogdestination-resource).
      summary: |
        Authorization parameters required for the ClusterLogDestination resource.
      severity: "9"
      markupFormat: markdown
    - name: D8LogShipperCollectLogErrors
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Deckhouse has detected that the {{ $labels.host }} `log-shipper-agent` on the `{{ $labels.node }}` node has failed to collect metrics for more than 10 minutes.

        This is caused by the `{{ $labels.error_type }}` errors occurred during the `{{ $labels.stage }}` stage while reading `{{ $labels.component_type }}`.

        To resolve this, check the pod logs or follow advanced instructions:

        ```bash
        d8 k -n d8-log-shipper logs {{ $labels.host }}` -c vector
        ```
      summary: |
        The log-shipper-agent pods can't collect logs to {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8LogShipperDestinationErrors
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Deckhouse has detected that the {{ $labels.host }} `log-shipper-agent` on the {{ $labels.node }} node has failed to send a log for more than 10 minutes.

        This is caused by the `{{ $labels.error_type }}` errors occurred during the `{{ $labels.stage }}` stage while sending logs to `{{ $labels.component_type }}`.

        To resolve this, check the pod logs or follow advanced instructions:

        ```bash
        d8 k -n d8-log-shipper logs {{ $labels.host }} -c vector
        ```
      summary: |
        The log-shipper-agent pods can't send logs to {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8LogShipperLogsDroppedByRateLimit
      sourceFile: modules/460-log-shipper/monitoring/prometheus-rules/log-shipper-agent.yaml
      moduleUrl: 460-log-shipper
      module: log-shipper
      edition: ce
      description: |
        Rate-limiting rules have been applied, and the `log-shipper-agent` on the `{{ $labels.node }}` node has been dropping logs for more than 10 minutes.

        To resolve this, check the pod logs or follow advanced instructions:

        ```bash
        d8 k -n d8-log-shipper get pods -o wide | grep {{ $labels.node }}
        ```
      summary: |
        The log-shipper-agent pods are dropping logs to {{ $labels.component_id }} on the {{ $labels.node }} node.
      severity: "4"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        The {{$labels.pod}} Pod is NOT Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsNotRunning
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The {{$labels.pod}} Pod is {{$labels.phase}}.

        To check the Pod's status, run the following command:

        ```shell
        d8 k -n {{$labels.namespace}} get pods {{$labels.pod}} -o json | jq .status
        ```
      summary: |
        The machine-controller-manager Pod is NOT Running.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerPodIsRestartingTooOften
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The `machine-controller-manager` has restarted {{ $value }} times in the past hour.

        Frequent restarts may indicate a problem.
        The `machine-controller-manager` is expected to run continuously without interruption.

        Check the logs for details:

        ```shell
        d8 k -n d8-cloud-instance-manager logs -f -l app=machine-controller-manager -c controller
        ```
      summary: |
        Too many machine-controller-manager restarts detected.
      severity: "9"
      markupFormat: markdown
    - name: D8MachineControllerManagerTargetAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        `Machine-controller-manager` controls ephemeral nodes in the cluster.
        If it becomes unavailable, it will be impossible to create or delete nodes in the cluster.

        To resolve the issue, follow these steps:

        1. Check availability and status of `machine-controller-manager` Pods:

           ```shell
           d8 k -n d8-cloud-instance-manager get pods -l app=machine-controller-manager
           ```

        2. Verify availability of the `machine-controller-manager` Deployment:

           ```shell
           d8 k -n d8-cloud-instance-manager get deployment machine-controller-manager
           ```

        3. Check the Deployments status:

           ```shell
           d8 k -n d8-cloud-instance-manager describe deployment machine-controller-manager
           ```
      summary: |
        Machine-controller-manager target is missing in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8MachineControllerManagerTargetDown
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/machine-controller-manager.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: ""
      summary: |
        Prometheus is unable to scrape the machine-controller-manager's metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8MetallbBGPSessionDown
      sourceFile: ee/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: ee
      description: |
        {{ $labels.job }}, MetalLB {{ $labels.container }} on {{ $labels.pod}} has BGP session {{ $labels.peer }} down.

        Check the logs for details:

        ```bash
        d8 k -n d8-metallb logs daemonset/speaker -c speaker
        ```
      summary: |
        MetalLB BGP session is down.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbConfigNotLoaded
      sourceFile: ee/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: ee
      description: |
        {{ $labels.job }}, MetalLB {{ $labels.container }} on {{ $labels.pod}} hasn't been loaded.

        To find the cause of the issue, review the controller logs:

        ```bash
        d8 k -n d8-metallb logs deploy/controller -c controller
        ```
      summary: |
        The MetalLB configuration hasn't been loaded.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbConfigStale
      sourceFile: ee/modules/380-metallb/monitoring/prometheus-rules/metallb.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: ee
      description: |
        {{ $labels.job }}, MetalLB {{ $labels.container }} on {{ $labels.pod}} is running on a stale configuration because the latest configuration failed to load.

        To find the cause of the issue, review the controller logs:

        ```bash
        d8 k -n d8-metallb logs deploy/controller -c controller
        ```
      summary: |
        MetalLB is running on a stale configuration.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbNotSupportedServiceAnnotationsDetected
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        The annotation '{{$labels.annotation}}' has been deprecated for the service '{{$labels.name}}' in the '{{$labels.namespace}}' namespace.

        The following service annotations are no longer effective:
        - `metallb.universe.tf/ip-allocated-from-pool`: Remove this annotation.
        - `metallb.universe.tf/address-pool`: Replace it with the `.spec.loadBalancerClass` parameter or use the `network.deckhouse.io/metal-load-balancer-class` annotation, referencing the appropriate MetalLoadBalancerClass.
        - `metallb.universe.tf/loadBalancerIPs`: Replace it with `network.deckhouse.io/load-balancer-ips: <IP>`.
        - `metallb.universe.tf/allow-shared-ip`: Replace it with `network.deckhouse.io/load-balancer-shared-ip-key`.

        **Please note.** Existing LoadBalancer services of Deckhouse have been migrated automatically, but the new ones will not be.
      summary: |
        The annotation '{{$labels.annotation}}' has been deprecated for the service '{{$labels.name}}' in the '{{$labels.namespace}}' namespace.
      severity: "4"
      markupFormat: markdown
    - name: D8MetallbObsoleteLayer2PoolsAreUsed
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        In ModuleConfig version 2, addressPool '{{$labels.name}}' of type layer2 are ignored. They should be removed from the configuration.
      summary: |
        The metallb module has obsolete layer2 pools configured.
      severity: "7"
      markupFormat: markdown
    - name: D8MetallbUpdateMCVersionRequired
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        D8 MetalLB settings are outdated.

        To resolve this issue, increase version in the ModuleConfig `metallb`.
      summary: |
        The metallb ModuleConfig settings are outdated.
      severity: "5"
      markupFormat: markdown
    - name: D8ModuleOutdatedByMajorVersion
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module `{{ $labels.moduleName }}` is running {{ $labels.majorReleaseDepth }} major versions behind on major version releases.
        Major version updates may contain breaking changes and require immediate attention and careful planning.

        To check the module releases status, run the following command:

        ```bash
        d8 k get mr -l module={{ $labels.moduleName }}
        ```{{ if ne $labels.majorReleaseName "nil" }}{{ if ne $labels.fromToName "nil" }}

        This major release has from-to version constraints that must be met before applying the update.
        To check the update instructions (from-to versions), run the following command:

        ```bash
        d8 k get mr {{ $labels.name }} -o jsonpath='{.spec.update.versions[*]}'
        ```

        This will show you the update paths including the from-to versions defined in the release.{{ end }}

        To approve the module major release update, run the following command:

        ```bash
        d8 k annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"
        ```{{ end }}
      summary: |
        Module {{ $labels.moduleName }} is running {{ $labels.majorReleaseDepth }} major versions behind.
      severity: "9"
      markupFormat: markdown
    - name: D8NeedDecreaseEtcdQuotaBackendBytes
      sourceFile: modules/040-control-plane-manager/monitoring/prometheus-rules/etcd-maintenance.yaml
      moduleUrl: 040-control-plane-manager
      module: control-plane-manager
      edition: ce
      description: |
        When the control plane node memory is reduced, Deckhouse may suggest reducing `quota-backend-bytes`.
        While Deckhouse is capable of automatically increasing this value, reducing it must be done manually.

        To modify `quota-backend-bytes`, set the `controlPlaneManager.etcd.maxDbSize` parameter. Before setting a new value, check the current database usage on every control plane node by running:

        ```
        for pod in $(d8 k get pod -n kube-system -l component=etcd,tier=control-plane -o name); do
          d8 k -n kube-system exec -ti "$pod" -- /usr/bin/etcdctl \
            --cacert /etc/kubernetes/pki/etcd/ca.crt \
            --cert /etc/kubernetes/pki/etcd/ca.crt \
            --key /etc/kubernetes/pki/etcd/ca.key \
            endpoint status -w json | jq --arg a "$pod" -r \
            '.[0].Status.dbSize / 1024 / 1024 | tostring | $a + ": " + . + " MB"';
        done
        ```

        Things to note:

        - The maximum value for `controlPlaneManager.etcd.maxDbSize` is 8 GB.
        - If control plane nodes have less than 24 GB, set `controlPlaneManager.etcd.maxDbSize` to 2 GB.
        - Starting from 24 GB, `quota-backend-bytes` will increase by 1 GB for every extra 8 GB of memory.

          Example:

          | Node memory | quota-backend-bytes |
          | ----------- | ------------------- |
          | 16 GB       | 2147483648 (2 GB)   |
          | 24 GB       | 3221225472 (3 GB)   |
          | 32 GB       | 4294967296 (4 GB)   |
          | 40 GB       | 5368709120 (5 GB)   |
          | 48 GB       | 6442450944 (6 GB)   |
          | 56 GB       | 7516192768 (7 GB)   |
          | 64 GB       | 8589934592 (8 GB)   |
          | 72 GB       | 8589934592 (8 GB)   |
          | ...         | ...                 |
      summary: |
        Deckhouse suggests reducing quota-backend-bytes.
      severity: "6"
      markupFormat: markdown
    - name: D8NeedMigrateStateToOpenTofu
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/migrate-to-tofu.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        This likely means the automatic migration to OpenTofu was canceled due to destructive changes detected in the cluster state.

        Verify the following:

        - Current Terraform version:

          ```shell
          d8 k -n d8-system exec -it deployments/terraform-state-exporter -c exporter -- terraform version
          ```

        - Version in the Terraform state:

          ```shell
          d8 k -n d8-system get secret d8-cluster-terraform-state -o json | jq -r '.data["cluster-tf-state.json"] | @base64d | fromjson | .terraform_version'
          ```

        - Check for destructive changes:

          ```shell
          d8 k exec -it deploy/terraform-state-exporter -n d8-system -- dhctl terraform check
          ```

        To resolve the issue and migrate to OpenTofu manually, follow these steps:

        1. Using the `install` container with the previous major Deckhouse release (for example, `1.69.X`, while the cluster is now on `1.70.X`), run:

           ```shell
           dhctl converge
           ```

           This should resolve all destructive changes in the cluster.

        2. After that, the `terraform-auto-converger` Pod should complete the migration automatically.

        If the automatic migration doesn't happen, use the `install` container with the current Deckhouse release and run the following command to enforce migration of the Terraform state to OpenTofu:

        ```shell
        dhctl converge-migration
        ```

        If this alert appears in a cluster created by Deckhouse Commander, it means the cluster has been updated to a new Deckhouse version without approving state changes (for example, destructive changes).

        To resolve this, follow these steps:

        1. In Deckhouse Commander, click **Delete manually** to temporarily remove the cluster from control.
        2. In the cluster, remove the `commander-agent` module using the following command:

           ```shell
           d8 k delete mc commander-agent
           ```

        3. Using the `install` container with the previous major Deckhouse release (for example, `1.69.X`, while the cluster is now on `1.70.X`), run the following command to resolve all destructive changes in the cluster:

           ```shell
           dhctl converge
           ```

        4. In Deckhouse Commander, click **Attach** to reattach the cluster.
      summary: |
        Terraform state must be migrated to OpenTofu.
      severity: "4"
      markupFormat: markdown
    - name: D8NginxIngressKruiseControllerPodIsRestartingTooOften
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        {{ $value }} Kruise controller restarts detected in the last hour.

        Excessive Kruise controller restarts indicate that something is wrong. Normally, it should be up and running all the time.

        Steps to resolve:

        1. Check events associated with `kruise-controller-manager` in the `d8-ingress-nginx` namespace. Look for issues related to node failures or memory shortages (OOM events):

           ```bash
           d8 k -n d8-ingress-nginx get events | grep kruise-controller-manager
           ```

        2. Analyze the controller's pod descriptions to identify restarted containers and possible causes. Pay attention to exit codes and other details:

           ```bash
           d8 k -n d8-ingress-nginx describe pod -lapp=kruise,control-plane=controller-manager
           ```

        3. In case the `kruise` container has restarted, get a list of relevant container logs to identify any meaningful errors:

           ```bash
           d8 k -n d8-ingress-nginx logs -lapp=kruise,control-plane=controller-manager -c kruise
           ```
      summary: |
        Too many Kruise controller restarts detected.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeGroupIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update available for Nodes in the `{{ $labels.node_group }}` NodeGroup.
        Although Nodes have detected the update, none of them have received approval to start the update process.

        Most likely, there is a problem with the `update_approval` hook of the `node-manager` module.
      summary: |
        NodeGroup {{ $labels.node_group }} is not handling the update correctly.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeHasDeprecatedOSVersion
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/node-os-requirements.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Deckhouse has detected nodes running deprecated OS versions.

        Steps to troubleshoot:

        1. Get a list of affected nodes by running the following Prometheus query:

           ```prometheus
           kube_node_info{os_image=~"Ubuntu 18.04.*|Debian GNU/Linux 10.*|CentOS Linux 7.*"}
           ```

        1. Update the affected nodes to a supported OS version.
      summary: |
        Nodes with deprecated OS versions detected.
      severity: "4"
      markupFormat: markdown
    - name: D8NodeHasUnmetKernelRequirements
      sourceFile: modules/021-cni-cilium/monitoring/prometheus-rules/kernel-requirements.yaml
      moduleUrl: 021-cni-cilium
      module: cni-cilium
      edition: ce
      description: |-
        Deckhouse has detected that some nodes don't meet the required kernel constraints.
        As a result, certain modules can't run on these nodes.

        Kernel requirements for each module:

        - **Cilium module**: Kernel version must be **>= 5.8**.

        To list all affected nodes, use the `d8_node_kernel_does_not_satisfy_requirements == 1` expression in Prometheus.
      summary: |
        Nodes have unmet kernel requirements.
      severity: "4"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` detected a new update, requested and received approval, started the update, and encountered a step that could cause downtime.
        The update manager (the `update_approval` hook of the `node-group` module) granted downtime approval, but no success message was received, which indicates that the update has not completed.

        To investigate the details, view Bashible logs on the Node:

        ```shell
        journalctl -fu bashible
        ```
      summary: |
        Node {{ $labels.node }} cannot complete the update.
      severity: "7"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` has detected a new update, requested and received approval, but failed to complete the update.

        To investigate the details, view Bashible logs on the Node:

        ```shell
        journalctl -fu bashible
        ```
      summary: |
        Node {{ $labels.node }} cannot complete the update.
      severity: "8"
      markupFormat: markdown
    - name: D8NodeIsNotUpdating
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` has a pending update but it's neither receiving it nor attempting to.

        Most likely, Bashible is not handling the update correctly.
        It should annotate the Node with `update.node.deckhouse.io/waiting-for-approval` before the update can proceed.

        Options to investigate the details:

        - Check the expected configuration checksum for the NodeGroup:

          ```shell
          d8 k -n d8-cloud-instance-manager get secret configuration-checksums -o jsonpath={.data.{{ $labels.node_group }}} | base64 -d
          ```

        - Check the current configuration checksum on the Node:

          ```shell
          d8 k get node {{ $labels.node }} -o jsonpath='{.metadata.annotations.node\.deckhouse\.io/configuration-checksum}'
          ```

        - View Bashible logs on the Node:

          ```shell
          journalctl -fu bashible
          ```
      summary: |
        Node {{ $labels.node }} is not updating.
      severity: "9"
      markupFormat: markdown
    - name: D8NodeIsUnmanaged
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-unmanaged.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The {{ $labels.node }} Node is not managed by the [`node-manager`](https://deckhouse.io/modules/node-manager/) module.

        To resolve this issue, follow the [instructions](https://deckhouse.io/modules/node-manager/faq.html#how-to-clean-up-a-node-for-adding-to-the-cluster) how to clean up the node and add it to the cluster.
      summary: |
        The {{ $labels.node }} Node is not managed by the node-manager module.
      severity: "9"
      markupFormat: markdown
    - name: D8NodeLocalDNSCacheDenialEvictionsHigh
      sourceFile: ee/be/modules/350-node-local-dns/monitoring/prometheus-rules/cache-plugin.yaml
      moduleUrl: 350-node-local-dns
      module: node-local-dns
      edition: be
      description: |
        The `denial` (NXDOMAIN) cache in node-local-dns is evicting entries frequently.

        This might indicate that there are Services that are being recreated too often in cluster. So, denial cache size is not enough (`denial 9984` is currently configured).

        Affected node: `{{$labels.node}}`
      summary: |
        node-local-dns denial cache is being frequently evicted.
      severity: "6"
      markupFormat: markdown
    - name: D8NodeLocalDNSCacheSuccessEvictionsHigh
      sourceFile: ee/be/modules/350-node-local-dns/monitoring/prometheus-rules/cache-plugin.yaml
      moduleUrl: 350-node-local-dns
      module: node-local-dns
      edition: be
      description: |
        The `success` cache in node-local-dns is evicting entries frequently (>100 in 5 minutes).

        This could mean there are too many different DNS records application have to resolve (to many Services?). So, node-local-dns cache capacity is too small (`success 39936` is currently configured), or traffic patterns changed.

        Affected node: `{{$labels.node}}`
      summary: |
        node-local-dns success cache is being frequently evicted.
      severity: "6"
      markupFormat: markdown
    - name: D8NodeLocalDNSForwardMaxConcurrentRejects
      sourceFile: ee/be/modules/350-node-local-dns/monitoring/prometheus-rules/forward-plugin.yaml
      moduleUrl: 350-node-local-dns
      module: node-local-dns
      edition: be
      description: |
        More than 20 queries were rejected within the last 5 minutes on node `{{$labels.node}}`
        because the max concurrent query limit was reached.
      summary: |
        The forward plugin is rejecting DNS queries due to max concurrency limit.
      severity: "5"
      markupFormat: default
    - name: D8NodeLocalDNSKubeforwardRequestLatencyP95High
      sourceFile: ee/be/modules/350-node-local-dns/monitoring/prometheus-rules/kubeforward-plugin.yaml
      moduleUrl: 350-node-local-dns
      module: node-local-dns
      edition: be
      description: |
        The 95th percentile DNS request latency for A-records through kubeforward on node `{{$labels.node}}`
        exceeded 300 milliseconds over the last 5 minutes.
        This may indicate slow upstreams, network issues, or DNS overload on this node.
      summary: |
        High DNS request latency in kubeforward (p95 &gt; 300ms).
      severity: "4"
      markupFormat: default
    - name: D8NodeLocalDNSUpstreamHealthcheckFailed
      sourceFile: ee/be/modules/350-node-local-dns/monitoring/prometheus-rules/forward-plugin.yaml
      moduleUrl: 350-node-local-dns
      module: node-local-dns
      edition: be
      description: |
        The health check system of the `forward` plugin failed more than 10 times within 5 minutes on node `{{$labels.node}}`.
        As a result, all upstreams were considered unhealthy.
      summary: |
        Too many upstream health check failures in forward plugin.
      severity: "5"
      markupFormat: default
    - name: D8NodeUpdateStuckWaitingForDisruptionApproval
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` detected a new update, requested and received initial approval, and started the update.
        However, it reached a stage that could cause downtime and was unable to obtain disruption approval.
        A disruption approval is normally issued automatically by the `update_approval` hook of the `node-manager` module.

        To resolve this issue, investigate why the approval could not be granted to proceed with the update.
      summary: |
        Node {{ $labels.node }} cannot obtain disruption approval.
      severity: "8"
      markupFormat: markdown
    - name: D8OkmeterAgentPodIsNotReady
      sourceFile: modules/500-okmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-okmeter
      module: okmeter
      edition: ce
      description: ""
      summary: |
        Okmeter agent is not Ready.
      severity: "6"
      markupFormat: markdown
    - name: D8OldPrometheusCustomTargetFormat
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        Deckhouse has detected that services with the `prometheus-custom-target` label are being used to collect metrics in the cluster.

        The label format has been changed. To resolve the issue, replace the `prometheus-custom-target` label with `prometheus.deckhouse.io/custom-target`.

        To list all services labeled with `prometheus-custom-target`, run the following command:

        ```bash
        d8 k get service --all-namespaces --show-labels | grep prometheus-custom-target
        ```

        For more information on metric collection, refer to the [Prometheus module FAQ](https://deckhouse.io/modules/prometheus/faq.html).
      summary: |
        Services with the prometheus-custom-target label are being used for metric collection.
      severity: "9"
      markupFormat: markdown
    - name: D8OldPrometheusTargetFormat
      sourceFile: ee/fe/modules/340-monitoring-applications/monitoring/prometheus-rules/warnings/warnings.yaml
      moduleUrl: 340-monitoring-applications
      module: monitoring-applications
      edition: fe
      description: |-
        Deckhouse has detected that services with the `prometheus-target` label are being used to collect metrics in the cluster.

        The label format has been changed. To resolve the issue, replace the `prometheus-target` label with `prometheus.deckhouse.io/target`.

        To list all services labeled with `prometheus-target`, run the following command:

        ```bash
        d8 k get service --all-namespaces --show-labels | grep prometheus-target
        ```
      summary: |
        Services with the prometheus-target label are being used for metric collection.
      severity: "6"
      markupFormat: markdown
    - name: D8ProblematicNodeGroupConfiguration
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There is a new update available for Nodes in the `{{ $labels.node_group }}` NodeGroup.
        However, Node `{{ $labels.node }}` cannot begin the update.

        The Node is missing the `node.deckhouse.io/configuration-checksum` annotation, which may indicate that its bootstrap process did not complete correctly.

        Troubleshooting options:

        - Check the `cloud-init` log (`/var/log/cloud-init-output.log`) on the node.
        - Check the NodeGroupConfiguration resource associated with the `{{ $labels.node_group }}` NodeGroup for potential issues.
      summary: |
        Node {{ $labels.node }} cannot begin the update.
      severity: "8"
      markupFormat: markdown
    - name: D8PrometheusLongtermFederationTargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/longterm-target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The `prometheus-longterm` instance is unable to scrape the `/federate` endpoint from Prometheus.

        Troubleshooting options:

        - Check the `prometheus-longterm` logs.
        - Open the corresponding web UI to check scrape errors.
      summary: |
        Prometheus-longterm cannot scrape Prometheus.
      severity: "5"
      markupFormat: default
    - name: D8PrometheusLongtermTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The `prometheus-longterm` component is used to display historical monitoring data and is not crucial.
        However, its extended downtime may prevent access to statistics.

        This issue is often caused by problems with disk availability. For example, if the disk cannot be mounted to a Node.

        Troubleshooting steps:

        1. Check the StatefulSet status:

           ```shell
           d8 k -n d8-monitoring describe statefulset prometheus-longterm
           ```

        2. Inspect the PersistentVolumeClaim (if used):

           ```shell
           d8 k -n d8-monitoring describe pvc prometheus-longterm-db-prometheus-longterm-0
           ```

        3. Inspect the Pod's state:

           ```shell
           d8 k -n d8-monitoring describe pod prometheus-longterm-0
           ```
      summary: |
        Prometheus-longterm target is missing in Prometheus.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorPodIsNotReady
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        As a result, new `Prometheus`, `PrometheusRules`, and `ServiceMonitor` configurations cannot be applied in the cluster.
        However, all existing and configured components will continue to operate normally.
        This problem will not affect alerting or monitoring in the short term (for a few days).

        Troubleshooting steps:

        1. Analyze the Deployment details:

           ```shell
           d8 k -n d8-operator-prometheus describe deployment prometheus-operator
           ```

        2. Examine the Pod's to determine why it is not running:

           ```shell
           d8 k -n d8-operator-prometheus describe pod -l app=prometheus-operator
           ```
      summary: |
        The prometheus-operator Pod is NOT Ready.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorPodIsNotRunning
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        As a result, new `Prometheus`, `PrometheusRules`, and `ServiceMonitor` configurations cannot be applied in the cluster.
        However, all existing and configured components will continue to operate normally.
        This problem will not affect alerting or monitoring in the short term (for a few days).

        Troubleshooting steps:

        1. Analyze the Deployment details:

           ```shell
           d8 k -n d8-operator-prometheus describe deployment prometheus-operator
           ```

        2. Examine the Pod's to determine why it is not running:

           ```shell
           d8 k -n d8-operator-prometheus describe pod -l app=prometheus-operator
           ```
      summary: |
        The prometheus-operator Pod is NOT Running.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorTargetAbsent
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |
        As a result, new `Prometheus`, `PrometheusRules`, and `ServiceMonitor` configurations cannot be applied in the cluster.
        However, all existing and configured components will continue to operate normally.
        This problem will not affect alerting or monitoring in the short term (for a few days).

        To resolve the issue, analyze the Deployment details:

        ```shell
        d8 k -n d8-operator-prometheus describe deployment prometheus-operator
        ```
      summary: |
        Prometheus-operator target is missing in Prometheus.
      severity: "7"
      markupFormat: markdown
    - name: D8PrometheusOperatorTargetDown
      sourceFile: modules/200-operator-prometheus/monitoring/prometheus-rules/operator.yaml
      moduleUrl: 200-operator-prometheus
      module: operator-prometheus
      edition: ce
      description: |-
        The `prometheus-operator` Pod is unavailable.

        As a result, new `Prometheus`, `PrometheusRules`, and `ServiceMonitor` configurations cannot be applied in the cluster.
        However, all existing and configured components will continue to operate normally.
        This problem will not affect alerting or monitoring in the short term (for a few days).

        Troubleshooting steps:

        1. Analyze the Deployment details:

           ```shell
           d8 k -n d8-operator-prometheus describe deployment prometheus-operator
           ```

        2. Examine the Pod's to determine why it is not running:

           ```shell
           d8 k -n d8-operator-prometheus describe pod -l app=prometheus-operator
           ```
      summary: |
        Prometheus is unable to scrape prometheus-operator metrics.
      severity: "8"
      markupFormat: markdown
    - name: D8ReservedNodeLabelOrTaintFound
      sourceFile: modules/340-monitoring-custom/monitoring/prometheus-rules/reserved-domain.tpl
      moduleUrl: 340-monitoring-custom
      module: monitoring-custom
      edition: ce
      description: |-
        Deckhouse has detected that node {{ $labels.name }} is using one of the following:
        - A reserved `metadata.labels` object `node-role.deckhouse.io/`, which doesn't end with `(system|frontend|monitoring|_deckhouse_module_name_)`.
        - A reserved `spec.taints` object `dedicated.deckhouse.io`, with a value other than `(system|frontend|monitoring|_deckhouse_module_name_)`.

        For instructions on how to resolve this issue, refer to the [node allocation guide](https://deckhouse.io/modules/node-manager/faq.html#how-do-i-allocate-nodes-to-specific-loads).
      summary: |
        Node {{ $labels.name }} is using a reserved label or taint.
      severity: "6"
      markupFormat: markdown
    - name: D8SecretCopierDeprecatedLabels
      sourceFile: modules/600-secret-copier/monitoring/prometheus-rules/deprecated-label.yaml
      moduleUrl: 600-secret-copier
      module: secret-copier
      edition: ce
      description: |-
        The [`secret-copier` module](https://github.com/deckhouse/deckhouse/tree/main/modules/600-secret-copier/) has changed its labeling approach for original secrets in the `default` namespace.

        The label `antiopa-secret-copier: "yes"` is deprecated and will be removed soon.

        To resolve this issue, replace the label `antiopa-secret-copier: "yes"` with `secret-copier.deckhouse.io/enabled: ""` for all secrets used by the `secret-copier` module in the `default` namespace.
      summary: |
        Obsolete antiopa_secret_copier=yes label detected.
      severity: "9"
      markupFormat: markdown
    - name: D8SmokeMiniNotBoundPersistentVolumeClaims
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The PersistentVolumeClaim `{{ $labels.persistentvolumeclaim }}` is in the `{{ $labels.phase }}` phase.

        This indicates a problem with PersistentVolume provisioning.

        To investigate the cause, check the PersistentVolumeClaim phase:

        ```shell
        d8 k -n d8-upmeter get pvc {{ $labels.persistentvolumeclaim }}
        ```

        If your cluster doesn't have a disk provisioning system,
        you can disable volume ordering for `smoke-mini` using the module settings.
      summary: |
        Smoke-mini has unbound or lost PersistentVolumeClaims.
      severity: "9"
      markupFormat: markdown
    - name: D8TerraformStateExporterClusterStateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The current Kubernetes cluster state is `{{ $labels.status }}` compared to the Terraform state.

        It is important to reconcile the states.

        Troubleshooting steps:

        1. View the differences:

           ```shell
           dhctl terraform check
           ```

        2. Apply the necessary changes to bring the cluster in sync:

           ```shell
           dhctl converge
           ```
      summary: |
        Terraform-state-exporter cluster state change detected.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterClusterStateError
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The `terraform-state-exporter` can't check difference between the Kubernetes cluster state and the Terraform state.

        That was likely caused by `terraform-state-exporter` failing to run Terraform with the current state and configuration.

        Troubleshooting steps:

        1. View the differences:

           ```shell
           dhctl terraform check
           ```

        2. Apply the necessary changes to bring the cluster in sync:

           ```shell
           dhctl converge
           ```
      summary: |
        Terraform-state-exporter cluster state error.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterHasErrors
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Errors occurred during the operation of the `terraform-state-exporter`.

        To get more details, check the Pod logs:

        ```shell
        d8 k -n d8-system logs -l app=terraform-state-exporter -c exporter
        ```
      summary: |
        Terraform-state-exporter has encountered errors.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeStateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The current state of node `{{ $labels.node_group }}/{{ $labels.name }}` is `{{ $labels.status }}` compared to the Terraform state.

        It is important to reconcile the states.

        Troubleshooting steps:

        1. View the differences:

           ```shell
           dhctl terraform check
           ```

        2. Apply the necessary changes to bring the cluster in sync:

           ```shell
           dhctl converge
           ```
      summary: |
        Terraform-state-exporter node state change detected.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeStateError
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The `terraform-state-exporter` can't check the difference between the node `{{ $labels.node_group }}/{{ $labels.name }}` state and the Terraform state.

        Probably, it occurred because `terraform-manager` had failed to run Terraform with the current state and configuration.

        Troubleshooting steps:

        1. View the differences:

           ```shell
           dhctl terraform check
           ```

        2. Apply the necessary changes to bring the cluster in sync:

           ```shell
           dhctl converge
           ```
      summary: |
        Terraform-state-exporter node state error.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterNodeTemplateChanged
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The `terraform-state-exporter` has detected a mismatch between the node template in the cluster provider configuration and the one specified in the NodeGroup {{ $labels.name }}`.

        Node template is `{{ $labels.status }}`.

        Troubleshooting steps:

        1. View the differences:

           ```shell
           dhctl terraform check
           ```

        2. Adjust NodeGroup settings to fix the issue or bring the cluster in sync via the following command:

           ```shell
           dhctl converge
           ```
      summary: |
        Terraform-state-exporter node template change detected.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterPodIsNotReady
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The `terraform-state-exporter` cannot check the difference between the actual Kubernetes cluster state and the Terraform state.

        To resolve the issue, check the following:

        1. Deployment description:

           ```shell
           d8 k -n d8-system describe deployment terraform-state-exporter
           ```

        2. Pod status:

           ```shell
           d8 k -n d8-system describe pod -l app=terraform-state-exporter
           ```
      summary: |
        Terraform-state-exporter Pod is not Ready.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterPodIsNotRunning
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        The `terraform-state-exporter` cannot check the difference between the actual Kubernetes cluster state and the Terraform state.

        To resolve the issue, check the following:

        1. Deployment description:

           ```shell
           d8 k -n d8-system describe deployment terraform-state-exporter
           ```

        2. Pod status:

           ```shell
           d8 k -n d8-system describe pod -l app=terraform-state-exporter
           ```
      summary: |
        Terraform-state-exporter Pod is not Running.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterTargetAbsent
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Prometheus cannot find the `terraform-state-exporter` target.

        To investigate the details:

        - Check the Pod status:

          ```shell
          d8 k -n d8-system get pod -l app=terraform-state-exporter
          ```

        - Check the container logs:

          ```shell
          d8 k -n d8-system logs -l app=terraform-state-exporter -c exporter
          ```
      summary: |
        Terraform-state-exporter target is missing in Prometheus.
      severity: "8"
      markupFormat: markdown
    - name: D8TerraformStateExporterTargetDown
      sourceFile: modules/040-terraform-manager/monitoring/prometheus-rules/terraform-manager/terraform-state-exporter.tpl
      moduleUrl: 040-terraform-manager
      module: terraform-manager
      edition: ce
      description: |
        Prometheus is unable to scrape metrics from the `terraform-state-exporter`.

        To investigate the details:

        - Check the Pod status:

          ```shell
          d8 k -n d8-system get pod -l app=terraform-state-exporter
          ```

        - Check the container logs:

          ```shell
          d8 k -n d8-system logs -l app=terraform-state-exporter -c exporter
          ```
      summary: |
        Prometheus can't scrape terraform-state-exporter.
      severity: "8"
      markupFormat: markdown
    - name: D8TricksterTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The following modules use the Trickster component:

        * `prometheus-metrics-adapter`: Its unavailability means horizontal pod autoscaling (HPA) is not working, and you cannot view resource consumption using `d8 k`.
        * `vertical-pod-autoscaler`: Short-term unavailability for this module is tolerable, as VPA looks at the consumption history for 8 days.
        * `grafana`: All dashboards use Trickster by default to cache Prometheus queries. You can retrieve data directly from Prometheus (bypassing the Trickster). However, this may lead to high memory usage by Prometheus and cause unavailability.

        Troubleshooting steps:

        1. Inspect the Deployment's stats:

           ```shell
           d8 k -n d8-monitoring describe deployment trickster
           ```

        2. Inspect the Pod's stats:

           ```shell
           d8 k -n d8-monitoring describe pod -l app=trickster
           ```

        3. Trickster often becomes unavailable due to Prometheus issues, since its `readinessProbe` depends on Prometheus being accessible.

           Make sure Prometheus is running:

           ```shell
           d8 k -n d8-monitoring describe pod -l app.kubernetes.io/name=prometheus,prometheus=main
           ```
      summary: |
        Trickster target is missing in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: D8TricksterTargetAbsent
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/base.tpl
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The following modules use the Trickster component:

        * `prometheus-metrics-adapter`: Its unavailability means horizontal pod autoscaling (HPA) is not working, and you cannot view resource consumption using `d8 k`.
        * `vertical-pod-autoscaler`: Short-term unavailability for this module is tolerable, as VPA looks at the consumption history for 8 days.
        * `grafana`: All dashboards use Trickster by default to cache Prometheus queries. You can retrieve data directly from Prometheus (bypassing the Trickster). However, this may lead to high memory usage by Prometheus and cause unavailability.

        Troubleshooting steps:

        1. Inspect the Deployment's stats:

           ```shell
           d8 k -n d8-monitoring describe deployment trickster
           ```

        2. Inspect the Pod's stats:

           ```shell
           d8 k -n d8-monitoring describe pod -l app=trickster
           ```

        3. Trickster often becomes unavailable due to Prometheus issues, since its `readinessProbe` depends on Prometheus being accessible.

           Make sure Prometheus is running:

           ```shell
           d8 k -n d8-monitoring describe pod -l app.kubernetes.io/name=prometheus,prometheus=main
           ```
      summary: |
        Trickster target is missing in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: D8UpmeterAgentPodIsNotReady
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: ""
      summary: |
        Upmeter agent is not Ready.
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterAgentReplicasUnavailable
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |-
        Some `upmeter-agent` Pods are not in the Running state.

        To investigate the details, do the following:

        - Check the DaemonSet status:

          ```shell
          d8 k -n d8-upmeter get daemonset upmeter-agent -o json | jq .status
          ```

        - Check the Pod status:

          ```shell
          d8 k -n d8-upmeter get pods -l app=upmeter-agent -o json | jq '.items[] | {(.metadata.name):.status}'
          ```
      summary: |
        One or more upmeter-agent Pods are NOT Running.
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterDiskUsage
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Disk usage for Upmeter has exceeded 80%.

        The only way to resolve this is to recreate the PersistentVolumeClaim (PVC) in the following steps:

        1. Save the PVC data if you need it.

        1. Delete the PVC and restart the `upmeter` Pod:

           ```shell
           d8 k -n d8-upmeter delete persistentvolumeclaim/data-upmeter-0 pod/upmeter-0
           ```

        1. Check the status of the created PVC:

           ```shell
           d8 k -n d8-upmeter get pvc
           ```
      summary: |
        Upmeter disk usage exceeds 80%.
      severity: "5"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageConfigmap
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Probe-generated ConfigMaps were found but not cleaned up as expected.

        `upmeter-agent` should automatically delete ConfigMaps produced by the `control-plane/basic` probe.
        There should be no more ConfigMaps than there are master nodes (since `upmeter-agent` runs as a DaemonSet with a master `nodeSelector`), and they should be deleted within seconds.

        This may indicate:

        - A problem with the `kube-apiserver`.
        - Stale ConfigMaps left by outdated `upmeter-agent` Pods after an Upmeter update.

        Troubleshooting steps:

        1. Check the `upmeter-agent` logs:

           ```shell
           d8 k -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "basic-functionality") | [.time, .level, .msg] | @tsv'
           ```

        2. Ensure the control plane is operating normally.

        3. Delete stale ConfigMaps manually:

           ```shell
           d8 k -n d8-upmeter delete cm -l heritage=upmeter
           ```
      summary: |
        Garbage ConfigMaps from the basic probe are not being cleaned up.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageDeployment
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The average number of probe-generated Deployments per `upmeter-agent` Pod is {{ $value }}.

        `upmeter-agent` should automatically delete Deployments created by the `control-plane/controller-manager` probe.
        There should not be more Deployments than master nodes (since `upmeter-agent` runs as a DaemonSet with a master `nodeSelector`), and they should be deleted within seconds.

        This may indicate:

        - A problem with the `kube-apiserver`.
        - Stale Deployments left by outdated `upmeter-agent` Pods after an Upmeter update.

        Troubleshooting steps:

        1. Check the `upmeter-agent` logs:

           ```shell
           d8 k -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "controller-manager") | [.time, .level, .msg] | @tsv'
           ```

        2. Ensure the control plane is operating normally. Pay close attention to `kube-controller-manager`.

        3. Delete stale Deployments manually:

           ```shell
           d8 k -n d8-upmeter delete deployment -l heritage=upmeter
           ```
      summary: |
        Garbage Deployments from the controller-manager probe are not being cleaned up.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageNamespaces
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: "The average number of probe-created namespaces per `upmeter-agent` Pod is {{ $value }}.\n\n`upmeter-agent` should automatically clean up namespaces created by the `control-plane/namespace` probe.  \nThere should not be more of these namespaces than there are master nodes (since `upmeter-agent` runs as a DaemonSet with a master `nodeSelector`), and they should be deleted within seconds.\n\nThis may indicate:\n\n- A problem with the `kube-apiserver`.\n- Stale namespaces left from older `upmeter-agent` Pods after an Upmeter update.\n\nTroubleshooting steps:\n\n1. Check `upmeter-agent` logs:\n\n   ```shell\n   d8 k -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group==\"control-plane\" and .probe == \"namespace\") | [.time, .level, .msg] | @tsv'\n   ```\n\n2. Ensure the control plane is operating normally.\n\n3. Delete stale namespaces manually:\n\n   ```shell\n   d8 k -n d8-upmeter delete ns -l heritage=upmeter\n   ```\n"
      summary: |
        Garbage namespaces from the namespace probe are not being cleaned up.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbagePods
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: "The average number of probe Pods per `upmeter-agent` Pod is {{ $value }}.\n\n`upmeter-agent` should automatically clean up Pods created by the `control-plane/scheduler` probe.  \nThere should not be more of these Pods than there are master nodes (since `upmeter-agent` runs as a DaemonSet with a master `nodeSelector`), and they should be deleted within seconds.\n\nThis may indicate:\n\n- A problem with the `kube-apiserver`.\n- Stale Pods left from old `upmeter-agent` Pods after an Upmeter update.\n\nTroubleshooting steps:\n\n1. Check `upmeter-agent` logs:\n\n   ```shell\n   d8 k -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group==\"control-plane\" and .probe == \"scheduler\") | [.time, .level, .msg] | @tsv'\n   ```\n\n2. Ensure the control plane is operating normally.\n\n3. Delete stale Pods manually:\n\n   ```shell\n   d8 k -n d8-upmeter delete pods -l upmeter-probe=scheduler\n   ```\n"
      summary: |
        Garbage Pods from the scheduler probe are not being cleaned up.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbagePodsFromDeployments
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The average number of probe Pods per `upmeter-agent` Pod is {{ $value }}.

        `upmeter-agent` is expected to clean Deployments created by the `control-plane/controller-manager` probe,
        and `kube-controller-manager` should remove the associated Pods.
        There should not be more of these Pods than there are master nodes (since `upmeter-agent` runs as a DaemonSet with a master `nodeSelector`), and the Pods should be deleted within seconds.

        This may indicate:

        - A problem with the `kube-apiserver` or `kube-controller-manager`.
        - Stale Pods left from outdated `upmeter-agent` Pods after an Upmeter update.

        Troubleshooting steps:

        1. Check the `upmeter-agent` logs:

           ```shell
           d8 k -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "controller-manager") | [.time, .level, .msg] | @tsv'
           ```

        2. Ensure the control plane is operating normally. Pay close attention to `kube-controller-manager`.

        3. Delete stale Pods manually:

           ```shell
           d8 k -n d8-upmeter delete pods -l upmeter-probe=controller-manager
           ```
      summary: |
        Garbage Pods from the controller-manager probe are not being cleaned up.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterProbeGarbageSecretsByCertManager
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        Probe-generated Secrets were found.

        `upmeter-agent` should clean up certificates created by the `cert-manager` probe, and `cert-manager` in turn should clean up the associated Secrets.
        There should not be more of these Secrets than there are master nodes (since `upmeter-agent` runs as a DaemonSet with a master `nodeSelector`), and they should be deleted within seconds.

        This may indicate:

        - A problem with `kube-apiserver`, `cert-manager`, or `upmeter`.
        - Stale Secrets left from older `upmeter-agent` Pods after an Upmeter update.

        Troubleshooting steps:

        1. Check `upmeter-agent` logs:

           ```shell
           d8 k -n d8-upmeter logs -l app=upmeter-agent --tail=-1 | jq -rR 'fromjson? | select(.group=="control-plane" and .probe == "cert-manager") | [.time, .level, .msg] | @tsv'
           ```

        2. Check that the control plane and `cert-manager` are operating normally.

        3. Delete stale certificates and Secrets manually:

           ```shell
           d8 k -n d8-upmeter delete certificate -l upmeter-probe=cert-manager
           d8 k -n d8-upmeter get secret -ojson | jq -r '.items[] | .metadata.name' | grep upmeter-cm-probe | xargs -n 1 -- d8 k -n d8-upmeter delete secret
           ```
      summary: |
        Garbage Secrets from the cert-manager probe are not being cleaned up.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterServerPodIsNotReady
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: ""
      summary: |
        Upmeter server is not Ready.
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterServerPodIsRestartingTooOften
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The `upmeter` server has restarted {{ $value }} times in the last hour.

        Frequent restarts may indicate an issue.
        It is expected to run continuously and collect availability episodes without interruption.

        To investigate the cause, check the logs:

        ```shell
        d8 k -n d8-upmeter logs -f upmeter-0 upmeter
        ```
      summary: |
        Upmeter server is restarting too frequently.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterServerReplicasUnavailable
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |-
        Some `upmeter` Pods are not in the Running state.

        To investigate the details, do the following:

        - Check the StatefulSet status:

          ```shell
          d8 k -n d8-upmeter get statefulset upmeter -o json | jq .status
          ```

        - Check the Pod status:

          ```shell
          d8 k -n d8-upmeter get pods upmeter-0 -o json | jq '.items[] | {(.metadata.name):.status}'
          ```
      summary: |
        One or more Upmeter server Pods are NOT Running.
      severity: "6"
      markupFormat: markdown
    - name: D8UpmeterSmokeMiniMoreThanOnePVxPVC
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The number of unnecessary `smoke-mini` PersistentVolumes (PVs) is {{ $value }}.

        These PVs should be deleted automatically when released.

        Possible causes:

        - The `smoke-mini` StorageClass may be set to `Retain` by default.
        - There may be issues with the CSI driver or cloud storage integration.

        These PVs do not contain valuable data and should be deleted.

        To list all the PVs, run the following command:

        ```shell
        d8 k get pv | grep disk-smoke-mini
        ```
      summary: |
        Unnecessary smoke-mini PersistentVolumes detected in the cluster.
      severity: "9"
      markupFormat: markdown
    - name: D8UpmeterTooManyHookProbeObjects
      sourceFile: modules/500-upmeter/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 500-upmeter
      module: upmeter
      edition: ce
      description: |
        The average number of `UpmeterHookProbe` objects per `upmeter-agent` Pod is {{ $value }}, but it should always be exactly 1 per agent.

        This likely happened because older `upmeter-agent` Pods left behind their `UpmeterHookProbe` objects during an Upmeter update or downscale.

        Once the cause has been investigated, remove outdated objects and leave only the ones corresponding to currently running `upmeter-agent` Pods.

        To view all `UpmeterHookProbe` objects, run the following command:

        ```shell
        d8 k get upmeterhookprobes.deckhouse.io
        ```
      summary: |
        Too many UpmeterHookProbe objects in the cluster.
      severity: "9"
      markupFormat: markdown
    - name: D8YandexNatInstanceConnectionsQuotaUtilization
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/nat-instance.tpl
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: "The connection quota for the Yandex NAT instance has exceeded 85% utilization over the past 5 minutes. \n\nTo prevent potential issues, contact Yandex technical support and request an increase in the connection quota.\n"
      summary: |
        Connection quota utilization of the Yandex NAT instance exceeds 85% over the last 5 minutes.
      severity: "4"
      markupFormat: markdown
    - name: DaemonSetAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DaemonSetUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The DaemonSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeadMansSwitch
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: This is a dead man's switch meant to ensure that the entire Alerting pipeline is functional.
      summary: |
        Alerting dead man's switch.
      severity: "4"
      markupFormat: default
    - name: DeckhouseEditionNotFound
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Specify a Deckhouse Kubernetes Platform edition in `{{ $labels.module }}` ModuleConfig.

        ```bash
        d8 k patch moduleconfig {{ $labels.module }} --type merge --patch '{"spec": {"settings": {"license": {"edition": "DKP_EDITION"}}}}'
        ```

        Where `DKP_EDITION` is the edition you want to use: `CE`, `BE`, `EE`, `SE`, or `SE-plus`.
      summary: |
        Incorrect Deckhouse Kubernetes Platform edition.
      severity: "6"
      markupFormat: markdown
    - name: DeckhouseHighMemoryUsage
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse pod `{{ $labels.pod }}` on node `{{ $labels.node }}` has high memory usage.

        Please, if possible, get the dumps for debugging purposes (it will take around 30 seconds):

        ```bash
        curl -s "http://$(d8 k -n d8-system get pod {{ $labels.pod }} -o jsonpath='{.status.hostIP}'):4222/debug/pprof/heap" -o /tmp/heap.pprof
        curl -s "http://$(d8 k -n d8-system get pod {{ $labels.pod }} -o jsonpath='{.status.hostIP}'):4222/debug/pprof/goroutine" -o /tmp/goroutine.pprof
        curl -s "http://$(d8 k -n d8-system get pod {{ $labels.pod }} -o jsonpath='{.status.hostIP}'):4222/debug/pprof/profile?seconds=30" -o /tmp/cpu.pprof
        ```

        and send these files (/tmp/heap.pprof, /tmp/cpu.pprof and /tmp/goroutine.pprof) to the support team.
      summary: |
        Deckhouse memory usage is too high.
      severity: "3"
      markupFormat: markdown
    - name: DeckhouseMigratedModuleNotFound
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module `{{ $labels.module_name }}` specified in DeckhouseRelease was moved to external source and not found in any available ModuleSource registry. DeckhouseRelease will be lock until found module in any available ModuleSource.

        This may indicate:
        - The module source registry is not properly configured.
        - The module has not been published to the registry yet.
        - There is a network issue preventing access to the registry.

        Please check:
        1. ModuleSource configurations: `d8 k get modulesources`.
        2. Registry accessibility and credentials.
        3. Module availability in the configured registries.

        To investigate, run the following commands:

        ```bash
        # Check ModuleSource status
        d8 k get modulesources

        # Check available modules in all sources
        d8 k -n d8-system exec -it deployment/deckhouse -c deckhouse -- deckhouse-controller registry get sources
        d8 k -n d8-system exec -it deployment/deckhouse -c deckhouse -- deckhouse-controller registry get modules <source_name>

        # Check Deckhouse logs for more details
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
      summary: |
        Migrated module {{ $labels.module_name }} not found in registry.
      severity: "6"
      markupFormat: markdown
    - name: DeckhouseModuleUpdatingFailedBrokenSequence
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The '{{ $labels.module }}' Module update has failed.

        Current version: {{ $labels.actual_version }}.
        Desired version: {{ $labels.version }}.

        Attempt to download interim releases failed.

        Possible reasons:
         - The necessary versions of the Module image is not available in the registry.

        To resolve this issue, ensure that all minor versions of the '{{ $labels.module }}' Module image between {{ $labels.actual_version }} and {{ $labels.version }} is available in the registry '{{ $labels.registry }}'.
      summary: |
        Module update has failed. Updating sequence is broken.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseModuleUpdatingFailedModuleIsNotValid
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The '{{ $labels.module }}' Module update has failed.

        Desired version: {{ $labels.version }}.

        Possible reasons:
         - The Module image is corrupted.

        To resolve this issue, ensure that {{ $labels.version }} image in the registry '{{ $labels.registry }}' is valid .
      summary: |
        Module update has failed. Module is not valid.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseModuleUseEmptyDir
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/emptydir.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The Deckhouse module `{{ $labels.module_name }}` is using `emptyDir` as its storage.

        If the associated Pod is removed from the node for any reason, the data in the `emptyDir` is deleted permanently.
        Consider using persistent storage if data durability is critical for the module.
      summary: |
        Deckhouse module {{ $labels.module_name }} is using emptyDir for storage.
      severity: "9"
      markupFormat: markdown
    - name: DeckhouseReleaseDisruptionApprovalRequired
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The new Deckhouse release includes a disruptive update that requires manual approval.

        To check the details, run the following command:

        ```bash
        d8 k describe DeckhouseRelease {{ $labels.name }}
        ```

        To approve the disruptive update, run the following command:

        ```bash
        d8 k annotate DeckhouseRelease {{ $labels.name }} release.deckhouse.io/disruption-approved=true
        ```
      summary: |
        Deckhouse release disruption approval required.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseReleaseIsBlocked
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The requirements for the Deckhouse release haven't been met.

        To check the details, run the following command:

        ```bash
        d8 k describe DeckhouseRelease {{ $labels.name }}
        ```
      summary: |
        Deckhouse release requirements haven't been met.
      severity: "5"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new Deckhouse release is available but requires manual approval before it can be applied.

        To approve the release, run the following command:

        ```bash
        d8 k patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'
        ```
      summary: |
        A new Deckhouse release is awaiting manual approval.
      severity: "3"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new Deckhouse release is available but requires manual approval before it can be applied.

        To approve the release, run the following command:

        ```bash
        d8 k patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'
        ```
      summary: |
        A new Deckhouse release is awaiting manual approval.
      severity: "6"
      markupFormat: markdown
    - name: DeckhouseReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new Deckhouse release is available but requires manual approval before it can be applied.

        To approve the release, run the following command:

        ```bash
        d8 k patch DeckhouseRelease {{ $labels.name }} --type=merge -p='{"approved": true}'
        ```
      summary: |
        A new Deckhouse release is awaiting manual approval.
      severity: "9"
      markupFormat: markdown
    - name: DeckhouseReleaseNotificationNotSent
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The Deckhouse release notification webhook failed to send.

        To check the notification webhook address, run the following command:

        ```bash
        d8 k get mc deckhouse -o yaml
        ```
      summary: |
        Deckhouse release notification webhook hasn't been sent.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseUpdating
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: ""
      summary: |
        Deckhouse is being updated to {{ $labels.deployingRelease }}.
      severity: "4"
      markupFormat: markdown
    - name: DeckhouseUpdatingFailed
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/deckhouse.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The Deckhouse update has failed.

        Possible reasons:

        - The next minor or patch version of the Deckhouse image is not available in the registry.
        - The Deckhouse image is corrupted.

        Current version: {{ $labels.version }}.

        To resolve this issue, ensure that the next version of the Deckhouse image is available in the registry.
      summary: |
        Deckhouse update has failed.
      severity: "4"
      markupFormat: markdown
    - name: DeploymentAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentGenerationMismatch
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kube-state-metrics.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The observed deployment generation doesn't match the expected one for Deployment `{{$labels.namespace}}/{{$labels.deployment}}`.
      summary: |
        Deployment is outdated.
      severity: "4"
      markupFormat: default
    - name: DeploymentImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeploymentUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The Deployment `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: DeprecatedGeoIPVersion
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/deprecated-geoip-version.tpl
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        An IngressNginxController and/or Ingress object in the cluster is using variables from the deprecated NGINX GeoIPv1 module. Support for this module has been discontinued in Ingress NGINX Controller version 1.10 and higher.

        It's recommended that you update your configuration to use the [GeoIPv2 module](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-geoip2).

        To get a list of the IngressNginxControllers using GeoIPv1 variables, run the following command:

        ```shell
        d8 k get ingressnginxcontrollers.deckhouse.io -o json | jq '.items[] | select(..|strings | test("\\$geoip_(country_(code3|code|name)|area_code|city_continent_code|city_country_(code3|code|name)|dma_code|latitude|longitude|region|region_name|city|postal_code|org)([^_a-zA-Z0-9]|$)+")) | .metadata.name'
        ```

        To get a list of the Ingress objects using GeoIPv1 variables, run the following command:

        ```shell
        d8 k get ingress -A -o json | jq '.items[] | select(..|strings | test("\\$geoip_(country_(code3|code|name)|area_code|city_continent_code|city_country_(code3|code|name)|dma_code|latitude|longitude|region|region_name|city|postal_code|org)([^_a-zA-Z0-9]|$)+")) | "\(.metadata.namespace)/\(.metadata.name)"' | sort | uniq
        ```
      summary: |
        Deprecated GeoIP version 1 is used in the cluster.
      severity: "9"
      markupFormat: markdown
    - name: EarlyOOMPodIsNotReady
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/early-oom.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        The Pod {{$labels.pod}} detected that the Pressure Stall Information (PSI) subsystem is unavailable.

        For details, check the logs:

        ```shell
        d8 k -n d8-cloud-instance-manager logs {{$labels.pod}}
        ```

        Troubleshooting options:

        - Upgrade the Linux kernel to version 4.20 or higher.
        - Enable the [Pressure Stall Information](https://docs.kernel.org/accounting/psi.html).
        - [Disable early OOM](https://deckhouse.io/products/kubernetes-platform/documentation/v1/modules/node-manager/configuration.html#parameters-earlyoomenabled).
      summary: |
        Pod {{$labels.pod}} has detected an unavailable PSI subsystem.
      severity: "8"
      markupFormat: markdown
    - name: EbpfExporterKernelNotSupported
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/ebpf-exporter.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Possible options to resolve the issue:

        * Build the kernel with [BTF type information](https://github.com/libbpf/libbpf?tab=readme-ov-file#bpf-co-re-compile-once--run-everywhere).
        * [Disable ebpf_exporter](https://deckhouse.io/modules/monitoring-kubernetes/configuration.html#parameters-ebpfexporterenabled).
      summary: |
        The BTF module required for ebpf_exporter is missing from the kernel.
      severity: "8"
      markupFormat: markdown
    - name: ExtendedMonitoringTargetDown
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/self.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        The pod running `extended-monitoring-exporter` is currently unavailable.

        As a result, the following alerts will not be triggered:

        * Low disk space and inode usage on volumes.
        * CPU overloads and container throttling.
        * `500` errors on Ingress.
        * Insufficient replicas of Deployments, StatefulSets, and DaemonSets.
        * [Other alerts](https://deckhouse.io/modules/extended-monitoring/) associated with this exporter.

        To resolve this issue, investigate its possible causes:

        1. Print detailed information about the `extended-monitoring-exporter` deployment:

           ```bash
           d8 k -n d8-monitoring describe deploy extended-monitoring-exporter
           ```

        2. Print detailed information about the pods associated with the `extended-monitoring-exporter`:

           ```bash
           d8 k -n d8-monitoring describe pod -l app=extended-monitoring-exporter
           ```
      summary: |
        Extended monitoring is unavailable.
      severity: "5"
      markupFormat: markdown
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.instance }}` is expected to exhaust its available file/socket descriptors within the next hour.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.instance }} are almost exhausted.
      severity: "3"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }}` is expected to exhaust its available file/socket descriptors within the next hour.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} are almost exhausted.
      severity: "3"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.instance }}` is expected to exhaust its available file/socket descriptors within the next 4 hours.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.instance }} are exhausting soon.
      severity: "4"
      markupFormat: default
    - name: FdExhaustionClose
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/general.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The instance `{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }}` is expected to exhaust its available file/socket descriptors within the next 4 hours.
      summary: |
        File descriptors for {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} are exhausting soon.
      severity: "4"
      markupFormat: default
    - name: GeoIPDownloadErrorDetected
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The controller `{{ $labels.controller }}` failed to download the GeoIP database.

        **Reason:** `{{ $labels.reason }}`

        **Type:** `{{ $labels.type }}`

        If this issue persists, investigate network connectivity or database availability.
      summary: |
        GeoIP DB download error in controller {{ $labels.controller }}.
      severity: "4"
      markupFormat: markdown
    - name: GrafanaDashboardAlertRulesDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before upgrading to Grafana 10, migrate outdated alerts from Grafana to an external alertmanager (or `exporter-alertmanager` stack).

        To list all deprecated alert rules, use the following expression:

        ```prometheus
        sum by (dashboard, panel, alert_rule) (d8_grafana_dashboards_deprecated_alert_rule) > 0
        ```

        Note that this check runs once per hour, so the alert may take up to an hour to clear after deprecated resources are migrated.
      summary: |
        Deprecated Grafana alerts detected.
      severity: "8"
      markupFormat: markdown
    - name: GrafanaDashboardPanelIntervalDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before upgrading to Grafana 10, update outdated panel expressions that use `$interval_rv`, `interval_sx3`, or `interval_sx4` to the new variable `$__rate_interval`.

        To list all deprecated panel intervals, use the following expression:

        ```prometheus
        sum by (dashboard, panel, interval) (d8_grafana_dashboards_deprecated_interval) > 0
        ```

        Note that this check runs once per hour, so the alert may take up to an hour to clear after deprecated intervals are removed.
      summary: |
        Deprecated Grafana panel intervals detected.
      severity: "8"
      markupFormat: markdown
    - name: GrafanaDashboardPluginsDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Before upgrading to Grafana 10, make sure that the currently installed plugins will work correctly with Grafana 10.

        To list all potentially outdated plugins, use the following expression:

        ```prometheus
        sum by (dashboard, panel, plugin) (d8_grafana_dashboards_deprecated_plugin) > 0
        ```

        The `flant-statusmap-panel` plugin is deprecated and no longer supported.
        It's recommended that you migrate to the [state timeline plugin](https://grafana.com/docs/grafana/latest/panels-visualizations/visualizations/state-timeline/) instead.

        Note that this check runs once per hour, so the alert may take up to an hour to clear after deprecated resources are migrated.
      summary: |
        Deprecated Grafana plugins detected.
      severity: "8"
      markupFormat: markdown
    - name: HelmReleasesHasResourcesWithDeprecatedVersions
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/helm/deprecated-versions.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        To list all affected resources, run the following Prometheus query:

        ```prometheus
        max by (helm_release_namespace, helm_release_name, helm_version, resource_namespace, resource_name, api_version, kind, k8s_version) (resource_versions_compatibility) == 1
        ```

        For more details on upgrading deprecated resources, refer to the Kubernetes deprecation guide available at `https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v{{ $labels.k8s_version | reReplaceAll "\\." "-" }}`.

        Note that the check runs once per hour, so this alert should resolve within an hour after migrating deprecated resources.
      summary: |
        At least one Helm release contains resources with deprecated apiVersion, which will be removed in Kubernetes version {{ $labels.k8s_version }}.
      severity: "5"
      markupFormat: markdown
    - name: HelmReleasesHasResourcesWithUnsupportedVersions
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/helm/deprecated-versions.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        To list all affected resources, run the following Prometheus query:

        ```prometheus
        max by (helm_release_namespace, helm_release_name, helm_version, resource_namespace, resource_name, api_version, kind, k8s_version) (resource_versions_compatibility) == 2
        ```

        For more details on migrating deprecated resources, refer to the Kubernetes deprecation guide available at `https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v{{ $labels.k8s_version | reReplaceAll "\\." "-" }}`.

        Note that the check runs once per hour, so this alert should resolve within an hour after migrating deprecated resources.
      summary: |
        At least one Helm release contains resources with unsupported apiVersion for Kubernetes version {{ $labels.k8s_version }}.
      severity: "4"
      markupFormat: markdown
    - name: IngressResponses5xx
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/ingress.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that URL {{$labels.vhost}}{{$labels.location}} on Ingress `{{$labels.ingress}}`, using service `{{$labels.service}}` on port {{$labels.service_port}} has more than {{ printf "extended_monitoring_ingress_threshold{threshold=\"5xx-critical\", namespace=\"%s\", ingress=\"%s\"}" $labels.namespace $labels.ingress | query | first | value }}% of `5xx` responses from the backend.

        Current rate of `5xx` responses: {{ .Value }}%
      summary: |
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} has more than {{ printf &quot;extended_monitoring_ingress_threshold{threshold=&quot;5xx-critical&quot;, namespace=&quot;%s&quot;, ingress=&quot;%s&quot;}&quot; $labels.namespace $labels.ingress | query | first | value }}% of 5xx responses from the backend.
      severity: "4"
      markupFormat: default
    - name: IngressResponses5xx
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/ingress.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that URL {{$labels.vhost}}{{$labels.location}} on Ingress `{{$labels.ingress}}`, using service `{{$labels.service}}` on port {{$labels.service_port}}, has more than {{ printf "extended_monitoring_ingress_threshold{threshold=\"5xx-warning\", namespace=\"%s\", ingress=\"%s\"}" $labels.namespace $labels.ingress | query | first | value }}% of `5xx` responses from the backend.

        Current rate of `5xx` responses: {{ .Value }}%
      summary: |
        URL {{$labels.vhost}}{{$labels.location}} on Ingress {{$labels.ingress}} has more than {{ printf &quot;extended_monitoring_ingress_threshold{threshold=&quot;5xx-warning&quot;, namespace=&quot;%s&quot;, ingress=&quot;%s&quot;}&quot; $labels.namespace $labels.ingress | query | first | value }}% of 5xx responses from the backend.
      severity: "5"
      markupFormat: default
    - name: IstioIrrelevantExternalServiceFound
      sourceFile: modules/110-istio/monitoring/prometheus-rules/services.yaml
      moduleUrl: 110-istio
      module: istio
      edition: ce
      description: |
        A service `{{$labels.name}}` in the `{{$labels.namespace}}` namespace has an irrelevant port specification.

        The `.spec.ports[]` field isn't applicable for services of the `ExternalName` type.
        However, Istio renders port listeners for external services as `0.0.0.0:port`, which captures all traffic to the specified port. This can cause problems for services that aren't registered in the Istio registry.

        To resolve the issue, remove the `.spec.ports` section from the service configuration. It is safe.
      summary: |
        External service found with irrelevant ports specifications.
      severity: "5"
      markupFormat: markdown
    - name: K8SApiserverDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: No API servers are reachable, or they have all disappeared from service discovery.
      summary: |
        API servers can't be reached.
      severity: "3"
      markupFormat: default
    - name: K8sCertificateExpiration
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: "Some clients are connecting to {{$labels.component}} with certificates that will expire in less than a day on node `{{$labels.component}}`.\n\nTo check control plane certificates, use kubeadm:\n\n1. Install kubeadm using the following command:\n   \n   ```bash\n   apt install kubeadm=1.24.*\n   ```\n\n2. Check certificates:\n\n   ```bash\n   kubeadm alpha certs check-expiration\n   ```\n\nTo check kubelet certificates, do the following on each node:\n\n1. Check kubelet configuration:\n\n   ```bash\n   ps aux \\\n     | grep \"/usr/bin/kubelet\" \\\n     | grep -o -e \"--kubeconfig=\\S*\" \\\n     | cut -f2 -d\"=\" \\\n     | xargs cat\n   ```\n\n2. Locate the `client-certificate` or `client-certificate-data` field.\n3. Check certificate expiration using OpenSSL.\n\nNote that there are no tools to find other stale kubeconfig files.\nConsider enabling the `control-plane-manager` module for advanced debugging.\n"
      summary: |
        Kubernetes has API clients with soon-to-expire certificates.
      severity: "5"
      markupFormat: markdown
    - name: K8sCertificateExpiration
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kubernetes.tpl
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: "Some clients are connecting to {{$labels.component}} with certificates that will expire in less than 7 days on node `{{$labels.node}}`.\n\nTo check control plane certificates, use kubeadm:\n\n1. Install kubeadm using the following command:\n   \n   ```bash\n   apt install kubeadm=1.24.*\n   ```\n\n2. Check certificates:\n\n   ```bash\n   kubeadm alpha certs check-expiration\n   ```\n\nTo check kubelet certificates, do the following on each node:\n\n1. Check kubelet configuration:\n\n   ```bash\n   ps aux \\\n     | grep \"/usr/bin/kubelet\" \\\n     | grep -o -e \"--kubeconfig=\\S*\" \\\n     | cut -f2 -d\"=\" \\\n     | xargs cat\n   ```\n\n2. Locate the `client-certificate` or `client-certificate-data` field.\n3. Check certificate expiration using OpenSSL.\n\nNote that there are no tools to find other stale kubeconfig files.\nConsider enabling the `control-plane-manager` module for advanced debugging.\n"
      summary: |
        Kubernetes has API clients with soon-to-expire certificates.
      severity: "6"
      markupFormat: markdown
    - name: K8SControllerManagerTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-controller-manager.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: There is no running kube-controller-manager. As a result, deployments and replication controllers are not progressing.
      summary: |
        Controller manager is down.
      severity: "3"
      markupFormat: default
    - name: K8SKubeletDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus failed to scrape {{ $value }}% of kubelets.
      summary: |
        Multiple kubelets couldn't be scraped.
      severity: "3"
      markupFormat: default
    - name: K8SKubeletDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus failed to scrape {{ $value }}% of kubelets.
      summary: |
        Several kubelets couldn't be scraped.
      severity: "4"
      markupFormat: default
    - name: K8SKubeletTooManyPods
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: The kubelet on node {{ $labels.node }} is running {{ $value }} pods, which is close to the limit of {{ printf "kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\",unit=\"integer\",node=\"%s\"}" $labels.node | query | first | value }}.
      summary: |
        The kubelet on node {{ $labels.node }} is approaching the pod limit.
      severity: "7"
      markupFormat: default
    - name: K8SManyNodesNotReady
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: '{{ $value }}% of Kubernetes nodes are not ready.'
      summary: |
        Too many nodes are not ready.
      severity: "3"
      markupFormat: default
    - name: K8SNodeNotReady
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kubelet.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The kubelet on node {{ $labels.node }} has either failed to check in with the API server or has set itself to `NotReady` for more than 10 minutes.
      summary: |
        The status of node {{ $labels.node }} is NotReady.
      severity: "3"
      markupFormat: default
    - name: K8SSchedulerTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-scheduler.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        The Kubernetes scheduler is not running.
        As a result, new pods are not being assigned to nodes.
      summary: |
        Scheduler is down.
      severity: "3"
      markupFormat: default
    - name: K8STooManyNodes
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/nodes.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        The cluster is currently running {{ $value }} nodes,
        which is close to the maximum of {{ print "d8_max_nodes_amount{}" | query | first | value }} allowed nodes.
      summary: |
        Node count is approaching the maximum allowed.
      severity: "7"
      markupFormat: default
    - name: KubeEtcdHighFsyncDurations
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        In the last 15 minutes, the 99th percentile of the fsync duration for WAL files exceeded 0.5 seconds: {{ $value }}.

        Possible causes:

        - High disk latency where etcd data is stored.
        - High CPU usage on the node.
      summary: |
        Syncing (fsync) WAL files to disk is slow.
      severity: "7"
      markupFormat: markdown
    - name: KubeEtcdHighNumberOfLeaderChanges
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        There have been {{ $value }} leader re-elections for the etcd cluster member running on node `{{ $labels.node }}` in the last 10 minutes.

        Possible causes:

        - High disk latency where etcd data is stored.
        - High CPU usage on the node.
        - Degradation of network connectivity between cluster members in the multi-master mode.
      summary: |
        The etcd cluster is re-electing the leader too frequently.
      severity: "5"
      markupFormat: markdown
    - name: KubeEtcdInsufficientMembers
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        The etcd cluster has too few members, increasing the risk of failure if another member becomes unavailable.
        To resolve this issue, check the status of etcd Pods:
        ```bash d8 k -n kube-system get pod -l component=etcd ```
      summary: |
        Insufficient members in the etcd cluster.
      severity: "4"
      markupFormat: markdown
    - name: KubeEtcdNoLeader
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        To resolve this issue, check the status of the etcd Pods:
        ```bash d8 k -n kube-system get pod -l component=etcd | grep {{ $labels.node }} ```
      summary: |
        The etcd cluster member running on node {{ $labels.node }} has lost the leader.
      severity: "4"
      markupFormat: markdown
    - name: KubeEtcdTargetAbsent
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Steps to troubleshoot:
        1. Check the status of the etcd Pods:

           ```bash
           d8 k -n kube-system get pod -l component=etcd
           ```

        1. Review Prometheus logs:

           ```bash
           d8 k -n d8-monitoring logs -l app.kubernetes.io/name=prometheus -c prometheus
           ```
      summary: |
        There is no etcd target in Prometheus.
      severity: "5"
      markupFormat: markdown
    - name: KubeEtcdTargetDown
      sourceFile: modules/340-monitoring-kubernetes-control-plane/monitoring/prometheus-rules/kube-etcd3.yaml
      moduleUrl: 340-monitoring-kubernetes-control-plane
      module: monitoring-kubernetes-control-plane
      edition: ce
      description: |
        Steps to troubleshoot:
        1. Check the status of the etcd Pods:

           ```bash
           d8 k -n kube-system get pod -l component=etcd
           ```

        1. Review Prometheus logs:

           ```bash
           d8 k -n d8-monitoring logs -l app.kubernetes.io/name=prometheus -c prometheus
           ```
      summary: |
        Prometheus is unable to scrape etcd metrics.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free space remaining on imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The imagefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletImageFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_imagefs_bytes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free inodes remaining on imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The imagefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_imagefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletImageFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of imagefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_imagefs_inodes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of imagefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free space remaining on nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The nodefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_bytes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletNodeFSBytesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_nodefs_bytes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: ""
      summary: |
        No free inodes remaining on nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "5"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Hard eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Hard eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "6"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The nodefs usage on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is nearing the hard eviction threshold.

        Hard eviction threshold: {{ printf "kubelet_eviction_nodefs_inodes{type=\"hard\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Approaching hard eviction threshold of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}}.
      severity: "7"
      markupFormat: markdown
    - name: KubeletNodeFSInodesUsage
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-disk-usage.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        Soft eviction of nodefs on node `{{$labels.node}}` at mountpoint `{{$labels.mountpoint}}` is currently in progress.

        Soft eviction threshold: {{ printf "kubelet_eviction_nodefs_inodes{type=\"soft\", node=\"%s\", mountpoint=\"%s\"}" $labels.node $labels.mountpoint | query | first | value }}%

        Current usage: {{ .Value }}%
      summary: |
        Soft eviction of nodefs on node {{$labels.node}} at mountpoint {{$labels.mountpoint}} is in progress.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesCoreDNSHasCriticalErrors
      sourceFile: modules/042-kube-dns/monitoring/prometheus-rules/kubernetes/dns.yaml
      moduleUrl: 042-kube-dns
      module: kube-dns
      edition: ce
      description: |-
        Deckhouse has detected at least one critical error in the CoreDNS pod {{$labels.pod}}.

        To resolve the issue, review the container logs:

        ```bash
        d8 k -n kube-system logs {{$labels.pod}}
        ```
      summary: |
        Critical errors found in CoreDNS.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDaemonSetNotUpToDate
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected {{ .Value }} outdated pods in DaemonSet `{{ $labels.namespace }}/{{ $labels.daemonset }}` over the last 15 minutes.

        Steps to resolve:

        1. Check the DaemonSet's status:

           ```bash
           d8 k -n {{ $labels.namespace }} get ds {{ $labels.daemonset }}
           ```

        2. Analyze the DaemonSet's description:

           ```bash
           d8 k -n {{ $labels.namespace }} describe ds {{ $labels.daemonset }}
           ```

        3. If the parameter `Number of Nodes Scheduled with Up-to-date Pods` does not match `Current Number of Nodes Scheduled`, check the DaemonSet's `updateStrategy`:

           ```bash
           d8 k -n {{ $labels.namespace }} get ds {{ $labels.daemonset }} -o json | jq '.spec.updateStrategy'
           ```

           If `updateStrategy` is set to `OnDelete`, the DaemonSet is updated only when pods are deleted.
      summary: |
        There were {{ .Value }} outdated pods in DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} over the last 15 minutes.
      severity: "9"
      markupFormat: markdown
    - name: KubernetesDaemonSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas remaining in DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        d8 k -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        No available replicas remaining in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}}.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDaemonSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that the number of unavailable replicas in DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}` exceeds the threshold.

        - Current number: `{{ .Value }}` unavailable replica(s).
        - Threshold number: `{{ printf "extended_monitoring_daemonset_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", daemonset=\"%s\"}" $labels.namespace $labels.daemonset | query | first | value }}` unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        d8 k -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        The number of unavailable replicas in DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} exceeds the threshold.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesDeploymentReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas remaining in deployment `{{$labels.namespace}}/{{$labels.deployment}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"Deployment\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        No available replicas remaining in deployment {{$labels.namespace}}/{{$labels.deployment}}.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesDeploymentReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that the number of unavailable replicas in deployment `{{$labels.namespace}}/{{$labels.deployment}}` exceeds the value set in `spec.strategy.rollingupdate.maxunavailable`.

        - Current number: `{{ .Value }}` unavailable replica(s).
        - Threshold number: `{{ printf "extended_monitoring_deployment_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", deployment=\"%s\"}" $labels.namespace $labels.deployment | query | first | value }}` unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"Deployment\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        The number of unavailable replicas in deployment {{$labels.namespace}}/{{$labels.deployment}} exceeds spec.strategy.rollingupdate.maxunavailable.
      severity: "6"
      markupFormat: markdown
    - name: KubernetesDnsTargetDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/kube-dns.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Prometheus is unable to collect metrics from `kube-dns`, which makes its status unknown.

        Steps to troubleshoot:

        1. Check the deployment details:

           ```bash
           d8 k -n kube-system describe deployment -l k8s-app=kube-dns
           ```

        2. Check the pod details:

           ```bash
           d8 k -n kube-system describe pod -l k8s-app=kube-dns
           ```
      summary: |
        Kube-dns or CoreDNS are not being monitored.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesStatefulSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that there are no ready replicas remaining in StatefulSet `{{$labels.namespace}}/{{$labels.statefulset}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"StatefulSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        No ready replicas remaining in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}}.
      severity: "5"
      markupFormat: markdown
    - name: KubernetesStatefulSetReplicasUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/application-controllers.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that the number of unavailable replicas in StatefulSet `{{$labels.namespace}}/{{$labels.statefulset}}` exceeds the threshold.

        - Current number: `{{ .Value }}` unavailable replica(s).
        - Threshold number: `{{ printf "extended_monitoring_statefulset_threshold{threshold=\"replicas-not-ready\", namespace=\"%s\", statefulset=\"%s\"}" $labels.namespace $labels.statefulset | query | first | value }}` unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_ready{namespace=\"%s\", condition!=\"true\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"StatefulSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.deployment | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        The number of unavailable replicas in StatefulSet {{$labels.namespace}}/{{$labels.statefulset}} exceeds the threshold.
      severity: "6"
      markupFormat: markdown
    - name: KubeStateMetricsDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/kube-state-metrics.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Deckhouse has detected that no metrics about cluster resources have been available for the past 5 minutes.
        As a result, most alerts and monitoring panels aren't working.

        Steps to troubleshoot:

        1. Check the `kube-state-metrics` pods:

           ```bash
           d8 k -n d8-monitoring describe pod -l app=kube-state-metrics
           ```

        2. Check the deployment logs:

           ```bash
           d8 k -n d8-monitoring describe deploy kube-state-metrics
           ```
      summary: |
        Kube-state-metrics isn't working in the cluster.
      severity: "3"
      markupFormat: markdown
    - name: L2LoadBalancerOrphanServiceFound
      sourceFile: ee/se/modules/380-metallb/monitoring/prometheus-rules/services.yaml
      moduleUrl: 380-metallb
      module: metallb
      edition: se
      description: |
        The cluster contains an orphaned service `{{$labels.name}}` in the `{{$labels.namespace}}` namespace  with an irrelevant L2LoadBalancer name.

        To resolve this issue, verify the L2LoadBalancer name specified in the annotation `network.deckhouse.io/l2-load-balancer-name`.
      summary: |
        Orphaned service with an irrelevant L2LoadBalancer name has been found.
      severity: "4"
      markupFormat: markdown
    - name: LoadAverageHigh
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Over the last 5 minutes, the average load on node `{{ $labels.node }}` has been higher than {{ printf "extended_monitoring_node_threshold{threshold=\"load-average-per-core-critical\", node=\"%s\"}" $labels.node | query | first | value }} per core.

        There are more processes in the queue than the CPU can handle.

        Possible causes:

        - A process has created too many threads or child processes.
        - The CPU is overloaded.
      summary: |
        Average load on node {{ $labels.node }} is too high.
      severity: "4"
      markupFormat: markdown
    - name: LoadAverageHigh
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Over the last 30 minutes, the average load on node `{{ $labels.node }}` has been higher than or equal to {{ printf "extended_monitoring_node_threshold{threshold=\"load-average-per-core-warning\", node=\"%s\"}" $labels.node | query | first | value }} per core.

        There are more processes in the queue than the CPU can handle.

        Possible causes:

        - A process has created too many threads or child processes.
        - The CPU is overloaded.
      summary: |
        Average load on node {{ $labels.node }} is too high.
      severity: "5"
      markupFormat: markdown
    - name: LoadBalancerServiceWithoutExternalIP
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/loadbalancer.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        One or more services of the `LoadBalancer` type have not received an external address.

        To list affected services, run the following command:

        ```bash
        d8 k get svc -Ao json | jq -r '.items[] | select(.spec.type == "LoadBalancer") | select(.status.loadBalancer.ingress[0].ip == null) | "namespace: \(.metadata.namespace), name: \(.metadata.name), ip: \(.status.loadBalancer.ingress[0].ip)"'
        ```

        Steps to troubleshoot:

        - Check the `cloud-controller-manager` logs in the `d8-cloud-provider-*` namespace.
        - If you are using a bare-metal cluster with the `metallb` module enabled, ensure the address pool has not been exhausted.
      summary: |
        A LoadBalancer has not been created.
      severity: "4"
      markupFormat: default
    - name: LokiInsufficientDiskForRetention
      sourceFile: modules/462-loki/monitoring/prometheus-rules/alerts.tpl
      moduleUrl: 462-loki
      module: loki
      edition: ce
      description: |-
        Not enough disk space to retain logs for 168 hours. Current effective retention period is {{ $value }} hours.

        You need either decrease expected `retentionPeriodHours` or increase resize Loki PersistentVolumeClaim
      summary: |
        Not enough disk space to retain logs for 168 hours
      severity: "4"
      markupFormat: markdown
    - name: ModuleAtConflict
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse has detected conflicting sources for the {{ $labels.moduleName }} module.

        To resolve this issue, specify the correct source in the module configuration.
      summary: |
        Conflict detected for module {{ $labels.moduleName }}.
      severity: "4"
      markupFormat: markdown
    - name: ModuleConfigDeprecated
      sourceFile: modules/810-documentation/monitoring/prometheus-rules/deprecated-mc.yaml
      moduleUrl: 810-documentation
      module: documentation
      edition: ce
      description: |-
        The `deckhouse-web` module has been renamed to `documentation`, and a new `documentation` ModuleConfig is generated automatically.

        Steps to troubleshoot:

        1. Remove the deprecated `deckhouse-web` ModuleConfig from the CI deployment process.
        1. Delete it using the following command:

           ```bash
           d8 k delete mc deckhouse-web
           ```
      summary: |
        Deprecated deckhouse-web ModuleConfig detected.
      severity: "9"
      markupFormat: markdown
    - name: ModuleConfigObsoleteVersion
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Deckhouse has detected that ModuleConfig `{{ $labels.name }}` is outdated.

        To resolve this issue, update ModuleConfig `{{ $labels.name }}` to the latest version.
      summary: |
        ModuleConfig {{ $labels.name }} is outdated.
      severity: "4"
      markupFormat: markdown
    - name: ModuleHasDeprecatedUpdatePolicy
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |-
        Deckhouse has detected that the module `{{ $labels.moduleName }}` is using the deprecated update policy `{{ $labels.updatePolicy }}`. The `v1alpha1` policy has a selector that no longer functions.

        To specify the update policy in the module configuration, run the following command:

        ```bash
        d8 k patch moduleconfig {{ $labels.moduleName }} --type='json' -p='[{"op": "add", "path": "/spec/updatePolicy", "value": "{{ $labels.updatePolicy }}"}]'
        ```

        After resolving all alerts related to the update policy `{{ $labels.updatePolicy }}`, clear the selector by running the following command:

        ```bash
        d8 k patch moduleupdatepolicies.v1alpha1.deckhouse.io {{ $labels.updatePolicy }} --type='json' -p='[{"op": "replace", "path": "/spec/moduleReleaseSelector/labelSelector/matchLabels", "value": {"": ""}}]'
        ```
      summary: |
        Module {{ $labels.moduleName }} is matched by the deprecated update policy {{ $labels.updatePolicy }}.
      severity: "4"
      markupFormat: markdown
    - name: ModuleIsDeprecated
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        The module `{{ $labels.module }}` is deprecated and will not receive new updates. The module may be removed in the future, so please disable it in a controlled manner.
      summary: |
        Module {{ $labels.module }} is deprecated.
      severity: "9"
      markupFormat: markdown
    - name: ModuleIsInMaintenanceMode
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module `{{ $labels.moduleName }}` is running in maintenance mode. In this mode, its state is not reconciled, which prevents configuration or hook changes from being applied automatically.

        To switch the module back to normal mode, edit the corresponding ModuleConfig resource with the following command:

        ```bash
        d8 k patch moduleconfig {{ $labels.moduleName }} --type=json -p='[{"op": "remove", "path": "/spec/maintenance"}]'
        ```
      summary: |
        Module {{ $labels.moduleName }} is running in maintenance mode.
      severity: "6"
      markupFormat: markdown
    - name: ModuleReleaseIsBlockedByRequirements
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new release for module `{{ $labels.moduleName }}` has been blocked because it doesn't meet the required conditions.

        To check the requirements, run the following command:

        ```bash
        d8 k  get mr {{ $labels.name }} -o json | jq .spec.requirements
        ```
      summary: |
        A new release for module {{ $labels.moduleName }} has been blocked due to unmet requirements.
      severity: "6"
      markupFormat: markdown
    - name: ModuleReleaseIsOutdated
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module `{{ $labels.moduleName }}` is running {{ $labels.versionLag }} or more minor versions behind the latest available release.
        This module is significantly outdated and may have compatibility issues.

        To approve the module release update, run the following command:

        ```bash
        d8 k annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"
        ```
      summary: |
        Module {{ $labels.moduleName }} is {{ $labels.versionLag }} or more minor versions behind the latest available release.
      severity: "5"
      markupFormat: markdown
    - name: ModuleReleaseIsOutdated
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module `{{ $labels.moduleName }}` is running 2 minor versions behind the latest available release.
        Updating is recommended to maintain compatibility and access to latest features.

        To approve the module release update, run the following command:

        ```bash
        d8 k annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"
        ```
      summary: |
        Module {{ $labels.moduleName }} is 2 minor versions behind the latest available release.
      severity: "7"
      markupFormat: markdown
    - name: ModuleReleaseIsOutdated
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        Module `{{ $labels.moduleName }}` is running 1 minor version behind the latest available release.
        Consider updating to maintain compatibility and security.

        To approve the module release update, run the following command:

        ```bash
        d8 k annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"
        ```
      summary: |
        Module {{ $labels.moduleName }} is 1 minor version behind the latest available release.
      severity: "9"
      markupFormat: markdown
    - name: ModuleReleaseIsWaitingManualApproval
      sourceFile: modules/340-monitoring-deckhouse/monitoring/prometheus-rules/alerting.yaml
      moduleUrl: 340-monitoring-deckhouse
      module: monitoring-deckhouse
      edition: ce
      description: |
        A new release for module `{{ $labels.moduleName }}` is available but requires manual approval before it can be applied.

        To approve the module release, run the following command:

        ```bash
        d8 k annotate mr {{ $labels.name }} modules.deckhouse.io/approved="true"
        ```
      summary: |
        A new release for module {{ $labels.moduleName }} is awaiting manual approval.
      severity: "6"
      markupFormat: markdown
    - name: NATInstanceWithDeprecatedAvailabilityZone
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/deprecated_availability_zone.yaml
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: |
        The NAT instance `{{ $labels.name }}` is located in the availability zone `ru-central1-c`, which has been deprecated by Yandex Cloud. To resolve this issue, migrate the NAT instance to either `ru-central1-a` or `ru-central1-b` by following the instructions below.

        > The migration process involves irreversible changes and may result in a significant downtime. The duration typically depends on Yandex Clouds response time and can last several tens of minutes.

        1. Migrate the NAT instance. To get `providerClusterConfiguration.withNATInstance`, run the following command:

           ```bash
           d8 k -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.cloudProviderYandex.internal.providerClusterConfiguration.withNATInstance'
           ```

           - If you specified `withNATInstance.natInstanceInternalAddress` and/or `withNATInstance.internalSubnetID` in `providerClusterConfiguration`, remove them using the following command:

             ```bash
             d8 system edit provider-cluster-configuration
             ```

           - If you specified `withNATInstance.externalSubnetID` and/or `withNATInstance.natInstanceExternalAddress` in `providerClusterConfiguration`, change them to the appropriate values.

             To get the address and subnet ID, use the Yandex Cloud Console or CLI.

             To change `withNATInstance.externalSubnetID` and `withNATInstance.natInstanceExternalAddress`, run the following command:

             ```bash
             d8 system edit provider-cluster-configuration
             ```

        1. Run the appropriate edition and version of the Deckhouse installer container on the **local** machine. You may have to change the container registry address to do that. After that, perform the converge.

           - Get the appropriate edition and version of Deckhouse:

             ```bash
             DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}')
             DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]')
             echo "DH_VERSION=$DH_VERSION DH_EDITION=$DH_EDITION"
             ```

           - Run the installer:

             ```bash
             docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
             ```

           - Perform the converge:

             ```bash
             dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
             ```

        1. Update the route table.

           - Get the route table name:

             ```bash
             d8 k -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.global.clusterConfiguration.cloud.prefix'
             ```

           - Get the NAT instance name:

             ```bash
             d8 k -n d8-system exec -ti svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values -g cloud-provider-yandex -o json | jq -c | jq '.cloudProviderYandex.internal.providerDiscoveryData.natInstanceName'
             ```

           - Get the NAT instance internal IP address:

             ```bash
             yc compute instance list | grep -e "INTERNAL IP" -e <NAT_INSTANCE_NAME_FROM_PREVIOUS_STEP>
             ```

           - Update the route:

             ```bash
             yc vpc route-table update --name <ROUTE_TABLE_NAME_FROM_PREVIOUS_STEP> --route "destination=0.0.0.0/0,next-hop=<NAT_INSTANCE_INTERNAL_IP_FROM_PREVIOUS_STEP>"
             ```
      summary: |
        NAT instance {{ $labels.name }} is in a deprecated availability zone.
      severity: "9"
      markupFormat: markdown
    - name: NginxIngressConfigTestFailed
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The configuration test (`nginx -t`) for the `{{ $labels.controller }}` Ingress controller in the `{{ $labels.controller_namespace }}` namespace has failed.

        Steps to resolve:

        1. Check the controller logs:

           ```bash
           d8 k -n {{ $labels.controller_namespace }} logs {{ $labels.controller_pod }} -c controller
           ```

        2. Find the most recently created Ingress in the cluster:

           ```bash
           d8 k get ingress --all-namespaces --sort-by="metadata.creationTimestamp"
           ```

        3. Check for errors in the `configuration-snippet` or `server-snippet` annotations.
      summary: |
        Configuration test failed on Ingress NGINX {{ $labels.controller_namespace }}/{{ $labels.controller }}.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressDaemonSetNotUpToDate
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected {{ .Value }} outdated pods in Ingress NGINX DaemonSet `{{ $labels.namespace }}/{{ $labels.daemonset }}` over the last 20 minutes.

        Steps to resolve:

        1. Check the DaemonSet's status:

           ```bash
           d8 k -n {{ $labels.namespace }} get ads {{ $labels.daemonset }}
           ```

        2. Analyze the DaemonSet's description:

           ```bash
           d8 k -n {{ $labels.namespace }} describe ads {{ $labels.daemonset }}
           ```

        3. If the parameter `Number of Nodes Scheduled with Up-to-date Pods` does not match
        `Current Number of Nodes Scheduled`, check the 'nodeSelector' and 'toleration' settings of the corresponding Ingress NGINX Controller and compare them to the 'labels' and 'taints' settings of the relevant nodes.
      summary: |
        There were {{ .Value }} outdated pods in Ingress NGINX DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} over the last 20 minutes.
      severity: "9"
      markupFormat: markdown
    - name: NginxIngressDaemonSetReplicasUnavailable
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas remaining in Ingress NGINX DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}`.

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_phase{namespace=\"%s\", phase!~\"Running|Succeeded\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        d8 k -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        No available replicas remaining in Ingress NGINX DaemonSet {{$labels.namespace}}/{{$labels.daemonset}}.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressDaemonSetReplicasUnavailable
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected that some replicas of Ingress NGINX DaemonSet `{{$labels.namespace}}/{{$labels.daemonset}}` are unavailable.

        Current number: {{ .Value }} unavailable replica(s).

        List of unavailable pods:

        ```text
        {{range $index, $result := (printf "(max by (namespace, pod) (kube_pod_status_phase{namespace=\"%s\", phase!~\"Running|Succeeded\"} == 1)) * on (namespace, pod) kube_controller_pod{namespace=\"%s\", controller_type=\"DaemonSet\", controller_name=\"%s\"}" $labels.namespace $labels.namespace $labels.daemonset | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```

        If you know where the DaemonSet should be scheduled, run the command below to identify the problematic nodes. Use a label selector for pods, if needed.

        ```bash
        d8 k -n {{$labels.namespace}} get pod -ojson | jq -r '.items[] | select(.metadata.ownerReferences[] | select(.name =="{{$labels.daemonset}}")) | select(.status.phase != "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 ) | .spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchFields[].values[]'
        ```
      summary: |
        Some replicas of Ingress NGINX DaemonSet {{$labels.namespace}}/{{$labels.daemonset}} are unavailable.
      severity: "6"
      markupFormat: markdown
    - name: NginxIngressPodIsRestartingTooOften
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        {{ $value }} Ingress NGINX Controller restarts detected in the last hour.

        Excessive Ingress NGINX restarts indicate that something is wrong. Normally, it should be up and running all the time.
      summary: |
        Too many Ingress NGINX restarts detected.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressProfilingIsEnabled
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Profiling mode is enabled for the Ingress NGINX Controller **"{{ $labels.controller_name }}"**.
        This may increase memory consumption, slow down request processing, and the process may run as root.
        It is recommended to disable profiling unless it is actively needed for debugging or performance analysis.
        To disable profiling, set `nginxProfilingEnabled: false` in the `ingressnginxcontroller` resource configuration for this controller.
      summary: |
        Warning: Profiling mode is enabled in the Ingress NGINX Controller &quot;{{ $labels.controller_name }}&quot;.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressProtobufExporterHasErrors
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Deckhouse has detected that the Ingress NGINX sidecar container with `protobuf_exporter` has {{ $labels.type }} errors.

        To resolve the issue, check the Ingress controller's logs:

        ```bash
        d8 k -n d8-ingress-nginx logs $(d8 k -n d8-ingress-nginx get pods -l app=controller,name={{ $labels.controller }} -o wide | grep {{ $labels.node }} | awk '{print $1}') -c protobuf-exporter
        ```
      summary: |
        The Ingress NGINX sidecar container with protobuf_exporter has {{ $labels.type }} errors.
      severity: "8"
      markupFormat: markdown
    - name: NginxIngressSslExpired
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The SSL certificate for {{ $labels.host }} in the `{{ $labels.namespace }}` namespace has expired.

        To verify the certificate, run the following command:

        ```bash
        d8 k -n {{ $labels.namespace }} get secret {{ $labels.secret_name }} -o json | jq -r '.data."tls.crt" | @base64d' | openssl x509 -noout -alias -subject -issuer -dates
        ```

        The site at `https://{{ $labels.host }}` is not accessible.
      summary: |
        Certificate has expired.
      severity: "4"
      markupFormat: markdown
    - name: NginxIngressSslWillExpire
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        The SSL certificate for {{ $labels.host }} in the `{{ $labels.namespace }}` namespace will expire in less than two weeks.

        To verify the certificate, run the following command:

        ```bash
        d8 k -n {{ $labels.namespace }} get secret {{ $labels.secret_name }} -o json | jq -r '.data."tls.crt" | @base64d' | openssl x509 -noout -alias -subject -issuer -dates
        ```
      summary: |
        Certificate is expiring soon.
      severity: "5"
      markupFormat: markdown
    - name: NginxIngressValidationIsDisabled
      sourceFile: modules/402-ingress-nginx/monitoring/prometheus-rules/ingress-nginx.yaml
      moduleUrl: 402-ingress-nginx
      module: ingress-nginx
      edition: ce
      description: |-
        Validation is disabled to reduce load on the master nodes, as it requires additional resources.
        To re-enable validation, remove the annotation `network.deckhouse.io/ingress-nginx-validation-suspended`
        from the `ingressnginxcontroller` resource.

        To find which `IngressNginxController` resources have the annotation, use the following command:
        ```shell
        d8 k get ingressnginxcontrollers.deckhouse.io -o json | jq -r '.items[] | select(.metadata.annotations."network.deckhouse.io/ingress-nginx-validation-suspended" != null) | "\(.metadata.name)"'
        ```
      summary: |
        Warning: Ingress resource validation in the Ingress NGINX Controller is currently disabled.
      severity: "4"
      markupFormat: markdown
    - name: NodeConntrackTableFull
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        As a result, no new connections created or accepted on the node. Keeping the `conntrack` table at full capacity may lead to erratic software behavior.

        To identify the source of excessive `conntrack` entries, use Okmeter or Grafana dashboards.
      summary: |
        The conntrack table on node {{ $labels.node }} is full.
      severity: "3"
      markupFormat: markdown
    - name: NodeConntrackTableFull
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The `conntrack` table on node `{{ $labels.node }}` is currently at {{ $value }}% of its maximum capacity.

        This is acceptable as long as the `conntrack` table remains 70-80% full. However, if it reaches full capacity, new connections may fail, causing network disruptions and erratic software behavior.

        To identify the source of excessive `conntrack` entries, use Okmeter or Grafana dashboards.
      summary: |
        The conntrack table on node {{ $labels.node }} is approaching the size limit.
      severity: "4"
      markupFormat: markdown
    - name: NodeDiskBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-bytes-critical\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%

        Steps to resolve:

        1. Retrieve disk usage information on the node:

           ```bash
           ncdu -x {{$labels.mountpoint}}
           ```

        1. If the output shows high disk usage in the `/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/` directory, identify the pods with the highest usage:

           ```bash
           crictl stats -o json | jq '.stats[] | select((.writableLayer.usedBytes.value | tonumber) > 1073741824) | { meta: .attributes.labels, diskUsage: ((.writableLayer.usedBytes.value | tonumber) / 1073741824 * 100 | round / 100 | tostring + " GiB")}'
           ```
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-bytes-critical&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: NodeDiskBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-bytes-warning\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%

        Steps to resolve:

        1. Retrieve disk usage information on the node:

           ```bash
           ncdu -x {{$labels.mountpoint}}
           ```

        1. If the output shows high disk usage in the `/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/` directory, identify the pods with the highest usage:

           ```bash
           crictl stats -o json | jq '.stats[] | select((.writableLayer.usedBytes.value | tonumber) > 1073741824) | { meta: .attributes.labels, diskUsage: ((.writableLayer.usedBytes.value | tonumber) / 1073741824 * 100 | round / 100 | tostring + " GiB")}'
           ```
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-bytes-warning&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "6"
      markupFormat: markdown
    - name: NodeDiskInodesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-inodes-critical\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-inodes-critical&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: NodeDiskInodesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/node.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that node disk `{{$labels.device}}` on mount point `{{$labels.mountpoint}}` is using more than {{ printf "extended_monitoring_node_threshold{threshold=\"disk-inodes-warning\", node=\"%s\"}" $labels.node | query | first | value }}% of its storage capacity.

        Current storage usage: {{ .Value }}%
      summary: |
        Node disk {{$labels.device}} on mount point {{$labels.mountpoint}} is using more than {{ printf &quot;extended_monitoring_node_threshold{threshold=&quot;disk-inodes-warning&quot;, node=&quot;%s&quot;}&quot; $labels.node | query | first | value }}% of its storage capacity.
      severity: "6"
      markupFormat: markdown
    - name: NodeExporterDown
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: Prometheus has been unable to scrape a `node-exporter` for more than 10 minutes, or `node-exporters` have disappeared from service discovery.
      summary: |
        Prometheus couldn't scrape a node-exporter.
      severity: "3"
      markupFormat: default
    - name: NodeFilesystemIsRO
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node-ro-fs.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The file system on the node has been switched to read-only mode.

        To investigate the cause, check the node logs.
      summary: |
        Node file system is read-only.
      severity: "4"
      markupFormat: default
    - name: NodeGroupHasStaticInternalNetworkCIDRsField
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group-deprecate.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The `spec.static.internalNetworkCIDRs` field is deprecated and has been moved to the static cluster configuration.

        To resolve this issue, delete the field from NodeGroup {{ $labels.name }}.
        This is safe, as the setting has already been migrated automatically.
      summary: |
        NodeGroup {{ $labels.name }} uses deprecated field spec.static.internalNetworkCIDRs.
      severity: "9"
      markupFormat: markdown
    - name: NodeGroupMasterTaintIsAbsent
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The `master` NodeGroup doesn't have the `node-role.kubernetes.io/control-plane: NoSchedule` taint.
        This may indicate a misconfiguration where control-plane nodes can run non-control-plane Pods.

        To resolve the issue, add the following to the `master` NodeGroup spec:

        ```yaml
          nodeTemplate:
            taints:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
        ```

        Note that the taint `key: node-role.kubernetes.io/master` is deprecated and has no effect starting from Kubernetes 1.24.
      summary: |
        The master NodeGroup is missing the required control-plane taint.
      severity: "4"
      markupFormat: markdown
    - name: NodeGroupNodeWithDeprecatedAvailabilityZone
      sourceFile: modules/030-cloud-provider-yandex/monitoring/prometheus-rules/deprecated_availability_zone.yaml
      moduleUrl: 030-cloud-provider-yandex
      module: cloud-provider-yandex
      edition: ce
      description: |
        Certain nodes in the node group `{{ $labels.node_group }}` are located in the availability zone `ru-central1-c`, which has been deprecated by Yandex Cloud.

        Steps to troubleshoot:

        1. Identify the nodes that need to be migrated by running the following command:

           ```bash
           d8 k get node -l "topology.kubernetes.io/zone=ru-central1-c"
           ```

        2. Migrate your nodes, disks, and load balancers to one of the supported zones: `ru-central1-a`, `ru-central1-b`, or `ru-central1-d`. Refer to the [Yandex migration guide](https://cloud.yandex.com/en/docs/overview/concepts/zone-migration) for detailed instructions.

           > You can't migrate public IP addresses between zones. For details, refer to the migration guide.
      summary: |
        NodeGroup {{ $labels.node_group }} contains nodes in a deprecated availability zone.
      severity: "9"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        This probably means that `machine-controller-manager` is unable to create Machines using the cloud provider module.

        Possible causes:

        1. Cloud provider resource limits.
        2. No access to the cloud provider API.
        3. Misconfiguration of the cloud provider or instance class.
        4. Problems with bootstrapping the Machine.

        Recommended actions:

        1. Check the status of the NodeGroup:

           ```shell
           d8 k get ng {{ $labels.node_group }} -o yaml
           ```

           Look for errors in the `.status.lastMachineFailures` field.

        2. If no Machines stay in the Pending state for more than a couple of minutes, it likely means that Machines are being continuously created and deleted due to an error:

           ```shell
           d8 k -n d8-cloud-instance-manager get machine
           ```

        3. If logs dont show errors, and a Machine continues to be Pending, check its bootstrap status:

           ```shell
           d8 k -n d8-cloud-instance-manager get machine <MACHINE_NAME> -o json | jq .status.bootstrapStatus
           ```

        4. If the output looks like the example below, connect via `nc` to examine bootstrap logs:

           ```json
           {
             "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
             "tcpEndpoint": "192.168.199.158"
           }
           ```

          5. If there's no bootstrap log endpoint, `cloudInit` may not be working correctly. This could indicate a misconfigured instance class in the cloud provider.
      summary: |
        NodeGroup {{ $labels.node_group }} has no available instances.
      severity: "7"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        The number of simultaneously unavailable instances in the {{ $labels.node_group }} NodeGroup exceeds the allowed threshold.
        This may indicate that the autoscaler has provisioned too many nodes at once. Check the state of the Machine in the cluster.
        This probably means that `machine-controller-manager` is unable to create Machines using the cloud provider module.

        Possible causes:

        1. Cloud provider resource limits.
        2. No access to the cloud provider API.
        3. Misconfiguration of the cloud provider or instance class.
        4. Problems with bootstrapping the Machine.

        Recommended actions:

        1. Check the status of the NodeGroup:

           ```shell
           d8 k get ng {{ $labels.node_group }} -o yaml
           ```

           Look for errors in the `.status.lastMachineFailures` field.

        2. If no Machines stay in the Pending state for more than a couple of minutes, it likely means that Machines are being continuously created and deleted due to an error:

           ```shell
           d8 k -n d8-cloud-instance-manager get machine
           ```

        3. If logs dont show errors, and a Machine continues to be Pending, check its bootstrap status:

           ```shell
           d8 k -n d8-cloud-instance-manager get machine <MACHINE_NAME> -o json | jq .status.bootstrapStatus
           ```

        4. If the output looks like the example below, connect via `nc` to examine bootstrap logs:

           ```json
           {
             "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
             "tcpEndpoint": "192.168.199.158"
           }
           ```

          5. If there's no bootstrap log endpoint, `cloudInit` may not be working correctly. This could indicate a misconfigured instance class in the cloud provider.
      summary: |
        Too many unavailable instances in the {{ $labels.node_group }} NodeGroup.
      severity: "8"
      markupFormat: markdown
    - name: NodeGroupReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-group.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        There are {{ $value }} unavailable instances in the NodeGroup {{ $labels.node_group }}.
        This probably means that `machine-controller-manager` is unable to create Machines using the cloud provider module.

        Possible causes:

        1. Cloud provider resource limits.
        2. No access to the cloud provider API.
        3. Misconfiguration of the cloud provider or instance class.
        4. Problems with bootstrapping the Machine.

        Recommended actions:

        1. Check the status of the NodeGroup:

           ```shell
           d8 k get ng {{ $labels.node_group }} -o yaml
           ```

           Look for errors in the `.status.lastMachineFailures` field.

        2. If no Machines stay in the Pending state for more than a couple of minutes, it likely means that Machines are being continuously created and deleted due to an error:

           ```shell
           d8 k -n d8-cloud-instance-manager get machine
           ```

        3. If logs dont show errors, and a Machine continues to be Pending, check its bootstrap status:

           ```shell
           d8 k -n d8-cloud-instance-manager get machine <MACHINE_NAME> -o json | jq .status.bootstrapStatus
           ```

        4. If the output looks like the example below, connect via `nc` to examine bootstrap logs:

           ```json
           {
             "description": "Use 'nc 192.168.199.158 8000' to get bootstrap logs.",
             "tcpEndpoint": "192.168.199.158"
           }
           ```

          5. If there's no bootstrap log endpoint, `cloudInit` may not be working correctly. This could indicate a misconfigured instance class in the cloud provider.
      summary: |
        NodeGroup {{ $labels.node_group }} has unavailable instances.
      severity: "8"
      markupFormat: markdown
    - name: NodePingPacketLoss
      sourceFile: modules/340-monitoring-ping/monitoring/prometheus-rules/node-ping.yaml
      moduleUrl: 340-monitoring-ping
      module: monitoring-ping
      edition: ce
      description: ICMP packet loss to node `{{$labels.destination_node}}` has exceeded 5%.
      summary: |
        Ping loss exceeds 5%.
      severity: "4"
      markupFormat: default
    - name: NodeRequiresDisruptionApprovalForUpdate
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` has detected a new update, received initial approval, and started the update.
        However, it encountered a stage that may cause downtime and requires manual disruption approval, because the NodeGroup is configured with `disruptions.approvalMode: Manual`.

        To resolve the issue, ensure the Node is ready for unsafe updates (drained) and grant disruption approval by annotating the Node with `update.node.deckhouse.io/disruption-approved=`.

        **Caution**:

        - Nodes in manual mode aren't drained automatically.
        - Do not drain master nodes.

        1. To drain the Node and grant the update approval, run the following command:

           ```shell
           d8 k drain {{ $labels.node }} --delete-emptydir-data --ignore-daemonsets --force=true &&
             d8 k annotate node {{ $labels.node }} update.node.deckhouse.io/disruption-approved=
           ```

        2. Uncordon the Node once the update is complete and the annotation `update.node.deckhouse.io/approved` is removed:

           ```shell
           while d8 k get node {{ $labels.node }} -o json | jq -e '.metadata.annotations | has("update.node.deckhouse.io/approved")' > /dev/null; do sleep 1; done
           d8 k uncordon {{ $labels.node }}
           ```

        If the NodeGroup has multiple Nodes, repeat this process for each one, since only one Node is updated at a time.
        Consider temporarily switching to automatic disruption approval (`disruptions.approvalMode: Automatic`).
      summary: |
        Node {{ $labels.node }} requires disruption approval to proceed with the update.
      severity: "8"
      markupFormat: markdown
    - name: NodeStuckInDraining
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` is stuck in the draining process.

        To get more details, run the following:

        ```shell
        d8 k -n default get event --field-selector involvedObject.name={{ $labels.node }},reason=DrainFailed --sort-by='.metadata.creationTimestamp'
        ```

        Error message: {{ $labels.message }}
      summary: |
        Node {{ $labels.node }} is stuck in draining.
      severity: "6"
      markupFormat: markdown
    - name: NodeStuckInDrainingForDisruptionDuringUpdate
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/node-update.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Node `{{ $labels.node }}` in NodeGroup `{{ $labels.node_group }}` has detected a new update, requested and received approval, started the update, and reached a step that could cause downtime.
        It is currently stuck in the draining process while waiting for automatic disruption approval.

        To get more details, run the following:

        ```shell
        d8 k -n default get event --field-selector involvedObject.name={{ $labels.node }},reason=ScaleDown --sort-by='.metadata.creationTimestamp'
        ```
      summary: |
        Node {{ $labels.node }} is stuck in draining.
      severity: "6"
      markupFormat: markdown
    - name: NodeSUnreclaimBytesUsageHigh
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The node `{{ $labels.node }}` has a potential kernel memory leak. One known issue could be causing this problem.

        Steps to troubleshoot:

        1. Check the `cgroupDriver` setting on node `{{ $labels.node }}`:

           ```bash
           cat /var/lib/kubelet/config.yaml | grep 'cgroupDriver: systemd'
           ```

        1. If `cgroupDriver` is set to `systemd`, a reboot is required to switch back to the `cgroupfs` driver.
           In this case, drain and reboot the node.

        For further details, refer to the [issue](https://github.com/deckhouse/deckhouse/issues/2152).
      summary: |
        Node {{ $labels.node }} has high kernel memory usage.
      severity: "4"
      markupFormat: markdown
    - name: NodeSystemExporterDoesNotExistsForNode
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        To resolve the issue, follow these steps:

        1. Find the `node-exporter` Pod running on the affected node:

           ```bash
           d8 k -n d8-monitoring get pod -l app=node-exporter -o json | jq -r ".items[] | select(.spec.nodeName==\"{{$labels.node}}\") | .metadata.name"
           ```

        2. Describe the `node-exporter` Pod:

           ```bash
           d8 k -n d8-monitoring describe pod <POD_NAME>
           ```

        3. Verify that the kubelet is running on node `{{ $labels.node }}`.
      summary: |
        Some system exporters aren't functioning correctly on node {{ $labels.node }}.
      severity: "4"
      markupFormat: markdown
    - name: NodeTCPMemoryExhaust
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The TCP stack on node `{{ $labels.node }}` is experiencing high memory pressure.
        This may be caused by applications with intensive TCP networking usage.

        Steps to troubleshoot:

        1. Identify applications consuming excessive TCP memory.
        1. Adjust TCP memory configuration if needed.
        1. Investigate network traffic sources.
      summary: |
        Node {{ $labels.node }} has high TCP stack memory usage.
      severity: "6"
      markupFormat: markdown
    - name: NodeTimeOutOfSync
      sourceFile: modules/470-chrony/monitoring/prometheus-rules/chrony.yaml
      moduleUrl: 470-chrony
      module: chrony
      edition: ce
      description: |
        Time on the node `{{$labels.node}}` is out of sync and drifts apart from the NTP server clock by {{ $value }} seconds.

        To resolve the time synchronization issues:

        - Fix network errors:
          - Ensure the upstream time synchronization servers defined in the [chrony configuration](https://deckhouse.io/modules/chrony/configuration.html) are available.
          - Eliminate large packet loss and excessive latency to upstream time synchronization servers.
        - Modify the NTP servers list defined in the [chrony configuration](https://deckhouse.io/modules/chrony/configuration.html).
      summary: |
        Clock on the node {{$labels.node}} is drifting.
      severity: "5"
      markupFormat: markdown
    - name: NodeUnschedulable
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/node.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        The node `{{ $labels.node }}` is in a cordon-protected state, meaning no new pods can be scheduled onto it.

        Someone may have executed one of the following commands on this node:

        - To cordon the node:

          ```bash
          d8 k cordon {{ $labels.node }}
          ```

        - To drain the node (if draining has been running for more than 20 minutes):

          ```bash
          d8 k drain {{ $labels.node }}
          ```

        The was likely caused by the scheduled maintenance of that node.
      summary: |
        Node {{ $labels.node }} is cordon-protected, preventing new pods from being scheduled.
      severity: "8"
      markupFormat: markdown
    - name: NTPDaemonOnNodeDoesNotSynchronizeTime
      sourceFile: modules/470-chrony/monitoring/prometheus-rules/chrony.yaml
      moduleUrl: 470-chrony
      module: chrony
      edition: ce
      description: |
        Steps to troubleshoot:

        1. Check if the chrony pod is running on the node by executing the following command:

           ```bash
           d8 k -n d8-chrony --field-selector spec.nodeName="{{$labels.node}}" get pods
           ```

        2. Verify the chrony daemon's status by executing the following command:

           ```bash
           d8 k -n d8-chrony exec <POD_NAME> -- /opt/chrony-static/bin/chronyc sources
           ```

        3. Resolve the time synchronization issues:
           - Fix network errors:
             - Ensure the upstream time synchronization servers defined in the [chrony configuration](https://deckhouse.io/modules/chrony/configuration.html) are available.
             - Eliminate large packet loss and excessive latency to upstream time synchronization servers.
           - Modify the NTP servers list defined in the [chrony configuration](https://deckhouse.io/modules/chrony/configuration.html).
      summary: |
        NTP daemon on the node {{$labels.node}} haven't synchronized time for too long.
      severity: "5"
      markupFormat: markdown
    - name: OpenVPNClientCertificateExpired
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN client certificate for **{{ $labels.client }}** has expired.

        Check the certificate information and make sure it actually expires:

        ```shell
        d8 k -n d8-openvpn get secrets -l name="{{ $labels.client }}"  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```

        Renew or delete the expired certificate.
        Check [the documentation](https://deckhouse.io/modules/openvpn/faq.html#how-to-revoke-rotate-or-delete-a-user-certificate) for details.
      summary: |
        OpenVPN client certificate expired for {{ $labels.client }}
      severity: "4"
      markupFormat: markdown
    - name: OpenVPNClientCertificateExpired
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN client certificate for **{{ $labels.client }}** is set to expire in {{ $value }} days.

        Renewal of certificate required.
        Check [the documentation](https://deckhouse.io/modules/openvpn/faq.html#how-to-revoke-rotate-or-delete-a-user-certificate) for details.

        View certificate details to display the exact expiration date:

        ```shell
        d8 k -n d8-openvpn get secrets -l name={{ $labels.client }}  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```
      summary: |
        OpenVPN client certificate expires for {{ $labels.client }} in less than 7 days.
      severity: "5"
      markupFormat: markdown
    - name: OpenVPNClientCertificateExpired
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN client certificate for **{{ $labels.client }}** is set to expire in {{ $value }} days.

        Renewal of certificate required.
        Check [the documentation](https://deckhouse.io/modules/openvpn/faq.html#how-to-revoke-rotate-or-delete-a-user-certificate) for details.

        View certificate details to display the exact expiration date:

        ```shell
        d8 k -n d8-openvpn get secrets -l name={{ $labels.client }}  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```
      summary: |
        OpenVPN client certificate expires for {{ $labels.client }} in less than 30 days.
      severity: "6"
      markupFormat: markdown
    - name: OpenVPNServerCACertificateExpired
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        OpenVPN CA certificate renews automatically 1 days before expiration.
        Automatic rotation did not work.
        Check the module status or perform [manual rotation](https://deckhouse.io/modules/openvpn/faq.html#how-to-rotate-a-root-certificate-ca).

        View certificate details to display the exact expiration date:

        ```shell
        d8 k -n d8-openvpn get secrets openvpn-pki-ca  -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```

        The hook should rotate CA and server certificates before expiry.
      summary: |
        OpenVPN CA certificate has expired.
      severity: "4"
      markupFormat: markdown
    - name: OpenVPNServerCACertificateExpiresTomorrow
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        OpenVPN CA certificate renews automatically 1 days before expiration.
        Automatic rotation did not work.
        Check the module status or perform [manual rotation](https://deckhouse.io/modules/openvpn/faq.html#how-to-rotate-a-root-certificate-ca).

        View certificate details to display the exact expiration date:

        ```shell
        d8 k -n d8-openvpn get secrets openvpn-pki-ca  -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```

        The hook should rotate CA and server certificates before expiry.
      summary: |
        OpenVPN CA certificate will expire tomorrow
      severity: "4"
      markupFormat: markdown
    - name: OpenVPNServerCACertificateExpiringInAWeek
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN CA certificate is set to expire in {{ $value }} days.

        Renew the CA certificate if necessary.
        After renewing the certificate, **you will need to reissue all client and server certificates**.
        Check [the documentation](https://deckhouse.io/modules/openvpn/faq.html#how-to-rotate-a-root-certificate-ca) for details.

        View certificate details to display the exact expiration date:

        ```shell
        d8 k -n d8-openvpn get secrets -l name=server  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```
      summary: |
        OpenVPN CA certificate expires in less than 7 days.
      severity: "5"
      markupFormat: markdown
    - name: OpenVPNServerCACertificateExpiringSoon
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN CA certificate is set to expire in {{ $value }} days.

        Renew the CA certificate if necessary.
        After renewing the certificate, **you will need to reissue all client and server certificates**.
        Check [the documentation](https://deckhouse.io/modules/openvpn/faq.html#how-to-rotate-a-root-certificate-ca) for details.

        View certificate details to display the exact expiration date:

        ```shell
        d8 k -n d8-openvpn get secrets -l name=server  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```
      summary: |
        OpenVPN CA certificate expires in less than 30 days.
      severity: "5"
      markupFormat: markdown
    - name: OpenVPNServerCertificateExpired
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN server certificate has expired.

        OpenVPN server certificate renews automatically 1 days before expiration.
        Automatic rotation did not work.
        Check the module status or perform [manual rotation](https://deckhouse.io/modules/openvpn/faq.html#how-to-rotate-a-server-certificate).

        Check the certificate information and make sure it actually expires.
        ```shell
        d8 k -n d8-openvpn get secrets -l name=server  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```
      summary: |
        OpenVPN server certificate has expired.
      severity: "4"
      markupFormat: markdown
    - name: OpenVPNServerCertificateExpiresTomorrow
      sourceFile: modules/500-openvpn/monitoring/prometheus-rules/ovpn-admin.yaml
      moduleUrl: 500-openvpn
      module: openvpn
      edition: ce
      description: |-
        The OpenVPN server certificate is set to expire in less than 1 day.

        OpenVPN server certificate renews automatically 1 days before expiration.
        Automatic rotation did not work.
        Check the module status or perform [manual rotation](https://deckhouse.io/modules/openvpn/faq.html#how-to-rotate-a-server-certificate).

        Check the certificate information and make sure it actually expires:

        ```shell
        d8 k -n d8-openvpn get secrets -l name=server  -o jsonpath='{.items[0].data.tls\.crt}' | base64 -d | openssl x509 -text -noout
        ```
      summary: |
        OpenVPN server certificate will expire tomorrow.
      severity: "4"
      markupFormat: markdown
    - name: OperationPolicyViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured operation policies for the cluster, and one or more existing objects are violating these policies.

        To identify violating objects:

        - Run the following Prometheus query:

          ```prometheus
          count by (violating_namespace, violating_kind, violating_name, violation_msg) (
            d8_gatekeeper_exporter_constraint_violations{
              violation_enforcement="deny",
              source_type="OperationPolicy"
            }
          )
          ```

        - Alternatively, check the admission-policy-engine Grafana dashboard.
      summary: |
        At least one object violates the configured cluster operation policies.
      severity: "3"
      markupFormat: markdown
    - name: PersistentVolumeClaimBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-bytes-critical\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.

        Current volume storage usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-bytes-critical&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.
      severity: "4"
      markupFormat: markdown
    - name: PersistentVolumeClaimBytesUsage
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-bytes-warning\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.

        Currently volume storage usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-bytes-warning&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume storage capacity.
      severity: "5"
      markupFormat: markdown
    - name: PersistentVolumeClaimInodesUsed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-inodes-critical\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.

        Current volume inode usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-inodes-critical&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.
      severity: "4"
      markupFormat: markdown
    - name: PersistentVolumeClaimInodesUsed
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/extended-monitoring/persistent-volume-claim.yaml
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |-
        Deckhouse has detected that PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is using more than {{ printf "extended_monitoring_pod_threshold{threshold=\"disk-inodes-warning\", namespace=\"%s\", pod=\"%s\"}" $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.

        Current volume inode usage: {{ .Value }}%

        PersistentVolumeClaim `{{$labels.namespace}}/{{$labels.persistentvolumeclaim}}` is used by the following pods:

        ```text
        {{range $index, $result := (print "kube_pod_spec_volumes_persistentvolumeclaims_info{namespace='" $labels.namespace "', persistentvolumeclaim='" $labels.persistentvolumeclaim "'}" | query)}}{{if not (eq $index 0)}}, {{ end }}{{ $result.Labels.pod }}{{ end }}
        ```
      summary: |
        PersistentVolumeClaim {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} is using more than {{ printf &quot;extended_monitoring_pod_threshold{threshold=&quot;disk-inodes-warning&quot;, namespace=&quot;%s&quot;, pod=&quot;%s&quot;}&quot; $labels.namespace $labels.pod | query | first | value }}% of the volume inode capacity.
      severity: "5"
      markupFormat: markdown
    - name: PodSecurityStandardsViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/), and one or more running pods are violating these standards.

        To identify violating pods:

        - Run the following Prometheus query:

          ```prometheus
          count by (violating_namespace, violating_name, violation_msg) (
            d8_gatekeeper_exporter_constraint_violations{
              violation_enforcement="deny",
              violating_namespace=~".*",
              violating_kind="Pod",
              source_type="PSS"
            }
          )
          ```

        - Alternatively, check the admission-policy-engine Grafana dashboard.
      summary: |
        At least one pod violates the configured cluster pod security standards.
      severity: "3"
      markupFormat: markdown
    - name: PodStatusIsIncorrect
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/pod-status.yaml
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |
        The Pod `{{ $labels.namespace }}/{{ $labels.pod }}` running on node `{{ $labels.node }}` is listed as `NotReady` while all the Pod's containers are `Ready`.

        This could have been caused by the [known Kubernetes bug](https://github.com/kubernetes/kubernetes/issues/80968).

        Steps to troubleshoot:

        1. Identify all pods with this state:

           ```bash
           d8 k get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | "\(.spec.nodeName)/\(.metadata.namespace)/\(.metadata.name)"'
           ```

        2. Identify all affected nodes:

           ```bash
           d8 k get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | .spec.nodeName' -r | sort | uniq -c
           ```

        3. Restart kubelet on each node:

           ```bash
           systemctl restart kubelet
           ```
      summary: |
        Incorrect state of Pod {{ $labels.namespace }}/{{ $labels.pod }} running on node {{ $labels.node }}.
      severity: "6"
      markupFormat: markdown
    - name: PrometheusDirectAccessDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The cluster has an Ingress that allows direct access to Prometheus with client certificate authentication, e.g. for [the external metrics access](https://deckhouse.io/modules/prometheus/usage.html#connecting-prometheus-to-an-external-grafana-instance)
        This approach is deprecated and the direct access to Prometheus will not be possible in the future Deckhouse Kubernetes Platform releases.

        To resolve this issue, remove the deprecated Ingress and use the [observability module to manage Prometheus metrics access](https://deckhouse.io/products/kubernetes-platform/modules/observability/stable/faq.html#how-to-grant-access-to-cluster-metrics-outside-of-cluster).
      summary: |
        Deprecated Ingress with direct Prometheus access found in cluster.
      severity: "6"
      markupFormat: markdown
    - name: PrometheusDiskUsage
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        Deckhouse has detected that the Prometheus disk is over 95% full.

        To check the current usage, run the following command:

        ```shell
        d8 k -n {{ $labels.namespace }} exec -ti {{ $labels.pod_name }} -c prometheus -- df -PBG /prometheus
        ```

        Consider expanding disk size following the [guide](https://deckhouse.io/modules/prometheus/faq.html#how-to-expand-disk-size).
      summary: |
        Prometheus disk usage exceeds 95%.
      severity: "4"
      markupFormat: markdown
    - name: PrometheusLongtermRotatingEarlierThanConfiguredRetentionDays
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The `prometheus-longterm` instance is rotating data earlier than specified by the `longtermRetentionDays` parameter.

        Troubleshooting options:

        - Increase the Prometheus disk size.
        - Reduce the number of metrics collected.
        - Lower the `longtermRetentionDays` parameter in the module configuration.
      summary: |
        Prometheus-longterm data is rotating earlier than the configured retention period.
      severity: "4"
      markupFormat: markdown
    - name: PrometheusMainRotatingEarlierThanConfiguredRetentionDays
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/prometheus-storage.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The `prometheus-main` instance is rotating data earlier than specified by the `retentionDays` parameter.

        Troubleshooting options:

        - Increase the Prometheus disk size.
        - Reduce the number of metrics collected.
        - Lower the `retentionDays` parameter in the module configuration.
      summary: |
        Prometheus-main data is rotating earlier than the configured retention period.
      severity: "4"
      markupFormat: markdown
    - name: PrometheusScapeConfigDeclarationDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |-
        Defining additional scrape configurations using Secrets will be deprecated in `prometheus-operator` versions later than v0.65.1.

        Migrate to using the [ScrapeConfig custom resource](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/proposals/implemented/202212-scrape-config.md) instead.
      summary: |
        AdditionalScrapeConfigs from Secrets will be deprecated soon.
      severity: "8"
      markupFormat: markdown
    - name: PrometheusServiceMonitorDeprecated
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/deprecation.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The Kubernetes cluster uses the modern `EndpointSlice` network mechanism, but the ServiceMonitor `{{ $labels.namespace }}/{{ $labels.name }}` still uses relabeling rules based on the old `Endpoints` mechanism (starting with `__meta_kubernetes_endpoints_`).

        Support for the old relabeling rules will be removed in a future Deckhouse release (1.60).
        To avoid issues, update your ServiceMonitor to use `EndpointSlice`-based relabeling.

        Update relabeling rules as follows:

        ```shell
        __meta_kubernetes_endpoints_name -> __meta_kubernetes_endpointslice_name
        __meta_kubernetes_endpoints_label_XXX -> __meta_kubernetes_endpointslice_label_XXX
        __meta_kubernetes_endpoints_labelpresent_XXX -> __meta_kubernetes_endpointslice_labelpresent_XXX
        __meta_kubernetes_endpoints_annotation_XXX -> __meta_kubernetes_endpointslice_annotation_XXX
        __meta_kubernetes_endpoints_annotationpresent_XXX -> __meta_kubernetes_endpointslice_annotationpresent_XXX
        __meta_kubernetes_endpoint_node_name -> __meta_kubernetes_endpointslice_endpoint_topology_kubernetes_io_hostname
        __meta_kubernetes_endpoint_ready -> __meta_kubernetes_endpointslice_endpoint_conditions_ready
        __meta_kubernetes_endpoint_port_name -> __meta_kubernetes_endpointslice_port_name
        __meta_kubernetes_endpoint_port_protocol -> __meta_kubernetes_endpointslice_port_protocol
        __meta_kubernetes_endpoint_address_target_kind -> __meta_kubernetes_endpointslice_address_target_kind
        __meta_kubernetes_endpoint_address_target_name -> __meta_kubernetes_endpointslice_address_target_name
        ```
      summary: |
        Deprecated Prometheus ServiceMonitor detected.
      severity: "8"
      markupFormat: markdown
    - name: SecurityPolicyViolation
      sourceFile: modules/015-admission-policy-engine/monitoring/prometheus-rules/audit.yaml
      moduleUrl: 015-admission-policy-engine
      module: admission-policy-engine
      edition: ce
      description: |-
        You have configured security policies for the cluster, and one or more existing objects are violating these policies.

        To identify violating objects:

        - Run the following Prometheus query:

          ```prometheus
          count by (violating_namespace, violating_kind, violating_name, violation_msg) (
            d8_gatekeeper_exporter_constraint_violations{
              violation_enforcement="deny",
              source_type="SecurityPolicy"
            }
          )
          ```

        - Alternatively, check the admission-policy-engine Grafana dashboard.
      summary: |
        At least one object violates the configured cluster security policies.
      severity: "3"
      markupFormat: markdown
    - name: StandbyHolderDeploymentReplicasUnavailable
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/standby-deployment-unavailable.yaml
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |-
        Deckhouse has detected that there are no available replicas in the `standby-holder-{{$labels.node_group_name}}` Deployment in the `d8-cloud-instance-manager` namespace.

        Check the Deployment and associated Pods for issues.
      summary: |
        No available replicas in Deployment standby-holder-{{$labels.node_group_name}}.
      severity: "5"
      markupFormat: markdown
    - name: StatefulSetAuthenticationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse was unable to log in to the container registry using `imagePullSecrets` for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Unable to log in to the container registry using imagePullSecrets for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetAuthorizationFailure
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has insufficient privileges to pull the `{{ $labels.image }}` image using the specified `imagePullSecrets`.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        Insufficient privileges to pull the {{ $labels.image }} image using the specified imagePullSecrets.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetBadImageFormat
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image name is incorrect.

        To resolve this issue, check that the `{{ $labels.image }}` image name is spelled correctly in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image name is incorrect.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetImageAbsent
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the `{{ $labels.image }}` image is missing from the container registry.

        To resolve this issue, check whether the `{{ $labels.image }}` image is available in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The {{ $labels.image }} image is missing from the registry.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetRegistryUnavailable
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected that the container registry is not available for the `{{ $labels.image }}` image.

        To resolve this issue, investigate the possible causes in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.
      summary: |
        The container registry is not available for the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StatefulSetUnknownError
      sourceFile: modules/340-extended-monitoring/monitoring/prometheus-rules/image-availability/image-checks.tpl
      moduleUrl: 340-extended-monitoring
      module: extended-monitoring
      edition: ce
      description: |
        Deckhouse has detected an unknown error with the `{{ $labels.image }}` image in the following sources:

        - The `{{ $labels.namespace }}` namespace.
        - The StatefulSet `{{ $labels.name }}`.
        - The `{{ $labels.container }}` container in the registry.

        To resolve this issue, review the exporter logs:

        ```bash
        d8 k -n d8-monitoring logs -l app=image-availability-exporter -c image-availability-exporter
        ```
      summary: |
        An unknown error occurred with the {{ $labels.image }} image.
      severity: "7"
      markupFormat: markdown
    - name: StorageClassCloudManual
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/storage-class.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        A StorageClass using a `cloud-provider` provisioner shouldn't be deployed manually.
        Such StorageClasses are managed by the `cloud-provider` module.

        Instead of the manual deployment, modify the `cloud-provider` module configuration as needed.
      summary: |
        Manually deployed StorageClass {{ $labels.name }} found in the cluster.
      severity: "6"
      markupFormat: markdown
    - name: StorageClassDefaultDuplicate
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/kubernetes/storage-class.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Deckhouse has detected that more than one StorageClass in the cluster is annotated as default.

        This may have been caused by a manually deployed StorageClass that is overlapping with the default storage configuration provided by the `cloud-provider` module.
      summary: |
        Multiple default StorageClasses found in the cluster.
      severity: "6"
      markupFormat: markdown
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: Deckhouse has detected that the `{{ $labels.job }}` target is is currently unreachable.
      summary: |
        A target is down.
      severity: "5"
      markupFormat: default
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: Deckhouse has detected that the `{{ $labels.job }}` target is is currently unreachable.
      summary: |
        A target is down.
      severity: "6"
      markupFormat: default
    - name: TargetDown
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: Deckhouse has detected that the `{{ $labels.job }}` target is is currently unreachable.
      summary: |
        A target is down.
      severity: "7"
      markupFormat: default
    - name: TargetSampleLimitExceeded
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        One or more targets are down because the Prometheus sample limit was exceeded during a scrape.
      summary: |
        Scrapes are exceeding the sample limit.
      severity: "6"
      markupFormat: markdown
    - name: TargetSampleLimitExceeded
      sourceFile: modules/300-prometheus/monitoring/prometheus-rules/target-down.yaml
      moduleUrl: 300-prometheus
      module: prometheus
      edition: ce
      description: |
        The target is close to exceeding the Prometheuss sampling limit. Less than 10% of the allowed samples are left.
      summary: |
        Sampling limit is nearly reached.
      severity: "7"
      markupFormat: markdown
    - name: UnmetCloudConditions
      sourceFile: modules/040-node-manager/monitoring/prometheus-rules/cloud-data-discovery.tpl
      moduleUrl: 040-node-manager
      module: node-manager
      edition: ce
      description: |
        Deckhouse has detected that some cloud providerspecific conditions have not been met.
        Until these issues are resolved, updating to the new Deckhouse release is not possible.

        Troubleshooting details:

        - Name: {{ $labels.name }}
        - Message: {{ $labels.message }}
      summary: |
        Deckhouse update is unavailable due to unmet cloud provider conditions.
      severity: "6"
      markupFormat: markdown
    - name: UnsupportedContainerRuntimeVersion
      sourceFile: modules/340-monitoring-kubernetes/monitoring/prometheus-rules/generic/cri-version.tpl
      moduleUrl: 340-monitoring-kubernetes
      module: monitoring-kubernetes
      edition: ce
      description: |-
        Deckhouse has detected that the node {{$labels.node}} is running an unsupported version of CRI {{$labels.container_runtime_version}}.

        Supported CRI versions for Kubernetes {{$labels.kubelet_version}}:

        * Containerd 1.4.
        * Containerd 1.5.
        * Containerd 1.6.
        * Containerd 1.7.
      summary: |
        Unsupported version of CRI {{$labels.container_runtime_version}} installed for Kubernetes version {{$labels.kubelet_version}}.
      severity: undefined
      markupFormat: markdown
modules-having-alerts:
    - admission-policy-engine
    - cert-manager
    - chrony
    - cloud-provider-yandex
    - cni-cilium
    - control-plane-manager
    - documentation
    - extended-monitoring
    - ingress-nginx
    - istio
    - kube-dns
    - log-shipper
    - loki
    - metallb
    - monitoring-applications
    - monitoring-custom
    - monitoring-deckhouse
    - monitoring-kubernetes
    - monitoring-kubernetes-control-plane
    - monitoring-ping
    - node-local-dns
    - node-manager
    - okmeter
    - openvpn
    - operator-prometheus
    - prometheus
    - secret-copier
    - terraform-manager
    - upmeter
    - user-authn
