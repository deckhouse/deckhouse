---
title: "Deckhouse Platform Certified Security Edition: Руководство администратора"
permalink: ru/deckhouse-admin-guide.html
lang: ru
sidebar: none
toc: true
layout: pdf
---

## Введение в документацию

Приветствуем вас на главной странице документации Deckhouse Platform Certified Security Edition — платформы для управления Kubernetes-кластерами.

Как быстро найти то, что нужно:

- Если знаете, что вам необходимо — используйте поиск.
- Для поиска по области применения воспользуйтесь меню.

Если возникнут вопросы, вы можете обратиться за помощью в наш [Telegram-канал]({{ site.social_links[page.lang]['telegram'] }}). Мы обязательно поможем и проконсультируем.

Вы можете написать нам [на почту](mailto:support@deckhouse.ru), мы также окажем вам поддержку.
## Безопасность
### События безопасности

#### Аудит событий API Kubernetes

Процедура аудита (Kubernetes auditing)
позволяет отслеживать обращения к API-серверу и анализировать события, происходящие в кластере.
Аудит может быть полезен для отладки неожиданных сценариев поведения, а также для соблюдения требований безопасности.

Kubernetes поддерживает настройку аудита через механизм Audit policy,
который позволяет задавать правила логирования интересующих операций.
Результаты аудита по умолчанию записываются в лог-файл `/var/log/kube-audit/audit.log`.

##### Встроенные политики аудита

В Deckhouse Platform Certified Security Edition по умолчанию создается базовый набор политик аудита, которые записывают:

- операции по созданию, изменению и удалению ресурсов;
- запросы от имени сервисных аккаунтов из системных пространств имен `kube-system` и `d8-*`;
- обращения к ресурсам в системных пространствах имен `kube-system` и `d8-*`.

Эти политики активны по умолчанию.
Чтобы отключить их, установите [параметр `basicAuditPolicyEnabled`](/modules/control-plane-manager/configuration.html#parameters-apiserver-basicauditpolicyenabled) в значение `false`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    apiserver:
      auditPolicyEnabled: true
      basicAuditPolicyEnabled: false
```

##### Настройка собственной политики аудита

Чтобы создать расширенную политику аудита API Kubernetes, выполните следующие шаги:

1. Включите [параметр `auditPolicyEnabled`](/modules/control-plane-manager/configuration.html#parameters-apiserver-auditpolicyenabled) в настройках модуля [`control-plane-manager`](/modules/control-plane-manager/):

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: control-plane-manager
   spec:
     version: 1
     settings:
       apiserver:
         auditPolicyEnabled: true
   ```

1. Создайте Secret `kube-system/audit-policy`, содержащий YAML-файл политики в формате Base64:

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: audit-policy
     namespace: kube-system
   data:
     audit-policy.yaml: <Base64>
   ```

   Пример содержимого файла `audit-policy.yaml` с минимальной рабочей конфигурацией:

   ```yaml
   apiVersion: audit.k8s.io/v1
   kind: Policy
   rules:
   - level: Metadata
     omitStages:
     - RequestReceived
   ```

##### Работа с лог-файлом аудита

Предполагается, что на master-узлах кластера Deckhouse установлен инструмент
для отслеживания лог-файла `/var/log/kube-audit/audit.log`: `log-shipper`, `promtail` или `filebeat`.

Параметры ротации логов в файле предустановлены и не подлежат изменению:

- максимальный размер файла: 1000 МБ;
- максимальная глубина записи: 30 дней.

В зависимости от настроек политики и объема запросов к API-серверу количество записей может быть очень большим.
В таких условиях глубина хранения может составлять менее 30 минут.

{% alert level="warning" %}
Наличие неподдерживаемых опций или опечаток в конфигурационном файле может привести к ошибкам при запуске API-сервера.
{% endalert %}

В случае возникновения проблем с запуском API-сервера выполните следующие шаги:

1. Вручную удалите параметры `--audit-log-*` из манифеста `/etc/kubernetes/manifests/kube-apiserver.yaml`;
1. Перезапустите API-сервер с помощью следующей команды:

   ```shell
   docker stop $(docker ps | grep kube-apiserver- | awk '{print $1}')
   
   ## Альтернативный вариант (в зависимости от используемого CRI).
   crictl stopp $(crictl pods --name=kube-apiserver -q)
   ```

1. После перезапуска исправьте Secret или удалите его с помощью следующей команды:

   ```shell
   d8 k -n kube-system delete secret audit-policy
   ```

##### Перенаправление лог-файла аудита в stdout

По умолчанию лог аудита сохраняется в файл `/var/log/kube-audit/audit.log` на master-узлах.
При необходимости вы можете перенаправить его вывод в stdout процесса `kube-apiserver` вместо файла,
установив [параметр `apiserver.auditLog.output`](/modules/control-plane-manager/configuration.html#parameters-apiserver-auditlog-output) модуля [`control-plane-manager`](/modules/control-plane-manager/) в значение `Stdout`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    apiserver:
      auditPolicyEnabled: true
      auditLog:
        output: Stdout
```

В этом случае лог будет доступен в stdout контейнера `kube-apiserver`.

Далее, используя встроенный механизм логирования в Deckhouse Platform Certified Security Edition,
вы можете настроить сбор и отправку логов в собственную систему безопасности.

#### Аудит событий безопасности

Deckhouse Platform Certified Security Edition предоставляет встроенные средства поиска угроз безопасности
за счёт анализа событий ядра Linux и аудита событий Kubernetes API.
Deckhouse Platform Certified Security Edition позволяет:

- находить угрозы в окружениях, анализируя приложения и контейнеры;
- обнаруживать попытки применения уязвимостей из базы CVE и признаки запуска криптовалютных майнеров;
- выявлять угрозы Kubernetes, включая:
  - оболочки командной строки, запущенные в контейнерах или подах;
  - контейнеры, работающие в привилегированном режиме;
  - монтирование небезопасных путей в контейнеры (например, `/proc`);
  - попытки чтения секретных данных (например, из `/etc/shadow`).

##### Источники данных для аудита безопасности

Deckhouse Platform Certified Security Edition использует два основных источника событий:

- события ядра Linux — с помощью eBPF-драйвера для системы обнаружения угроз Falco;
- события [аудита API Kubernetes](./kubernetes-api-audit.html) — через интеграцию с механизмом Kubernetes auditing и вебхук-интерфейс.

##### Минимальные требования

Для получения событий ядра требуется:

- ядро Linux версии 5.8 или выше;
- поддержка eBPF.
  Проверить наличие поддержки можно одним из следующих способов:
  - убедитесь в наличии файла `/sys/kernel/btf/vmlinux`:

    ```shell
    ls -lah /sys/kernel/btf/vmlinux
    ```
  
  - убедитесь, что включён параметр `CONFIG_DEBUG_INFO_BTF`:
  
    ```shell
    grep -E "CONFIG_DEBUG_INFO_BTF=(y|m)" /boot/config-*
    ```

На каждом узле кластера запускаются агенты Falco,
которые потребляют ресурсы в зависимости от количества применяемых правил или собираемых событий.

{% alert level="info" %}
На некоторых системах могут не работать пробы eBPF.
{% endalert %}

##### Как включить аудит событий безопасности

1. Убедитесь, что узлы соответствуют [минимальным требованиям](#минимальные-требования).
1. Включите аудит в Deckhouse, используя следующую конфигурацию:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: runtime-audit-engine
   spec:
     enabled: true
   ```

1. (**Опционально**) Если control plane в кластере не управляется Deckhouse Platform Certified Security Edition при помощи [`control-plane-manager`](/modules/control-plane-manager/),
   настройте вебхук аудита API Kubernetes вручную.

Все доступные параметры аудита безопасности доступны [в разделе документации модуля `runtime-audit-engine`](/modules/runtime-audit-engine/configuration.html).

###### Настройка вебхука API Kubernetes вручную

{% alert level="info" %}
Настройка вебхука не требуется, если включён модуль [`control-plane-manager`](/modules/control-plane-manager/).
В этом случае при включении модуля [`runtime-audit-engine`](/modules/runtime-audit-engine/)
настройки сбора событий аудита API Kubernetes применятся автоматически.
{% endalert %}

Чтобы настроить вебхук на получение событий аудита от `kube-apiserver`, выполните следующие шаги:

1. Создайте файл `kubeconfig` для вебхука с адресом `https://127.0.0.1:9765/k8s-audit`
   и данными сертификата (`ca.crt`) из Secret `d8-runtime-audit-engine/runtime-audit-engine-webhook-tls`.

   Пример:

   ```yaml
   apiVersion: v1
   kind: Config
   clusters:
   - name: webhook
     cluster:
       certificate-authority-data: BASE64_CA
       server: "https://127.0.0.1:9765/k8s-audit"
   users:
   - name: webhook
   contexts:
   - context:
      cluster: webhook
      user: webhook
     name: webhook
   current-context: webhook
   ```

1. Укажите путь к созданному файлу конфигурации с помощью флага `--audit-webhook-config-file` в манифесте `kube-apiserver`.
1. (**Опционально**) Чтобы собирать события аудита API Kubernetes не только из системных,
   но и пользовательских пространств имён,
   настройте [политики аудита](./kubernetes-api-audit.html#настройка-собственной-политики-аудита).

##### Работа с правилами аудита

Для анализа событий безопасности используются правила, определяющие критерии подозрительного поведения.
В Deckhouse Platform Certified Security Edition предусмотрены:

- **встроенные правила**, включая:
  - правила для аудита Kubernetes (располагаются в контейнере `falco` по пути `/etc/falco/k8s_audit_rules.yaml`);
  - правила, удовлетворяющие требованиям приказа ФСТЭК России №118 от 4 июля 2022 г.
    «Требования по безопасности информации к средствам контейнеризации»
    (`fstec`, в формате [кастомного ресурса FalcoAuditRules](/modules/runtime-audit-engine/cr.html#falcoauditrules));

  Чтобы настроить список встроенных правил,
  используйте [параметр `settings.builtInRulesList`](/modules/runtime-audit-engine/configuration.html#parameters-builtinruleslist) модуля [`runtime-audit-engine`](/modules/runtime-audit-engine/).

- **пользовательские правила**, которые задаются через [кастомный ресурс FalcoAuditRules](/modules/runtime-audit-engine/cr.html#falcoauditrules).


###### Добавление пользовательского правила

Чтобы добавить правило, создайте [ресурс FalcoAuditRules](/modules/runtime-audit-engine/cr.html#falcoauditrules) с необходимыми условиями.
Используйте синтаксис условий Falco.
Агенты Falco автоматически применят созданное правило.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: FalcoAuditRules
metadata:
  name: ownership-permissions
spec:
  rules:
  - macro:
      name: spawned_process
      condition: (evt.type in (execve, execveat) and evt.dir=<)
  - rule:
      name: Detect Ownership Change
      desc: detect file permission/ownership change
      condition: >
        spawned_process and proc.name in (chmod, chown) and proc.args contains "/tmp/"
      output: >
        The file or directory below has had its permissions or ownership changed (user=%user.name
        command=%proc.cmdline file=%fd.name parent=%proc.pname pcmdline=%proc.pcmdline gparent=%proc.aname[2])
      priority: Warning
      tags: [filesystem]
```

Дополнительные примеры правил можно найти на следующих ресурсах:

- официальный репозиторий правил Falco;
- раздел с правилами Falco на Artifact Hub.

###### Применение стороннего правила

Поскольку структура правил Falco отличается от схемы кастомных ресурсов Deckhouse Platform Certified Security Edition,
сторонние правила из интернета необходимо сконвертировать [в ресурс FalcoAuditRules](/modules/runtime-audit-engine/cr.html#falcoauditrules) перед применением.

Используйте следующий скрипт для конвертации:

```shell
git clone github.com/deckhouse/deckhouse
cd deckhouse/ee/modules/runtime-audit-engine/hack/far-converter
go run main.go -input /path/to/falco/rule_example.yaml > ./my-rules-cr.yaml
```

Пример результата работы скрипта:

- Изначальное правило:

  ```yaml
  ## /path/to/falco/rule_example.yaml
  - macro: spawned_process
    condition: (evt.type in (execve, execveat) and evt.dir=<)

  - rule: Linux Cgroup Container Escape Vulnerability (CVE-2022-0492)
    desc: "This rule detects an attempt to exploit a container escape vulnerability in the Linux Kernel."
    condition: container.id != "" and proc.name = "unshare" and spawned_process and evt.args contains "mount" and evt.args contains "-o rdma" and evt.args contains "/release_agent"
    output: "Detect Linux Cgroup Container Escape Vulnerability (CVE-2022-0492) (user=%user.loginname uid=%user.loginuid command=%proc.cmdline args=%proc.args)"
    priority: CRITICAL
    tags: [process, mitre_privilege_escalation]
  ```

- Ресурс с правилом после конвертации:

  ```yaml
  ## ./my-rules-cr.yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: FalcoAuditRules
  metadata:
    name: rule-example
  spec:
    rules:
      - macro:
          name: spawned_process
          condition: (evt.type in (execve, execveat) and evt.dir=<)
      - rule:
          name: Linux Cgroup Container Escape Vulnerability (CVE-2022-0492)
          condition: container.id != "" and proc.name = "unshare" and spawned_process and evt.args contains "mount" and evt.args contains "-o rdma" and evt.args contains "/release_agent"
          desc: This rule detects an attempt to exploit a container escape vulnerability in the Linux Kernel.
          output: Detect Linux Cgroup Container Escape Vulnerability (CVE-2022-0492) (user=%user.loginname uid=%user.loginuid command=%proc.cmdline args=%proc.args)
          priority: Critical
          tags:
            - process
            - mitre_privilege_escalation
  ```

##### Сбор логов и оповещения

Deckhouse Platform Certified Security Edition экспортирует события аудита безопасности в формате метрик Prometheus,
по которым можно настроить оповещения через [ресурс CustomPrometheusRules](/modules/prometheus/cr.html#customprometheusrules).
Это позволяет:

- подключить внешнее хранилище для сбора логов (например, Loki или Elasticsearch);
- настроить оповещения о критических событиях.

###### Настройка сбора логов и событий

Все события аудита безопасности выводятся в stdout.
Для сбора и отправки событий в хранилище логов создайте [ресурс ClusterLoggingConfig](/modules/log-shipper/cr.html#clusterloggingconfig), следуя примеру:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLoggingConfig
metadata:
  name: falco-events
spec:
  destinationRefs:
  - xxxx
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [d8-runtime-audit-engine]
  labelFilter:
  - operator: Regex
    values: ["\{.*"] ## Для сбора логов только в формате JSON.
    field: "message"
  type: KubernetesPods
```

###### Настройка оповещений о критических событиях

Для создания оповещений о критических событиях создайте [объект CustomPrometheusRules](/modules/prometheus/cr.html#customprometheusrules), следуя примеру:
{% raw %}

```yaml
apiVersion: deckhouse.io/v1
kind: CustomPrometheusRules
metadata:
  name: falco-critical-alerts
spec:
  groups:
  - name: falco-critical-alerts
    rules:
    - alert: FalcoCriticalAlertsAreFiring
      for: 1m
      annotations:
        description: |
          There is a suspicious activity on a node {{ $labels.node }}. 
          Check you events journal for more details.
        summary: Falco detects a critical security incident
      expr: |
        sum by (node) (rate(falco_events{priority="Critical"}[5m]) > 0)
```

{% endraw %}

###### Просмотр метрик

Для получения Prometheus-метрик используйте PromQL-запрос `falcosecurity_falcosidekick_falco_events_total{}`:

```shell
d8 k -n d8-monitoring exec -it prometheus-main-0 prometheus -- \
  curl -s "http://127.0.0.1:9090/api/v1/query?query=falcosecurity_falcosidekick_falco_events_total" | jq
```

##### Отладка и эмуляция событий

Для отладки и эмуляции событий безопасности в Deckhouse Platform Certified Security Edition можно использовать:

- утилиту `event-generator`;
- HTTP-эндпоинт `/test` сервиса `falcosidekick`.

###### Включение логов для отладки

В Falco по умолчанию используется отладочный уровень логирования `debug`.

В Falcosidekick по умолчанию отладочное логирование отключено.
Для включения установите [параметр `spec.settings.debugLogging`](/modules/runtime-audit-engine/configuration.html#parameters-debuglogging) в `true`, следуя примеру

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: runtime-audit-engine
spec:
  enabled: true
  settings:
    debugLogging: true
```

###### Настройка эмуляции событий

####### Falco

Утилита `event-generator` позволяет генерировать
различные подозрительные действия (например, системные вызовы или события аудита API Kubernetes).

Используйте следующую команду для запуска тестового набора событий в кластере Kubernetes:

```shell
d8 k run falco-event-generator --image=falcosecurity/event-generator run
```

Если вам нужно сымитировать определённое действие,
обратитесь к руководству утилиты.

####### Falcosidekick

Чтобы имитировать отправку тестовых событий для сервиса `falcosidekick`,
используйте HTTP-эндпоинт `/test`:

1. Создайте тестовое событие, выполнив следующую команду:

   ```shell
   nsenter -t $(pidof falcosidekick) curl -X POST -H "Content-Type: application/json" -H "Accept: application/json" http://localhost:2801/test
   ```

1. Проверьте метрику события:

   ```shell
   d8 k -n d8-monitoring exec -it prometheus-main-0 prometheus -- \
     curl -s "http://127.0.0.1:9090/api/v1/query?query=falcosecurity_falcosidekick_falco_events_total" \
     | jq '.data.result.[] | select (.metric.priority_raw == "debug")'
   ```

   Пример вывода:

   ```console
   {
     "metric": {
       "__name__": "falcosecurity_falcosidekick_falco_events_total",
       "container": "kube-rbac-proxy",
       "hostname": "falcosidekick",
       "instance": "192.168.208.7:4212",
       "job": "runtime-audit-engine",
       "node": "dev-master-0",
       "priority": "1",
       "priority_raw": "debug",
       "rule": "Test rule",
       "source": "internal",
       "tier": "cluster"
     },
     "value": [
       1744234729.799,
       "1"
     ]
   }
   ```

### Политики безопасности

Deckhouse Platform Certified Security Edition позволяет управлять безопасностью приложений в кластере с помощью набора политик,
соответствующих модели Kubernetes Pod Security Standards и дополнительно расширяемых через встроенные механизмы Deckhouse Platform Certified Security Edition.

Для реализации политик безопасности в Deckhouse Platform Certified Security Edition используется Gatekeeper.

#### Применение Pod Security Standards

В Deckhouse Platform Certified Security Edition поддерживаются три уровня политики безопасности:

- `privileged` — неограничивающая политика с максимально широким уровнем разрешений;
- `baseline` — минимально ограничивающая политика,
  которая предотвращает наиболее известные и популярные способы повышения привилегий.
  Позволяет использовать стандартную (минимально заданную) конфигурацию пода;
- `restricted` — политика со значительными ограничениями. Предъявляет самые жесткие требования к подам.

##### Политика по умолчанию

Используемая по умолчанию политика определяется следующим образом:

- в версиях Deckhouse Platform Certified Security Edition до v1.55 политика по умолчанию — `privileged`;
- начиная с версии Deckhouse Platform Certified Security Edition v1.55, политика по умолчанию — `baseline`.

{% alert level="info" %}
При обновлении Deckhouse Platform Certified Security Edition на версию v1.55 или выше политика по умолчанию не изменится автоматически.
{% endalert %}

##### Назначение политики

Варианты назначения политики:

- глобально — с помощью [параметра `settings.podSecurityStandards.defaultPolicy`](/modules/admission-policy-engine/configuration.html#parameters-podsecuritystandards-defaultpolicy) модуля [`admission-policy-engine`](/modules/admission-policy-engine/);
- для конкретного пространства имён — с помощью лейбла `security.deckhouse.io/pod-policy=<POLICY_NAME>`.

  Пример команды для назначения политики `restricted` на все поды в пространстве имён `my-namespace`:

  ```shell
  d8 k label ns my-namespace security.deckhouse.io/pod-policy=restricted
  ```

##### Режимы применения политики

Допустимые режимы применения политик:

- `deny` — запрещает выполнений действий.
- `dryrun` — не влияет на выполнение действий и используется для отладки.
  Информацию о событиях можно посмотреть в Grafana или в консоли с помощью команды `kubectl`.
- `warn` — работает как `dryrun`, но дополнительно выводит предупреждение с указанием причины,
  по которой бы произошёл запрет действия в режиме `deny`.

По умолчанию, политики Pod Security Standards в Deckhouse Platform Certified Security Edition применяются в режиме `deny`.
В этом режиме поды приложений, не удовлетворяющие политикам, не могут быть запущены в кластере.

Как и в случае с назначением политик, режим их применения можно задать:

- глобально — с помощью [параметра `settings.podSecurityStandards.enforcementAction`](/modules/admission-policy-engine/configuration.html#parameters-podsecuritystandards-enforcementaction) модуля [`admission-policy-engine`](/modules/admission-policy-engine/);
- для конкретного пространства имён — с помощью лейбла `security.deckhouse.io/pod-policy-action=<POLICY_ACTION>`.

  Пример команды для установки режима `warn` на все поды в пространстве имён `my-namespace`:

  ```shell
  d8 k label ns my-namespace security.deckhouse.io/pod-policy-action=warn
  ```

##### Расширение политики

Вы можете расширить политики `baseline` и `restricted` с помощью шаблонов Gatekeeper,
добавив необходимые проверки к уже существующим.

Чтобы расширить политику, выполните следующее:

1. Создайте шаблон проверки с помощью ресурса ConstraintTemplate.
1. Примените созданный шаблон к политике `baseline` или `restricted`.

Пример шаблона для проверки адреса репозитория с образом контейнера:

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package d8.pod_security_standards.extended

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, input.parameters.repos])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.initContainers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, input.parameters.repos])
        }
```

Пример применения шаблона к политике `restricted`:

```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: prod-repo
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaceSelector:
      matchLabels:
        security.deckhouse.io/pod-policy: restricted
  parameters:
    repos:
      - "mycompany.registry.com"
```

В этом примере проверяется адрес репозитория в поле `image`
у всех подов в пространстве имён с лейблом `security.deckhouse.io/pod-policy: restricted`.
Если адрес в поле `image` создаваемого пода начинается не с `mycompany.registry.com`, под создан не будет.

#### Операционные политики

Deckhouse Platform Certified Security Edition предоставляет механизм создания операционных политик с помощью [ресурса OperationPolicy](/modules/admission-policy-engine/cr.html#operationpolicy).
В операционных политиках задаются требования к объектам в кластере:
допустимые репозитории, требуемые ресурсы, наличие проб и т. д.

Команда разработки Deckhouse Platform Certified Security Edition рекомендует установить следующую политику с минимально необходимым набором требований:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: OperationPolicy
metadata:
  name: common
spec:
  policies:
    allowedRepos:
      - myrepo.example.com
      - registry.deckhouse.io
    requiredResources:
      limits:
        - memory
      requests:
        - cpu
        - memory
    disallowedImageTags:
      - latest
    requiredProbes:
      - livenessProbe
      - readinessProbe
    maxRevisionHistoryLimit: 3
    imagePullPolicy: Always
    priorityClassNames:
    - production-high
    - production-low
    checkHostNetworkDNSPolicy: true
    checkContainerDuplicates: true
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          operation-policy.deckhouse.io/enabled: "true"
```

Эта политика задаёт базовые операционные требования к объектам в кластере,
включая разрешённые container registries контейнеров, обязательные ресурсы и пробы, запрет на использование образов с тегом `latest`,
допустимые классы приоритетов и другие настройки, повышающие безопасность и стабильность работы приложений.

Чтобы назначить данную операционную политику,
примените лейбл `operation-policy.deckhouse.io/enabled=true` к необходимому пространству имён:

```shell
d8 k label ns my-namespace operation-policy.deckhouse.io/enabled=true
```

#### Политики безопасности

Используя [ресурс SecurityPolicy](/modules/admission-policy-engine/cr.html#securitypolicy),
вы можете создавать политики безопасности, задающие ограничения на поведение контейнеров в кластере:
доступ к host-сетям, привилегии, использование AppArmor и т. д.

Пример политики безопасности:

```yaml
---
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: mypolicy
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: true
    allowHostNetwork: true
    allowHostPID: false
    allowPrivileged: false
    allowPrivilegeEscalation: false
    allowedFlexVolumes:
    - driver: vmware
    allowedHostPorts:
    - max: 4000
      min: 2000
    allowedProcMount: Unmasked
    allowedAppArmor:
    - unconfined
    allowedUnsafeSysctls:
    - kernel.*
    allowedVolumes:
    - hostPath
    - projected
    fsGroup:
      ranges:
      - max: 200
        min: 100
      rule: MustRunAs
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
    - ALL
    runAsGroup:
      ranges:
      - max: 500
        min: 300
      rule: RunAsAny
    runAsUser:
      ranges:
      - max: 200
        min: 100
      rule: MustRunAs
    seccompProfiles:
      allowedLocalhostFiles:
      - my_profile.json
      allowedProfiles:
      - Localhost
    supplementalGroups:
      ranges:
      - max: 133
        min: 129
      rule: MustRunAs
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          enforce: mypolicy
```

Чтобы назначить данную политику безопасности,
примените лейбл `enforce: "mypolicy"` к необходимому пространству имён.

##### Частичное применение политик

Чтобы применить отдельные политики безопасности, не отключая весь предустановленный набор, выполните следующие шаги:

1. Добавьте в необходимое пространство имён лейбл `security.deckhouse.io/pod-policy: privileged`,
   чтобы отключить встроенный набор политик.
1. Создайте [ресурс SecurityPolicy](/modules/admission-policy-engine/cr.html#securitypolicy), соответствующий уровню `baseline` или `restricted`.
   В секции `policies` укажите только необходимые вам настройки безопасности.
1. Добавьте в пространство имён дополнительный лейбл, соответствующий селектору `namespaceSelector` в SecurityPolicy.

Пример конфигурации SecurityPolicy, соответствующий уровню `baseline`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: baseline
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowPrivilegeEscalation: true
    allowPrivileged: false
    allowedAppArmor:
      - runtime/default
      - localhost/*
    allowedCapabilities:
      - AUDIT_WRITE
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - FSETID
      - KILL
      - MKNOD
      - NET_BIND_SERVICE
      - SETFCAP
      - SETGID
      - SETPCAP
      - SETUID
      - SYS_CHROOT
    allowedHostPaths: []
    allowedHostPorts:
      - max: 0
        min: 0
    allowedProcMount: Default
    allowedUnsafeSysctls:
      - kernel.shm_rmid_forced
      - net.ipv4.ip_local_port_range
      - net.ipv4.ip_unprivileged_port_start
      - net.ipv4.tcp_syncookies
      - net.ipv4.ping_group_range
      - net.ipv4.ip_local_reserved_ports
      - net.ipv4.tcp_keepalive_time
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_keepalive_intvl
      - net.ipv4.tcp_keepalive_probes
    seLinux:
      - type: ""
      - type: container_t
      - type: container_init_t
      - type: container_kvm_t
      - type: container_engine_t
    seccompProfiles:
      allowedProfiles:
        - RuntimeDefault
        - Localhost
        - undefined
        - ''
      allowedLocalhostFiles:
        - '*'
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          operation-policy.deckhouse.io/baseline-enabled: "true"
```

Пример конфигурации SecurityPolicy, соответствующий уровню `restricted`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: restricted
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowPrivilegeEscalation: false
    allowPrivileged: false
    allowedAppArmor:
      - runtime/default
      - localhost/*
    allowedCapabilities:
      - NET_BIND_SERVICE
    allowedHostPaths: []
    allowedHostPorts:
      - max: 0
        min: 0
    allowedProcMount: Default
    allowedUnsafeSysctls:
      - kernel.shm_rmid_forced
      - net.ipv4.ip_local_port_range
      - net.ipv4.ip_unprivileged_port_start
      - net.ipv4.tcp_syncookies
      - net.ipv4.ping_group_range
      - net.ipv4.ip_local_reserved_ports
      - net.ipv4.tcp_keepalive_time
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_keepalive_intvl
      - net.ipv4.tcp_keepalive_probes
    allowedVolumes:
      - configMap
      - csi
      - downwardAPI
      - emptyDir
      - ephemeral
      - persistentVolumeClaim
      - projected
      - secret
    requiredDropCapabilities:
      - ALL
    runAsUser:
      rule: MustRunAsNonRoot
    seLinux:
      - type: ""
      - type: container_t
      - type: container_init_t
      - type: container_kvm_t
      - type: container_engine_t
    seccompProfiles:
      allowedProfiles:
        - RuntimeDefault
        - Localhost
      allowedLocalhostFiles:
        - '*'
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          operation-policy.deckhouse.io/restricted-enabled: "true"
```

#### Кастомные ресурсы Gatekeeper

Gatekeeper предоставляет расширенные возможности для модификации ресурсов Kubernetes
с помощью настраиваемых политик (mutation policies).
Эти политики описываются через следующие кастомные ресурсы:

- [AssignMetadata](/modules/admission-policy-engine/gatekeeper-cr.html#assignmetadata) — для изменения секции `metadata` в ресурсе;
- [Assign](/modules/admission-policy-engine/gatekeeper-cr.html#assign) — для изменения других полей, кроме `metadata`;
- [ModifySet](/modules/admission-policy-engine/gatekeeper-cr.html#modifyset) — для добавления или удаления значений из списка, например, аргументов для запуска контейнера;
- [AssignImage](/modules/admission-policy-engine/gatekeeper-cr.html#assignimage) — для изменения параметра `image` ресурса.

Подробнее о возможности изменения ресурсов Kubernetes с помощью настраиваемых политик
можно прочитать [в документации Gatekeeper](https://open-policy-agent.github.io/gatekeeper/website/docs/mutation/).

#### Проверка подписи образов

Deckhouse Platform Certified Security Edition поддерживает проверку подписей образов контейнеров с помощью инструмента Cosign.
Проверка позволяет убедиться в целостности и подлинности образов.

Чтобы подписать образ с помощью Cosign, выполните следующее:

1. Сгенерируйте пару ключей:

   ```shell
   cosign generate-key-pair
   ```

1. Подпишите образ:

   ```shell
   cosign sign --key <KEY> <IMAGE>
   ```

Чтобы включить проверку подписи образов контейнеров в кластере Deckhouse Platform Certified Security Edition,
используйте [параметр `policies.verifyImageSignatures`](/modules/admission-policy-engine/cr.html#securitypolicy-v1alpha1-spec-policies-verifyimagesignatures) ресурса SecurityPolicy.

Пример конфигурации SecurityPolicy для проверки подписи образов контейнеров:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: verify-image-signatures
spec:
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          kubernetes.io/metadata.name: default
  policies:
    verifyImageSignatures:
      - reference: docker.io/myrepo/*
        publicKeys:
        - |-
          -----BEGIN PUBLIC KEY-----
          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM
          5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA==
          -----END PUBLIC KEY-----
      - reference: company.registry.com/*
        dockerCfg: zxc==
        publicKeys:
        - |-
          -----BEGIN PUBLIC KEY-----
          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM
          5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA==
          -----END PUBLIC KEY-----
```

Согласно данной политике, если адрес какого-либо образа контейнера совпадает со значением параметра `reference`
и образ не подписан или подпись не соответствует указанным ключам, создание пода будет запрещено.

Пример вывода ошибки при создании пода с образом контейнера, не прошедшим проверку подписи:

```console
[verify-image-signatures] Image signature verification failed: nginx:1.17.2
```

#### Использование альтернатив для управления политиками безопасности

Если вместо встроенного механизма управления политиками безопасности
в кластере Deckhouse Platform Certified Security Edition используется альтернативное решение,
настройте исключения для следующих пространств имён:

- `kube-system`;
- все пространства имён с префиксом `d8-*` (например, `d8-system`).

Без этих исключений политики могут блокировать или нарушать работу системных компонентов.

### Сканирование контейнерных образов на уязвимости

Deckhouse Platform Certified Security Edition предоставляет встроенное средство для автоматического поиска уязвимостей
в контейнерных образах на основе проекта Trivy.

#### Поиск уязвимостей

Deckhouse Platform Certified Security Edition сканирует все контейнерные образы, используемые в подах кластера.
Проверка охватывает:

- известные уязвимости (CVE) в используемых образах;
- соответствие CIS-стандартам (compliance-проверки).

Для сканирования используются как публичные базы уязвимостей,
так и обогащённые данные из [Astra Linux](https://astralinux.ru/), [ALT Linux](https://www.basealt.ru/products) и [РЕД ОС](https://redos.red-soft.ru/product/server/).

#### Сканирование в пространствах имён

По умолчанию сканируется только пространство имён `default`.

Чтобы выполнить сканирование в конкретном пространстве имён, добавьте для него лейбл `security-scanning.deckhouse.io/enabled=""`.

Как только в кластере обнаруживается хотя бы одно пространство имён с указанным лейблом, сканирование пространства имён `default` прекращается.
Чтобы снова включить сканирование для пространства имён `default`, добавьте для него лейбл следующей командой:

```shell
d8 k label namespace default security-scanning.deckhouse.io/enabled=""
```

В текущей версии функциональности ограничения перечня ресурсов для сканирования в пространстве имён не предусмотрено.  
Deckhouse Platform Certified Security Edition сканирует **все ресурсы**, находящиеся в пространстве имён, помеченном лейблом `security-scanning.deckhouse.io/enabled=""`.

#### Условия запуска и алгоритм сканирования

Сканирование запускается:

- автоматически каждые 24 часа,
- при запуске компонентов с новыми образами контейнеров в пространствах имен, для которых включено сканирование (в частности, при появлении новых объектов).

Сканирование происходит согласно следующему алгоритму:

1. В пространстве имён c каждым просканированным ресурсом создаётся объект VulnerabilityReport.
1. Этот объект содержит аннотацию `trivy-operator.aquasecurity.github.io/report-ttl`,
   которая определяет срок жизни отчёта (по умолчанию - `24h`).
1. По истечении этого срока объект VulnerabilityReport удаляется и сканирование запускается повторно.

##### Принудительный повтор сканирования

Чтобы запустить повторное сканирование ресурса вручную, воспользуйтесь одним из двух способов:

- Перезапишите аннотацию `trivy-operator.aquasecurity.github.io/report-ttl`, указав короткий срок жизни отчёта.

  Пример команды:

  ```shell
  d8 k annotate VulnerabilityReport -n <NAMESPACE> <REPORT_NAME> trivy-operator.aquasecurity.github.io/report-ttl=1s --overwrite
  ```

- Удалите объект VulnerabilityReport из пространства имён, где находится просканированный ресурс.

  Пример команды:

  ```shell
  d8 k delete VulnerabilityReport -n <NAMESPACE> <REPORT_NAME>
  ```

### Управление сертификатами

Deckhouse Platform Certified Security Edition предоставляет встроенные средства управления TLS-сертификатами в кластере
и поддерживает:

- заказ сертификатов во всех поддерживаемых источниках;
- выпуск самоподписанных сертификатов;
- автоматический перевыпуск и контроль срока действия сертификатов;
- установку `cm-acme-http-solver` на master-узлы и выделенные узлы.

На этой странице описаны доступные в Deckhouse Platform Certified Security Edition возможности управления сертификатами,
а также порядок работы с издателями сертификатов.

#### Мониторинг

Deckhouse Platform Certified Security Edition экспортирует метрики в Prometheus, что позволяет отслеживать:

- срок действия сертификатов;
- статус перевыпуска сертификатов.

#### Роли доступа

В Deckhouse Platform Certified Security Edition предопределены несколько ролей для доступа к ресурсам:

| Роль       | Права доступа |
| ---------- | ------------- |
| `User`     | Просмотр ресурсов Certificate и Issuer в доступных пространствах имён, а также глобальных ресурсов ClusterIssuer. |
| `Editor`   | Управление ресурсами Certificate и Issuer в доступных пространствах имён. |
| `ClusterEditor` | Управление ресурсами Certificate и Issuer во всех пространствах имён. |
| `SuperAdmin` | Управление внутренними служебными объектами. |

#### Работа с издателями сертификатов

В Deckhouse Platform Certified Security Edition по умолчанию поддерживаются следующие издатели сертификатов (ClusterIssuer):

- `letsencrypt` — выпускает TLS-сертификаты, используя публичный удостоверяющий центр Let’s Encrypt
  и HTTP-валидацию по протоколу ACME.
  Используется для автоматического получения доверенных сертификатов, подходящих для большинства публичных сервисов.

- `letsencrypt-staging` — аналогичен `letsencrypt`, но использует тестовый сервер Let’s Encrypt.
  Подходит для отладки конфигурации и проверки процесса выпуска сертификатов.
  
- `selfsigned` — выпускает самоподписанные сертификаты.
  Используется в ситуациях, когда не требуется внешнее доверие к сертификату (например, для внутренних сервисов).

- `selfsigned-no-trust` — также выпускает самоподписанные сертификаты,
  но без автоматического добавления корневого сертификата в доверенные.
  Используется для ручного управления доверием.

В некоторых случаях вам могут понадобиться дополнительные виды ClusterIssuer:

- если вы хотите использовать сертификат от Let’s Encrypt, но с DNS-валидацией через стороннего DNS-провайдера;
- когда необходимо использовать удостоверяющий центр (CA), отличный от Let's Encrypt.

##### Добавление ClusterIssuer с валидацией DNS-01 через вебхук

Для подтверждения владения доменом через Let’s Encrypt с помощью метода `DNS-01` необходимо,
чтобы [модуль `cert-manager`](/modules/cert-manager/) мог создавать TXT-записи в зоне DNS, связанной с доменом.

У модуля `cert-manager` есть встроенная поддержка популярных DNS-провайдеров.

Если провайдер не поддерживается напрямую,
можно настроить вебхук и разместить в кластере собственный обработчик ACME-запросов,
который будет выполнять нужные операции для обновления DNS-записей.

Данный пример основан на использовании сервиса Yandex Cloud DNS:

1. Для обработки вебхука разместите в кластере сервис `Yandex Cloud DNS ACME webhook`
   согласно официальной документации.
1. Создайте ресурс ClusterIssuer, следуя примеру:

   ```yaml
   apiVersion: cert-manager.io/v1
   kind: ClusterIssuer
   metadata:
     name: yc-clusterissuer
     namespace: default
   spec:
     acme:
       # Заменить этот адрес электронной почты на свой собственный.
       # Let's Encrypt будет использовать его, чтобы связаться с вами по поводу истекающих
       # сертификатов и вопросов, связанных с вашей учетной записью.
       email: your@email.com
       server: https://acme-staging-v02.api.letsencrypt.org/directory
       privateKeySecretRef:
         # Ресурс секретов, который будет использоваться для хранения закрытого ключа аккаунта.
         name: secret-ref
       solvers:
         - dns01:
             webhook:
               config:
                 # Идентификатор папки, в которой расположена DNS-зона.
                 folder: <your-folder-ID>
                 # Секрет, используемый для доступа к учетной записи сервиса.
                 serviceAccountSecretRef:
                   name: cert-manager-secret
                   key: iamkey.json
               groupName: acme.cloud.yandex.com
               solverName: yandex-cloud-dns
   ```

##### Добавление ClusterIssuer, использующего собственный удостоверяющий центр (CA)

1. Сгенерируйте сертификат:

   ```shell
   openssl genrsa -out rootCAKey.pem 2048
   openssl req -x509 -sha256 -new -nodes -key rootCAKey.pem -days 3650 -out rootCACert.pem
   ```

1. В пространстве имён `d8-cert-manager` создайте секрет с произвольным именем, содержащий данные файлов сертификатов.

   - Пример создания секрета с помощью команды `d8 k`:

     ```shell
     d8 k create secret tls internal-ca-key-pair -n d8-cert-manager --key="rootCAKey.pem" --cert="rootCACert.pem"
     ```

   - Пример создания секрета из YAML-файла (содержимое файлов сертификатов должно быть закодировано в Base64):

     ```yaml
     apiVersion: v1
     data:
       tls.crt: <результат команды `cat rootCACert.pem | base64 -w0`>
       tls.key: <результат команды `cat rootCAKey.pem | base64 -w0`>
     kind: Secret
     metadata:
       name: internal-ca-key-pair
       namespace: d8-cert-manager
     type: Opaque
     ```

1. Создайте ClusterIssuer с произвольным именем из созданного секрета:

   ```yaml
   apiVersion: cert-manager.io/v1
   kind: ClusterIssuer
   metadata:
     name: inter-ca
   spec:
     ca:
       secretName: internal-ca-key-pair    # Имя созданного секрета.
   ```

Теперь можно использовать созданный ClusterIssuer для получения сертификатов
для всех компонентов Deckhouse Platform Certified Security Edition или конкретного компонента.

Например, чтобы использовать ClusterIssuer для получения сертификатов для всех компонентов Deckhouse Platform Certified Security Edition,
укажите его имя [в глобальном параметре `clusterIssuerName`](/reference/api/global.html#parameters-modules-https-certmanager-clusterissuername):

```yaml
  spec:
    settings:
      modules:
        https:
          certManager:
            clusterIssuerName: inter-ca
          mode: CertManager
        publicDomainTemplate: '%s.<public_domain_template>'
    version: 1
```

##### Добавление Issuer и ClusterIssuer, использующих HashiCorp Vault для заказа сертификатов

После настройки PKI и включения авторизации в Kubernetes, выполните следующее:

1. Создайте ServiceAccount и скопируйте ссылку на его Secret:

   ```shell
   d8 k create serviceaccount issuer
     
   ISSUER_SECRET_REF=$(d8 k get serviceaccount issuer -o json | jq -r ".secrets[].name")
   ```

1. Создайте ресурс Issuer:

   ```shell
   d8 k apply -f - <<EOF
   apiVersion: cert-manager.io/v1
   kind: Issuer
   metadata:
     name: vault-issuer
     namespace: default
   spec:
     vault:
       server: http://vault.default.svc.cluster.local:8200
       # Указывается на этапе конфигурации PKI.
       path: pki/sign/example-dot-com 
       auth:
         kubernetes:
           mountPath: /v1/auth/kubernetes
           role: issuer
           secretRef:
             name: $ISSUER_SECRET_REF
             key: token
   EOF
   ```

1. Создайте ресурс Certificate для получения TLS-сертификата, подписанного CA Vault:

   ```shell
   d8 k apply -f - <<EOF
   apiVersion: cert-manager.io/v1
   kind: Certificate
   metadata:
     name: example-com
     namespace: default
   spec:
     secretName: example-com-tls
     issuerRef:
       name: vault-issuer
     # Домены указываются на этапе конфигурации PKI в Vault.
     commonName: www.example.com 
     dnsNames:
     - www.example.com
   EOF
   ```

### Интеграция с KUMA и антивирусным ПО

Deckhouse Platform Certified Security Edition поддерживает интеграцию с [Kaspersky Unified Monitoring and Analysis Platform (KUMA)](https://lp.kaspersky.com/ru/kuma/),
единой системой мониторинга и анализа от «Лаборатории Касперского».
В рамках интеграции события безопасности и журналы аудита из кластера передаются в KUMA для дальнейшего анализа.

#### Отправка логов в KUMA

Для отправки логов в систему KUMA настройте сбор и доставку логов на стороне Deckhouse Platform Certified Security Edition,
используя следующие ресурсы:

- [ClusterLogDestination](/modules/log-shipper/cr.html#clusterlogdestination) — задаёт параметры хранилища логов;
- [ClusterLoggingConfig](/modules/log-shipper/cr.html#clusterloggingconfig) — задаёт параметры сбора логов из кластера.

{% alert level="info" %}
На стороне KUMA настройте соответствующие ресурсы для приёма событий.
{% endalert %}

##### Примеры конфигурации ClusterLogDestination и ClusterLoggingConfig

- Отправка логов в формате JSON по UDP:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: kuma-udp-json
  spec:
    type: Socket
    socket:
      address: IP_ADDRESS:PORT # Замените при настройке.
      mode: UDP
      encoding:
        codec: "JSON"
  ---
  apiVersion: deckhouse.io/v1alpha2
  kind: ClusterLoggingConfig
  metadata:
    name: kubelet-audit-logs
  spec:
    type: File
    file:
      include:
      - /var/log/kube-audit/audit.log
    destinationRefs:
      - kuma-udp-json
  ```

- Отправка логов в формате JSON по TCP:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: kuma-tcp-json
  spec:
    type: Socket
    socket:
      address: IP_ADDRESS:PORT # Замените при настройке.
      mode: TCP
      tcp:
        verifyCertificate: false
        verifyHostname: false
      encoding:
        codec: "JSON"
  ---
  apiVersion: deckhouse.io/v1alpha2
  kind: ClusterLoggingConfig
  metadata:
    name: kubelet-audit-logs
  spec:
    type: File
    file:
      include:
      - /var/log/kube-audit/audit.log
    destinationRefs:
      - kuma-tcp-json
  ```

- Отправка логов в формате CEF по TCP:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: kuma-tcp-cef
  spec:
    type: Socket
    socket:
      extraLabels:
        cef.name: d8
        cef.severity: "1"
      address: IP_ADDRESS:PORT # Замените при настройке.
      mode: TCP
      tcp:
        verifyCertificate: false
        verifyHostname: false
      encoding:
        codec: "CEF"
  ---
  apiVersion: deckhouse.io/v1alpha2
  kind: ClusterLoggingConfig
  metadata:
    name: kubelet-audit-logs
  spec:
    type: File
    file:
      include:
      - /var/log/kube-audit/audit.log
    logFilter:
      - field: userAgent
        operator: Regex
        values: [ "kubelet.*" ]
    destinationRefs:
      - kuma-tcp-cef
  ```

- Отправка логов в формате Syslog по TCP:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: kuma-tcp-syslog
  spec:
    type: Socket
    socket:
      address: IP_ADDRESS:PORT # Замените при настройке.
      mode: TCP
      tcp:
        verifyCertificate: false
        verifyHostname: false
      encoding:
        codec: "Syslog"
  ---
  apiVersion: deckhouse.io/v1alpha2
  kind: ClusterLoggingConfig
  metadata:
    name: kubelet-audit-logs
  spec:
    type: File
    file:
      include:
      - /var/log/kube-audit/audit.log
    logFilter:
      - field: userAgent
        operator: Regex
        values: [ "kubelet.*" ]
    destinationRefs:
      - kuma-tcp-syslog
  ```

- Отправка логов в Apache Kafka:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: kuma-kafka
  spec:
    type: Kafka
    kafka:
      bootstrapServers:
        - kafka-address:9092 # Замените при настройке.
      topic: k8s-logs
  ---
  apiVersion: deckhouse.io/v1alpha2
  kind: ClusterLoggingConfig
  metadata:
    name: kubelet-audit-logs
  spec:
    destinationRefs:
      - kuma-kafka
    file:
      include:
        - /var/log/kube-audit/audit.log
    logFilter:
      - field: userAgent
        operator: Regex
        values:
          - kubelet.*
    type: File
  ```

#### Исключения при антивирусном сканировании узлов

Если на узлах кластера Deckhouse Platform Certified Security Edition используются антивирусные средства (например, Kaspersky Endpoint Security (KESL)),
вам может понадобиться исключить из анализа служебные директории Deckhouse, чтобы избежать ложных срабатываний.

| Директория | Назначение |
| ---------- | ---------- |
| `/etc/cni/` (любой узел) | Файлы настройки модуля CNI |
| `/etc/kubernetes` (любой узел) | Манифесты статических подов, файлы сертификатов PKI |
| `/mnt/kubernetes-data` (master-узел) | Существует только в кластерах, развёрнутых в облаке, когда используется отдельный диск для базы данных etcd |
| `/mnt/vector-data` (любой узел, модуль `log-shipper`) | Служебные данные статуса отправленных журналов |
| `/opt/cni/bin/` (любой узел) | Исполняемые файлы модуля CNI |
| `/opt/deckhouse/bin/` (любой узел) | Исполняемые файлы, необходимые для работы Deckhouse |
| `/var/lib/bashible` (любой узел) | Файлы настройки узла |
| `/var/lib/containerd` (любой узел) | Используется для хранения данных, связанных с работой CRI (например, containerd). Содержит слои образов контейнеров, снимки файловых систем контейнеров метаинформацию, логи и другую информацию контейнеров |
| `/var/lib/deckhouse/` (master-узел) | Файлы модулей Deckhouse, динамически загружаемых из хранилища образов |
| `/var/lib/etcd` (master-узел) | База данных etcd |
| `/var/lib/kubelet/` (любой узел) | Файлы настройки kubelet |
| `/var/lib/upmeter` (master-узел, модуль `upmeter`) | База данных [модуля `upmeter`](/modules/upmeter/) |
| `/var/log/containers` (любой узел) | Журналы контейнеров (при использовании containerd) |
| `/var/log/pods/` (любой узел) | Журналы всех контейнеров подов, запущенных на узле |

##### Рекомендации по настройке KESL

Для корректной работы Deckhouse Platform Certified Security Edition при наличии установленного решения KESL выполните следующие шаги:

1. Отключите следующие задачи на стороне KESL:

   - `Firewall Management (ID: 12)`;
   - `Web Threat Protection (ID: 14)`;
   - `Network Threat Protection (ID: 17)`;
   - `Web Control (ID: 26)`.

   > В будущих версиях KESL список задач может отличаться.

1. Убедитесь, что ресурсы узлов соответствуют требованиям. В том числе для [KESL](https://support.kaspersky.com/KES4Linux/12.1.0/ru-RU/197642.htm).

1. Для оптимизации производительности следуйте [официальным рекомендациям «Лаборатории Касперского»](https://support.kaspersky.com/KES4Linux/12.1.0/ru-RU/206054.htm).
## Подсистема Кластер Kubernetes

### Модуль chrony

Обеспечивает синхронизацию времени на всех узлах кластера с помощью утилиты chrony.

#### Как работает

Модуль запускает `chrony` агенты на всех узлах кластера.
По умолчанию используется NTP сервер `pool.ntp.org`. NTP сервер можно изменить через [настройки](/modules/chrony/configuration.html) модуля.
Для просмотра используемых NTP серверов можно воспользоваться командой:

```bash
d8 k exec -it -n d8-chrony chrony-master-r7v6c -- chronyc -N sources
Defaulted container "chrony" out of: chrony, chrony-exporter, kube-rbac-proxy
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^* pool.ntp.org.                 2  10   377   171   -502us[ -909us] +/- 5388us
^- pool.ntp.org.                 2  10   377   666  -5317us[-5698us] +/-  103ms
^+ pool.ntp.org.                 2  10   377   938   -201us[ -567us] +/- 5346us
^+ pool.ntp.org.                 2  10   377   843   -159us[ -530us] +/-   12ms
```

`^+` - комбинируемый NTP сервер(`chrony` комбинирует информацию из `combined` серверов для уменьшения неточностей);  
`^*` - текущий NTP сервер;  
`^-` - некомбинируемый NTP сервер.

`chrony` агенты на мастер узлах и на остальных узлах имеют одно главное отличие - на всех узлах, которые не являются мастерами, в списке NTP серверов находятся не только NTP сервера из `module config`, но и адреса всех мастер узлов кластера.  

Таким образом, агенты на мастер узлах синхронизируют время только из списка хостов, указанных в `module config`(по умолчанию с `pool.ntp.org`). А агенты на остальных узлах синхронизируют время со списком NTP серверов из `module config` плюс с `chrony` агентов на мастер узлах.  

Это сделано для того, чтобы в случае недоступности NTP серверов, указанных в `module config`, время синхронизировалось с мастер узлами.

### Модуль chrony: настройки

 
<!-- SCHEMA -->

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: chrony
spec:
  enabled: true
  settings:
    ntpServers:
      - pool.ntp.org
      - ntp.ubuntu.com
      - time.google.com
  version: 1
```
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['chrony'].config-values | format_module_configuration: moduleKebabName }}

### Модуль chrony: FAQ

#### Как запретить использование chrony и использовать NTP-демоны на узлах?

1. [Выключите](configuration.html) модуль chrony.

1. Создайте `NodeGroupConfiguration` custom step, чтобы включить NTP-демоны на узлах (пример для `systemd-timesyncd`):

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: enable-ntp-on-node.sh
   spec:
     weight: 100
     nodeGroups: ["*"]
     bundles: ["*"]
     content: |
       systemctl enable systemd-timesyncd
       systemctl start systemd-timesyncd
   ```

### Управление control plane

Управление компонентами control plane кластера осуществляется с помощью модуля `control-plane-manager`, который запускается на всех master-узлах кластера (узлы с лейблом `node-role.kubernetes.io/control-plane: ""`).

Функции управления control plane:

- **Управление сертификатами**, необходимыми для работы control-plane, в том числе продление, выпуск при изменении конфигурации и т. п. Позволяет автоматически поддерживать безопасную конфигурацию control plane и быстро добавлять дополнительные SAN для организации защищенного доступа к API Kubernetes.
- **Настройка компонентов**. Автоматически создает необходимые конфигурации и манифесты компонентов `control-plane`.
- **Upgrade/downgrade компонентов**. Поддерживает в кластере одинаковые версии компонентов.
- **Управление конфигурацией etcd-кластера** и его членов. Масштабирует master-узлы, выполняет миграцию из single-master в multi-master и обратно.
- **Настройка kubeconfig**. Обеспечивает всегда актуальную конфигурацию для работы kubectl. Генерирует, продлевает, обновляет kubeconfig с правами cluster-admin и создает symlink пользователю root, чтобы kubeconfig использовался по умолчанию.
- **Расширение работы планировщика**, за счет подключения внешних плагинов через вебхуки. Управляется ресурсом [KubeSchedulerWebhookConfiguration](cr.html#kubeschedulerwebhookconfiguration). Позволяет использовать более сложную логику при решении задач планирования нагрузки в кластере. Например:
  - размещение подов приложений организации хранилища данных ближе к самим данным,
  - приоритизация узлов в зависимости от их состояния (сетевой нагрузки, состояния подсистемы хранения и т. д.),
  - разделение узлов на зоны, и т. п.

#### Управление сертификатами

Управляет SSL-сертификатами компонентов `control-plane`:

- Серверными сертификатами для `kube-apiserver` и `etcd`. Они хранятся в Secret'е `d8-pki` пространства имен `kube-system`:
  - корневой CA kubernetes (`ca.crt` и `ca.key`);
  - корневой CA etcd (`etcd/ca.crt` и `etcd/ca.key`);
  - RSA-сертификат и ключ для подписи Service Account'ов (`sa.pub` и `sa.key`);
  - корневой CA для extension API-серверов (`front-proxy-ca.key` и `front-proxy-ca.crt`).
- Клиентскими сертификатами для подключения компонентов `control-plane` друг к другу. Выписывает, продлевает и перевыписывает, если что-то изменилось (например, список SAN). Следующие сертификаты хранятся только на узлах:
  - серверный сертификат API-сервера (`apiserver.crt` и `apiserver.key`);
  - клиентский сертификат для подключения `kube-apiserver` к `kubelet` (`apiserver-kubelet-client.crt` и `apiserver-kubelet-client.key`);
  - клиентский сертификат для подключения `kube-apiserver` к `etcd` (`apiserver-etcd-client.crt` и `apiserver-etcd-client.key`);
  - клиентский сертификат для подключения `kube-apiserver` к extension API-серверам (`front-proxy-client.crt` и `front-proxy-client.key`);
  - серверный сертификат `etcd` (`etcd/server.crt` и `etcd/server.key`);
  - клиентский сертификат для подключения `etcd` к другим членам кластера (`etcd/peer.crt` и `etcd/peer.key`);
  - клиентский сертификат для подключения `kubelet` к `etcd` для helthcheck'ов (`etcd/healthcheck-client.crt` и `etcd/healthcheck-client.key`).

Также позволяет добавить дополнительные SAN в сертификаты, это дает возможность быстро и просто добавлять дополнительные «точки входа» в API Kubernetes.

При изменении сертификатов также автоматически обновляется соответствующая конфигурация kubeconfig.

#### Масштабирование

Поддерживается работа `control-plane` в конфигурации как *single-master*, так и *multi-master*.

В конфигурации *single-master*:

- `kube-apiserver` использует только тот экземпляр `etcd`, который размещен с ним на одном узле;
- На узле настраивается прокси-сервер, отвечающий на localhost,`kube-apiserver` отвечает на IP-адрес master-узла.

В конфигурации *multi-master* компоненты `control-plane` автоматически разворачиваются в отказоустойчивом режиме:

- `kube-apiserver` настраивается для работы со всеми экземплярами `etcd`.
- На каждом master-узле настраивается дополнительный прокси-сервер, отвечающий на localhost. Прокси-сервер по умолчанию обращается к локальному экземпляру `kube-apiserver`, но в случае его недоступности последовательно опрашивает остальные экземпляры `kube-apiserver`.

##### Масштабирование master-узлов

Масштабирование узлов `control-plane` осуществляется автоматически, с помощью лейбла `node-role.kubernetes.io/control-plane=""`:

- Установка лейбла `node-role.kubernetes.io/control-plane=""` на узле приводит к развертыванию на нем компонентов `control-plane`, подключению нового узла `etcd` в etcd-кластер, а также перегенерации необходимых сертификатов и конфигурационных файлов.
- Удаление лейбла `node-role.kubernetes.io/control-plane=""` с узла приводит к удалению всех компонентов `control-plane`, перегенерации необходимых конфигурационных файлов и сертификатов, а также корректному исключению узла из etcd-кластера.

{% alert level="warning" %}
При масштабировании узлов с 2 до 1 требуются [ручные действия](./faq.html#что-делать-если-кластер-etcd-развалился) с `etcd`. В остальных случаях все необходимые действия происходят автоматически. Обратите внимание, что при масштабировании с любого количества master-узлов до 1 рано или поздно на последнем шаге возникнет ситуация масштабирования узлов с 2 до 1.
{% endalert %}

#### Управление версиями

Обновление **patch-версии** компонентов control plane (то есть в рамках минорной версии, например с `1.29.13` на `1.29.14`) происходит автоматически вместе с обновлением версии Deckhouse. Управлять обновлением patch-версий нельзя.

Обновлением **минорной-версии** компонентов control plane (например, с `1.29.*` на `1.30.*`) можно управлять с помощью параметра [kubernetesVersion](/reference/api/cr.html#clusterconfiguration-kubernetesversion), в котором можно выбрать автоматический режим обновления (значение `Automatic`) или указать желаемую минорную версию control plane. Версию control plane, которая используется по умолчанию (при `kubernetesVersion: Automatic`).

Обновление control plane выполняется безопасно и для single-master-, и для multi-master-кластеров. Во время обновления может быть кратковременная недоступность API-сервера. На работу приложений в кластере обновление не влияет и может выполняться без выделения окна для регламентных работ.

Если указанная для обновления версия (с параметром [kubernetesVersion](/reference/api/cr.html#clusterconfiguration-kubernetesversion)) не соответствует текущей версии control plane в кластере, запускается умная стратегия изменения версий компонентов:

- Общие замечания:
  - Обновление в разных NodeGroup выполняется параллельно. Внутри каждой NogeGroup узлы обновляются последовательно, по одному.
- При upgrade:
  - Обновление происходит **последовательными этапами**, по одной минорной версии: 1.29 -> 1.30, 1.30 -> 1.31, 1.31 -> 1.32.
  - На каждом этапе сначала обновляется версия control plane, затем происходит обновление kubelet на узлах кластера.  
- При downgrade:
  - Успешное понижение версии гарантируется только на одну версию вниз от максимальной минорной версии control plane, когда-либо использовавшейся в кластере.
  - Сначала на узлах кластера выполняется понижение версии kubelet, после чего производится понижение версии компонентов control plane.

#### Аудит

Если требуется журналировать операции с API или отдебажить неожиданное поведение, для этого в Kubernetes предусмотрен Auditing. Его можно настроить путем создания правил Audit Policy, а результатом работы аудита будет лог-файл `/var/log/kube-audit/audit.log` со всеми интересующими операциями.

В установках Deckhouse по умолчанию созданы базовые политики, которые отвечают за логирование событий, которые:

- связаны с операциями создания, удаления и изменения ресурсов;
- совершаются от имен сервисных аккаунтов из системных Namespace `kube-system`, `d8-*`;
- совершаются с ресурсами в системных пространствах имен `kube-system`, `d8-*`.

Для выключения базовых политик установите флаг [basicAuditPolicyEnabled](configuration.html#parameters-apiserver-basicauditpolicyenabled) в `false`.

При настройке OIDC-аутентификации в аудит-логах дополнительно включается информация о пользователе в поле `user.extra`:
- `user-authn.deckhouse.io/name` — отображаемое имя пользователя
- `user-authn.deckhouse.io/preferred_username` — предпочитаемое имя пользователя
- `user-authn.deckhouse.io/dex-provider` — идентификатор провайдера Dex (требует scope `federated:id`)

Настройка политик аудита подробнее рассмотрена в [одноименной секции FAQ](faq.html#как-настроить-дополнительные-политики-аудита).

### Управление control plane: настройки

Некоторые параметры кластера, влияющие на управление control plane, также берутся из ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['control-plane-manager'].config-values | format_module_configuration: moduleKebabName }}

### Управление control plane: Custom Resources
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}

### Управление control plane: примеры

#### Подключение внешнего плагина планировщика

Пример подключения внешнего плагина планировщика через вебхук.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: KubeSchedulerWebhookConfiguration
metadata:
  name: sds-replicated-volume
webhooks:
- weight: 5
  failurePolicy: Ignore
  clientConfig:
    service:
      name: scheduler
      namespace: d8-sds-replicated-volume
      port: 8080
      path: /scheduler
    caBundle: ABCD=
  timeoutSeconds: 5
```

### Управление control plane: FAQ

<div id='как-добавить-master-узел'></div>

#### Как добавить master-узел в статическом или гибридном кластере?

> Важно иметь нечетное количество master-узлов для обеспечения кворума.

В процессе установки Deckhouse Platform Certified Security Edition с настройками по умолчанию в NodeGroup `master` отсутствует секция [`spec.staticInstances.labelSelector`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-labelselector) с настройками фильтра меток (label) по ресурсам `staticInstances`. Из-за этого после изменения количества узлов `staticInstances` в NodeGroup `master` (параметр [`spec.staticInstances.count`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-count)) при добавлении обычного узла с помощью Cluster API Provider Static (CAPS) он может быть «перехвачен» и добавлен в NodeGroup `master`, даже если в соответствующем ему `StaticInstance` (в `metadata`) указан лейбл с `role`, отличающейся от `master`.
Чтобы избежать этого «перехвата», после установки Deckhouse Platform Certified Security Edition измените NodeGroup `master` — добавьте в нее секцию [`spec.staticInstances.labelSelector`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-labelselector) с настройками фильтра меток (label) по ресурсам `staticInstances`. Пример NodeGroup `master` с `spec.staticInstances.labelSelector`:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: master
spec:
  nodeType: Static
  staticInstances:
    count: 2
    labelSelector:
      matchLabels:
        role: master
```

Далее при добавлении в кластер master-узлов с помощью CAPS указывайте в соответствующих им `StaticInstance` лейбл, заданный в `spec.staticInstances.labelSelector` NodeGroup `master`. Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: StaticInstance
metadata:
  name: static-master-1
  labels:
    # Лейбл, указанный в spec.staticInstances.labelSelector NodeGroup master.
    role: master
spec:
  # Укажите IP-адрес сервера статического узла.
  address: "<SERVER-IP>"
  credentialsRef:
    kind: SSHCredentials
    name: credentials
```

{% alert level="info" %}
При добавлении новых master-узлов с помощью CAPS и изменении в NodeGroup `master` количества master-узлов (параметр [`spec.staticInstances.count`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-count)) учитывайте следующее:

При бутстрапе кластера в конфигурации указывается первый master-узел, на который происходит установка.
Если после бутстрапа нужно сделать мультимастер и добавить master-узлы с помощь CAPS, в параметре `spec.staticInstances.count` NodeGroup `master` необходимо указать количество узлов на один меньше желаемого.

Например, если нужно сделать мультимастер с тремя master-узлами в `spec.staticInstances.count` NodeGroup `master` укажите значение `2` и создайте два `staticInstances` для добавляемых узлов. После их добавления в кластер количество master-узлов будет равно трём: master-узел, на который происходила установка и два master-узла, добавленные с помощью CAPS.
{% endalert %}

В остальном добавление master-узла в статический или гибридный кластер аналогично добавлению обычного узла.
Воспользуйтесь для этого соответствующими [примерами](./node-manager/examples.html#добавление-статического-узла-в-кластер). Все необходимые действия по настройке компонентов control plane кластера на новом узле будут выполнены автоматически, дождитесь их завершения — появления master-узлов в статусе `Ready`.

<div id='как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master'></div>

#### Как добавить master-узлы в облачном кластере?

Далее описана конвертация кластера с одним master-узлом в мультимастерный кластер.

> Перед добавлением узлов убедитесь в наличии необходимых квот.
>
> Важно иметь нечетное количество master-узлов для обеспечения кворума.

1. Сделайте [резервную копию `etcd`](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать созданию новых master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. **На локальной машине** запустите контейнер установщика Deckhouse соответствующей редакции и версии (измените адрес container registry при необходимости):

   ```bash
   DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}') 
   DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]' ) 
   docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" \
     registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
   ```

1. **В контейнере с инсталлятором** выполните следующую команду, чтобы проверить состояние перед началом работы:

   ```bash
   dhctl terraform check --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

   Ответ должен сообщить, что Terraform не нашел расхождений и изменений не требуется.

1. **В контейнере с инсталлятором** выполните следующую команду и укажите требуемое количество master-узлов в параметре `masterNodeGroup.replicas`:

   ```bash
   dhctl config edit provider-cluster-configuration --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST>
   ```

   > Для **Yandex Cloud**, при использовании внешних адресов на master-узлах, количество элементов массива в параметре [masterNodeGroup.instanceClass.externalIPAddresses](/modules/cloud-provider-yandex/cluster_configuration.html#yandexclusterconfiguration-masternodegroup-instanceclass-externalipaddresses) должно равняться количеству master-узлов. При использовании значения `Auto` (автоматический заказ публичных IP-адресов), количество элементов в массиве все равно должно соответствовать количеству master-узлов.
   >
   > Например, при трех master-узлах (`masterNodeGroup.replicas: 3`) и автоматическом заказе адресов, параметр `masterNodeGroup.instanceClass.externalIPAddresses` будет выглядеть следующим образом:
   >
   > ```bash
   > externalIPAddresses:
   > - "Auto"
   > - "Auto"
   > - "Auto"
   > ```

1. **В контейнере с инсталлятором** выполните следующую команду для запуска масштабирования:

   ```bash
   dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

1. Дождитесь появления необходимого количества master-узлов в статусе `Ready` и готовности всех экземпляров `control-plane-manager`:

   ```bash
   d8 k -n kube-system wait pod --timeout=10m --for=condition=ContainersReady -l app=d8-control-plane-manager
   ```

<div id='как-удалить-master-узел'></div>
<div id='как-уменьшить-число-master-узлов-в-облачном-кластере-multi-master-в-single-master'></div>

#### Как уменьшить число master-узлов в облачном кластере?

Далее описана конвертация мультимастерного кластера в кластер с одним master-узлом.

{% alert level="warning" %}
Описанные ниже шаги необходимо выполнять с первого по порядку master-узла кластера (master-0). Это связано с тем, что кластер всегда масштабируется по порядку: например, невозможно удалить узлы master-0 и master-1, оставив master-2.
{% endalert %}

1. Сделайте [резервную копию `etcd`](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать обновлению master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. **На локальной машине** запустите контейнер установщика Deckhouse соответствующей редакции и версии (измените адрес container registry при необходимости):

   ```bash
   DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}') 
   DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]' ) 
   docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" \
     registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
   ```

1. **В контейнере с инсталлятором** выполните следующую команду и укажите `1` в параметре `masterNodeGroup.replicas`:

   ```bash
   dhctl config edit provider-cluster-configuration --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> \
     --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

   > Для **Yandex Cloud** при использовании внешних адресов на master-узлах количество элементов массива в параметре [masterNodeGroup.instanceClass.externalIPAddresses](/modules/cloud-provider-yandex/cluster_configuration.html#yandexclusterconfiguration-masternodegroup-instanceclass-externalipaddresses) должно равняться количеству master-узлов. При использовании значения `Auto` (автоматический заказ публичных IP-адресов) количество элементов в массиве все равно должно соответствовать количеству master-узлов.
   >
   > Например, при одном master-узле (`masterNodeGroup.replicas: 1`) и автоматическом заказе адресов параметр `masterNodeGroup.instanceClass.externalIPAddresses` будет выглядеть следующим образом:
   >
   > ```yaml
   > externalIPAddresses:
   > - "Auto"
   > ```

1. Снимите следующие лейблы с удаляемых master-узлов:
   * `node-role.kubernetes.io/control-plane`
   * `node-role.kubernetes.io/master`
   * `node.deckhouse.io/group`

   Команда для снятия лейблов:

   ```bash
   d8 k label node <MASTER-NODE-N-NAME> node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- node.deckhouse.io/group-
   ```

1. Убедитесь, что удаляемые master-узлы пропали из списка узлов кластера etcd:

   ```bash
   d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o name | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ member list -w table
   ```

1. Выполните `drain` для удаляемых узлов:

   ```bash
   d8 k drain <MASTER-NODE-N-NAME> --ignore-daemonsets --delete-emptydir-data
   ```

1. Выключите виртуальные машины, соответствующие удаляемым узлам, удалите инстансы соответствующих узлов из облака и подключенные к ним диски (`kubernetes-data-master-<N>`).

1. Удалите в кластере поды, оставшиеся на удаленных узлах:

   ```bash
   d8 k delete pods --all-namespaces --field-selector spec.nodeName=<MASTER-NODE-N-NAME> --force
   ```

1. Удалите в кластере объекты `Node` удаленных узлов:

   ```bash
   d8 k delete node <MASTER-NODE-N-NAME>
   ```

1. **В контейнере с инсталлятором** выполните следующую команду для запуска масштабирования:

   ```bash
   dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

#### Как убрать роль master-узла, сохранив узел?

1. Сделайте [резервную копию etcd](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать обновлению master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. Снимите следующие лейблы:
   * `node-role.kubernetes.io/control-plane`
   * `node-role.kubernetes.io/master`
   * `node.deckhouse.io/group`

   Команда для снятия лейблов:

   ```bash
   d8 k label node <MASTER-NODE-N-NAME> node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- node.deckhouse.io/group-
   ```

1. Убедитесь, что удаляемый master-узел пропал из списка узлов кластера:

   ```bash
   d8 k -n kube-system exec -ti \
   $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o json | jq -r '.items[] | select( .status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | .metadata.name' | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ member list -w table
   ```

1. Зайдите на узел и выполните следующие команды:

   ```shell
   rm -f /etc/kubernetes/manifests/{etcd,kube-apiserver,kube-scheduler,kube-controller-manager}.yaml
   rm -f /etc/kubernetes/{scheduler,controller-manager}.conf
   rm -f /etc/kubernetes/authorization-webhook-config.yaml
   rm -f /etc/kubernetes/admin.conf /root/.kube/config
   rm -rf /etc/kubernetes/deckhouse
   rm -rf /etc/kubernetes/pki/{ca.key,apiserver*,etcd/,front-proxy*,sa.*}
   rm -rf /var/lib/etcd/member/
   ```

<div id='как-изменить-образ-ос-в-multi-master-кластере'></div>

#### Как изменить образ ОС в мультимастерном кластере?

1. Сделайте [резервную копию `etcd`](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать обновлению master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. **На локальной машине** запустите контейнер установщика Deckhouse соответствующей редакции и версии (измените адрес container registry при необходимости):

   ```bash
   DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}') 
   DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]' ) 
   docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" \
     registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
   ```

1. **В контейнере с инсталлятором** выполните следующую команду, чтобы проверить состояние перед началом работы:

   ```bash
   dhctl terraform check --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST> --ssh-host <MASTER-NODE-1-HOST> --ssh-host <MASTER-NODE-2-HOST>
   ```

   Ответ должен сообщить, что Terraform не нашел расхождений и изменений не требуется.

1. **В контейнере с инсталлятором** выполните следующую команду и укажите необходимый образ ОС в параметре `masterNodeGroup.instanceClass` (укажите адреса всех master-узлов в параметре `--ssh-host`):

   ```bash
   dhctl config edit provider-cluster-configuration --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST> --ssh-host <MASTER-NODE-1-HOST> --ssh-host <MASTER-NODE-2-HOST>
   ```

1. **В контейнере с инсталлятором** выполните следующую команду, чтобы провести обновление узлов:

   Внимательно изучите действия, которые планирует выполнить converge, когда запрашивает подтверждение.

   При выполнении команды узлы будут замены на новые с подтверждением на каждом узле. Замена будет выполняться по очереди в обратном порядке (2,1,0).

   ```bash
   dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST> --ssh-host <MASTER-NODE-1-HOST> --ssh-host <MASTER-NODE-2-HOST>
   ```

   Следующие действия (П. 9-12) **выполняйте поочередно на каждом** master-узле, начиная с узла с наивысшим номером (с суффиксом 2) и заканчивая узлом с наименьшим номером (с суффиксом 0).

1. **На созданном узле** откройте журнал systemd-юнита `bashible.service`. Дождитесь окончания настройки узла — в журнале должно появиться сообщение `nothing to do`:

   ```bash
   journalctl -fu bashible.service
   ```

1. Проверьте, что узел etcd отобразился в списке узлов кластера:

   ```bash
   d8 k -n kube-system exec -ti \
   $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o json | jq -r '.items[] | select( .status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | .metadata.name' | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ member list -w table
   ```

1. Убедитесь, что `control-plane-manager` функционирует на узле.

   ```bash
   d8 k -n kube-system wait pod --timeout=10m --for=condition=ContainersReady \
     -l app=d8-control-plane-manager --field-selector spec.nodeName=<MASTER-NODE-N-NAME>
   ```

1. Перейдите к обновлению следующего узла.

<div id='как-изменить-образ-ос-в-single-master-кластере'></div>

#### Как изменить образ ОС в кластере с одним master-узлом?

1. Преобразуйте кластер с одним master-узлом в мультимастерный в соответствии с [инструкцией](#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master).
1. Обновите master-узлы в соответствии с [инструкцией](#как-изменить-образ-ос-в-multi-master-кластере).
1. Преобразуйте мультимастерный кластер в кластер с одним master-узлом в соответствии с [инструкцией](#как-уменьшить-число-master-узлов-в-облачном-кластере)

<div id='как-посмотреть-список-memberов-в-etcd'></div>

#### Как посмотреть список узлов кластера в etcd?

##### Вариант 1

Используйте команду `etcdctl member list`.

Пример:

```shell
d8 k -n kube-system exec -ti \
$(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o json | jq -r '.items[] | select( .status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | .metadata.name' | head -n1) -- \
etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
--endpoints https://127.0.0.1:2379/ member list -w table
```

**Внимание.** Последний параметр в таблице вывода показывает, что узел находится в состоянии `learner`, а не в состоянии `leader`.

##### Вариант 2

Используйте команду `etcdctl endpoint status`. Для этой команды, после флага `--endpoints` нужно подставить адрес каждого узла control-plane. В пятом столбце таблицы вывода будет указано значение `true` для лидера.

Пример скрипта, который автоматически передает все адреса узлов control-plane:

```shell
MASTER_NODE_IPS=($(d8 k get nodes -l \
node-role.kubernetes.io/control-plane="" \
-o 'custom-columns=IP:.status.addresses[?(@.type=="InternalIP")].address' \
--no-headers))
unset ENDPOINTS_STRING
for master_node_ip in ${MASTER_NODE_IPS[@]}
do ENDPOINTS_STRING+="--endpoints https://${master_node_ip}:2379 "
done
d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod \
-l component=etcd,tier=control-plane -o name | head -n1) \
-- etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt  --cert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/ca.key \
$(echo -n $ENDPOINTS_STRING) endpoint status -w table
```

#### Что делать, если что-то пошло не так?

В процессе работы `control-plane-manager` автоматически создает резервные копии конфигурации и данных, которые могут пригодиться в случае возникновения проблем. Эти резервные копии сохраняются в директории `/etc/kubernetes/deckhouse/backup`. Если в процессе работы возникли ошибки или непредвиденные ситуации, вы можете использовать эти резервные копии для восстановления до предыдущего исправного состояния.

<div id='что-делать-если-кластер-etcd-развалился'></div>

#### Что делать, если кластер etcd не функционирует?

Если кластер etcd не функционирует и не удается восстановить его из резервной копии, вы можете попытаться восстановить его с нуля, следуя шагам ниже.

1. Сначала на всех узлах, которые являются частью вашего кластера etcd, кроме одного, удалите манифест `etcd.yaml`, который находится в директории `/etc/kubernetes/manifests/`. После этого только один узел останется активным, и с него будет происходить восстановление состояния мультимастерного кластера.
1. На оставшемся узле откройте файл манифеста `etcd.yaml` и укажите параметр `--force-new-cluster` в `spec.containers.command`.
1. После успешного восстановления кластера, удалите параметр `--force-new-cluster`.

 {% alert level="warning" %}
 Эта операция является деструктивной, так как она полностью уничтожает текущие данные и инициализирует кластер с состоянием, которое сохранено на узле. Все pending-записи будут утеряны.
 {% endalert %}

##### Что делать, если etcd постоянно перезапускается с ошибкой?

Этот способ может понадобиться, если использование параметра `--force-new-cluster` не восстанавливает работу etcd. Это может произойти, если converge master-узлов прошел неудачно, в результате чего новый master-узел был создан на старом диске etcd, изменил свой адрес в локальной сети, а другие master-узлы отсутствуют. Этот метод стоит использовать если контейнер etcd находится в бесконечном цикле перезапуска, а в его логах появляется ошибка: `panic: unexpected removal of unknown remote peer`.

1. Установите утилиту etcdutl.
1. С текущего локального снапшота базы etcd (`/var/lib/etcd/member/snap/db`) выполните создание нового снапшота:

   ```shell
   ./etcdutl snapshot restore /var/lib/etcd/member/snap/db --name <HOSTNAME> \
   --initial-cluster=HOSTNAME=https://<ADDRESS>:2380 --initial-advertise-peer-urls=https://ADDRESS:2380 \
   --skip-hash-check=true --data-dir /var/lib/etcdtest
   ```

   * `<HOSTNAME>` — название master-узла;
   * `<ADDRESS>` — адрес master-узла.

1. Выполните следующие команды для использования нового снапшота:

   ```shell
   cp -r /var/lib/etcd /tmp/etcd-backup
   rm -rf /var/lib/etcd
   mv /var/lib/etcdtest /var/lib/etcd
   ```

1. Найдите контейнеры `etcd` и `api-server`:

   ```shell
   crictl ps -a | egrep "etcd|apiserver"
   ```

1. Удалите найденные контейнеры `etcd` и `api-server`:

   ```shell
   crictl rm <CONTAINER-ID>
   ```

1. Перезапустите master-узел.

##### Что делать, если объем базы данных etcd достиг лимита, установленного в quota-backend-bytes?

Когда объем базы данных etcd достигает лимита, установленного параметром `quota-backend-bytes`, доступ к ней становится "read-only". Это означает, что база данных etcd перестает принимать новые записи, но при этом остается доступной для чтения данных. Вы можете понять, что столкнулись с подобной ситуацией, выполнив команду:

   ```shell
   d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o name | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ endpoint status -w table --cluster
   ```

Если в поле `ERRORS` вы видите подобное сообщение `alarm:NOSPACE`, значит вам нужно предпринять следующие шаги:

1. Найдите строку с `--quota-backend-bytes` в файле манифеста пода etcd, расположенного по пути `/etc/kubernetes/manifests/etcd.yaml` и увеличьте значение, умножив указанный параметр в этой строке на два. Если такой строки нет — добавьте, например: `- --quota-backend-bytes=8589934592`. Эта настройка задает лимит на 8 ГБ.

1. Сбросьте активное предупреждение (alarm) о нехватке места в базе данных. Для этого выполните следующую команду:

   ```shell
   d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o name | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ alarm disarm
   ```

1. Измените параметр [maxDbSize](configuration.html#parameters-etcd-maxdbsize) в настройках `control-plane-manager` на тот, который был задан в манифесте.

#### Как выполнить дефрагментацию etcd

{% alert level="warning" %}
Перед дефрагментацией [создайте резервную копию etcd](#как-сделать-резервную-копию-etcd-вручную).
{% endalert %}

Для просмотра размера БД etcd на определенном узле перед дефрагментацией и после ее выполнения используйте команду (здесь `NODE_NAME` — имя master-узла):

```bash
d8 k -n kube-system exec -it etcd-NODE_NAME -- /usr/bin/etcdctl \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  endpoint status --cluster -w table
```

Пример вывода (размер БД etcd на узле указывается в колонке `DB SIZE`):

```console
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
|          ENDPOINT           |        ID        | VERSION | STORAGE VERSION | DB SIZE | IN USE | PERCENTAGE NOT IN USE | QUOTA  | IS LEADER  | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | DOWNGRADE TARGET VERSION | DOWNGRADE ENABLED |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| https://192.168.199.80:2379 | 489a8af1e7acd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |       true |      false |        56 |  258054684 |          258054684 |        |                          |             false |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| https://192.168.199.81:2379 | 589a8ad1e7ccd7b0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| https://192.168.199.82:2379 | 229a8cd1e7bcd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
```

##### Как выполнить дефрагментацию etcd узла в кластере с одним master-узлом

{% alert level="warning" %}
Дефрагментация etcd — ресурсоемкая операция, которая на время полностью блокирует работу etcd на данном узле.
Учитывайте это при выборе времени для проведения операции в кластере с одним master-узлом.
{% endalert %}

Чтобы выполнить дефрагментацию etcd в кластере с одним master-узлом, используйте следующую команду (здесь `NODE_NAME` — имя master-узла):

```bash
d8 k -n kube-system exec -ti etcd-NODE_NAME -- /usr/bin/etcdctl \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --cert /etc/kubernetes/pki/etcd/ca.crt \
  --key /etc/kubernetes/pki/etcd/ca.key \
  --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
```

Пример вывода при успешном выполнении операции:

```console
Finished defragmenting etcd member[https://localhost:2379]. took 848.948927ms
```

> При появлении ошибки из-за таймаута увеличивайте значение параметра `–command-timeout` из команды выше, пока дефрагментация не выполнится успешно.

##### Как выполнить дефрагментацию etcd в кластере с несколькими master-узлами

Чтобы выполнить дефрагментацию etcd в кластере с несколькими master-узлами:

1. Получите список подов etcd. Для этого используйте следующую команду:

   ```bash
   d8 k -n kube-system get pod -l component=etcd -o wide
   ```

   Пример вывода:

   ```console
   NAME           READY    STATUS    RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES
   etcd-master-0   1/1     Running   0          3d21h   192.168.199.80  master-0    <none>           <none>
   etcd-master-1   1/1     Running   0          3d21h   192.168.199.81  master-1    <none>           <none>
   etcd-master-2   1/1     Running   0          3d21h   192.168.199.82  master-2    <none>           <none>
   ```

1. Определите master-узел — лидер. Для этого обратитесь к любому поду etcd и получите список узлов — участников кластера etcd с помощью команды (где `NODE_NAME` — имя master-узла):

   ```bash
   d8 k -n kube-system exec -it etcd-NODE_NAME -- /usr/bin/etcdctl \
     --cert=/etc/kubernetes/pki/etcd/server.crt \
     --key=/etc/kubernetes/pki/etcd/server.key \
     --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     endpoint status --cluster -w table
   ```

   Пример вывода (у лидера в колонке `IS LEADER` будет значение `true`):

   ```console
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   |          ENDPOINT           |        ID        | VERSION | STORAGE VERSION | DB SIZE | IN USE | PERCENTAGE NOT IN USE | QUOTA  | IS LEADER  | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | DOWNGRADE TARGET VERSION | DOWNGRADE ENABLED |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   | https://192.168.199.80:2379 | 489a8af1e7acd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |       true |      false |        56 |  258054684 |          258054684 |        |                          |             false |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   | https://192.168.199.81:2379 | 589a8ad1e7ccd7b0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   | https://192.168.199.82:2379 | 229a8cd1e7bcd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   ```

1. Поочередно выполните дефрагментацию etcd узлов — участников etcd кластера. Для дефрагментации используйте команду (здесь `NODE_NAME` — имя master-узла):

   > Важно: дефрагментацию лидера необходимо выполнять в последнюю очередь.
   >
   > Восстановление etcd на узле после дефрагментации может занять некоторое время. Рекомендуется подождать не менее минуты прежде чем переходить к дефрагментации etcd следующего узла.

   ```bash
   d8 k -n kube-system exec -ti etcd-NODE_NAME -- /usr/bin/etcdctl \
     --cacert /etc/kubernetes/pki/etcd/ca.crt \
     --cert /etc/kubernetes/pki/etcd/ca.crt \
     --key /etc/kubernetes/pki/etcd/ca.key \
     --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
   ```

   Пример вывода при успешном выполнении операции:

   ```console
   Finished defragmenting etcd member[https://localhost:2379]. took 848.948927ms
   ```

   > При появлении ошибки из-за таймаута увеличивайте значение параметра `–command-timeout` из команды выше, пока дефрагментация не выполнится успешно.

#### Как настроить дополнительные политики аудита?

1. Включите параметр [auditPolicyEnabled](configuration.html#parameters-apiserver-auditpolicyenabled) в настройках модуля:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: control-plane-manager
   spec:
     version: 1
     settings:
       apiserver:
         auditPolicyEnabled: true
   ```

2. Создайте Secret `kube-system/audit-policy` с YAML-файлом политик, закодированным в Base64:

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: audit-policy
     namespace: kube-system
   data:
     audit-policy.yaml: <base64>
   ```

   Минимальный рабочий пример `audit-policy.yaml` выглядит так:

   ```yaml
   apiVersion: audit.k8s.io/v1
   kind: Policy
   rules:
   - level: Metadata
     omitStages:
     - RequestReceived
   ```

##### Как исключить встроенные политики аудита?

Установите параметр [apiserver.basicAuditPolicyEnabled](configuration.html#parameters-apiserver-basicauditpolicyenabled) модуля в `false`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    apiserver:
      auditPolicyEnabled: true
      basicAuditPolicyEnabled: false
```

##### Как вывести аудит-лог в стандартный вывод вместо файлов?

Установите параметр [apiserver.auditLog.output](configuration.html#parameters-apiserver-auditlog) модуля в значение `Stdout`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    apiserver:
      auditPolicyEnabled: true
      auditLog:
        output: Stdout
```

##### Как работать с журналом аудита?

Предполагается, что на master-узлах установлен «скрейпер логов»: [log-shipper](./log-shipper/cr.html#clusterloggingconfig), `promtail`, `filebeat`,  который будет мониторить файл с логами:

```bash
/var/log/kube-audit/audit.log
```

Параметры ротации логов в файле журнала предустановлены и их изменение не предусмотрено:

* Максимальное занимаемое место на диске `1000 МБ`.
* Максимальная глубина записи `30 дней`.

В зависимости от настроек политики (`Policy`) и количества запросов к `apiserver` логов может быть очень много, соответственно глубина хранения может быть менее 30 минут.

{% alert level="warning" %}
Текущая реализация функционала не гарантирует безопасность, так как существует риск временного нарушения работы control plane.

Если в Secret'е с конфигурационным файлом окажутся неподдерживаемые опции или опечатка, `apiserver` не сможет запуститься.
{% endalert %}

В случае возникновения проблем с запуском `apiserver`, потребуется вручную отключить параметры `--audit-log-*` в манифесте `/etc/kubernetes/manifests/kube-apiserver.yaml` и перезапустить `apiserver` следующей командой:

```bash
docker stop $(docker ps | grep kube-apiserver- | awk '{print $1}')
### Или (в зависимости используемого вами CRI).
crictl stopp $(crictl pods --name=kube-apiserver -q)
```

После перезапуска будет достаточно времени исправить Secret или удалить его:

```bash
d8 k -n kube-system delete secret audit-policy
```

#### Как ускорить перезапуск подов при потере связи с узлом?

По умолчанию, если узел в течении 40 секунд не сообщает свое состояние, он помечается как недоступный. И еще через 5 минут поды узла начнут перезапускаться на других узлах.  В итоге общее время недоступности приложений составляет около 6 минут.

В специфических случаях, когда приложение не может быть запущено в нескольких экземплярах, есть способ сократить период их недоступности:

1. Уменьшить время перехода узла в состояние `Unreachable` при потере с ним связи настройкой параметра `nodeMonitorGracePeriodSeconds`.
1. Установить меньший таймаут удаления подов с недоступного узла в параметре `failedNodePodEvictionTimeoutSeconds`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    nodeMonitorGracePeriodSeconds: 10
    failedNodePodEvictionTimeoutSeconds: 50
```

В этом случае при потере связи с узлом приложения будут перезапущены примерно через 1 минуту.

Оба упомянутых параметра напрямую влияют на использование процессора и памяти control-plane'ом. Снижая таймауты, системные компоненты чаще отправляют статусы и проверяют состояние ресурсов.

При выборе оптимальных значений учитывайте графики использования ресурсов управляющих узлов. Чем меньше значения параметров, тем больше ресурсов может понадобиться для их обработки на этих узлах.

#### Резервное копирование и восстановление etcd

##### Что выполняется автоматически

Автоматически запускаются CronJob `kube-system/d8-etcd-backup-*` в 00:00 по UTC+0. Результат сохраняется в `/var/lib/etcd/etcd-backup.tar.gz` на всех узлах с `control-plane` в кластере (master-узлы).

<div id='как-сделать-бэкап-etcd-вручную'></div>

##### Как сделать резервную копию etcd вручную

###### Используя Deckhouse CLI (Deckhouse Platform Certified Security Edition v1.65+)

Начиная с релиза Deckhouse Platform Certified Security Edition v1.65, стала доступна утилита `d8 backup etcd`, которая предназначена для быстрого создания снимков состояния etcd.

```bash
d8 backup etcd ./etcd-backup.snapshot
```

###### Используя bash (Deckhouse Platform Certified Security Edition v1.64 и старше)

Войдите на любой control-plane узел под пользователем `root` и используйте следующий bash-скрипт:

```bash
###!/usr/bin/env bash
set -e

pod=etcd-`hostname`
d8 k -n kube-system exec "$pod" -- /usr/bin/etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key --endpoints https://127.0.0.1:2379/ snapshot save /var/lib/etcd/${pod##*/}.snapshot && \
mv /var/lib/etcd/"${pod##*/}.snapshot" etcd-backup.snapshot && \
cp -r /etc/kubernetes/ ./ && \
tar -cvzf kube-backup.tar.gz ./etcd-backup.snapshot ./kubernetes/
rm -r ./kubernetes ./etcd-backup.snapshot
```

В текущей директории будет создан файл `kube-backup.tar.gz` со снимком базы etcd одного из узлов кластера.
Из полученного снимка можно будет восстановить состояние кластера.

Рекомендуем сделать резервную копию директории `/etc/kubernetes`, в которой находятся:

* манифесты и конфигурация компонентов control-plane;
* PKI кластера Kubernetes.

Данная директория поможет быстро восстановить кластер при полной потере control-plane узлов без создания нового кластера и без повторного присоединения узлов в новый кластер.

Рекомендуем хранить резервные копии снимков состояния кластера etcd, а также резервную копию директории `/etc/kubernetes/` в зашифрованном виде вне кластера Deckhouse.

##### Как выполнить полное восстановление состояния кластера из резервной копии etcd?

Далее описаны шаги по восстановлению кластера до предыдущего состояния из резервной копии при полной потере данных.

<div id='восстановление-кластера-single-master'></div>

###### Восстановление кластера с одним master-узлом

Для корректного восстановления выполните следующие шаги на master-узле:

1. Найдите утилиту `etcdutl` на master-узле и скопируйте исполняемый файл в `/usr/local/bin/`:

   ```shell
   cp $(find /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/ \
   -name etcdutl -print | tail -n 1) /usr/local/bin/etcdutl
   etcdutl version
   ```

   Должен отобразиться корректный вывод `etcdutl version` без ошибок.

   Также вы можете загрузить исполняемый файл etcdutl:

   ```shell
   wget "https://github.com/etcd-io/etcd/releases/download/v3.6.1/etcd-v3.6.1-linux-amd64.tar.gz"
   tar -xzvf etcd-v3.6.1-linux-amd64.tar.gz && mv etcd-v3.6.1-linux-amd64/etcdutl /usr/local/bin/etcdutl
   ```

   Проверить версию etcd в кластере можно выполнив следующую команду (команда может не сработать, если etcd и Kubernetes API недоступны):

   ```shell
   d8 k -n kube-system exec -ti etcd-$(hostname) -- etcdutl version
   ```

1. Остановите etcd.

   ```shell
   mv /etc/kubernetes/manifests/etcd.yaml ~/etcd.yaml
   ```

1. Сохраните текущие данные etcd.

   ```shell
   cp -r /var/lib/etcd/member/ /var/lib/deckhouse-etcd-backup
   ```

1. Очистите директорию etcd.

   ```shell
   rm -rf /var/lib/etcd
   ```

1. Положите резервную копию etcd в файл `~/etcd-backup.snapshot`.

1. Восстановите базу данных etcd.

   ```shell
   ETCDCTL_API=3 etcdutl snapshot restore ~/etcd-backup.snapshot  --data-dir=/var/lib/etcd
   ```

1. Запустите etcd. Запуск может занять некоторое время.

   ```shell
   mv ~/etcd.yaml /etc/kubernetes/manifests/etcd.yaml
      ```

   Чтобы убедиться, что etcd запущена, воспользуйтесь командой:

   ```shell
   crictl ps --label io.kubernetes.pod.name=etcd-$HOSTNAME
   ```

   Пример вывода:

   ```console
   CONTAINER        IMAGE            CREATED              STATE     NAME      ATTEMPT     POD ID          POD
   4b11d6ea0338f    16d0a07aa1e26    About a minute ago   Running   etcd      0           ee3c8c7d7bba6   etcd-gs-test
   ```

1. Перезапустите master-узел.

<div id='восстановление-кластера-multi-master'></div>

###### Восстановление мультимастерного кластера

Для корректного восстановления выполните следующие шаги:

1. Включите режим High Availability (HA) с помощью глобального параметра [highAvailability](/reference/api/global.html#parameters-highavailability). Это необходимо для сохранения хотя бы одной реплики Prometheus и его PVC, поскольку в режиме кластера с одним master-узлом HA по умолчанию отключён.

1. Переведите кластер в режим с одним master-узлом в соответствии с [инструкцией](#как-уменьшить-число-master-узлов-в-облачном-кластере) для облачных кластеров, или самостоятельно выведите статические master-узлы из кластера.

1. На оставшемся единственном master-узле выполните шаги по восстановлению etcd из резервной копии в соответствии с [инструкцией](#восстановление-кластера-single-master) для кластера с одним master-узлом.

1. Когда работа etcd будет восстановлена, удалите из кластера информацию об уже удаленных в первом пункте master-узлах, воспользовавшись следующей командой (укажите название узла):

   ```shell
   d8 k delete node <MASTER_NODE_I>
   ```

1. Перезапустите все узлы кластера.

1. Дождитесь выполнения заданий из очереди Deckhouse:

   ```shell
   d8 s queue main
   ```

1. Переведите кластер обратно в режим мультимастерного в соответствии с [инструкцией](#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master) для облачных кластеров или [инструкцией](#как-добавить-master-узел-в-статическом-или-гибридном-кластере) для статических или гибридных кластеров.

##### Как восстановить объект Kubernetes из резервной копии etcd?

Чтобы получить данные определенных объектов кластера из резервной копии etcd:

1. Запустите временный экземпляр etcd.
1. Заполните его данными из [резервной копии](#как-сделать-бэкап-etcd-вручную).
1. Получите описания нужных объектов с помощью `auger`.

###### Пример шагов по восстановлению объектов из резервной копии etcd

В следующем примере `etcd-backup.snapshot` — [резервная копия](#как-сделать-бэкап-etcd-вручную) etcd (snapshot), `infra-production` — пространство имен, в котором нужно восстановить объекты.

* Для выгрузки бинарных данных из etcd потребуется утилита auger.

  ```shell
  git clone -b v1.0.1 --depth 1 https://github.com/etcd-io/auger
  cd auger
  make release
  build/auger -h
  ```
  
* Получившийся исполняемый файл `build/auger`, а также `snapshot` из резервной копии etcd нужно загрузить на master-узел, с которого будет выполняться дальнейшие действия.

Данные действия выполняются на master-узле в кластере, на который предварительно был загружен файл `snapshot` и утилита `auger`:

1. Установите корректные права доступа для файла с резервной копией:

   ```shell
   chmod 644 etcd-backup.snapshot
   ```

1. Установите полный путь до `snapshot` и до утилиты в переменных окружения:

   ```shell
   SNAPSHOT=/root/etcd-restore/etcd-backup.snapshot
   AUGER_BIN=/root/auger 
   chmod +x $AUGER_BIN
   ```

1. Запустите под с временным экземпляром etcd:

   * Создайте манифест пода. Он будет запускаться именно на текущем master-узле, выбрав его по переменной `$HOSTNAME`, и смонтирует `snapshot` по пути `$SNAPSHOT` для загрузки во временный экземпляр etcd:

     ```shell
     cat <<EOF >etcd.pod.yaml 
     apiVersion: v1
     kind: Pod
     metadata:
       name: etcdrestore
       namespace: default
     spec:
       nodeName: $HOSTNAME
       tolerations:
       - operator: Exists
       initContainers:
       - command:
         - etcdutl
         - snapshot
         - restore
         - "/tmp/etcd-snapshot"
         - --data-dir=/default.etcd
         image: $(kubectl -n kube-system get pod -l component=etcd -o jsonpath="{.items[*].spec.containers[*].image}" | cut -f 1 -d ' ')
         imagePullPolicy: IfNotPresent
         name: etcd-snapshot-restore
         # Раскоментируйте фрагмент ниже, чтобы задать лимиты для контейнера, если ресурсов узла недостаточно для его запуска.
         # resources:
         #   requests:
         #     ephemeral-storage: "200Mi"
         #   limits:
         #     ephemeral-storage: "500Mi"
         volumeMounts:
         - name: etcddir
           mountPath: /default.etcd
         - name: etcd-snapshot
           mountPath: /tmp/etcd-snapshot
           readOnly: true
       containers:
       - command:
         - etcd
         image: $(kubectl -n kube-system get pod -l component=etcd -o jsonpath="{.items[*].spec.containers[*].image}" | cut -f 1 -d ' ')
         imagePullPolicy: IfNotPresent
         name: etcd-temp
         volumeMounts:
         - name: etcddir
           mountPath: /default.etcd
       volumes:
       - name: etcddir
         emptyDir: {}
         # Используйте фрагмент ниже вместо emptyDir: {}, чтобы задать лимиты для контейнера, если ресурсов узла недостаточно для его запуска.
         # emptyDir:
         #  sizeLimit: 500Mi
       - name: etcd-snapshot
         hostPath:
           path: $SNAPSHOT
           type: File
     EOF
     ```

   * Запустите под:

     ```shell
     d8 k create -f etcd.pod.yaml
     ```

1. Установите нужные переменные. В текущем примере:

   * `infra-production` - пространство имен, в котором мы будем искать ресурсы.

   * `/root/etcd-restore/output` - каталог для восстановленных манифестов.

   * `/root/auger` - путь до исполняемого файла утилиты `auger`:

     ```shell
     FILTER=infra-production
     BACKUP_OUTPUT_DIR=/root/etcd-restore/output
     mkdir -p $BACKUP_OUTPUT_DIR && cd $BACKUP_OUTPUT_DIR
     ```

1. Следующие команды отфильтруют список нужных ресурсов по переменной `$FILTER` и выгрузят их в каталог `$BACKUP_OUTPUT_DIR`:

   ```shell
   files=($(d8 k -n default exec etcdrestore -c etcd-temp -- etcdctl  --endpoints=localhost:2379 get / --prefix --keys-only | grep "$FILTER"))
   for file in "${files[@]}"
   do
     OBJECT=$(d8 k -n default exec etcdrestore -c etcd-temp -- etcdctl  --endpoints=localhost:2379 get "$file" --print-value-only | $AUGER_BIN decode)
     FILENAME=$(echo $file | sed -e "s#/registry/##g;s#/#_#g")
     echo "$OBJECT" > "$BACKUP_OUTPUT_DIR/$FILENAME.yaml"
     echo $BACKUP_OUTPUT_DIR/$FILENAME.yaml
   done
   ```

1. Удалите из полученных описаний объектов информацию о времени создания (`creationTimestamp`), `UID`, `status` и прочие оперативные данные, после чего восстановите объекты:

   ```bash
   d8 k create -f deployments_infra-production_supercronic.yaml
   ```

1. Удалите под с временным экземпляром etcd:

   ```bash
   d8 k -n default delete pod etcdrestore
   ```

#### Как выбирается узел, на котором будет запущен под?

За распределение подов по узлам отвечает планировщик Kubernetes (компонент `scheduler`).
Он проходит через две основные фазы — `Filtering` и `Scoring` (на самом деле, фаз больше, например, `pre-filtering` и `post-filtering`, но в общем можно выделить две ключевые фазы).

##### Общее устройство планировщика Kubernetes

Планировщик состоит из плагинов, которые работают в рамках какой-либо фазы (фаз).

Примеры плагинов:

* **ImageLocality** — отдает предпочтение узлам, на которых уже есть образы контейнеров, которые используются в запускаемом поде. Фаза: `Scoring`.
* **TaintToleration** — реализует механизм taints and tolerations. Фазы: `Filtering`, `Scoring`.
* **NodePorts** — проверяет, есть ли у узла свободные порты, необходимые для запуска пода. Фаза: `Filtering`.

##### Логика работы

###### Профили планировщика

Есть два преднастроенных профиля планировщика:

* `default-scheduler` — профиль по умолчанию, который распределяет поды на узлы с наименьшей загрузкой;
* `high-node-utilization` — профиль, при котором поды размещаются на узлах с наибольшей загрузкой.

Чтобы задать профиль планировщика, укажите его параметре `spec.schedulerName` манифеста пода.

Пример использования профиля:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: scheduler-example
  labels:
    name: scheduler-example
spec:
  schedulerName: high-node-utilization
  containers:
  - name: example-pod
    image: registry.k8s.io/pause:2.0  
```

###### Этапы планирования подов

На первой фазе — `Filtering` — активируются плагины фильтрации (filter-плагины), которые из всех доступных узлов выбирают те, которые удовлетворяют определенным условиям фильтрации (например, `taints`, `nodePorts`, `nodeName`, `unschedulable` и другие). Если узлы расположены в разных зонах, планировщик чередует выбор зон, чтобы избежать размещения всех подов в одной зоне.

Предположим, что узлы распределяются по зонам следующим образом:

```text
Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
```

В этом случае они будут выбираться в следующем порядке:

```text
Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
```

Обратите внимание, что с целью оптимизации выбираются не все попадающие под условия узлы, а только их часть. По умолчанию функция выбора количества узлов линейная. Для кластера из ≤50 узлов будут выбраны 100% узлов, для кластера из 100 узлов — 50%, а для кластера из 5000 узлов — 10%. Минимальное значение — 5% при количестве узлов более 5000. Таким образом, при настройках по умолчанию узел может не попасть в список возможных узлов для запуска.

После того как были выбраны узлы, соответствующие условиям фильтрации, запускается фаза `Scoring`. Каждый плагин анализирует список отфильтрованных узлов и назначает оценку (score) каждому узлу. Оценки от разных плагинов суммируются. На этой фазе оцениваются доступные ресурсы на узлах: `pod capacity`, `affinity`, `volume provisioning` и другие. По итогам этой фазы выбирается узел с наибольшей оценкой. Если сразу несколько узлов получили максимальную оценку, узел выбирается случайным образом.

В итоге под запускается на выбранном узле.

###### Документация

##### Как изменить или расширить логику работы планировщика

Для изменения логики работы планировщика можно использовать механизм плагинов расширения.

Каждый плагин представляет собой вебхук, отвечающий следующим требованиям:

* Использование TLS.
* Доступность через сервис внутри кластера.
* Поддержка стандартных `Verbs` (`filterVerb = filter`, `prioritizeVerb = prioritize`).
* Также, предполагается что все подключаемые плагины могут кэшировать информацию об узле (`nodeCacheCapable: true`).

Подключить `extender` можно при помощи ресурса [KubeSchedulerWebhookConfiguration](cr.html#kubeschedulerwebhookconfiguration).

{% alert level="danger" %}
При использовании опции `failurePolicy: Fail`, в случае ошибки в работе вебхука планировщик Kubernetes прекратит свою работу, и новые поды не смогут быть запущены.
{% endalert %}

#### Как происходит ротация сертификатов kubelet?

В Deckhouse Platform Certified Security Edition ротация сертификатов kubelet происходит автоматически.

Kubelet использует клиентский TLS-сертификат (`/var/lib/kubelet/pki/kubelet-client-current.pem`), при помощи которого может запросить у kube-apiserver новый клиентский сертификат или новый серверный сертификат (`/var/lib/kubelet/pki/kubelet-server-current.pem`).

Когда до истечения времени жизни сертификата остается 5-10% (случайное значение из диапазона) времени, kubelet запрашивает у kube-apiserver новый сертификат.

##### Время жизни сертификатов

По умолчанию время жизни сертификатов равно 1 году (8760 часов). При необходимости это значение можно изменить с помощью аргумента `--cluster-signing-duration` в манифесте `/etc/kubernetes/manifests/kube-controller-manager.yaml`. Но чтобы kubelet успел установить сертификат до его истечения, рекомендуем устанавливать время жизни сертификатов более, чем 1 час.

{% alert level="warning" %}
Если истекло время жизни клиентского сертификата, то kubelet не сможет делать запросы к kube-apiserver и не сможет обновить сертификаты. В данном случае узел (Node) будет помечен как `NotReady` и пересоздан.
{% endalert %}

##### Особенности работы с серверными сертификатами kubelet в Deckhouse Platform Certified Security Edition

В Deckhouse Platform Certified Security Edition для запросов в kubelet API используются IP-адреса. Поэтому в конфигурации kubelet поля `tlsCertFile` и `tlsPrivateKeyFile` не указываются, а используется динамический сертификат, который kubelet генерирует самостоятельно. Также, из-за использования динамического сертификата, в Deckhouse Platform Certified Security Edition (в модуле `operator-trivy`) отключены проверки CIS benchmark `AVD-KCV-0088` и `AVD-KCV-0089`, которые отслеживают, были ли переданы аргументы `--tls-cert-file` и `--tls-private-key-file` для kubelet.

{% offtopic title="Информация о логике работы с серверными сертификатами в Kubernetes" %}

В kubelet реализована следующая логика работы с серверными сертификатами:

* Если `tlsCertFile` и `tlsPrivateKeyFile` не пустые, то kubelet будет использовать их как сертификат и ключ по умолчанию.
  * При запросе клиента в kubelet API с указанием IP-адреса (например `https://10.1.1.2:10250/`), для установления соединения по TLS-протоколу будет использован закрытый ключ по умолчанию (`tlsPrivateKeyFile`). В данном случае ротация сертификатов не будет работать.
  * При запросе клиента в kubelet API с указанием названия хоста (например `https://k8s-node:10250/`), для установления соединения по TLS-протоколу будет использован динамически сгенерированный закрытый ключ из директории `/var/lib/kubelet/pki/`. В данном случае ротация сертификатов будет работать.
* Если `tlsCertFile` и `tlsPrivateKeyFile` пустые, то для установления соединения по TLS-протоколу будет использован динамически сгенерированный закрытый ключ из директории `/var/lib/kubelet/pki/`. В данном случае ротация сертификатов будет работать.
{% endofftopic %}

#### Как вручную обновить сертификаты компонентов управляющего слоя?

Может возникнуть ситуация, когда master-узлы кластера находятся в выключенном состоянии долгое время. За это время может истечь срок действия сертификатов компонентов управляющего слоя. После включения узлов сертификаты не обновятся автоматически, поэтому это необходимо сделать вручную.

Обновление сертификатов компонентов управляющего слоя происходит с помощью утилиты `kubeadm`.
Чтобы обновить сертификаты, выполните следующие действия на каждом master-узле:

1. Найдите утилиту `kubeadm` на master-узле и создайте символьную ссылку c помощью следующей команды:

   ```shell
   ln -s  $(find /var/lib/containerd  -name kubeadm -type f -executable -print) /usr/bin/kubeadm
   ```

2. Обновите сертификаты:

   ```shell
   kubeadm certs renew all
   ```

### Управление узлами

#### Основные функции

Управление узлами осуществляется с помощью модуля `node-manager`, основные функции которого:

1. Управление несколькими узлами как связанной группой (**NodeGroup**):
    * Возможность определить метаданные, которые наследуются всеми узлами группы.
    * Мониторинг группы узлов как единой сущности (группировка узлов на графиках по группам, группировка алертов о недоступности узлов, алерты о недоступности N узлов или N% узлов группы).
2. Систематическое прерывание работы узлов — **Chaos Monkey**. Предназначено для верификации отказоустойчивости элементов кластера и запущенных приложений.
3. Установка/обновление и настройка ПО узла (containerd, kubelet и др.), подключение узла в кластер:
    * Установка операционной системы вне зависимости от типа используемой инфраструктуры (в любом облаке или на любом железе).
    * Базовая настройка операционной системы (отключение автообновления, установка необходимых пакетов, настройка параметров журналирования и т. д.).
    * Настройка nginx (и системы автоматического обновления перечня upstream’ов) для балансировки запросов от узла (kubelet) по API-серверам.
    * Установка и настройка CRI containerd и Kubernetes, включение узла в кластер.
    * Управление обновлениями узлов и их простоем (disruptions):
        * Автоматическое определение допустимой минорной версии Kubernetes группы узлов на основании ее
          настроек (указанной для группы kubernetesVersion), версии по умолчанию для всего кластера и текущей
          действительной версии control plane (не допускается обновление узлов в опережение обновления control plane).
        * Из группы одновременно производится обновление только одного узла и только если все узлы группы доступны.
        * Два варианта обновлений узлов:
            * обычные — всегда происходят автоматически;
            * требующие disruption (например, обновление ядра, смена версии containerd, значительная смена версии kubelet и пр.) — можно выбрать ручной или автоматический режим. В случае, если разрешены автоматические disruptive-обновления, перед обновлением производится drain узла (можно отключить).
    * Мониторинг состояния и прогресса обновления.
4. Масштабирование кластера.
   * Автоматическое масштабирование.

     Доступно при использовании поддерживаемых облачных провайдеров ([подробнее](#масштабирование-узлов-в-облаке)) и недоступно для статических узлов. Облачный провайдер в автоматическом режиме может создавать или удалять виртуальные машины, подключать их к кластеру или отключать.

   * Поддержание желаемого количества узлов в группе.

     Доступно как для [облачных провайдеров](#масштабирование-узлов-в-облаке), так и для статических узлов (при использовании [Cluster API Provider Static](#работа-со-статическими-узлами)).
5. Управление Linux-пользователями на узлах.
6. Поддержка GPU на узлах:
   * Автоматическое обнаружение и настройка NVIDIA GPU.
   * Различные режимы разделения ресурсов GPU на уровне NodeGroup: Exclusive, TimeSlicing, MIG.
   * Интеграция с мониторингом — в Grafana доступны готовые дашборды с ключевыми метриками GPU.

Управление узлами осуществляется через управление группой узлов (ресурс [NodeGroup](cr.html#nodegroup)), где каждая такая группа выполняет определенные для нее задачи. Примеры групп узлов по выполняемым задачам:

- группы master-узлов;
- группа узлов маршрутизации HTTP(S)-трафика (front-узлы);
- группа узлов мониторинга;
- группа узлов приложений (worker-узлы) и т. п.

Узлы в группе имеют общие параметры и настраиваются автоматически в соответствии с параметрами группы. Deckhouse масштабирует группы, добавляя, исключая и обновляя ее узлы. Допускается иметь в одной группе как развернутые в облаке узлы типа Static, так и статические узлы (серверы bare-metal, виртуальные машины). Это позволяет получать узлы на физических серверах, которые могут масштабироваться за счет облачных узлов (гибридные кластеры).

Работа в [облачной инфраструктуре](#работа-с-узлами-в-поддерживаемых-облаках) осуществляется с помощью поддерживаемых облачных провайдеров. Если поддержки необходимой облачной платформы нет, возможно использование ее ресурсов в виде статических узлов.

Работа со [статическими узлами](#работа-со-статическими-узлами) (например, серверами bare metal) выполняется с помощью в провайдера CAPS (Cluster API Provider Static).

#### Типы узлов

Типы узлов, с которыми возможна работа в группах узлов (ресурс [NodeGroup](cr.html#nodegroup)):

- `CloudEphemeral` — узлы автоматически заказываются, создаются и удаляются в настроенном облачном провайдере.
- `CloudPermanent` — узлы отличаются тем, что их конфигурация берется не из custom resource [nodeGroup](cr.html#nodegroup), а из специального ресурса `<PROVIDER>ClusterConfiguration`. Также важное отличие узлов  в том, что для применения их конфигурации необходимо выполнить `dhctl converge` (запустив инсталлятор Deckhouse). Примером CloudPermanent-узла облачного кластера является master-узел кластера.  
- `CloudStatic` — узел, созданный *вручную* (статический узел), размещенный в том же облаке, с которым настроена интеграция у одного из облачных провайдеров. На таком узле работает CSI и такой узел управляется `cloud-controller-manager'ом`. Объект `Node` кластера обогащается информацией о зоне и регионе, в котором работает узел. Также при удалении узла из облака соответствующий ему Node-объект будет удален в кластере.
- `Static` — статический узел, размещенный на сервере bare metal или виртуальной машине. В случае облака, такой узел не управляется `cloud-controller-manager'ом`, даже если включен один из облачных провайдеров. [Подробнее про работу со статическими узлами...](#работа-со-статическими-узлами)

#### Группировка узлов и управление группами

Группировка и управление узлами как связанной группой означает, что все узлы группы будут иметь одинаковые метаданные, взятые из кастомного ресурса [`NodeGroup`](cr.html#nodegroup).

Для групп узлов доступен мониторинг:

- с группировкой параметров узлов на графиках группы;
- с группировкой алертов о недоступности узлов;
- с алертами о недоступности N узлов или N% узлов группы и т. п.

#### Автоматическое развертывание, настройка и обновление узлов Kubernetes

Автоматическое развертывание (в *static/hybrid* — частично), настройка и дальнейшее обновление ПО работают на любых кластерах, независимо от его размещения в облаке или на bare metal.

##### Развертывание узлов Kubernetes

Deckhouse автоматически разворачивает узлы кластера, выполняя следующие **идемпотентные** операции:

- Настройку и оптимизацию операционной системы для работы с containerd и Kubernetes:
  - устанавливаются требуемые пакеты из репозиториев дистрибутива;
  - настраиваются параметры работы ядра, параметры журналирования, ротация журналов и другие параметры системы.
- Установку требуемых версий containerd и kubelet, включение узла в кластер Kubernetes.
- Настройку Nginx и обновление списка upstream для балансировки запросов от узла к Kubernetes API.

##### Поддержка актуального состояния узлов

Для поддержания узлов кластера в актуальном состоянии могут применяться два типа обновлений:

- **Обычные**. Такие обновления всегда применяются автоматически, и не приводят к остановке или перезагрузке узла.
- **Требующие прерывания** (disruption). Пример таких обновлений — обновление версии ядра или containerd, значительная смена версии kubelet и т. д. Для этого типа обновлений можно выбрать ручной или автоматический режим (секция параметров [disruptions](cr.html#nodegroup-v1-spec-disruptions)). В автоматическом режиме перед обновлением выполняется корректная приостановка работы узла (drain) и только после этого производится обновление.

В один момент времени производится обновление только одного узла из группы и только в том случае, когда все узлы группы доступны.

Модуль `node-manager` имеет набор встроенных метрик мониторинга, которые позволяют контролировать прогресс обновления, получать уведомления о возникающих во время обновления проблемах или о необходимости получения разрешения на обновление (ручное подтверждение обновления).

#### Работа с узлами в поддерживаемых облаках

У каждого поддерживаемого облачного провайдера существует возможность автоматического заказа узлов. Для этого необходимо указать требуемые параметры для каждого узла или группы узлов.

В зависимости от провайдера этими параметрами могут быть:

- тип узлов или количество ядер процессора и объем оперативной памяти;
- размер диска;
- настройки безопасности;
- подключаемые сети и др.

Создание, запуск и подключение виртуальных машин к кластеру выполняются автоматически.

##### Масштабирование узлов в облаке

Возможны два режима масштабирования узлов в группе:

- **Автоматическое масштабирование**.

  При дефиците ресурсов, наличии подов в состоянии `Pending`, в группу будут добавлены узлы. При отсутствии нагрузки на один или несколько узлов, они будут удалены из кластера. При работе автомасштабирования учитывается [приоритет](cr.html#nodegroup-v1-spec-cloudinstances-priority) группы (в первую очередь будет масштабироваться группа, у которой приоритет больше).
  
  Чтобы включить автоматическое масштабирование узлов, необходимо указать разные ненулевые значения [минимального](cr.html#nodegroup-v1-spec-cloudinstances-minperzone) и [максимального](cr.html#nodegroup-v1-spec-cloudinstances-maxperzone) количества узлов в группе.

- **Фиксированное количество узлов.**

  В этом случае Deckhouse будет поддерживать указанное количество узлов (например, заказывая новые в случае выхода из строя старых узлов).

  Чтобы указать фиксированное количество узлов в группе и отключить автоматическое масштабирование, необходимо указать одинаковые значения параметров [minPerZone](cr.html#nodegroup-v1-spec-cloudinstances-minperzone) и [maxPerZone](cr.html#nodegroup-v1-spec-cloudinstances-maxperzone).

#### Работа со статическими узлами

При работе со статическими узлами функции модуля `node-manager` выполняются со следующими ограничениями:

- **Отсутствует заказ узлов.** Непосредственное выделение ресурсов (серверов bare metal, виртуальных машин, связанных ресурсов) выполняется вручную. Дальнейшая настройка ресурсов  (подключение узла к кластеру, настройка мониторинга и т.п.) выполняются полностью автоматически (аналогично узлам в облаке) или частично.
- **Отсутствует автоматическое масштабирование узлов.** Доступно поддержание в группе указанного количества узлов при использовании [Cluster API Provider Static](#cluster-api-provider-static) (параметр [staticInstances.count](cr.html#nodegroup-v1-spec-staticinstances-count)). Т.е. Deckhouse будет пытаться поддерживать указанное количество узлов в группе, очищая лишние узлы и настраивая новые при необходимости (выбирая их из ресурсов [StaticInstance](cr.html#staticinstance), находящихся в состоянии *Pending*).

Настройка/очистка узла, его подключение к кластеру и отключение могут выполняться следующими способами:

- **Вручную,** с помощью подготовленных скриптов.

  Для настройки сервера (ВМ) и ввода узла в кластер нужно загрузить и выполнить специальный bootstrap-скрипт. Такой скрипт генерируется для каждой группы статических узлов (каждого ресурса `NodeGroup`). Он находится в секрете `d8-cloud-instance-manager/manual-bootstrap-for-<ИМЯ-NODEGROUP>`. Пример добавления статического узла в кластер можно найти в [FAQ](examples.html#вручную).

  Для отключения узла кластера и очистки сервера (виртуальной машины) нужно выполнить скрипт `/var/lib/bashible/cleanup_static_node.sh`, который уже находится на каждом статическом узле. Пример отключения узла кластера и очистки сервера можно найти в [FAQ](faq.html#как-вручную-очистить-статический-узел).

- **Автоматически,** с помощью [Cluster API Provider Static](#cluster-api-provider-static).

  Cluster API Provider Static (CAPS) подключается к серверу (ВМ) используя ресурсы [StaticInstance](cr.html#staticinstance) и [SSHCredentials](cr.html#sshcredentials), выполняет настройку, и вводит узел в кластер.

  При необходимости (например, если удален соответствующий серверу ресурс [StaticInstance](cr.html#staticinstance) или уменьшено [количество узлов группы](cr.html#nodegroup-v1-spec-staticinstances-count)), Cluster API Provider Static подключается к узлу кластера, очищает его и отключает от кластера.

- **Вручную с последующей передачей узла под автоматическое управление** [Cluster API Provider Static](#cluster-api-provider-static).

  > Функциональность доступна начиная с версии Deckhouse 1.63.

  Для передачи существующего узла кластера под управление CAPS необходимо подготовить для этого узла ресурсы [StaticInstance](cr.html#staticinstance) и [SSHCredentials](cr.html#sshcredentials), как при автоматическом управлении в пункте выше, однако ресурс [StaticInstance](cr.html#staticinstance) должен дополнительно быть помечен аннотацией `static.node.deckhouse.io/skip-bootstrap-phase: ""`.

##### Cluster API Provider Static

Cluster API Provider Static (CAPS), это реализация провайдера декларативного управления статическими узлами (серверами bare metal или виртуальными машинами) для проекта Cluster API Kubernetes. По сути, CAPS это дополнительный слой абстракции к уже существующему функционалу Deckhouse по автоматической настройке и очистке статических узлов с помощью скриптов, генерируемых для каждой группы узлов (см. раздел [Работа со статическими узлами](#работа-со-статическими-узлами)).

CAPS выполняет следующие функции:

- настройка сервера bare metal (или виртуальной машины) для подключения к кластеру Kubernetes;
- подключение узла в кластер Kubernetes;
- отключение узла от кластера Kubernetes;
- очистка сервера bare metal (или виртуальной машины) после отключения узла из кластера Kubernetes.

CAPS использует следующие ресурсы (CustomResource) при работе:

- **[StaticInstance](cr.html#staticinstance).** Каждый ресурс `StaticInstance` описывает конкретный хост (сервер, ВМ), который управляется с помощью CAPS.
- **[SSHCredentials](cr.html#sshcredentials)**. Содержит данные SSH, необходимые для подключения к хосту (`SSHCredentials` указывается в параметре [credentialsRef](cr.html#staticinstance-v1alpha1-spec-credentialsref) ресурса `StaticInstance`).
- **[NodeGroup](cr.html#nodegroup)**. Секция параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances) определяет необходимое количество узлов в группе и фильтр множества ресурсов `StaticInstance` которые могут использоваться в группе.

CAPS включается автоматически, если в NodeGroup заполнена секция параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances). Если в `NodeGroup` секция параметров `staticInstances` не заполнена, то настройка и очистка узлов для работы в этой группе выполняется *вручную* (см. примеры [добавления статического узла в кластер](examples.html#вручную) и [очистки узла](faq.html#как-вручную-очистить-статический-узел)), а не с помощью CAPS.

Схема работы со статичными узлами при использовании Cluster API Provider Static (CAPS) ([практический пример добавления узла](examples.html#с-помощью-cluster-api-provider-static)):

1. **Подготовка ресурсов.**

   Перед тем, как отдать bare-metal-сервер или виртуальную машину под управление CAPS, может быть необходима предварительная подготовка, например:

   - Подготовка системы хранения, добавление точек монтирования и т. п.;
   - Установка специфических пакетов ОС;
   - Настройка необходимой сетевой связности. Например, между сервером и узлами кластера;
   - Настройка доступа по SSH на сервер, создание пользователя для управления с root-доступом через `sudo`. Хорошей практикой является создание отдельного пользователя и уникальных ключей для каждого сервера.

1. **Создание ресурса [SSHCredentials](cr.html#sshcredentials).**

   В ресурсе `SSHCredentials` указываются параметры, необходимые CAPS для подключения к серверу по SSH. Один ресурс `SSHCredentials` может использоваться для подключения к нескольким серверам, но хорошей практикой является создание уникальных пользователей и ключей доступа для подключения к каждому серверу. В этом случае ресурс `SSHCredentials` также будет отдельный на каждый сервер.

1. **Создание ресурса [StaticInstance](cr.html#staticinstance).**

   На каждый сервер (ВМ) в кластере создается отдельный ресурс `StaticInstance`. В нем указан IP-адрес для подключения и ссылка на ресурс `SSHCredentials`, данные которого нужно использовать при подключении.

   Возможные состояния `StaticInstances` и связанных с ним серверов (ВМ) и узлов кластера:
   - `Pending`. Сервер не настроен, и в кластере нет соответствующего узла.
   - `Bootstrapping`. Выполняется процедура настройки сервера (ВМ) и подключения узла в кластер.
   - `Running`. Сервер настроен, и в кластер добавлен соответствующий узел.
   - `Cleaning`. Выполняется процедура очистки сервера и отключение узла из кластера.

   > Можно отдать существующий узел кластера, заранее введенный в кластер вручную, под управление CAPS, пометив его StaticInstance аннотацией `static.node.deckhouse.io/skip-bootstrap-phase: ""`.

1. **Создание ресурса [NodeGroup](cr.html#nodegroup).**

   В контексте CAPS в ресурсе `NodeGroup` нужно обратить внимание на параметр [nodeType](cr.html#nodegroup-v1-spec-nodetype) (должен быть `Static`) и секцию параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances).

   Секция параметров [staticInstances.labelSelector](cr.html#nodegroup-v1-spec-staticinstances-labelselector) определяет фильтр, по которому CAPS выбирает ресурсы `StaticInstance` для использования в группе. Фильтр позволяет использовать для разных групп узлов только определенные `StaticInstance`, а также позволяет использовать один `StaticInstance` в разных группах узлов. Фильтр можно не определять, чтобы использовать в группе узлов любой доступный `StaticInstance`.

   Параметр [staticInstances.count](cr.html#nodegroup-v1-spec-staticinstances-count) определяет желаемое количество узлов в группе.  При изменении параметра, CAPS начинает добавлять или удалять необходимое количество узлов, запуская этот процесс параллельно.

В соответствии с данными секции параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances), CAPS будет пытаться поддерживать указанное (параметр [count](cr.html#nodegroup-v1-spec-staticinstances-count)) количество узлов в группе. При необходимости добавить узел в группу, CAPS выбирает соответствующий [фильтру](cr.html#nodegroup-v1-spec-staticinstances-labelselector) ресурс [StaticInstance](cr.html#staticinstance) находящийся в статусе `Pending`, настраивает сервер (ВМ) и добавляет узел в кластер. При необходимости удалить узел из группы, CAPS выбирает [StaticInstance](cr.html#staticinstance) находящийся в статусе `Running`, очищает сервер (ВМ) и удаляет узел из кластера (после чего, соответствующий `StaticInstance` переходит в состояние `Pending` и снова может быть использован).

#### Пользовательские настройки на узлах

Для автоматизации действий на узлах группы предусмотрен ресурс [NodeGroupConfiguration](cr.html#nodegroupconfiguration). Ресурс позволяет выполнять на узлах bash-скрипты, в которых можно пользоваться набором команд bashbooster. Это удобно для автоматизации таких операций, как:

- Установка и настройки дополнительных пакетов ОС.  

  Примеры:  
  - [установка kubectl-плагина](examples.html#установка-плагина-cert-manager-для-kubectl-на-master-узлах);
  - [настройка containerd с поддержкой Nvidia GPU](faq.html#как-использовать-containerd-с-поддержкой-nvidia-gpu).

- Обновление ядра ОС на конкретную версию.

  Примеры:
  - [обновление ядра Debian](faq.html#для-дистрибутивов-основанных-на-debian);
  - [обновление ядра CentOS](faq.html#для-дистрибутивов-основанных-на-centos).

- Изменение параметров ОС.

  Примеры:  
  - [настройка параметра sysctl](examples.html#задание-параметра-sysctl);
  - [добавление корневого сертификата](examples.html#добавление-корневого-сертификата-в-хост).

- Сбор информации на узле и выполнение других подобных действий.

Ресурс `NodeGroupConfiguration` позволяет указывать [приоритет](cr.html#nodegroupconfiguration-v1alpha1-spec-weight) выполняемым скриптам, ограничивать их выполнение определенными [группами узлов](cr.html#nodegroupconfiguration-v1alpha1-spec-nodegroups) и [типами ОС](cr.html#nodegroupconfiguration-v1alpha1-spec-bundles).

Код скрипта указывается в параметре [content](cr.html#nodegroupconfiguration-v1alpha1-spec-content) ресурса. При создании скрипта на узле содержимое параметра `content` проходит через шаблонизатор Go Template, который позволят встроить дополнительный уровень логики при генерации скрипта. При прохождении через шаблонизатор становится доступным контекст с набором динамических переменных.

Переменные, которые доступны для использования в шаблонизаторе:
<ul>
<li><code>.cloudProvider</code> (для групп узлов с nodeType <code>CloudEphemeral</code> или <code>CloudPermanent</code>) — массив данных облачного провайдера.
</li>
<li><code>.cri</code> — используемый CRI (с версии Deckhouse 1.49 используется только <code>Containerd</code>).</li>
<li><code>.kubernetesVersion</code> — используемая версия Kubernetes.</li>
<li><code>.nodeUsers</code> — массив данных о пользователях узла, добавленных через ресурс <a href="cr.html#nodeuser">NodeUser</a>.
{% offtopic title="Пример данных..." %}
```yaml
nodeUsers:
- name: user1
  spec:
    isSudoer: true
    nodeGroups:
    - '*'
    passwordHash: PASSWORD_HASH
    sshPublicKey: SSH_PUBLIC_KEY
    uid: 1050
```
{% endofftopic %}
</li>
<li><code>.nodeGroup</code> — массив данных группы узлов.
{% offtopic title="Пример данных..." %}
```yaml
nodeGroup:
  cri:
    type: Containerd
  disruptions:
    approvalMode: Automatic
  kubelet:
    containerLogMaxFiles: 4
    containerLogMaxSize: 50Mi
    resourceReservation:
      mode: "Off"
  kubernetesVersion: "1.29"
  manualRolloutID: ""
  name: master
  nodeTemplate:
    labels:
      node-role.kubernetes.io/control-plane: ""
      node-role.kubernetes.io/master: ""
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
  nodeType: CloudPermanent
  updateEpoch: "1699879470"
```
{% endofftopic %}</li>
</ul>

{% raw %}
Пример использования переменных в шаблонизаторе:

```shell
{{- range .nodeUsers }}
echo 'Tuning environment for user {{ .name }}'
### Some code for tuning user environment
{{- end }}
```

Пример использования команд bashbooster:

```shell
bb-event-on 'bb-package-installed' 'post-install'
post-install() {
  bb-log-info "Setting reboot flag due to kernel was updated"
  bb-flag-set reboot
}
```

{% endraw %}

Ход выполнения скриптов можно увидеть на узле в журнале сервиса bashible c помощью команды:

```bash
journalctl -u bashible.service
```  

Сами скрипты находятся на узле в директории `/var/lib/bashible/bundle_steps/`.  

Сервис принимает решение о повторном запуске скриптов путем сравнения единой контрольной суммы всех файлов, расположенной по пути `/var/lib/bashible/configuration_checksum` с контрольной суммой размещенной в кластере `kubernetes` в секрете `configuration-checksums` namespace `d8-cloud-instance-manager`.
Проверить контрольную сумму можно следующей командой:  

```bash
d8 k -n d8-cloud-instance-manager get secret configuration-checksums -o yaml
```  

Сравнение контрольных суммы сервис совершает каждую минуту.  

Контрольная сумма в кластере изменяется раз в 4 часа, тем самым повторно запуская скрипты на всех узлах.  
Принудительный вызов исполнения bashible на узле можно произвести путем удаления файла с контрольной суммой скриптов с помощью следующей команды:  

```bash
rm /var/lib/bashible/configuration_checksum
```  

##### Особенности написания скриптов

При написании скриптов важно учитывать следующие особенности их использования в Deckhouse:

1. Скрипты в deckhouse выполняются раз в 4 часа или на основании внешних триггеров. Поэтому важно писать скрипты таким образом, чтобы они производили проверку необходимости своих изменений в системе перед выполнением действий, а не производили изменения каждый раз при запуске.
2. Существуют предзаготовленные скрипты пользовательских скриптов. Например, если в скрипте планируется произвести перезапуск сервиса, то данный скрипт должен вызываться после скрипта установки сервиса. В противном случае он не сможет выполниться при развертывании нового узла.

Полезные особенности некоторых скриптов:

* `032_configure_containerd.sh` - производит объединение всех конфигурационных файлов сервиса `containerd` расположенных по пути `/etc/containerd/conf.d/*.toml`, а также **перезапуск** сервиса. Следует учитывать что директория `/etc/containerd/conf.d/` не создается автоматически, а также что создание файлов в этой директории следует производить в скриптах с приоритетом менее `32`

#### Chaos Monkey

Инструмент (включается у каждой из `NodeGroup` отдельно), позволяющий систематически вызывать случайные прерывания работы узлов. Предназначен для проверки элементов кластера, приложений и инфраструктурных компонентов на реальную работу отказоустойчивости.

#### Мониторинг

Для групп узлов (ресурс `NodeGroup`) Deckhouse Platform Certified Security Edition экспортирует метрики доступности группы.

##### Какую информацию собирает Prometheus?

Все метрики групп узлов имеют префикс `d8_node_group_` в названии, и метку с именем группы `node_group_name`.

Следующие метрики собираются для каждой группы узлов:

- `d8_node_group_ready` — количество узлов группы, находящихся в статусе `Ready`;
- `d8_node_group_nodes` — количество узлов в группе (в любом статусе);
- `d8_node_group_instances` — количество инстансов в группе (в любом статусе);
- `d8_node_group_desired` — желаемое (целевое) количество объектов `Machines` в группе;
- `d8_node_group_min` — минимальное количество инстансов в группе;
- `d8_node_group_max` — максимальное количество инстансов в группе;
- `d8_node_group_up_to_date` — количество узлов в группе в состоянии up-to-date;
- `d8_node_group_standby` — количество резервных узлов (см. параметр [standby](cr.html#nodegroup-v1-spec-cloudinstances-standby)) в группе;
- `d8_node_group_has_errors` — единица, если в группе узлов есть какие-либо ошибки.

### Управление узлами: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['node-manager'].config-values | format_module_configuration: moduleKebabName }}

### Управление узлами: custom resources
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}

### Управление узлами: примеры

Ниже представлены несколько примеров описания NodeGroup, а также установки плагина cert-manager для `kubectl` и задания параметра `sysctl`.

#### Примеры описания NodeGroup

##### Статические узлы

<span id="пример-описания-статической-nodegroup"></span>

Для виртуальных машин на гипервизорах или физических серверов используйте статические узлы, указав `nodeType: Static` в NodeGroup.

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: worker
spec:
  nodeType: Static
```

Узлы в такую группу добавляются [вручную](#вручную) с помощью подготовленных скриптов.

Также можно использовать способ [добавления статических узлов с помощью Cluster API Provider Static](#с-помощью-cluster-api-provider-static).

##### Системные узлы

<span id="пример-описания-статичной-nodegroup-для-системных-узлов"></span>

Ниже представлен пример манифеста группы системных узлов.

При описании NodeGroup c узлами типа Static в поле `nodeType` укажите значение `Static` и используйте поле [`staticInstances`](./cr.html#nodegroup-v1-spec-staticinstances) для описания параметров настройки машин статических узлов.

При описании NodeGroup c облачными узлами типа CloudEphemeral в поле `nodeType` укажите значение `CloudEphemeral` и используйте поле [`cloudInstances`](./cr.html#nodegroup-v1-spec-cloudinstances) для описания параметров заказа облачных виртуальных машин.

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: system
spec:
  nodeTemplate:
    labels:
      node-role.deckhouse.io/system: ""
    taints:
      - effect: NoExecute
        key: dedicated.deckhouse.io
        value: system
  # Пример для узлов типа Static.
  nodeType: Static
  staticInstances:
    count: 2
    labelSelector:
      matchLabels:
        role: system
  # Пример для узлов типа CloudEphemeral.
  # nodeType: CloudEphemeral
  # cloudInstances:
  #   classReference:
  #     kind: YandexInstanceClass
  #     name: large
  #   maxPerZone: 2
  #   minPerZone: 1
  #   zones:
  #   - ru-central1-d
```

##### Узлы с GPU

Для работы узлов с GPU требуются **драйвер NVIDIA** и **NVIDIA Container Toolkit**. Возможны два варианта установки драйвера:

1. **Ручная установка** — администратор устанавливает драйвер до включения узла в кластер.
1. **Автоматизация через `NodeGroupConfiguration`** (подробнее – в разделе [Порядок действий по добавлению GPU-узла в кластер](./node-manager/faq.html#порядок-действий-по-добавлению-gpu-узла-в-кластер)).

После того как драйвер установлен и в NodeGroup добавлен блок `spec.gpu`,
`node-manager` включает полноценную поддержку GPU: автоматически разворачиваются
**NFD**, **GFD**, **NVIDIA Device Plugin**, **DCGM Exporter** и, при необходимости,
**MIG Manager**.

{% alert level="info" %}
Узлы с GPU часто помечают отдельным taint-ом (например, `node-role=gpu:NoSchedule`) — тогда по умолчанию туда не попадают обычные поды.
Сервисам, которым нужен GPU, достаточно добавить `tolerations` и `nodeSelector`.
{% endalert %}

Подробная схема параметров находится в [описании кастомного ресурса `NodeGroup`](./node-manager/cr.html#nodegroup-v1-spec-gpu).

Ниже представлены примеры манифестов NodeGroup для типовых режимов работы GPU (Exclusive,
TimeSlicing, MIG).

###### Эксклюзивный режим (Exclusive)

Каждому поду выделяется целый GPU, в кластере публикуется ресурс `nvidia.com/gpu`.

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: gpu-exclusive
spec:
  nodeType: Static
  gpu:
    sharing: Exclusive
  nodeTemplate:
    labels:
      node-role/gpu: ""
    taints:
    - key: node-role
      value: gpu
      effect: NoSchedule
```

###### TimeSlicing (4 слота)

GPU распределяется по временным слотам: до 4 подов могут последовательно
использовать одну карту. Подходит для экспериментов, CI и лёгких
inference-задач.

Поды по-прежнему запрашивают ресурс `nvidia.com/gpu`.

```yaml
spec:
  gpu:
    sharing: TimeSlicing
    timeSlicing:
      partitionCount: 4
```

###### MIG (профиль `all-1g.5gb`)

Физический GPU (A100, A30 и др.) делится на аппаратные экземпляры.
Планировщик увидит ресурсы `nvidia.com/mig-1g.5gb`.

Полный список поддерживаемых GPU-устройств и их профили можно увидеть, воспользовавшись  
[инструкцией](./node-manager/faq.html#как-посмотреть-доступные-mig-профили-в-кластере).

```yaml
spec:
  gpu:
    sharing: MIG
    mig:
      partedConfig: all-1g.5gb
```

###### Проверка работы: тестовая задача (CUDA vectoradd)

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: cuda-vectoradd
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        node-role/gpu: ""
      tolerations:
      - key: node-role
        value: gpu
        effect: NoSchedule
      containers:
      - name: cuda-vectoradd
        image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04
        resources:
          limits:
            nvidia.com/gpu: 1
```

Эта задача (Job) запускает демонстрационный пример **vectoradd** из набора CUDA-samples.
Если под завершается успешно (`Succeeded`), значит GPU-устройство доступно и корректно настроено.

#### Добавление статического узла в кластер

<span id="пример-описания-статичной-nodegroup"></span>

Добавление статического узла можно выполнить вручную или с помощью Cluster API Provider Static.

##### Вручную

Чтобы добавить новый статический узел (выделенная ВМ, bare-metal-сервер и т. п.) в кластер вручную, выполните следующие шаги:

1. Используйте существующий или создайте новый ресурс [NodeGroup](cr.html#nodegroup) ([пример](#статические-узлы) NodeGroup с именем `worker`). Параметр [nodeType](cr.html#nodegroup-v1-spec-nodetype) в ресурсе NodeGroup для статических узлов должен быть `Static` или `CloudStatic`.
1. Получите код скрипта в кодировке Base64 для добавления и настройки узла.

   Пример получения кода скрипта в кодировке Base64 для добавления узла в NodeGroup `worker`:

   ```shell
   NODE_GROUP=worker
   d8 k -n d8-cloud-instance-manager get secret manual-bootstrap-for-${NODE_GROUP} -o json | jq '.data."bootstrap.sh"' -r
   ```

1. Выполните предварительную настройку нового узла в соответствии с особенностями вашего окружения. Например:
   - добавьте необходимые точки монтирования в файл `/etc/fstab` (NFS, Ceph и т. д.);
   - установите необходимые пакеты;
   - настройте сетевую связность между новым узлом и остальными узлами кластера.
1. Зайдите на новый узел по SSH и выполните следующую команду, вставив полученную в п. 3 Base64-строку:

   ```shell
   echo <Base64-КОД-СКРИПТА> | base64 -d | bash
   ```

##### С помощью Cluster API Provider Static

{% alert level="warning" %}
Если вы ранее увеличивали количество master-узлов в кластере в NodeGroup `master` (параметр [`spec.staticInstances.count`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-count)), перед добавлением обычных узлов с помощью CAPS [убедитесь](./control-plane-manager/faq.html#как-добавить-master-узел-в-статическом-или-гибридном-кластере), что не произойдет их «перехват».
{% endalert %}

Простой пример добавления статического узла в кластер с помощью [Cluster API Provider Static (CAPS)](./#cluster-api-provider-static):

1. Подготовьте необходимые ресурсы.

   * Выделите сервер (или виртуальную машину), настройте сетевую связность и т. п., при необходимости установите специфические пакеты ОС и добавьте точки монтирования которые потребуются на узле.

   * Создайте пользователя (в примере — `caps`) с возможностью выполнять `sudo`, выполнив **на сервере** следующую команду:

     ```shell
     useradd -m -s /bin/bash caps 
     usermod -aG sudo caps
     ```

   * Разрешите пользователю выполнять команды через sudo без пароля. Для этого **на сервере** внесите следующую строку в конфигурацию sudo (отредактировав файл `/etc/sudoers`, выполнив команду `sudo visudo` или другим способом):

     ```text
     caps ALL=(ALL) NOPASSWD: ALL
     ```

   * Сгенерируйте **на сервере** пару SSH-ключей с пустой парольной фразой:

     ```shell
     ssh-keygen -t rsa -f caps-id -C "" -N ""
     ```

     Публичный и приватный ключи пользователя `caps` будут сохранены в файлах `caps-id.pub` и `caps-id` в текущей директории на сервере.

   * Добавьте полученный публичный ключ в файл `/home/caps/.ssh/authorized_keys` пользователя `caps`, выполнив в директории с ключами **на сервере** следующие команды:

     ```shell
     mkdir -p /home/caps/.ssh 
     cat caps-id.pub >> /home/caps/.ssh/authorized_keys 
     chmod 700 /home/caps/.ssh 
     chmod 600 /home/caps/.ssh/authorized_keys
     chown -R caps:caps /home/caps/
     ```

   В операционных системах семейства Astra Linux, при использовании модуля мандатного контроля целостности Parsec, сконфигурируйте максимальный уровень целостности для пользователя `caps`:

     ```shell
     pdpl-user -i 63 caps
     ```

1. Создайте в кластере ресурс [SSHCredentials](cr.html#sshcredentials).

   В директории с ключами пользователя **на сервере** выполните следующую команду для получения закрытого ключа в формате Base64:

   ```shell
   base64 -w0 caps-id
   ```

   На любом компьютере с `kubectl`, настроенным на управление кластером, создайте переменную окружения со значением закрытого ключа созданного пользователя в Base64, полученным на предыдущем шаге:

   ```shell
    CAPS_PRIVATE_KEY_BASE64=<ЗАКРЫТЫЙ_КЛЮЧ_В_BASE64>
   ```

   Выполните следующую команду, для создания в кластере ресурса `SSHCredentials` (здесь и далее также используйте `kubectl`, настроенный на управление кластером):

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: SSHCredentials
   metadata:
     name: credentials
   spec:
     user: caps
     privateSSHKey: "${CAPS_PRIVATE_KEY_BASE64}"
   EOF
   ```

1. Создайте в кластере ресурс [StaticInstance](cr.html#staticinstance), указав IP-адрес сервера статического узла:

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-worker-1
     labels:
       role: worker
   spec:
     # Укажите IP-адрес сервера статического узла.
     address: "<SERVER-IP>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   EOF
   ```

1. Создайте в кластере ресурс [NodeGroup](cr.html#nodegroup). Параметр `count` обозначает количество `staticInstances`, подпадающих под `labelSelector`, которые будут добавлены в кластер, в данном случае `1`:

   > Поле `labelSelector` в ресурсе `NodeGroup` является неизменным. Чтобы обновить `labelSelector`, нужно создать новую `NodeGroup` и перенести в неё статические узлы, изменив их лейблы (labels).

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1
   kind: NodeGroup
   metadata:
     name: worker
   spec:
     nodeType: Static
     staticInstances:
       count: 1
       labelSelector:
         matchLabels:
           role: worker
   EOF
   ```

   > Если необходимо добавить узлы в уже существующую группу узлов, укажите их желаемое количество в поле `.spec.count` NodeGroup.

##### С помощью Cluster API Provider Static для нескольких групп узлов

Пример использования фильтров в [label selector](cr.html#nodegroup-v1-spec-staticinstances-labelselector) StaticInstance, для группировки статических узлов и использования их в разных NodeGroup. В примере используются две группы узлов (`front` и `worker`), предназначенные для разных задач, которые должны содержать разные по характеристикам узлы — два сервера для группы `front` и один для группы `worker`.

1. Подготовьте необходимые ресурсы (3 сервера или виртуальные машины) и создайте ресурс `SSHCredentials`, аналогично п.1 и п.2 [примера](#с-помощью-cluster-api-provider-static).

1. Создайте в кластере два ресурса [NodeGroup](cr.html#nodegroup) (здесь и далее используйте `kubectl`, настроенный на управление кластером):

   > Поле `labelSelector` в ресурсе `NodeGroup` является неизменным. Чтобы обновить labelSelector, нужно создать новую NodeGroup и перенести в неё статические узлы, изменив их лейблы (labels).

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1
   kind: NodeGroup
   metadata:
     name: front
   spec:
     nodeType: Static
     staticInstances:
       count: 2
       labelSelector:
         matchLabels:
           role: front
   ---
   apiVersion: deckhouse.io/v1
   kind: NodeGroup
   metadata:
     name: worker
   spec:
     nodeType: Static
     staticInstances:
       count: 1
       labelSelector:
         matchLabels:
           role: worker
   EOF
   ```

1. Создайте в кластере ресурсы [StaticInstance](cr.html#staticinstance), указав актуальные IP-адреса серверов:

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-front-1
     labels:
       role: front
   spec:
     address: "<SERVER-FRONT-IP1>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   ---
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-front-2
     labels:
       role: front
   spec:
     address: "<SERVER-FRONT-IP2>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   ---
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-worker-1
     labels:
       role: worker
   spec:
     address: "<SERVER-WORKER-IP>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   EOF
   ```

##### Cluster API Provider Static: перемещение узлов между NodeGroup

В данном разделе описывается процесс перемещения статических узлов между различными NodeGroup с использованием Cluster API Provider Static (CAPS). Процесс включает изменение конфигурации NodeGroup и обновление лейблов у соответствующих StaticInstance.

###### Исходная конфигурация

Предположим, что в кластере уже существует NodeGroup с именем `worker`, настроенный для управления одним статическим узлом с лейблом `role: worker`.

`NodeGroup` worker:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: worker
spec:
  nodeType: Static
  staticInstances:
    count: 1
    labelSelector:
      matchLabels:
        role: worker
```

`StaticInstance` static-0:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: StaticInstance
metadata:
  name: static-worker-1
  labels:
    role: worker
spec:
  address: "192.168.1.100"
  credentialsRef:
    kind: SSHCredentials
    name: credentials
```

###### Шаги по перемещению узла между `NodeGroup`

{% alert level="warning" %}
В процессе переноса узлов между NodeGroup будет выполнена очистка и повторный бутстрап узла, объект `Node` будет пересоздан.
{% endalert %}

####### 1. Создание новой `NodeGroup` для целевой группы узлов

Создайте новый ресурс NodeGroup, например, с именем `front`, который будет управлять статическим узлом с лейблом `role: front`.

```shell
d8 k create -f - <<EOF
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: front
spec:
  nodeType: Static
  staticInstances:
    count: 1
    labelSelector:
      matchLabels:
        role: front
EOF
```

####### 2. Обновление лейбла у `StaticInstance`

Измените лейбл `role` у существующего StaticInstance с `worker` на `front`. Это позволит новой NodeGroup `front` начать управлять этим узлом.

```shell
d8 k label staticinstance static-worker-1 role=front --overwrite
```

####### 3. Уменьшение количества статических узлов в исходной `NodeGroup`

Обновите ресурс NodeGroup `worker`, уменьшив значение параметра `count` с `1` до `0`.

```shell
d8 k patch nodegroup worker -p '{"spec": {"staticInstances": {"count": 0}}}' --type=merge
```

#### Пример описания `NodeUser`

```yaml
apiVersion: deckhouse.io/v1
kind: NodeUser
metadata:
  name: testuser
spec:
  uid: 1100
  sshPublicKeys:
  - "<SSH_PUBLIC_KEY>"
  passwordHash: <PASSWORD_HASH>
  isSudoer: true
```

#### Пример описания `NodeGroupConfiguration`

##### Задание параметра sysctl

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: sysctl-tune.sh
spec:
  weight: 100
  bundles:
  - "*"
  nodeGroups:
  - "*"
  content: |
    sysctl -w vm.max_map_count=262144
```

##### Добавление корневого сертификата в хост

{% alert level="warning" %}
Данный пример приведен для ОС Ubuntu.  
Способ добавления сертификатов в хранилище может отличаться в зависимости от ОС.
  
При адаптации скрипта под другую ОС измените параметры [bundles](cr.html#nodegroupconfiguration-v1alpha1-spec-bundles) и [content](cr.html#nodegroupconfiguration-v1alpha1-spec-content).
{% endalert %}

{% alert level="warning" %}
Для использования сертификата в `containerd` (в т.ч. pull контейнеров из приватного репозитория) после добавления сертификата требуется произвести рестарт сервиса.
{% endalert %}

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: add-custom-ca.sh
spec:
  weight: 31
  nodeGroups:
  - '*'  
  bundles:
  - 'ubuntu-lts'
  content: |-
    CERT_FILE_NAME=example_ca
    CERTS_FOLDER="/usr/local/share/ca-certificates"
    CERT_CONTENT=$(cat <<EOF
    -----BEGIN CERTIFICATE-----
    MIIDSjCCAjKgAwIBAgIRAJ4RR/WDuAym7M11JA8W7D0wDQYJKoZIhvcNAQELBQAw
    JTEjMCEGA1UEAxMabmV4dXMuNTEuMjUwLjQxLjIuc3NsaXAuaW8wHhcNMjQwODAx
    MTAzMjA4WhcNMjQxMDMwMTAzMjA4WjAlMSMwIQYDVQQDExpuZXh1cy41MS4yNTAu
    NDEuMi5zc2xpcC5pbzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAL1p
    WLPr2c4SZX/i4IS59Ly1USPjRE21G4pMYewUjkSXnYv7hUkHvbNL/P9dmGBm2Jsl
    WFlRZbzCv7+5/J+9mPVL2TdTbWuAcTUyaG5GZ/1w64AmAWxqGMFx4eyD1zo9eSmN
    G2jis8VofL9dWDfUYhRzJ90qKxgK6k7tfhL0pv7IHDbqf28fCEnkvxsA98lGkq3H
    fUfvHV6Oi8pcyPZ/c8ayIf4+JOnf7oW/TgWqI7x6R1CkdzwepJ8oU7PGc0ySUWaP
    G5bH3ofBavL0bNEsyScz4TFCJ9b4aO5GFAOmgjFMMUi9qXDH72sBSrgi08Dxmimg
    Hfs198SZr3br5GTJoAkCAwEAAaN1MHMwDgYDVR0PAQH/BAQDAgWgMAwGA1UdEwEB
    /wQCMAAwUwYDVR0RBEwwSoIPbmV4dXMuc3ZjLmxvY2FsghpuZXh1cy41MS4yNTAu
    NDEuMi5zc2xpcC5pb4IbZG9ja2VyLjUxLjI1MC40MS4yLnNzbGlwLmlvMA0GCSqG
    SIb3DQEBCwUAA4IBAQBvTjTTXWeWtfaUDrcp1YW1pKgZ7lTb27f3QCxukXpbC+wL
    dcb4EP/vDf+UqCogKl6rCEA0i23Dtn85KAE9PQZFfI5hLulptdOgUhO3Udluoy36
    D4WvUoCfgPgx12FrdanQBBja+oDsT1QeOpKwQJuwjpZcGfB2YZqhO0UcJpC8kxtU
    by3uoxJoveHPRlbM2+ACPBPlHu/yH7st24sr1CodJHNt6P8ugIBAZxi3/Hq0wj4K
    aaQzdGXeFckWaxIny7F1M3cIWEXWzhAFnoTgrwlklf7N7VWHPIvlIh1EYASsVYKn
    iATq8C7qhUOGsknDh3QSpOJeJmpcBwln11/9BGRP
    -----END CERTIFICATE-----
    EOF
    )

    # bb-event           - Creating subscription for event function. More information: http://www.bashbooster.net/#event
    ## ca-file-updated   - Event name
    ## update-certs      - The function name that the event will call
    
    bb-event-on "ca-file-updated" "update-certs"
    
    update-certs() {          # Function with commands for adding a certificate to the store
      update-ca-certificates
    }

    # bb-tmp-file - Creating temp file function. More information: http://www.bashbooster.net/#tmp
    CERT_TMP_FILE="$( bb-tmp-file )"
    echo -e "${CERT_CONTENT}" > "${CERT_TMP_FILE}"  
    
    # bb-sync-file                                - File synchronization function. More information: http://www.bashbooster.net/#sync
    ## "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt"    - Destination file
    ##  ${CERT_TMP_FILE}                          - Source file
    ##  ca-file-updated                           - Name of event that will be called if the file changes.

    bb-sync-file \
      "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt" \
      ${CERT_TMP_FILE} \
      ca-file-updated   
```

##### Добавление в containerd возможности скачивать образы из insecure container registry

Возможность скачивания образов из insecure container registry включается с помощью параметра `insecure_skip_verify` в конфигурационном файле containerd. Подробнее — в разделе [Как добавить конфигурацию для дополнительного registry](faq.html#как-добавить-конфигурацию-для-дополнительного-registry).

### Управление узлами: FAQ

<div id='как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master'></div>

#### Как добавить master-узлы в облачном кластере?

Как конвертировать кластер с одним master-узлом в мультикластерный описано [в FAQ модуля control-plane-manager](./control-plane-manager/faq.html#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master).

<div id='как-уменьшить-число-master-узлов-в-облачном-кластере-multi-master-в-single-master'></div>

#### Как уменьшить число master-узлов в облачном кластере?

Как конвертировать мультимастерный кластер в кластер с одним master-узлом описано [в FAQ модуля control-plane-manager](./control-plane-manager/faq.html#как-уменьшить-число-master-узлов-в-облачном-кластере-multi-master-в-single-master).

#### Статические узлы

<span id='как-добавить-статический-узел-в-кластер'></span>
<span id='как-добавить-статичный-узел-в-кластер'></span>

Добавить статический узел в кластер можно вручную ([пример](examples.html#вручную)) или с помощью [Cluster API Provider Static](#как-добавить-статический-узел-в-кластер-cluster-api-provider-static).

##### Как добавить статический узел в кластер (Cluster API Provider Static)?

Чтобы добавить статический узел в кластер (сервер bare-metal или виртуальную машину), выполните следующие шаги:

1. Подготовьте необходимые ресурсы:

   - Выделите сервер или виртуальную машину и убедитесь, что узел имеет необходимую сетевую связанность с кластером.

   - При необходимости установите дополнительные пакеты ОС и настройте точки монтирования, которые будут использоваться на узле.

1. Создайте пользователя с правами `sudo`:

   - Добавьте нового пользователя (в данном примере — `caps`) с правами выполнения команд через `sudo`:

     ```shell
     useradd -m -s /bin/bash caps 
     usermod -aG sudo caps
     ```

   - Разрешите пользователю выполнять команды через `sudo` без ввода пароля. Для этого отредактируйте конфигурацию `sudo` (отредактировав файл `/etc/sudoers`, выполнив команду `sudo visudo` или другим способом):

     ```shell
     caps ALL=(ALL) NOPASSWD: ALL
     ```

1. На сервере откройте файл `/etc/ssh/sshd_config` и убедитесь, что параметр `UsePAM` установлен в значение `yes`. Затем перезапустите службу `sshd`:

   ```shell
   sudo systemctl restart sshd
   ```

1. Сгенерируйте на сервере пару SSH-ключей с пустой парольной фразой:

   ```shell
   ssh-keygen -t rsa -f caps-id -C "" -N ""
   ```

   Приватный и публичный ключи будут сохранены в файлах `caps-id` и `caps-id.pub` соответственно в текущей директории.

1. Добавьте полученный публичный ключ в файл `/home/caps/.ssh/authorized_keys` пользователя `caps`, выполнив в директории с ключами на сервере следующие команды:

   ```shell
   mkdir -p /home/caps/.ssh 
   cat caps-id.pub >> /home/caps/.ssh/authorized_keys 
   chmod 700 /home/caps/.ssh 
   chmod 600 /home/caps/.ssh/authorized_keys
   chown -R caps:caps /home/caps/
   ```

1. Создайте ресурс [SSHCredentials](cr.html#sshcredentials).
1. Создайте ресурс [StaticInstance](cr.html#staticinstance).
1. Создайте ресурс [NodeGroup](cr.html#nodegroup) с [nodeType](cr.html#nodegroup-v1-spec-nodetype) `Static`, указав [желаемое количество узлов](cr.html#nodegroup-v1-spec-staticinstances-count) в группе и, при необходимости, [фильтр](cr.html#nodegroup-v1-spec-staticinstances-labelselector) выбора `StaticInstance`.

[Пример](examples.html#с-помощью-cluster-api-provider-static) добавления статического узла.

##### Как добавить несколько статических узлов в кластер вручную?

Используйте существующий или создайте новый кастомный ресурс (Custom Resource) [NodeGroup](cr.html#nodegroup) ([пример](examples.html#пример-описания-статической-nodegroup) `NodeGroup` с именем `worker`).

Автоматизировать процесс добавления узлов можно с помощью любой платформы автоматизации. Далее приведен пример для Ansible.

1. Получите один из адресов Kubernetes API-сервера. Обратите внимание, что IP-адрес должен быть доступен с узлов, которые добавляются в кластер:

   ```shell
   d8 k -n default get ep kubernetes -o json | jq '.subsets[0].addresses[0].ip + ":" + (.subsets[0].ports[0].port | tostring)' -r
   ```

   Проверьте версию K8s. Если версия >= 1.25, создайте токен `node-group`:

   ```shell
   d8 k create token node-group --namespace d8-cloud-instance-manager --duration 1h
   ```

   Сохраните полученный токен, и добавьте в поле `token:` playbook'а Ansible на дальнейших шагах.

1. Если версия Kubernetes меньше 1.25, получите Kubernetes API-токен для специального ServiceAccount'а, которым управляет Deckhouse:

   ```shell
   d8 k -n d8-cloud-instance-manager get $(d8 k -n d8-cloud-instance-manager get secret -o name | grep node-group-token) \
     -o json | jq '.data.token' -r | base64 -d && echo ""
   ```

1. Создайте Ansible playbook с `vars`, которые заменены на полученные на предыдущих шагах значения:

{% raw %}

   ```yaml
   - hosts: all
     become: yes
     gather_facts: no
     vars:
       kube_apiserver: <KUBE_APISERVER>
       token: <TOKEN>
     tasks:
       - name: Check if node is already bootsrapped
         stat:
           path: /var/lib/bashible
         register: bootstrapped
       - name: Get bootstrap secret
         uri:
           url: "https://{{ kube_apiserver }}/api/v1/namespaces/d8-cloud-instance-manager/secrets/manual-bootstrap-for-{{ node_group }}"
           return_content: yes
           method: GET
           status_code: 200
           body_format: json
           headers:
             Authorization: "Bearer {{ token }}"
           validate_certs: no
         register: bootstrap_secret
         when: bootstrapped.stat.exists == False
       - name: Run bootstrap.sh
         shell: "{{ bootstrap_secret.json.data['bootstrap.sh'] | b64decode }}"
         args:
           executable: /bin/bash
         ignore_errors: yes
         when: bootstrapped.stat.exists == False
       - name: wait
         wait_for_connection:
           delay: 30
         when: bootstrapped.stat.exists == False
   ```

{% endraw %}

1. Определите дополнительную переменную `node_group`. Значение переменной должно совпадать с именем `NodeGroup`, которой будет принадлежать узел. Переменную можно передать различными способами, например с использованием inventory-файла:

   ```text
   [system]
   system-0
   system-1

   [system:vars]
   node_group=system

   [worker]
   worker-0
   worker-1

   [worker:vars]
   node_group=worker
   ```

1. Запустите выполнение playbook'а с использованием inventory-файла.

##### Как вручную очистить статический узел?

<span id='как-вывести-узел-из-под-управления-node-manager'></span>

{% alert level="info" %}
Инструкция справедлива как для узла, настроенного вручную (с помощью бутстрап-скрипта), так и для узла, настроенного с помощью CAPS.
{% endalert %}

Чтобы вывести из кластера узел и очистить сервер (ВМ), выполните следующую команду на узле:

```shell
bash /var/lib/bashible/cleanup_static_node.sh --yes-i-am-sane-and-i-understand-what-i-am-doing
```

##### Можно ли удалить StaticInstance?

StaticInstance, находящийся в состоянии `Pending` можно удалять без каких-либо проблем.

Чтобы удалить StaticInstance находящийся в любом состоянии, отличном от `Pending` (`Running`, `Cleaning`, `Bootstrapping`), выполните следующие шаги:

1. Добавьте лейбл `"node.deckhouse.io/allow-bootstrap": "false"` в StaticInstance.

   Пример команды для добавления лейбла:

   ```shell
   d8 k label staticinstance d8cluster-worker node.deckhouse.io/allow-bootstrap=false
   ```

1. Дождитесь, пока StaticInstance перейдет в статус `Pending`.

   Для проверки статуса StaticInstance используйте команду:

   ```shell
   d8 k get staticinstances
   ```

1. Удалите `StaticInstance`.

   Пример команды для удаления StaticInstance:

   ```shell
   d8 k delete staticinstance d8cluster-worker
   ```

1. Уменьшите значение параметра `NodeGroup.spec.staticInstances.count` на 1.

##### Как изменить IP-адрес StaticInstance?

Изменить IP-адрес в ресурсе `StaticInstance` нельзя. Если в `StaticInstance` указан ошибочный адрес, то нужно [удалить StaticInstance](#можно-ли-удалить-staticinstance) и создать новый.

##### Как мигрировать статический узел настроенный вручную под управление CAPS?

Необходимо выполнить [очистку узла](#как-вручную-очистить-статический-узел), затем [добавить](#как-добавить-статический-узел-в-кластер-cluster-api-provider-static) узел под управление CAPS.

#### Как изменить NodeGroup у статического узла?

<span id='как-изменить-nodegroup-у-статичного-узла'><span>

Если узел находится под управлением [CAPS](./#cluster-api-provider-static), то изменить принадлежность к `NodeGroup` у такого узла **нельзя**. Единственный вариант — [удалить StaticInstance](#можно-ли-удалить-staticinstance) и создать новый.

Чтобы перенести существующий статический узел созданный [вручную](./#работа-со-статическими-узлами) из одной `NodeGroup` в другую, необходимо изменить у узла лейбл группы:

```shell
d8 k label node --overwrite <node_name> node.deckhouse.io/group=<new_node_group_name>
d8 k label node <node_name> node-role.kubernetes.io/<old_node_group_name>-
```

Применение изменений потребует некоторого времени.

#### Как очистить узел для последующего ввода в кластер?

Это необходимо только в том случае, если нужно переместить статический узел из одного кластера в другой. Имейте в виду, что эти операции удаляют данные локального хранилища. Если необходимо просто изменить `NodeGroup`, следуйте [этой инструкции](#как-изменить-nodegroup-у-статического-узла).

{% alert level="warning" %}
Если на зачищаемом узле есть пулы хранения LINSTOR/DRBD, то предварительно перенесите ресурсы с узла и удалите узел LINSTOR/DRBD, следуя [инструкции](/modules/sds-replicated-volume/faq.html#как-выгнать-ресурсы-с-узла).
{% endalert %}

1. Удалите узел из кластера Kubernetes:

   ```shell
   d8 k drain <node> --ignore-daemonsets --delete-local-data
   d8 k delete node <node>
   ```

1. Запустите на узле скрипт очистки:

   ```shell
   bash /var/lib/bashible/cleanup_static_node.sh --yes-i-am-sane-and-i-understand-what-i-am-doing
   ```

1. После перезагрузки узла [запустите](#как-добавить-статический-узел-в-кластер) скрипт `bootstrap.sh`.

#### Как понять, что что-то пошло не так?

Если узел в NodeGroup не обновляется (значение `UPTODATE` при выполнении команды `d8 k get nodegroup` меньше значения `NODES`) или вы предполагаете какие-то другие проблемы, которые могут быть связаны с модулем `node-manager`, нужно проверить логи сервиса `bashible`. Сервис `bashible` запускается на каждом узле, управляемом модулем `node-manager`.

Чтобы проверить логи сервиса `bashible`, выполните на узле следующую команду:

```shell
journalctl -fu bashible
```

Пример вывода, когда все необходимые действия выполнены:

```console
May 25 04:39:16 kube-master-0 systemd[1]: Started Bashible service.
May 25 04:39:16 kube-master-0 bashible.sh[1976339]: Configuration is in sync, nothing to do.
May 25 04:39:16 kube-master-0 systemd[1]: bashible.service: Succeeded.
```

#### Как посмотреть, что в данный момент выполняется на узле при его создании?

Если необходимо узнать, что происходит на узле (например, узел долго создается), можно проверить логи `cloud-init`. Для этого выполните следующие шаги:

1. Найдите узел, который находится в стадии бутстрапа:

   ```shell
   d8 k get instances | grep Pending
   ```

   Пример:

   ```shell
   d8 k get instances | grep Pending
   dev-worker-2a6158ff-6764d-nrtbj   Pending   46s
   ```

1. Получите информацию о параметрах подключения для просмотра логов:

   ```shell
   d8 k get instances dev-worker-2a6158ff-6764d-nrtbj -o yaml | grep 'bootstrapStatus' -B0 -A2
   ```

   Пример:

   ```shell
   d8 k get instances dev-worker-2a6158ff-6764d-nrtbj -o yaml | grep 'bootstrapStatus' -B0 -A2
   bootstrapStatus:
     description: Use 'nc 192.168.199.178 8000' to get bootstrap logs.
     logsEndpoint: 192.168.199.178:8000
   ```

1. Выполните полученную команду (в примере выше — `nc 192.168.199.178 8000`), чтобы просмотреть логи `cloud-init` и определить, на каком этапе остановилась настройка узла.

Логи первоначальной настройки узла находятся в `/var/log/cloud-init-output.log`.

#### Как обновить ядро на узлах?

##### Для дистрибутивов, основанных на Debian

Создайте ресурс `NodeGroupConfiguration`, указав в переменной `desired_version` shell-скрипта (параметр `spec.content` ресурса) желаемую версию ядра:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: install-kernel.sh
spec:
  bundles:
    - '*'
  nodeGroups:
    - '*'
  weight: 32
  content: |
    # Copyright 2022 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    desired_version="5.15.0-53-generic"

    bb-event-on 'bb-package-installed' 'post-install'
    post-install() {
      bb-log-info "Setting reboot flag due to kernel was updated"
      bb-flag-set reboot
    }

    version_in_use="$(uname -r)"

    if [[ "$version_in_use" == "$desired_version" ]]; then
      exit 0
    fi

    bb-deckhouse-get-disruptive-update-approval
    bb-apt-install "linux-image-${desired_version}"
```

##### Для дистрибутивов, основанных на CentOS

Создайте ресурс `NodeGroupConfiguration`, указав в переменной `desired_version` shell-скрипта (параметр `spec.content` ресурса) желаемую версию ядра:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: install-kernel.sh
spec:
  bundles:
    - '*'
  nodeGroups:
    - '*'
  weight: 32
  content: |
    # Copyright 2022 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    desired_version="3.10.0-1160.42.2.el7.x86_64"

    bb-event-on 'bb-package-installed' 'post-install'
    post-install() {
      bb-log-info "Setting reboot flag due to kernel was updated"
      bb-flag-set reboot
    }

    version_in_use="$(uname -r)"

    if [[ "$version_in_use" == "$desired_version" ]]; then
      exit 0
    fi

    bb-deckhouse-get-disruptive-update-approval
    bb-dnf-install "kernel-${desired_version}"
```

#### Какие параметры NodeGroup к чему приводят?

| Параметр NG                           | Disruption update          | Перезаказ узлов   | Рестарт kubelet |
|---------------------------------------|----------------------------|-------------------|-----------------|
| chaos                                 | -                          | -                 | -               |
| cloudInstances.classReference         | -                          | +                 | -               |
| cloudInstances.maxSurgePerZone        | -                          | -                 | -               |
| cri.containerd.maxConcurrentDownloads | -                          | -                 | +               |
| cri.type                              | - (NotManaged) / + (other) | -                 | -               |
| disruptions                           | -                          | -                 | -               |
| kubelet.maxPods                       | -                          | -                 | +               |
| kubelet.rootDir                       | -                          | -                 | +               |
| kubernetesVersion                     | -                          | -                 | +               |
| nodeTemplate                          | -                          | -                 | -               |
| static                                | -                          | -                 | +               |
| update.maxConcurrent                  | -                          | -                 | -               |

Подробно о всех параметрах можно прочитать в описании кастомного ресурса [NodeGroup](cr.html#nodegroup).

В случае изменения параметров `InstanceClass` или `instancePrefix` в конфигурации Deckhouse не будет происходить `RollingUpdate`. Deckhouse создаст новые `MachineDeployment`, а старые удалит. Количество заказываемых одновременно `MachineDeployment` определяется параметром `cloudInstances.maxSurgePerZone`.

При обновлении, которое требует прерывания работы узла (disruption update), выполняется процесс вытеснения подов с узла. Если какой-либо под не может быть вытеснен, попытка повторяется каждые 20 секунд до достижения глобального таймаута в 5 минут. После истечения этого времени, поды, которые не удалось вытеснить, удаляются принудительно.

#### Как пересоздать эфемерные машины в облаке с новой конфигурацией?

При изменении конфигурации Deckhouse (как в модуле `node-manager`, так и в любом из облачных провайдеров) виртуальные машины не будут перезаказаны. Пересоздание происходит только после изменения ресурсов `InstanceClass` или `NodeGroup`.

Чтобы принудительно пересоздать все узлы, связанные с ресурсом `Machines`, следует добавить/изменить аннотацию `manual-rollout-id` в `NodeGroup`: `d8 k annotate NodeGroup имя_ng "manual-rollout-id=$(uuidgen)" --overwrite`.

#### Как выделить узлы под специфические нагрузки?

{% alert level="warning" %}
Запрещено использование домена `deckhouse.io` в ключах `labels` и `taints` у `NodeGroup`. Он зарезервирован для компонентов Deckhouse. Следует отдавать предпочтение в пользу ключей `dedicated` или `dedicated.client.com`.
{% endalert %}

Для решений данной задачи существуют два механизма:

1. Установка меток в `NodeGroup` `spec.nodeTemplate.labels` для последующего использования их в `Pod` spec.nodeSelector или spec.affinity.nodeAffinity. Указывает, какие именно узлы будут выбраны планировщиком для запуска целевого приложения.
1. Установка ограничений в `NodeGroup` `spec.nodeTemplate.taints` с дальнейшим снятием их в `Pod` spec.tolerations. Запрещает исполнение не разрешенных явно приложений на этих узлах.

{% alert level="info" %}
Deckhouse по умолчанию поддерживает использование taint'а с ключом `dedicated`, поэтому рекомендуется применять этот ключ с любым значением для taints на ваших выделенных узлах.

Если требуется использовать другие ключи для taints (например, `dedicated.client.com`), необходимо добавить соответствующее значение ключа в массив [`.spec.settings.modules.placement.customTolerationKeys`](/reference/api/global.html#parameters-modules-placement-customtolerationkeys). Это обеспечит разрешение системным компонентам, таким как `cni-flannel`, использовать эти узлы.
{% endalert %}

#### Как выделить узлы под системные компоненты?

##### Фронтенд

Для Ingress-контроллеров используйте `NodeGroup` со следующей конфигурацией:

```yaml
nodeTemplate:
  labels:
    node-role.deckhouse.io/frontend: ""
  taints:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      value: frontend
```

##### Системные

Для компонентов подсистем Deckhouse параметр `NodeGroup` будет настроен с параметрами:

```yaml
nodeTemplate:
  labels:
    node-role.deckhouse.io/system: ""
  taints:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      value: system
```

#### Как ускорить заказ узлов в облаке при горизонтальном масштабировании приложений?

Самое действенное — держать в кластере некоторое количество предварительно подготовленных узлов, которые позволят новым репликам ваших приложений запускаться мгновенно. Очевидным минусом данного решения будут дополнительные расходы на содержание этих узлов.

Необходимые настройки целевой `NodeGroup` будут следующие:

1. Указать абсолютное количество предварительно подготовленных узлов (или процент от максимального количества узлов в этой группе) в параметре `cloudInstances.standby`.
1. При наличии на узлах дополнительных служебных компонентов, не обслуживаемых Deckhouse (например, DaemonSet `filebeat`), задать их процентное потребление ресурсов узла можно в параметре `standbyHolder.overprovisioningRate`.
1. Для работы этой функции требуется, чтобы как минимум один узел из группы уже был запущен в кластере. Иными словами, либо должна быть доступна одна реплика приложения, либо количество узлов для этой группы `cloudInstances.minPerZone` должно быть `1`.

Пример:

```yaml
cloudInstances:
  maxPerZone: 10
  minPerZone: 1
  standby: 10%
  standbyHolder:
    overprovisioningRate: 30%
```

#### Как выключить machine-controller-manager/CAPI в случае выполнения потенциально деструктивных изменений в кластере?

{% alert level="danger" %}
Использовать эту настройку допустимо только тогда, когда вы четко понимаете, зачем это необходимо.
{% endalert %}

Для того чтобы временно отключить machine-controller-manager (MCM) и предотвратить его автоматические действия, которые могут повлиять на инфраструктуру кластера (например, удаление или пересоздание узлов), установите следующий параметр в конфигурации:

```yaml
mcmEmergencyBrake: true
```

Для отключения CAPI установите следующий параметр в конфигурации:

```yaml
capiEmergencyBrake: true
```

#### Как восстановить master-узел, если kubelet не может загрузить компоненты control plane?

Подобная ситуация может возникнуть, если в кластере с одним master-узлом на нем были удалены образы компонентов control plane (например, удалена директория `/var/lib/containerd`).
В этом случае kubelet при рестарте не сможет скачать образы компонентов `control plane`, поскольку на master-узле нет параметров авторизации в `registry.deckhouse.io`.

Далее приведена инструкция по восстановлению master-узла.

##### containerd

Для восстановления работоспособности master-узла нужно в любом рабочем кластере под управлением Deckhouse выполнить команду:

```shell
d8 k -n d8-system get secrets deckhouse-registry -o json |
jq -r '.data.".dockerconfigjson"' | base64 -d |
jq -r '.auths."registry.deckhouse.io".auth'
```

Вывод команды нужно скопировать и присвоить переменной `AUTH` на поврежденном master-узле.

Далее на поврежденном master-узле нужно загрузить образы компонентов `control-plane`:

```shell
for image in $(grep "image:" /etc/kubernetes/manifests/* | awk '{print $3}'); do
  crictl pull --auth $AUTH $image
done
```

После загрузки образов необходимо перезапустить `kubelet`.

#### Как изменить CRI для NodeGroup?

{% alert level="warning" %}
Смена CRI возможна только между `Containerd` на `NotManaged` и обратно (параметр [cri.type](cr.html#nodegroup-v1-spec-cri-type)).
{% endalert %}

Для изменения CRI для NodeGroup, установите параметр [cri.type](cr.html#nodegroup-v1-spec-cri-type) в `Containerd` или в `NotManaged`.

Пример YAML-манифеста NodeGroup:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: worker
spec:
  nodeType: Static
  cri:
    type: Containerd
```

Также эту операцию можно выполнить с помощью патча:

* Для `Containerd`:

  ```shell
  d8 k patch nodegroup <имя NodeGroup> --type merge -p '{"spec":{"cri":{"type":"Containerd"}}}'
  ```

* Для `NotManaged`:

  ```shell
  d8 k patch nodegroup <имя NodeGroup> --type merge -p '{"spec":{"cri":{"type":"NotManaged"}}}'
  ```

{% alert level="warning" %}
 При изменении `cri.type` для NodeGroup, созданных с помощью `dhctl`, необходимо обновить это значение в `dhctl config edit provider-cluster-configuration` и настройках объекта NodeGroup.
{% endalert %}

После изменения CRI для NodeGroup модуль `node-manager` будет поочередно перезагружать узлы, применяя новый CRI.  Обновление узла сопровождается простоем (disruption). В зависимости от настройки `disruption` для NodeGroup, модуль `node-manager` либо автоматически выполнит обновление узлов, либо потребует подтверждения вручную.

#### Как изменить CRI для всего кластера?

{% alert level="warning" %}
Смена CRI возможна только между `Containerd` на `NotManaged` и обратно (параметр [cri.type](cr.html#nodegroup-v1-spec-cri-type)).
{% endalert %}

Для изменения CRI для всего кластера, необходимо с помощью утилиты `dhctl` отредактировать параметр `defaultCRI` в конфигурационном файле `cluster-configuration`.

Также возможно выполнить эту операцию с помощью `d8 k patch`.

* Для `Containerd`:

  ```shell
  data="$(d8 k -n kube-system get secret d8-cluster-configuration -o json | jq -r '.data."cluster-configuration.yaml"' | base64 -d | sed "s/NotManaged/Containerd/" | base64 -w0)"
  d8 k -n kube-system patch secret d8-cluster-configuration -p "{\"data\":{\"cluster-configuration.yaml\":\"$data\"}}"
  ```

* Для `NotManaged`:

  ```shell
  data="$(d8 k -n kube-system get secret d8-cluster-configuration -o json | jq -r '.data."cluster-configuration.yaml"' | base64 -d | sed "s/Containerd/NotManaged/" | base64 -w0)"
  d8 k -n kube-system patch secret d8-cluster-configuration -p "{\"data\":{\"cluster-configuration.yaml\":\"$data\"}}"
  ```

Если необходимо, чтобы отдельные NodeGroup использовали другой CRI, перед изменением `defaultCRI` необходимо установить CRI для этой NodeGroup,
как описано [в документации](#как-изменить-cri-для-nodegroup).

{% alert level="danger" %}
Изменение `defaultCRI` влечет за собой изменение CRI на всех узлах, включая master-узлы.
Если master-узел один, данная операция является опасной и может привести к полной неработоспособности кластера.
Рекомендуется использовать multimaster-конфигурацию и менять тип CRI только после этого.
{% endalert %}

При изменении CRI в кластере для master-узлов необходимо выполнить дополнительные шаги:

1. Чтобы определить, какой узел в текущий момент обновляется в master NodeGroup, используйте следующую команду:

   ```shell
   d8 k get nodes -l node-role.kubernetes.io/control-plane="" -o json | jq '.items[] | select(.metadata.annotations."update.node.deckhouse.io/approved"=="") | .metadata.name' -r
   ```

1. Подтвердите остановку (disruption) для master-узла, полученного на предыдущем шаге:

   ```shell
   d8 k annotate node <имя master-узла> update.node.deckhouse.io/disruption-approved=
   ```

1. Дождитесь перехода обновленного master-узла в `Ready`. Выполните итерацию для следующего master-узла.

#### Как добавить шаг для конфигурации узлов?

Дополнительные шаги для конфигурации узлов задаются с помощью кастомного ресурса [NodeGroupConfiguration](cr.html#nodegroupconfiguration).

#### Как автоматически проставить на узел кастомные лейблы?

1. На узле создайте каталог `/var/lib/node_labels`.

1. Создайте в нём файл или файлы, содержащие необходимые лейблы. Количество файлов может быть любым, как и вложенность подкаталогов, их содержащих.

1. Добавьте в файлы нужные лейблы в формате `key=value`. Например:

   ```console
   example-label=test
   ```

1. Сохраните файлы.

При добавлении узла в кластер указанные в файлах лейблы будут автоматически проставлены на узел.

{% alert level="warning" %}
Обратите внимание, что добавить таким образом лейблы, использующиеся в Deckhouse Platform Certified Security Edition, невозможно. Работать такой метод будет только с кастомными лейблами, не пересекающимися с зарезервированными для Deckhouse.
{% endalert %}

#### Как развернуть кастомный конфигурационный файл containerd?

{% alert level="info" %}
Пример `NodeGroupConfiguration` основан на функциях, заложенных в скрипте [032_configure_containerd.sh](./#особенности-написания-скриптов).
{% endalert %}

{% alert level="danger" %}
Добавление кастомных настроек вызывает перезапуск сервиса containerd.
{% endalert %}

Bashible на узлах объединяет конфигурацию containerd для Deckhouse с конфигурацией из файла `/etc/containerd/conf.d/*.toml`.

{% alert level="warning" %}
Вы можете переопределять значения параметров, которые заданы в файле `/etc/containerd/deckhouse.toml`, но их работу придётся обеспечивать самостоятельно. Также, лучше изменением конфигурации не затрагивать master-узлы (nodeGroup `master`).
{% endalert %}

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-option-config.sh
spec:
  bundles:
    - '*'
  content: |
    # Copyright 2024 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_option.toml - << EOF
    oom_score = 500
    [metrics]
    address = "127.0.0.1"
    grpc_histogram = true
    EOF
  nodeGroups:
    - "worker"
  weight: 31
```

##### Как добавить конфигурацию для дополнительного registry?

В containerd существует два способа описания конфигурации registry: **устаревший** и **актуальный**.

Для проверки наличия **устаревшего** способа конфигурации выполните на узлах кластера следующие команды:

```bash
cat /etc/containerd/config.toml | grep 'plugins."io.containerd.grpc.v1.cri".registry.mirrors'
cat /etc/containerd/config.toml | grep 'plugins."io.containerd.grpc.v1.cri".registry.configs'

### Пример вывода:
### [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
###   [plugins."io.containerd.grpc.v1.cri".registry.mirrors."<REGISTRY_URL>"]
### [plugins."io.containerd.grpc.v1.cri".registry.configs]
###   [plugins."io.containerd.grpc.v1.cri".registry.configs."<REGISTRY_URL>".auth]
```

Для проверки наличия **актуального** способа конфигурации выполните на узлах кластера следующую команду:

```bash
cat /etc/containerd/config.toml | grep '/etc/containerd/registry.d'

### Пример вывода:
### config_path = "/etc/containerd/registry.d"
```

###### Устаревший способ

{% alert level="warning" %}
Этот формат конфигурации containerd устарел (deprecated).
{% endalert %}

{% alert level="info" %}
Используется в containerd v1, если Deckhouse не управляется с помощью модуля [registry](/modules/registry/).
{% endalert %}

Конфигурация описывается в основном конфигурационном файле containerd `/etc/containerd/config.toml`.

Пользовательская конфигурация добавляется через механизм `toml merge`. Конфигурационные файлы из директории `/etc/containerd/conf.d` объединяются с основным файлом `/etc/containerd/config.toml`. Применение merge происходит на этапе выполнения скрипта `032_configure_containerd.sh`, поэтому соответствующие файлы должны быть добавлены заранее.

Пример конфигурационного файла для директории `/etc/containerd/conf.d/`:

```toml
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${REGISTRY_URL}"]
          endpoint = ["https://${REGISTRY_URL}"]
      [plugins."io.containerd.grpc.v1.cri".registry.configs]
        [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".auth]
          auth = "${BASE_64_AUTH}"
          username = "${USERNAME}"
          password = "${PASSWORD}"
        [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".tls]
          ca_file = "${CERT_DIR}/${CERT_NAME}.crt"
          insecure_skip_verify = true
```

{% alert level="danger" %}
Добавление кастомных настроек через механизм `toml merge` вызывает перезапуск сервиса containerd.
{% endalert %}

####### Как добавить авторизацию в дополнительный registry (устаревший способ)?

Пример добавления авторизации в дополнительный registry при использовании **устаревшего** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-auth.sh
spec:
  # Для добавления файла перед шагом '032_configure_containerd.sh'
  weight: 31
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
    [plugins]
      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
            [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${REGISTRY_URL}"]
              endpoint = ["https://${REGISTRY_URL}"]
          [plugins."io.containerd.grpc.v1.cri".registry.configs]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".auth]
              username = "username"
              password = "password"
              # OR
              auth = "dXNlcm5hbWU6cGFzc3dvcmQ="
    EOF
```

####### Как настроить сертификат для дополнительного registry (устаревший способ)?

Пример настройки сертификата для дополнительного registry при использовании **устаревшего** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-tls.sh
spec:
  # Для добавления файла перед шагом '032_configure_containerd.sh'
  weight: 31
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example
    CERT_FILE_NAME=${REGISTRY_URL}
    CERTS_FOLDER="/var/lib/containerd/certs/"


    mkdir -p ${CERTS_FOLDER}
    bb-sync-file "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt" - << EOF
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    EOF

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
    [plugins]
      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          [plugins."io.containerd.grpc.v1.cri".registry.configs]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".tls]
              ca_file = "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt"
    EOF
```

{% alert level="info" %}
Помимо containerd, сертификат можно [добавить в операционную систему](examples.html#добавление-корневого-сертификата-в-хост).
{% endalert %}

####### Как добавить TLS skip verify (устаревший способ)?

Пример добавления TLS skip verify при использовании **устаревшего** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-skip-tls.sh
spec:
  # Для добавления файла перед шагом '032_configure_containerd.sh'
  weight: 31
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
    [plugins]
      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          [plugins."io.containerd.grpc.v1.cri".registry.configs]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".tls]
              insecure_skip_verify = true
    EOF
```

После применения конфигурационного файла проверьте доступ к registry с узлов, используя команду:

```bash
### Через cri-интерфейс
crictl pull private.registry.example/image/repo:tag
```

###### Новый способ

{% alert level="info" %}
Используется в containerd v2.  

Используется в containerd v1, если управление осуществляется через модуль [`registry`](/modules/registry/) (например, в режиме [`Direct`](./deckhouse/configuration.html#parameters-registry)).
{% endalert %}

Конфигурация описывается в каталоге `/etc/containerd/registry.d` и задаётся через создание подкаталогов с именами, соответствующими адресу registry:

```bash
/etc/containerd/registry.d
├── private.registry.example:5001
│   ├── ca.crt
│   └── hosts.toml
└── registry.deckhouse.ru
    ├── ca.crt
    └── hosts.toml
```

Пример содержимого файла `hosts.toml`:

```toml
[host]
  # Mirror 1.
  [host."https://${REGISTRY_URL_1}"]
    capabilities = ["pull", "resolve"]
    ca = ["${CERT_DIR}/${CERT_NAME}.crt"]

    [host."https://${REGISTRY_URL_1}".auth]
      username = "${USERNAME}"
      password = "${PASSWORD}"

  # Mirror 2.
  [host."http://${REGISTRY_URL_2}"]
    capabilities = ["pull", "resolve"]
    skip_verify = true
```

{% alert level="info" %}
Изменения конфигураций не приводят к перезапуску сервиса containerd.
{% endalert %}

####### Как добавить авторизацию в дополнительный registry (актуальный способ)?

Пример добавления авторизации в дополнительный registry при использовании **актуального** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-auth.sh
spec:
  # Шаг может быть любой, т.к. не требуется перезапуск сервиса containerd.
  weight: 0
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"
    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
    [host]
      [host."https://${REGISTRY_URL}"]
        capabilities = ["pull", "resolve"]
        [host."https://${REGISTRY_URL}".auth]
          username = "username"
          password = "password"
    EOF
```

####### Как настроить сертификат для дополнительного registry (актуальный способ)?

Пример настройки сертификата для дополнительного registry? при использовании **актуального** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-tls.sh
spec:
  # Шаг может быть любой, тк не требуется перезапуск сервиса containerd.
  weight: 0
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"

    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/ca.crt" - << EOF
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    EOF

    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
    [host]
      [host."https://${REGISTRY_URL}"]
        capabilities = ["pull", "resolve"]
        ca = ["/etc/containerd/registry.d/${REGISTRY_URL}/ca.crt"]
    EOF
```

{% alert level="info" %}
Помимо containerd, сертификат можно [добавить в операционную систему](examples.html#добавление-корневого-сертификата-в-хост).
{% endalert %}

####### Как добавить TLS skip verify (актуальный способ)?

Пример добавления TLS skip verify при использовании **актуального** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-skip-tls.sh
spec:
  # Шаг может быть любой, тк не требуется перезапуск сервиса containerd.
  weight: 0
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"
    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
    [host]
      [host."https://${REGISTRY_URL}"]
        capabilities = ["pull", "resolve"]
        skip_verify = true
    EOF
```

После применения конфигурационного файла проверьте доступ к registry с узлов, используя команды:

```bash
### Через cri интерфейс.
crictl pull private.registry.example/image/repo:tag

### Через ctr с указанием директории с конфигурациями.
ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ private.registry.example/image/repo:tag

### Через ctr для http репозитория.
ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ --plain-http private.registry.example/image/repo:tag
```

#### Как интерпретировать состояние группы узлов?

**Ready** — группа узлов содержит минимально необходимое число запланированных узлов с состоянием `Ready` для всех зон.

Пример 1. Группа узлов в состоянии `Ready`:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: ng1
spec:
  nodeType: CloudEphemeral
  cloudInstances:
    maxPerZone: 5
    minPerZone: 1
status:
  conditions:
  - status: "True"
    type: Ready
---
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    node.deckhouse.io/group: ng1
status:
  conditions:
  - status: "True"
    type: Ready
```

Пример 2. Группа узлов в состоянии `Not Ready`:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: ng1
spec:
  nodeType: CloudEphemeral
  cloudInstances:
    maxPerZone: 5
    minPerZone: 2
status:
  conditions:
  - status: "False"
    type: Ready
---
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    node.deckhouse.io/group: ng1
status:
  conditions:
  - status: "True"
    type: Ready
```

**Updating** — группа узлов содержит как минимум один узел, в котором присутствует аннотация с префиксом `update.node.deckhouse.io` (например, `update.node.deckhouse.io/waiting-for-approval`).

**WaitingForDisruptiveApproval** — группа узлов содержит как минимум один узел, в котором присутствует аннотация `update.node.deckhouse.io/disruption-required` и
отсутствует аннотация `update.node.deckhouse.io/disruption-approved`.

**Scaling** — рассчитывается только для групп узлов с типом `CloudEphemeral`. Состояние `True` может быть в двух случаях:

1. Когда число узлов меньше *желаемого числа узлов в группе, то есть когда нужно увеличить число узлов в группе*.
1. Когда какой-то узел помечается к удалению или число узлов больше *желаемого числа узлов*, то есть когда нужно уменьшить число узлов в группе.

*Желаемое число узлов* — это сумма всех реплик, входящих в группу узлов.

Пример. Желаемое число узлов равно 2:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: ng1
spec:
  nodeType: CloudEphemeral
  cloudInstances:
    maxPerZone: 5
    minPerZone: 2
status:
...
  desired: 2
...
```

**Error** — содержит последнюю ошибку, возникшую при создании узла в группе узлов.

#### Как заставить werf игнорировать состояние Ready в группе узлов?

werf проверяет состояние `Ready` у ресурсов и в случае его наличия дожидается, пока значение станет `True`.

Создание (обновление) ресурса [nodeGroup](cr.html#nodegroup) в кластере может потребовать значительного времени на развертывание необходимого количества узлов. При развертывании такого ресурса в кластере с помощью werf (например, в рамках процесса CI/CD) развертывание может завершиться по превышении времени ожидания готовности ресурса. Чтобы заставить werf игнорировать состояние `nodeGroup`, необходимо добавить к `nodeGroup` следующие аннотации:

```yaml
metadata:
  annotations:
    werf.io/fail-mode: IgnoreAndContinueDeployProcess
    werf.io/track-termination-mode: NonBlocking
```

#### Что такое ресурс Instance?

Ресурс `Instance` в Kubernetes представляет собой описание объекта эфемерной виртуальной машины, но без конкретной реализации. Это абстракция, которая используется для управления машинами, созданными с помощью таких инструментов, как MachineControllerManager или Cluster API Provider Static.

Объект не содержит спецификации. Статус содержит:

1. Ссылку на `InstanceClass`, если он существует для данной реализации.
1. Ссылку на объект Node Kubernetes.
1. Текущий статус машины.
1. Информацию о том, как проверить [логи создания машины](#как-посмотреть-что-в-данный-момент-выполняется-на-узле-при-его-создании) (появляется на этапе создания машины).

При создании или удалении машины создается или удаляется соответствующий объект Instance.
Самостоятельно ресурс `Instance` создать нельзя, но можно удалить. В таком случае машина будет удалена из кластера (процесс удаления зависит от деталей реализации).

#### Когда требуется перезагрузка узлов?

Некоторые операции по изменению конфигурации узлов могут потребовать перезагрузки.

Перезагрузка узла может потребоваться при изменении некоторых настроек sysctl, например, при изменении параметра `kernel.yama.ptrace_scope` (изменяется при использовании команды `astra-ptrace-lock enable/disable` в Astra Linux).

#### Как мониторить GPU?

Deckhouse Platform Certified Security Edition автоматически устанавливает **DCGM Exporter**; метрики GPU попадают в Prometheus и доступны в Grafana.

#### Какие режимы работы GPU поддерживаются?

Поддерживаются следующие режимы работы GPU:

- **Exclusive** — узел публикует ресурс `nvidia.com/gpu`; каждому поду выделяется целый GPU.
- **TimeSlicing** — временное разделение одного GPU между несколькими подами (по умолчанию `partitionCount: 4`), при этом под по-прежнему запрашивает `nvidia.com/gpu`.
- **MIG (Multi-Instance GPU)** — аппаратное разделение совместимых GPU на независимые экземпляры; при профиле `all-1g.5gb` появятся ресурсы вида `nvidia.com/mig-1g.5gb`.

Примеры приведены в разделе [Управление узлами: примеры](./node-manager/examples.html#пример-gpu-nodegroup).

#### Как посмотреть доступные MIG-профили в кластере?

<span id="как-посмотреть-доступные-mig-профили-в-кластере"></span>

Предустановленные профили находятся в ConfigMap `mig-parted-config` в пространстве имен `d8-nvidia-gpu`. Для их просмотра используйте команду:

```bash
d8 k -n d8-nvidia-gpu get cm mig-parted-config -o json | jq -r '.data["config.yaml"]'
```

В разделе mig-configs вы увидите конкретные модели ускорителей (по PCI-ID) и список совместимых MIG-профилей для каждой из них.
Найдите свою видеокарту и выберите подходящий профиль — его имя указывается в `spec.gpu.mig.partedConfig` вашего NodeGroup.
Это позволит применить правильный профиль именно к вашей карте.

#### Для GPU не активируется MIG-профиль — что проверить?

1. Модель GPU: MIG поддерживают H100/A100/A30, **не** поддерживает V100/T4.
1. Конфигурация NodeGroup:

   ```yaml
   gpu:
     sharing: MIG
     mig:
       partedConfig: all-1g.5gb
   ```

1. Дождитесь, пока `nvidia-mig-manager` выполнит drain узла и переконфигурирует GPU.

    **Этот процесс может занять несколько минут**.

    Пока операция идёт, на узле стоит taint `mig-reconfigure`. После успешного окончания taint удаляется.

1. Ход процесса можно отслеживать по label `nvidia.com/mig.config.state` на узле:

    `pending`, `rebooting`, `success` (или `error`, если что-то пошло не так).

1. Если ресурсы `nvidia.com/mig-*` не появились — проверьте:

   ```bash
   d8 k -n d8-nvidia-gpu logs daemonset/nvidia-mig-manager
   nvidia-smi -L
   ```

#### Поддерживаются ли AMD или Intel GPU?

Сейчас Deckhouse Platform Certified Security Edition автоматически настраивает **только NVIDIA GPU**. Поддержка **AMD (ROCm)** и **Intel GPU** находится в проработке и планируется к добавлению в будущих релизах.

### Модуль local-path-provisioner

Позволяет пользователям Kubernetes использовать локальное хранилище на узлах.

#### Как это работает?

Для каждого custom resource [LocalPathProvisioner](cr.html) создается соответствующий `StorageClass`.

Допустимая топология для `StorageClass` вычисляется на основе списка `nodeGroup` из custom resource. Топология используется при шедулинге подов.

Когда под заказывает диск, то:
- создается `HostPath` PV;
- `Provisioner` создает на нужном узле локальную папку по пути, состоящем из параметра `path` custom resource, имени PV и имени PVC.
  
  Пример пути:

  ```shell
  /opt/local-path-provisioner/pvc-d9bd3878-f710-417b-a4b3-38811aa8aac1_d8-monitoring_prometheus-main-db-prometheus-main-0
  ```

#### Ограничения

- Ограничение на размер диска не поддерживается для локальных томов.

### Модуль local-path-provisioner: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Модуль не требует конфигурации.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['local-path-provisioner'].config-values | format_module_configuration: moduleKebabName }}

### Модуль local-path-provisioner: custom resources
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}

### Модуль local-path-provisioner: примеры

#### Пример custom resource `LocalPathProvisioner`

Reclaim policy устанавливается по умолчанию в `Retain`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: LocalPathProvisioner
metadata:
  name: localpath-system
spec:
  nodeGroups:
  - system
  path: "/opt/local-path-provisioner"
```

#### Пример custom resource `LocalPathProvisioner` с установленным `reclaimPolicy`

Reclaim policy устанавливается в `Delete`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: LocalPathProvisioner
metadata:
  name: localpath-system
spec:
  nodeGroups:
  - system
  path: "/opt/local-path-provisioner"
  reclaimPolicy: "Delete"
```

### Модуль local-path-provisioner: FAQ

#### Как настроить Prometheus на использование локального хранилища?

Применить custom resource `LocalPathProvisioner`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: LocalPathProvisioner
metadata:
  name: localpath-system
spec:
  nodeGroups:
  - system
  path: "/opt/local-path-provisioner"
```

- `spec.nodeGroups` должен совпадать с NodeGroup, где запущен под Prometheus’а.
- `spec.path` - путь на узле, где будут лежать данные.

Добавить в конфигурацию модуля `prometheus` следующие параметры:

```yaml
longtermStorageClass: localpath-system
storageClass: localpath-system
```

Дождаться переката подов Prometheus.

### Модуль namespace-configurator

Модуль namespace-configurator позволяет автоматически управлять аннотациями и лейблами на пространствах имён.

Чтобы автоматически включать новые пространства имён в мониторинг, добавьте лейбл `extended-monitoring.deckhouse.io/enabled=true`.

#### Как работает

Модуль следит за изменениями пространства имён и своей конфигурации:

* назначает лейблы и аннотации из конфигурации всем пространствам имён, подпадающим под шаблон `includeNames` и не подпадающим под шаблон `excludeNames`;
* игнорируются пространства имён с метками `heritage` с одним из значений `upmeter`, `deckhouse` или `multitenancy-manager`;
* при изменении конфигурации модуля лейблы и аннотации пространств имён будут переназначены согласно конфигурации.

#### Что нужно настроить?

Перечислите список желаемых лейблов и аннотаций, а также список шаблонов поиска пространств имён в конфигурации модуля.

### Модуль namespace-configurator: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['namespace-configurator'].config-values | format_module_configuration: moduleKebabName }}

### Модуль namespace-configurator: примеры

#### Пример

Добавьте лейбл `extended-monitoring.deckhouse.io/enabled=true` и аннотацию `foo=bar` к каждому namespace, начинающемуся с `prod-` или `infra-`, за исключением `infra-test`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: namespace-configurator
spec:
  version: 1
  enabled: true
  settings:
    configurations:
    - annotations:
        foo: bar
      labels:
        extended-monitoring.deckhouse.io/enabled: "true"
      includeNames:
      - "^prod"
      - "^infra"
      excludeNames:
      - "infra-test"
```

### Модуль admission-policy-engine

Позволяет использовать в кластере политики безопасности согласно Pod Security Standards Kubernetes. Модуль для работы использует Gatekeeper.

Pod Security Standards определяют три политики, охватывающие весь спектр безопасности. Эти политики являются кумулятивными, то есть состоящими из набора политик, и варьируются по уровню ограничений от «неограничивающего» до «ограничивающего значительно».

{% alert level="info" %}
Модуль не применяет политики к системным пространствам имен.
{% endalert %}

Список политик, доступных для использования:
- `Privileged` — неограничивающая политика с максимально широким уровнем разрешений;
- `Baseline` — минимально ограничивающая политика, которая предотвращает наиболее известные и популярные способы повышения привилегий. Позволяет использовать стандартную (минимально заданную) конфигурацию пода;
- `Restricted` — политика со значительными ограничениями. Предъявляет самые жесткие требования к подам.

Политика кластера используемая по умолчанию определяется следующим образом:
- При установке Deckhouse версии **ниже v1.55**, для всех несистемных пространств имен используется политика по умолчанию `Privileged`;
- При установке Deckhouse версии **v1.55 и выше**, для всех несистемных пространств имен используется политика по умолчанию `Baseline`;

**Обратите внимание,** что обновление Deckhouse в кластере на версию v1.55 не вызывает автоматической смены политики по умолчанию.

Политику по умолчанию можно переопределить как глобально ([в настройках модуля](configuration.html#parameters-podsecuritystandards-defaultpolicy)), так и для каждого пространства имен отдельно (лейбл `security.deckhouse.io/pod-policy=<POLICY_NAME>` на соответствующем пространстве имен).

Пример установки политики `Restricted` для всех подов в пространстве имен `my-namespace`:

```bash
d8 k label ns my-namespace security.deckhouse.io/pod-policy=restricted
```

По умолчанию, политики Pod Security Standards применяются в режиме "Deny" и поды приложений, не удовлетворяющие данным политикам, не смогут быть запущены. Режим работы политик может быть задан как глобально для кластера так и для каждого namespace отдельно. Что бы задать режим работы политик глобально используйте [configuration](configuration.html#parameters-podsecuritystandards-enforcementaction). В случае если необходимо переопределить глобальный режим политик для определенного namespace, допускается использовать лейбл `security.deckhouse.io/pod-policy-action =<POLICY_ACTION>` на соответствующем namespace. Список допустимых режимом политик состоит из: "dryrun", "warn", "deny".

Пример установки "warn" режима политик PSS для всех подов в пространстве имен `my-namespace`:

```bash
d8 k label ns my-namespace security.deckhouse.io/pod-policy-action=warn
```

Предлагаемые модулем политики могут быть расширены. Примеры расширения политик можно найти в [FAQ](faq.html).

##### Операционные политики

Модуль предоставляет набор операционных политик и лучших практик для безопасной работы ваших приложений.
Операционные политики описываются с помощью кастомного ресурса [`OperationPolicy`](/modules/admission-policy-engine/cr.html#operationpolicy).

Мы рекомендуем устанавливать следующий минимальный набор операционных политик:

```yaml
---
apiVersion: deckhouse.io/v1alpha1
kind: OperationPolicy
metadata:
  name: common
spec:
  policies:
    allowedRepos:
      - myrepo.example.com
      - registry.deckhouse.io
    requiredResources:
      limits:
        - memory
      requests:
        - cpu
        - memory
    disallowedImageTags:
      - latest
    requiredProbes:
      - livenessProbe
      - readinessProbe
    maxRevisionHistoryLimit: 3
    imagePullPolicy: Always
    priorityClassNames:
    - production-high
    - production-low
    checkHostNetworkDNSPolicy: true
    checkContainerDuplicates: true
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          operation-policy.deckhouse.io/enabled: "true"
```

Для применения приведенной политики достаточно навесить лейбл `operation-policy.deckhouse.io/enabled: "true"` на желаемый namespace. Политика, приведенная в примере, рекомендована для использования командой Deckhouse. Аналогичным образом вы можете создать собственную политику с необходимыми настройками.

##### Политики безопасности

Модуль предоставляет возможность определять политики безопасности применимо к приложениям (контейнерам), запущенным в кластере.

Пример политики безопасности:

```yaml
---
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: mypolicy
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: true
    allowHostNetwork: true
    allowHostPID: false
    allowPrivileged: false
    allowPrivilegeEscalation: false
    allowedFlexVolumes:
    - driver: vmware
    allowedHostPorts:
    - max: 4000
      min: 2000
    allowedProcMount: Unmasked
    allowedAppArmor:
    - unconfined
    allowedUnsafeSysctls:
    - kernel.*
    allowedVolumes:
    - hostPath
    - projected
    fsGroup:
      ranges:
      - max: 200
        min: 100
      rule: MustRunAs
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
    - ALL
    runAsGroup:
      ranges:
      - max: 500
        min: 300
      rule: RunAsAny
    runAsUser:
      ranges:
      - max: 200
        min: 100
      rule: MustRunAs
    seccompProfiles:
      allowedLocalhostFiles:
      - my_profile.json
      allowedProfiles:
      - Localhost
    supplementalGroups:
      ranges:
      - max: 133
        min: 129
      rule: MustRunAs
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          enforce: mypolicy
```

Для применения приведенной политики достаточно навесить лейбл `enforce: "mypolicy"` на желаемое пространство имён.

> **Важно**. Параметры `allowPrivilegeEscalation` и `allowPrivileged` по умолчанию имеют значение `false` — даже если не указаны явно. Это означает, что контейнеры не смогут запускаться в привилегированном режиме или повышать привилегии. Чтобы разрешить такое поведение, задайте параметр в `true`.

##### Изменение ресурсов Kubernetes

Модуль позволяет использовать [кастомные ресурсы Gatekeeper](gatekeeper-cr.html) для модификации объектов в кластере, такие как:
- [AssignMetadata](gatekeeper-cr.html#assignmetadata) — для изменения секции `metadata` в ресурсе;
- [Assign](gatekeeper-cr.html#assign) — для изменения других полей, кроме `metadata`;
- [ModifySet](gatekeeper-cr.html#modifyset) — для добавления или удаления значений из списка, например аргументов для запуска контейнера.
- [AssignImage](gatekeeper-cr.html#assignimage) — для изменения параметра `image` ресурса.

### Модуль cert-manager

Устанавливает надежную и высокодоступную инсталляцию cert-manager release v1.17.1.

При установке модуля автоматически учитываются особенности кластера:

- компонент (webhook), к которому обращается `kube-apiserver`, устанавливается на master-узлы;
- в случае недоступности вебхука производится временное удаление `apiservice`, чтобы недоступность *cert-manager* не блокировала работу кластера.

Обновление самого модуля происходит в автоматическом режиме, в том числе с миграцией ресурсов cert-manager.

#### Возможности модуля cert-manager (с учетом внесенных изменений)

Модуль обеспечивает использование всех возможностей оригинального cert-manager, в том числе:

- заказ сертификатов во всех поддерживаемых источниках, таких как *Let’s Encrypt*, *HashiCorp Vault*, *Venafi*;
- выпуск самоподписанных сертификатов;
- поддержку актуальности сертификатов, автоматический перевыпуск и т. д.

Изменения в оригинальный cert-manager были внесены, чтобы поды `cm-acme-http-solver` могли выполняться на master-узлах и выделенных узлах.

#### Мониторинг

Модуль обеспечивает экспорт метрик в Prometheus для мониторинга:

- срока действия сертификатов;
- корректности перевыпуска сертификатов.

#### Роли доступа к ресурсам

В модуле предопределены несколько продуманных ролей для удобного доступа к ресурсам:

- `User` – доступ на чтение к ресурсам Certificate и Issuer в доступных ему namespace, а также к глобальным ClusterIssue;
- `Editor` – управление ресурсами Certificate и Issuer в доступных ему namespace;
- `ClusterEditor` – управление ресурсами Certificate и Issuer в любых namespace;
- `SuperAdmin` – управление внутренними служебными объектами.

### Модуль chrony

Обеспечивает синхронизацию времени на всех узлах кластера с помощью утилиты chrony.

#### Как работает

Модуль запускает `chrony` агенты на всех узлах кластера.
По умолчанию используется NTP сервер `pool.ntp.org`. NTP сервер можно изменить через [настройки](/modules/chrony/configuration.html) модуля.
Для просмотра используемых NTP серверов можно воспользоваться командой:

```bash
d8 k exec -it -n d8-chrony chrony-master-r7v6c -- chronyc -N sources
Defaulted container "chrony" out of: chrony, chrony-exporter, kube-rbac-proxy
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^* pool.ntp.org.                 2  10   377   171   -502us[ -909us] +/- 5388us
^- pool.ntp.org.                 2  10   377   666  -5317us[-5698us] +/-  103ms
^+ pool.ntp.org.                 2  10   377   938   -201us[ -567us] +/- 5346us
^+ pool.ntp.org.                 2  10   377   843   -159us[ -530us] +/-   12ms
```

`^+` - комбинируемый NTP сервер(`chrony` комбинирует информацию из `combined` серверов для уменьшения неточностей);  
`^*` - текущий NTP сервер;  
`^-` - некомбинируемый NTP сервер.

`chrony` агенты на мастер узлах и на остальных узлах имеют одно главное отличие - на всех узлах, которые не являются мастерами, в списке NTP серверов находятся не только NTP сервера из `module config`, но и адреса всех мастер узлов кластера.  

Таким образом, агенты на мастер узлах синхронизируют время только из списка хостов, указанных в `module config`(по умолчанию с `pool.ntp.org`). А агенты на остальных узлах синхронизируют время со списком NTP серверов из `module config` плюс с `chrony` агентов на мастер узлах.  

Это сделано для того, чтобы в случае недоступности NTP серверов, указанных в `module config`, время синхронизировалось с мастер узлами.

### Модуль cilium-hubble

Модуль `cilium-hubble` обеспечивает визуализацию сетевого стека кластера, если включен Cilium CNI.

#### Требования

Для работы модуля `cilium-hubble` необходимы:

- Версия ядра Linux >= 5.8 с поддержкой eBPF.
- Поддержка формата метаданных BPF Type Format (BTF). Проверить можно следующими способами:
  - выполнить команду `ls -lah /sys/kernel/btf/vmlinux` — наличие файла подтверждает поддержку BTF;
  - выполнить команду `grep -E "CONFIG_DEBUG_INFO_BTF=(y|m)" /boot/config-*` — если параметр включён, BTF поддерживается.

### Cloud provider — DVP

Взаимодействие с облачными ресурсами провайдера DVP осуществляется с помощью модуля `cloud-provider-dvp`. Он позволяет [модулю управления узлами `node-manager`](/modules/node-manager/) задействовать ресурсы DVP при создании узлов для [заданной группы узлов](/modules/node-manager/cr.html#nodegroup).

Основные возможности модуля `cloud-provider-dvp`:

- управление ресурсами DVP через модуль `cloud-controller-manager`;
- заказ дисков с использованием компонента `CSI storage`;
- интеграция с [модулем `node-manager`](/modules/node-manager/) для поддержки [DVPInstanceClass](cr.html#dvpinstanceclass) при описании [NodeGroup](/modules/node-manager/cr.html#nodegroup).

### Модуль cni-cilium

Модуль `cni-cilium` обеспечивает работу сети в кластере. Основан на проекте Cilium.

#### Ограничения

1. Сервисам с типом `NodePort` и `LoadBalancer` не подходят эндпоинты в LB-режиме `DSR`, работающие с hostNetwork. Если это необходимо, переключитесь на режим `SNAT`.
1. Поды `HostPort` связываются только с одним IP-адресом. Если в ОС есть несколько интерфейсов/IP, Cilium выберет один, предпочитая «серые» «белым».
1. Для обеспечения стабильной работы `cni-cilium` на узлах кластера отключите Elastic Agent или ограничьте доступ этого агента к серверу управления Elastic. В состав Elastic Agent входит компонент Elastic Endpoint, который использует технологию Extended Berkeley Packet Filter (eBPF) на узлах кластера и может удалять критически важные eBPF-программы, необходимые для корректной работы `cni-cilium`. Детальная информация и обсуждение проблемы доступны в публикациях проектов Cilium.
1. Требования к ядру:
   * ядро Linux версии не ниже `5.8` для работы модуля `cni-cilium` и его совместной работы с модулями [istio](./istio/), [openvpn](/modules/openvpn/), [node-local-dns]({% if site.d8Revision == 'CE' %}{{ site.urls.ru}}/modules/{% else %}..{% endif %}/node-local-dns/).
1. Совместимость с ОС:
   * Ubuntu:
     * несовместим с версией 18.04;
     * для работы с версией 20.04 необходима установка ядра HWE.
   * Astra Linux:
     * несовместим с изданием «Смоленск».

#### Обработка внешнего трафика в разных режимах работы `bpfLB` (замена kube-proxy от Cilium)

В Kubernetes обычно используются схемы, где трафик приходит на балансировщик, который распределяет его между многими серверами. Через балансировщик проходит и входящий, и исходящий трафик. Таким образом, общая пропускная способность ограничена ресурсами и шириной канала балансировщика. Для оптимизации трафика и разгрузки балансировщика и был придуман механизм `DSR`, в котором входящие пакеты проходят через балансировщик, а исходящие идут напрямую с терминирующих серверов. Так как обычно ответы имеют много больший размер чем запросы, то такой подход позволяет значительно увеличить общую пропускную способность схемы.

В модуле возможен [выбор режима работы](configuration.html#parameters-bpflbmode), влияющий на поведение `Service` с типом `NodePort` и `LoadBalancer`:

* `SNAT` (Source Network Address Translation) — один из подвидов NAT, при котором для каждого исходящего пакета происходит трансляция IP-адреса источника в IP-адрес шлюза из целевой подсети, а входящие пакеты, проходящие через шлюз, транслируются обратно на основе таблицы трансляций. В этом режиме `bpfLB` полностью повторяет логику работы `kube-proxy`:
  * если в `Service` указан `externalTrafficPolicy: Local`, то трафик будет передаваться и балансироваться только в те целевые поды, которые запущены на том же узле, на который этот трафик пришел. Если целевой под не запущен на этом узле, то трафик будет отброшен.
  * если в `Service` указан `externalTrafficPolicy: Cluster`, то трафик будет передаваться и балансироваться во все целевые поды в кластере. При этом, если целевые поды находятся на других узлах, то при передаче трафика на них будет произведен SNAT (IP-адрес источника будет заменен на InternalIP узла).

   ![Схема потоков данных SNAT](images/snat.png)

* `DSR` - (Direct Server Return) — метод, при котором весь входящий трафик проходит через балансировщик нагрузки, а весь исходящий трафик обходит его. Такой метод используется вместо `SNAT`. Часто ответы имеют много больший размер чем запросы и `DSR` позволяет значительно увеличить общую пропускную способность схемы:
  * если в `Service` указан `externalTrafficPolicy: Local`, то поведение абсолютно аналогично `kube-proxy` и `bpfLB` в режиме `SNAT`.
  * если в `Service` указан `externalTrafficPolicy: Cluster`, то трафик так же будет передаваться и балансироваться во все целевые поды в кластере.
  При этом важно учитывать следующие особенности:
    * если целевые поды находятся на других узлах, то при передаче на них входящего трафика будет сохранен IP-адрес источника;
    * исходящий трафик пойдет прямо с узла, на котором был запущен целевой под;
    * IP-адрес источника будет заменен на внешний IP-адрес узла, на которую изначально пришел входящий запрос.

   ![Схема потоков данных DSR](images/dsr.png)

{% alert level="warning" %}
В случае использования режима `DSR` и `Service` с `externalTrafficPolicy: Cluster` требуются дополнительные настройки сетевого окружения.
Сетевое оборудование должно быть готово к ассиметричному прохождению трафика: отключены или настроены соответствующим образом средства фильтрации IP адресов на входе в сеть (`uRPF`, `sourceGuard` и т.п.).
{% endalert %}

* `Hybrid` — в данном режиме TCP-трафик обрабатывается в режиме `DSR`, а UDP — в режиме `SNAT`.

#### Использование CiliumClusterwideNetworkPolicies

{% alert level="danger" %}
Использование CiliumClusterwideNetworkPolicies при отсутствии опции `policyAuditMode` в настройках модуля cni-cilium может привести к некорректной работе Control plane или потере доступа ко всем узлам кластера по SSH.
{% endalert %}

Для использования CiliumClusterwideNetworkPolicies выполните следующие шаги:

1. Примените первичный набор объектов `CiliumClusterwideNetworkPolicy`. Для этого в настройки модуля cni-cilium добавьте конфигурационную опцию [`policyAuditMode`](/modules/cni-cilium/configuration.html#parameters-policyauditmode) со значением `true`.
Опция `policyAuditMode` может быть удалена после применения всех `CniliumClusterwideNetworkPolicy`-объектов и проверки корректности их работы в Hubble UI.

1. Примените правило политики сетевой безопасности:

   ```yaml
   apiVersion: "cilium.io/v2"
   kind: CiliumClusterwideNetworkPolicy
   metadata:
     name: "allow-control-plane-connectivity"
   spec:
     ingress:
     - fromEntities:
       - kube-apiserver
     nodeSelector:
       matchLabels:
         node-role.kubernetes.io/control-plane: ""
   ```

В случае, если CiliumClusterwideNetworkPolicies не будут использованы, Control plane может некорректно работать до одной минуты во время перезагрузки `cilium-agent`-подов. Это происходит из-за сброса Conntrack-таблицы. Привязка к entity `kube-apiserver` позволяет избежать проблемы.

#### Смена режима работы Cilium

При смене режима работы Cilium (параметр [tunnelMode](configuration.html#parameters-tunnelmode)) c `Disabled` на `VXLAN` или обратно, необходимо перезагрузить все узлы, иначе возможны проблемы с доступностью подов.

#### Выключение модуля kube-proxy

Cilium полностью заменяет собой функционал модуля `kube-proxy`, поэтому `kube-proxy` автоматически отключается при включении модуля `cni-cilium`.

#### Использование выборочного алгоритма балансировки нагрузки для сервисов

В Deckhouse Platform Certified Security Edition для балансировки нагрузки трафика сервисов можно применять следующие алгоритмы:

* `Random` — случайный выбор бэкенда для каждого соединения. Прост в реализации, но не всегда обеспечивает равномерное распределение.
* `Maglev` — использует консистентное хеширование для равномерного распределения трафика, подходит для масштабных сервисов с множеством бэкендов, которые часто ротируются.
* `Least Connections` — направляет трафик на бэкенд с наименьшим количеством активных соединений, оптимизируя нагрузку для приложений с длительными соединениями.

По умолчанию для всех сервисов задан алгоритм балансировки **Random**. Однако Deckhouse позволяет переопределять алгоритм для отдельных сервисов. Чтобы использовать выборочный алгоритм балансировки для конкретного сервиса, выполните следующие шаги:

* Отредактируйте конфигурацию модуля `cni-cilium` в Deckhouse, включив параметр [`extraLoadBalancerAlgorithmsEnabled`](configuration.html#parameters-extralbalgorithmsenabled). Это активирует поддержку аннотаций сервисов для выборочных алгоритмов.
* В манифесте сервиса укажите аннотацию `service.cilium.io/lb-algorithm` с одним из значений: `random`, `maglev` или `least-conn`.

{% alert level="warning" %}
Для корректной работы данного механизма требуется версия ядра Linux 5.15 и выше.
{% endalert %}

#### Использование Egress Gateway

{% alert level="warning" %}Доступно в следующих редакциях Deckhouse Platform Certified Security Edition: SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

Egress Gateway в Deckhouse Platform Certified Security Edition может быть использован в одном из двух режимов: [Базовый](#базовый-режим) и [Режим с Virtual IP](#режим-с-virtual-ip). Для выбора режима используйте ресурс [EgressGateway](cr.html#egressgateway) (параметр `spec.sourceIP.node`).

##### Базовый режим

Используются предварительно настроенные IP-адреса на egress-узлах.

<div data-presentation="./presentations/cni-cilium/egressgateway_base_ru.pdf"></div>


##### Режим с Virtual IP

Реализована возможность динамически назначать дополнительные IP-адреса узлам.

<div data-presentation="./presentations/cni-cilium/egressgateway_virtualip_ru.pdf"></div>

### Модуль cni-flannel

Обеспечивает работу сети в кластере с помощью модуля flannel.

### Модуль cni-simple-bridge

Модуль не имеет настроек.

### Модуль commander-agent

Модуль является служебным компонентом для обеспечения корректной работы модуля [Коммандер](/modules/commander/)

### Веб-интерфейс

Веб-интерфейс упрощает управление Deckhouse Platform Certified Security Edition и делает состояние системы наглядным.

Доступ к интерфейсу доступен всем пользователям согласно их правам в платформе.

Если шаблон публичных доменов `%s.example.com`, то в веб-приложение можно зайти по адресу
`https://console.example.com`.

![Обзор](/images/console/screenshot-sys-overview.ru.png)

#### Основные возможности

- Обзор кластера, актуальной версии, состояния системы и обновлений
- Управление модулями и их настройками
- Управление узлами: конфигурация узлов, масштабирование, параметры обновления
- Управление тенантами: проекты, созданные на основании шаблонов
- Управление доступом: провайдеры аутентификации, права групп и пользователей
- Ингресс-контроллеры: заведение трафика в кластер
- Журналирование: сбор логов с узлов и подов, отправка в различные типы хранилищ
- Мониторинг: обработка и отправка метрик, создание алертов и recording rule, дашборды и источники данных для Grafana, настройки Prometheus и список горящих алертов
- Поддержка GitOps: специально отмечены ресурсы Kubernetes, созданные автоматикой (werf, Argo CD, Helm)
- Метрики и мониторинг в узлах, группах узлов и в ингресс-контроллерах
- Состояние подов Prometheus, ингресс-контроллеров и поды на узлах
- И многое другое!

#### Как включить

Чтобы включить модуль, создайте ModuleConfig:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: console
spec:
  enabled: true
```

#### Требования к ресурсам

Потребление ресурсов подами серверной части в зависимости от количества одновременных пользователей
отображено в таблице ниже

| Пользователей | ЦП, ядра | Память, МиБ |
| ------------: | -------: | ----------: |
|             0 |   0.0005 |          18 |
|             1 |   0.0500 |          25 |
|            10 |   0.4000 |          53 |
|           100 |   0.6500 |         130 |

Ограничение на вертикальное масштабирование подов: минимальные значения CPU/памяти в 100m/100MiB и максимальные значения в 1/512MiB.
Две реплики серверной части включаются автоматически для Deckhouse Platform Certified Security Edition в режиме высокой доступности.

### Управление control plane

Управление компонентами control plane кластера осуществляется с помощью модуля `control-plane-manager`, который запускается на всех master-узлах кластера (узлы с лейблом `node-role.kubernetes.io/control-plane: ""`).

Функции управления control plane:

- **Управление сертификатами**, необходимыми для работы control-plane, в том числе продление, выпуск при изменении конфигурации и т. п. Позволяет автоматически поддерживать безопасную конфигурацию control plane и быстро добавлять дополнительные SAN для организации защищенного доступа к API Kubernetes.
- **Настройка компонентов**. Автоматически создает необходимые конфигурации и манифесты компонентов `control-plane`.
- **Upgrade/downgrade компонентов**. Поддерживает в кластере одинаковые версии компонентов.
- **Управление конфигурацией etcd-кластера** и его членов. Масштабирует master-узлы, выполняет миграцию из single-master в multi-master и обратно.
- **Настройка kubeconfig**. Обеспечивает всегда актуальную конфигурацию для работы kubectl. Генерирует, продлевает, обновляет kubeconfig с правами cluster-admin и создает symlink пользователю root, чтобы kubeconfig использовался по умолчанию.
- **Расширение работы планировщика**, за счет подключения внешних плагинов через вебхуки. Управляется ресурсом [KubeSchedulerWebhookConfiguration](cr.html#kubeschedulerwebhookconfiguration). Позволяет использовать более сложную логику при решении задач планирования нагрузки в кластере. Например:
  - размещение подов приложений организации хранилища данных ближе к самим данным,
  - приоритизация узлов в зависимости от их состояния (сетевой нагрузки, состояния подсистемы хранения и т. д.),
  - разделение узлов на зоны, и т. п.

#### Управление сертификатами

Управляет SSL-сертификатами компонентов `control-plane`:

- Серверными сертификатами для `kube-apiserver` и `etcd`. Они хранятся в Secret'е `d8-pki` пространства имен `kube-system`:
  - корневой CA kubernetes (`ca.crt` и `ca.key`);
  - корневой CA etcd (`etcd/ca.crt` и `etcd/ca.key`);
  - RSA-сертификат и ключ для подписи Service Account'ов (`sa.pub` и `sa.key`);
  - корневой CA для extension API-серверов (`front-proxy-ca.key` и `front-proxy-ca.crt`).
- Клиентскими сертификатами для подключения компонентов `control-plane` друг к другу. Выписывает, продлевает и перевыписывает, если что-то изменилось (например, список SAN). Следующие сертификаты хранятся только на узлах:
  - серверный сертификат API-сервера (`apiserver.crt` и `apiserver.key`);
  - клиентский сертификат для подключения `kube-apiserver` к `kubelet` (`apiserver-kubelet-client.crt` и `apiserver-kubelet-client.key`);
  - клиентский сертификат для подключения `kube-apiserver` к `etcd` (`apiserver-etcd-client.crt` и `apiserver-etcd-client.key`);
  - клиентский сертификат для подключения `kube-apiserver` к extension API-серверам (`front-proxy-client.crt` и `front-proxy-client.key`);
  - серверный сертификат `etcd` (`etcd/server.crt` и `etcd/server.key`);
  - клиентский сертификат для подключения `etcd` к другим членам кластера (`etcd/peer.crt` и `etcd/peer.key`);
  - клиентский сертификат для подключения `kubelet` к `etcd` для helthcheck'ов (`etcd/healthcheck-client.crt` и `etcd/healthcheck-client.key`).

Также позволяет добавить дополнительные SAN в сертификаты, это дает возможность быстро и просто добавлять дополнительные «точки входа» в API Kubernetes.

При изменении сертификатов также автоматически обновляется соответствующая конфигурация kubeconfig.

#### Масштабирование

Поддерживается работа `control-plane` в конфигурации как *single-master*, так и *multi-master*.

В конфигурации *single-master*:

- `kube-apiserver` использует только тот экземпляр `etcd`, который размещен с ним на одном узле;
- На узле настраивается прокси-сервер, отвечающий на localhost,`kube-apiserver` отвечает на IP-адрес master-узла.

В конфигурации *multi-master* компоненты `control-plane` автоматически разворачиваются в отказоустойчивом режиме:

- `kube-apiserver` настраивается для работы со всеми экземплярами `etcd`.
- На каждом master-узле настраивается дополнительный прокси-сервер, отвечающий на localhost. Прокси-сервер по умолчанию обращается к локальному экземпляру `kube-apiserver`, но в случае его недоступности последовательно опрашивает остальные экземпляры `kube-apiserver`.

##### Масштабирование master-узлов

Масштабирование узлов `control-plane` осуществляется автоматически, с помощью лейбла `node-role.kubernetes.io/control-plane=""`:

- Установка лейбла `node-role.kubernetes.io/control-plane=""` на узле приводит к развертыванию на нем компонентов `control-plane`, подключению нового узла `etcd` в etcd-кластер, а также перегенерации необходимых сертификатов и конфигурационных файлов.
- Удаление лейбла `node-role.kubernetes.io/control-plane=""` с узла приводит к удалению всех компонентов `control-plane`, перегенерации необходимых конфигурационных файлов и сертификатов, а также корректному исключению узла из etcd-кластера.

{% alert level="warning" %}
При масштабировании узлов с 2 до 1 требуются [ручные действия](./faq.html#что-делать-если-кластер-etcd-развалился) с `etcd`. В остальных случаях все необходимые действия происходят автоматически. Обратите внимание, что при масштабировании с любого количества master-узлов до 1 рано или поздно на последнем шаге возникнет ситуация масштабирования узлов с 2 до 1.
{% endalert %}

#### Управление версиями

Обновление **patch-версии** компонентов control plane (то есть в рамках минорной версии, например с `1.29.13` на `1.29.14`) происходит автоматически вместе с обновлением версии Deckhouse. Управлять обновлением patch-версий нельзя.

Обновлением **минорной-версии** компонентов control plane (например, с `1.29.*` на `1.30.*`) можно управлять с помощью параметра [kubernetesVersion](/reference/api/cr.html#clusterconfiguration-kubernetesversion), в котором можно выбрать автоматический режим обновления (значение `Automatic`) или указать желаемую минорную версию control plane. Версию control plane, которая используется по умолчанию (при `kubernetesVersion: Automatic`).

Обновление control plane выполняется безопасно и для single-master-, и для multi-master-кластеров. Во время обновления может быть кратковременная недоступность API-сервера. На работу приложений в кластере обновление не влияет и может выполняться без выделения окна для регламентных работ.

Если указанная для обновления версия (с параметром [kubernetesVersion](/reference/api/cr.html#clusterconfiguration-kubernetesversion)) не соответствует текущей версии control plane в кластере, запускается умная стратегия изменения версий компонентов:

- Общие замечания:
  - Обновление в разных NodeGroup выполняется параллельно. Внутри каждой NogeGroup узлы обновляются последовательно, по одному.
- При upgrade:
  - Обновление происходит **последовательными этапами**, по одной минорной версии: 1.29 -> 1.30, 1.30 -> 1.31, 1.31 -> 1.32.
  - На каждом этапе сначала обновляется версия control plane, затем происходит обновление kubelet на узлах кластера.  
- При downgrade:
  - Успешное понижение версии гарантируется только на одну версию вниз от максимальной минорной версии control plane, когда-либо использовавшейся в кластере.
  - Сначала на узлах кластера выполняется понижение версии kubelet, после чего производится понижение версии компонентов control plane.

#### Аудит

Если требуется журналировать операции с API или отдебажить неожиданное поведение, для этого в Kubernetes предусмотрен Auditing. Его можно настроить путем создания правил Audit Policy, а результатом работы аудита будет лог-файл `/var/log/kube-audit/audit.log` со всеми интересующими операциями.

В установках Deckhouse по умолчанию созданы базовые политики, которые отвечают за логирование событий, которые:

- связаны с операциями создания, удаления и изменения ресурсов;
- совершаются от имен сервисных аккаунтов из системных Namespace `kube-system`, `d8-*`;
- совершаются с ресурсами в системных пространствах имен `kube-system`, `d8-*`.

Для выключения базовых политик установите флаг [basicAuditPolicyEnabled](configuration.html#parameters-apiserver-basicauditpolicyenabled) в `false`.

При настройке OIDC-аутентификации в аудит-логах дополнительно включается информация о пользователе в поле `user.extra`:
- `user-authn.deckhouse.io/name` — отображаемое имя пользователя
- `user-authn.deckhouse.io/preferred_username` — предпочитаемое имя пользователя
- `user-authn.deckhouse.io/dex-provider` — идентификатор провайдера Dex (требует scope `federated:id`)

Настройка политик аудита подробнее рассмотрена в [одноименной секции FAQ](faq.html#как-настроить-дополнительные-политики-аудита).

### Модуль csi-ceph

{% alert level="warning" %}
При переключении на данный модуль с модуля ceph-csi производится автоматическая миграция, но ее запуск требует подготовки:
1. Необходимо сделать scale всех операторов (redis, clickhouse, kafka и т.д) в ноль реплик, в момент миграции операторы в кластере работать не должны. Единственное исключение - оператор prometheus в составе Deckhouse, в процессе миграции его отключит автоматически
2. Выключить модуль ceph-csi и включить модуль csi-ceph
3. В логах Deckhouse дождаться окончания процесса миграции (Finished migration from Ceph CSI module)
4. Создать тестовые pod/pvc для проверки работоспособности CSI
5. Вернуть операторы в работоспособное состояние
При наличии Ceph StorageClass, созданного не с помощью ресурса CephCSIDriver потребуется ручная миграция.
В этом случае необходимо связаться с техподдержкой.
{% endalert %}

{% alert level="info" %}
Для работы с снапшотами требуется подключенный модуль [snapshot-controller](./snapshot-controller/).
{% endalert %}

Ceph — это масштабируемая распределённая система хранения, обеспечивающая высокую доступность и отказоустойчивость данных. В Deckhouse поддерживается интеграция с Ceph-кластерами, что позволяет динамически управлять хранилищем и использовать StorageClass на основе RBD (RADOS Block Device) или CephFS.

На этой странице представлены инструкции по подключению Ceph в Deckhouse, настройке аутентификации, созданию объектов StorageClass, а также проверке работоспособности хранилища.

#### Включение модуля

Для подключения Ceph-кластера в Deckhouse необходимо включить модуль `csi-ceph`. Для этого примените ресурс ModuleConfig:

```yaml
d8 k apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-ceph
spec:
  enabled: true
EOF
```

#### Подключение к Ceph-кластеру

Чтобы настроить подключение к Ceph-кластеру, необходимо применить ресурс [CephClusterConnection](cr.html#cephclusterconnection). Пример команды:

```yaml
d8 k apply -f - <<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephClusterConnection
metadata:
  name: ceph-cluster-1
spec:
  # FSID/UUID Ceph-кластера.
  # Получить FSID/UUID Ceph-кластера можно с помощью команды `ceph fsid`.
  clusterID: 2bf085fc-5119-404f-bb19-820ca6a1b07e
  # Список IP-адресов ceph-mon’ов в формате 10.0.0.10:6789.
  monitors:
    - 10.0.0.10:6789
  # Имя пользователя без `client.`.
  # Получить имя пользователя можно с помощью команды `ceph auth list`.
  userID: admin
  # Ключ авторизации, соответствующий userID.
  # Получить ключ авторизации можно с помощью команды `ceph auth get-key client.admin`.
  userKey: AQDiVXVmBJVRLxAAg65PhODrtwbwSWrjJwssUg==
EOF
```

Проверить создание подключения можно командой (фаза должна быть в статусе `Created`):

```shell
d8 k get cephclusterconnection ceph-cluster-1
```

#### Создание StorageClass

Создание объектов StorageClass осуществляется через ресурс [CephStorageClass](cr.html#cephstorageclass), который определяет конфигурацию для желаемого класса хранения. Ручное создание ресурса StorageClass без [CephStorageClass](cr.html#cephstorageclass) может привести к ошибкам. Пример создания StorageClass на основе RBD (RADOS Block Device):

```yaml
d8 k apply -f - <<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-rbd-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: RBD
  rbd:
    defaultFSType: ext4
    pool: ceph-rbd-pool
EOF
```

Пример создания StorageClass на основе файловой системы Ceph:

```yaml
d8 k apply -f - <<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-fs-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: CephFS
  cephFS:
    fsName: cephfs
EOF
```

Проверьте, что созданные ресурсы [CephStorageClass](cr.html#cephstorageclass) перешли в состояние `Created`, выполнив следующую команду:

```shell
d8 k get cephstorageclass
```

В результате будет выведена информация о созданных ресурсах [CephStorageClass](cr.html#cephstorageclass):

```console
NAME          PHASE     AGE
ceph-rbd-sc   Created   1h
ceph-fs-sc    Created   1h
```

Проверьте созданный StorageClass с помощью следующей команды:

```shell
d8 k get sc
```

В результате будет выведена информация о созданном StorageClass:

```console
NAME          PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ceph-rbd-sc   rbd.csi.ceph.com   Delete          WaitForFirstConsumer   true                   15s
ceph-fs-sc    rbd.csi.ceph.com   Delete          WaitForFirstConsumer   true                   15s
```

Если объекты StorageClass появились, значит настройка модуля `csi-ceph` завершена. Теперь пользователи могут создавать PersistentVolume, указывая созданные объекты StorageClass.

### Модуль csi-nfs

Модуль предоставляет CSI для управления NFS-томами и позволяет создавать StorageClass в Kubernetes через [пользовательские ресурсы Kubernetes](./cr.html#nfsstorageclass) `NFSStorageClass`.

{% alert level="warning" %}
**Предостережение про использовании снапшотов (Volume Snapshots)**

При создании снапшотов NFS-томов важно понимать схему их создания и связанные ограничения. Мы рекомендуем по возможности избегать использования snapshots в csi-nfs:

1. CSI-драйвер создает снапшот на уровне NFS-сервера.
2. Для этого используется tar, которой упаковывается содержимое тома, со всеми ограничениями, могущими возникнуть из-за этого
3. **Перед созданием снапшота обязательно остановите рабочую нагрузку** (pods), использующую NFS-том
4. NFS не обеспечивает атомарность операций на уровне файловой системы при создании снапшота

{% endalert %}

{% alert level="info" %}
Для работы с снапшотами требуется подключенный модуль [snapshot-controller](/modules/snapshot-controller/).
{% endalert %}

{% alert level="info" %}
Создание StorageClass для CSI-драйвера `nfs.csi.k8s.io` пользователем запрещено.
{% endalert %}

#### Системные требования и рекомендации

##### Требования

- Используйте стоковые ядра, поставляемые вместе с поддерживаемыми дистрибутивами;
- Убедитесь в наличии развернутого и настроенного NFS-сервера;
- Для поддержки RPC-with-TLS включите в ядре Linux опции `CONFIG_TLS` и `CONFIG_NET_HANDSHAKE`.

##### Рекомендации

Чтобы поды модуля перезапускались при изменении параметра `tlsParameters` в настройках модуля, должен быть включен модуль [pod-reloader](/modules/pod-reloader) (включен по умолчанию).

#### Ограничения режима RPC-with-TLS

- Для политики безопасности `mtls` поддерживается только один сертификат клиента.
- Один NFS-сервер не может одновременно работать в разных режимах безопасности: `tls`, `mtls` и стандартный режим (без TLS).
- На узлах кластера не должен быть запущен демон `tlshd`, иначе он будет конфликтовать с демоном нашего модуля. Для предотвращения конфликтов при включении TLS на узлах автоматически останавливается сторонний `tlshd` и отключается его автозапуск.

#### Быстрый старт

Все команды следует выполнять на машине, имеющей доступ к API Kubernetes с правами администратора.

##### Включение модуля

1. Включите модуль `csi-nfs`.  Это приведет к тому, что на всех узлах кластера будет:
   - Зарегистрирован CSI драйвер;
   - Запущены служебные поды компонентов `csi-nfs`.

   ```yaml
   kubectl apply -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: csi-nfs
   spec:
     enabled: true
     version: 1
   EOF
   ```

2. Дождитесь, когда модуль перейдет в состояние `Ready`:

   ```shell
   kubectl get module csi-nfs -w
   ```

##### Создание StorageClass

Для создания StorageClass необходимо использовать ресурс [NFSStorageClass](./cr.html#nfsstorageclass). Пример создания ресурса:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: NFSStorageClass
metadata:
  name: nfs-storage-class
spec:
  connection:
    host: 10.223.187.3
    share: /
    nfsVersion: "4.1"
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
EOF
```

Для каждого PV будет создаваться каталог `<директория из share>/<имя PV>`.

##### Проверка работоспособности модуля

Процесс проверки работоспособности модуля описан в разделе FAQ [Как проверить работоспособность модуля](./faq.html#как-проверить-работоспособность-модуля)

##### Выбор метода очистки тома перед удалением PV

На удаляемом томе могут остаться файлы с пользовательскими данными. Эти файлы будут удалены и не будут доступны другим пользователям через NFS.

Однако данные удаленных файлов могут оказаться доступными другим клиентам, если сервер предоставит доступ к своему хранилищу на уровне блочных устройств.

Выбрать метод очистки тома перед удалением поможет параметр `volumeCleanup`.

> **Внимание.** Эта опция не влияет на файлы, уже удаленные клиентским приложением.

> **Внимание.** Эта опция влияет только на команды отправляемые по протоколу NFS. Проведение этих команд на стороне сервера определено:
>
> - сервисом NFS сервера;
> - файловой системой;
> - уровнем блочных устройств и их виртуализации (например LVM);
> - самими физическими устройствами.
>
> Убедитесь в доверенности сервера. Не отправляйте деликатные данные на сервера, в которых нет уверенности.

###### Метод `SinglePass`

Используется, если для параметра `volumeCleanup` задано значение `RandomFillSinglePass`.

Содержимое файлов переписывается случайной последовательностью перед удалением. Случайная последовательность передается по сети.

###### Метод `ThreePass`

Используется, если для параметра `volumeCleanup` задано значение `RandomFillThreePass`.

Содержимое файлов трижды переписывается случайной последовательностью перед удалением. Три случайных последовательности передаются по сети. 
<!-- Имеет смысл только если сервер хранит данные на жестком диске, и есть риск, что у злоумышленника появится физический доступ к устройству. -->

###### Метод `Discard`

Используется, если для параметра `volumeCleanup` задано значение `Discard`.

Многие файловые системы реализуют поддержку твердотельных накопителей, позволяя освободить место, занятое файлом, на блочном уровне без записи новых данных для увеличения срока службы твердотельного накопителя. Однако не все накопители гарантируют недоступность данных освобожденных блоков.

Если для `volumeCleanup` установлено значение `Discard`, содержимое файлов помечается как свободное через системный вызов `falloc` с флагом `FALLOC_FL_PUNCH_HOLE`. Файловая система освободит полностью используемые файлом блоки, через вызов `blkdiscard`, а остальное место будет перезаписано нулями.

Преимущества этого метода:

- объем трафика не зависит от размера файлов, а только от их количества;
- метод может обеспечить недоступность старых данных при некоторых конфигурациях сервера;
- работает как для жестких дисков, так и для твердотельных накопителей;
- позволяет увеличить время жизни твердотельного накопителя.

<!-- TODO: Может разделим на две или три (PunchHole, ZeroOut, PunchHoleOrZeroOut)? -->

### Модуль csi-scsi-generic

Модуль предоставляет CSI для управления томами c использованием СХД с подключением через SCSI.

На данный момент поддерживается:
  - обнаружение LUN через iSCSI
  - создание PV из заранее подготовленных LUN
  - удаление PV и обнуление данных на LUN
  - подключение LUN к узлам через iSCSI
  - создание multipath устройств и монтирование их в поды
  - отключение LUN от узлов

Не поддерживается:
  - создание LUN на СХД
  - изменение размера LUN
  - создание снимков

#### Системные требования и рекомендации

##### Требования

- Наличие развернутой и настроенной СХД с подключением через SCSI.
- Уникальные iqn в /etc/iscsi/initiatorname.iscsi на каждой из Kubernetes Nodes

#### Быстрый старт

Все команды следует выполнять на машине, имеющей доступ к API Kubernetes с правами администратора.

##### Включение модуля

- Включить модуль `csi-scsi-generic`.  Это приведет к тому, что на всех узлах кластера будет:
    - зарегистрирован CSI драйвер;
    - запущены служебные поды компонентов `csi-scsi-generic`.

```shell
kubectl apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-scsi-generic
spec:
  enabled: true
  version: 1
EOF
```

- Дождаться, когда модуль перейдет в состояние `Ready`.

```shell
kubectl get module csi-scsi-generic -w
```

##### Создание SCSITarget

Для создания SCSITarget необходимо использовать ресурс [SCSITarget](./cr.html#scsitarget). Пример команд для создания такого ресурса:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: SCSITarget
metadata:
  name: hpe-3par-1
spec:
  deviceTemplate:
    metadata:
      labels:
        my-key: some-label-value
  iSCSI:
    auth:
      login: ""
      password: ""
    iqn: iqn.2000-05.com.3pardata:xxxx1
    portals:
    - 192.168.1.1

---
apiVersion: storage.deckhouse.io/v1alpha1
kind: SCSITarget
metadata:
  name: hpe-3par-2
spec:
  deviceTemplate:
    metadata:
      labels:
        my-key: some-label-value
  iSCSI:
    auth:
      login: ""
      password: ""
    iqn: iqn.2000-05.com.3pardata:xxxx2
    portals:
    - 192.168.1.2
EOF

```

Обратите внимание, что в примере выше используются два SCSITarget. Таким образом можно создать несколько SCSITarget как для одного, так и для разных СХД. Это позволяет использовать multipath для повышения отказоустойчивости и производительности.

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get scsitargets.storage.deckhouse.io <имя scsitarget>
```

##### Создание StorageClass

Для создания StorageClass необходимо использовать ресурс [SCSIStorageClass](./cr.html#scsistorageclass). Пример команд для создания такого ресурса:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: SCSIStorageClass
metadata:
  name: scsi-all
spec:
  scsiDeviceSelector:
    matchLabels:
      my-key: some-label-value
  reclaimPolicy: Delete
EOF
```

Обратите внимание на `scsiDeviceSelector`. Этот параметр позволяет выбрать SCSITarget для создания PV по меткам. В примере выше выбираются все SCSITarget с меткой `my-key: some-label-value`. Эта метка будет выставлена на все девайсы, которые будут обнаружены в указанных SCSITarget.

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get scsistorageclasses.storage.deckhouse.io <имя scsistorageclass>
```

##### Отчистка PV 

Поскольку мы не управляем сервером **iSCSI Target** и повторно используем доступные тома из **iSCSI Target**, мы должны очистить их после удаления **PV**.
Изменение режима очистки **PV** происходит автоматически и зависит от поддержки **trim**.
Пошаговый процесс очистки:
1. Проверка поддержки **trim**. Поддержка **trim** проверяется чтением значения «discard_max_bytes» и `/sys/block/${device name}/queue/discard_max_bytes`. Если размер > 0, **trim** поддерживается.
2. Очистка:
    1. Если **trim** поддерживается, команда выглядит так: `blkdiscard ${device name}`
    2. Если **trim** не поддерживается, команда выглядит так: `blkdiscard -z ${device name}`, `-z` означает, что устройство будет обнулено (полностью заполнено нулями).

### Модуль csi-yadro-tatlin-unified

Модуль предоставляет CSI для управления томами c использованием СХД TATLIN.UNIFIED. Модуль позволяет создавать `StorageClass` в `Kubernetes` через создание [пользовательских ресурсов Kubernetes](./cr.html#yadrotatlinunifiedstorageclass) `YadroTatlinUnifiedStorageClass`.

> **Внимание!** Создание `StorageClass` для CSI-драйвера `csi-tatlinunified.yadro.com` пользователем запрещено.

{% alert level="info" %}
Для работы с снапшотами требуется подключенный модуль [snapshot-controller](/modules/snapshot-controller/).
{% endalert %}

#### Системные требования и рекомендации

##### Требования

- Наличие развернутой и настроенной СХД TATLIN.
- Уникальные iqn в /etc/iscsi/initiatorname.iscsi на каждой из Kubernetes Nodes

#### Быстрый старт

Все команды следует выполнять на машине, имеющей доступ к API Kubernetes с правами администратора.

##### Включение модуля

- Включить модуль `csi-yadro-tatlin-unified`.  Это приведет к тому, что на всех узлах кластера будет:
    - зарегистрирован CSI драйвер;
    - запущены служебные поды компонентов `csi-yadro-tatlin-unified`.

```shell
kubectl apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-yadro-tatlin-unified
spec:
  enabled: true
  version: 1
EOF
```

- Дождаться, когда модуль перейдет в состояние `Ready`.

```shell
kubectl get module csi-yadro-tatlin-unified -w
```

##### Создание StorageClass

Для создания StorageClass необходимо использовать ресурсы [YadroTatlinUnifiedStorageClass](./cr.html#yadrotatlinunifiedstorageclass) и [YadroTatlinUnifiedStorageConnection](./cr.html#yadrotatlinunifiedstorageconnection). Пример команд для создания таких ресурсов:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: YadroTatlinUnifiedStorageConnection
metadata:
  name: yad1
spec:
  controlPlane:
    address: "172.19.28.184"
    username: "admin"
    password: "cGFzc3dvcmQ=" # ДОЛЖЕН БЫТЬ ЗАКОДИРОВАН В BASE64
    ca: "base64encoded"
    skipCertificateValidation: true
  dataPlane:
    protocol: "iscsi"
    iscsi:
      volumeExportPort: "p50,p51,p60,p61"
EOF
```

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: YadroTatlinUnifiedStorageClass
metadata:
  name: yad1
spec:
  fsType: "xfs"
  pool: "pool-hdd"
  storageConnectionName: "yad1"
  reclaimPolicy: Delete
EOF
```

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get yadrotatlinunifiedstorageconnections.storage.deckhouse.io <имя yadrotatlinunifiedstorageconnection>
```

```shell
kubectl get yadrotatlinunifiedstorageclasses.storage.deckhouse.io <имя yadrotatlinunifiedstorageclass>
```

##### Проверка работоспособности модуля.

Проверить работоспособность модуля можно [так](./faq.html#как-проверить-работоспособность-модуля)

### Модуль dashboard

Устанавливает Web UI Kubernetes Dashboard для ручного управления кластером, который интегрирован с модулями [user-authn](/modules/user-authn/) и [user-authz](/modules/user-authz/) (доступ в кластер осуществляется от имени пользователя и с учетом его прав).

Kubernetes Dashboard предоставляет следующие возможности:

- управление подами и другими высокоуровневыми ресурсами;
- доступ к контейнерам через веб-консоль для отладки;
- просмотр логов отдельных контейнеров.

{% alert level="warning" %}
Модуль не поддерживает работу через HTTP.
{% endalert %}

Для работы модуля необходимо:

1. Включить модуль [user-authz](./user-authz/);
1. Включить модуль [user-authn](./user-authn/), либо подключить внешнюю аутентификацию (секция параметров [externalAuthentication](configuration.html#parameters-auth-externalauthentication) модуля).

### Модуль deckhouse-tools

Этот модуль создает веб-интерфейс со ссылками на скачивание утилиты [Deckhouse CLI]({% if site.mode != 'module' %}{{ site.canonical_url_prefix_documentation }}{% endif %}/cli/d8/) под различные операционные системы.

Адрес веб-интерфейса формируется в соответствии с шаблоном [publicDomainTemplate](/reference/api/global.html#parameters-modules-publicdomaintemplate) глобального параметра конфигурации Deckhouse (ключ `%s` заменяется на `tools`).

Например, если `publicDomainTemplate` установлен как `%s-kube.company.my`, веб-интерфейс будет доступен по адресу `tools-kube.company.my`.

### Модуль deckhouse

Этот модуль настраивает в Deckhouse:

- **[Уровень логирования](configuration.html#parameters-loglevel)**
- **[Набор модулей](configuration.html#parameters-bundle), включенных по умолчанию**

  Обычно используется набор модулей `Default`, который подходит в большинстве случаев.

  Независимо от используемого набора включенных по умолчанию модулей любой модуль может быть явно включен или выключен в конфигурации Deckhouse.
- **[Канал обновлений](configuration.html#parameters-releasechannel)**

  В Deckhouse реализован механизм автоматического обновления. Этот механизм использует 5 каналов обновлений, различающиеся стабильностью и частотой выхода версий.
- **[Режим обновлений](configuration.html#parameters-update-mode)** и **[окна обновлений](configuration.html#parameters-update-windows)**

  Deckhouse может использовать **ручной** или **автоматический** режим обновлений.

  В ручном режиме обновлений автоматически применяются только важные исправления (patch-релизы), и для перехода на новый релиз Deckhouse требуется [ручное подтверждение](/reference/api/cr.html#deckhouserelease-v1alpha1-approved).

  В автоматическом режиме обновлений, если в кластере **не установлены** [окна обновлений](configuration.html#parameters-update-windows), переход на новый релиз Deckhouse осуществляется сразу после его появления на соответствующем канале обновлений. Если же в кластере **установлены** окна обновлений, переход на более свежий релиз Deckhouse начнется в ближайшее доступное окно обновлений после появления новой версии на канале обновлений.
  
- **Сервис валидирования кастомных ресурсов**

  Сервис валидирования предотвращает создание кастомных ресурсов с некорректными данными или внесение таких данных в уже существующие кастомные ресурсы. Отслеживаются только ресурсы, находящиеся под управлением модулей Deckhouse.

#### Обновление релизов Deckhouse

##### Просмотр статуса релизов Deckhouse

Список последних релизов в кластере можно получить командной `d8 k get deckhousereleases`. По умолчанию хранятся 10 последних релизов и все будущие.
Каждый релиз может иметь один из следующих статусов:

- `Pending` — релиз находится в ожидании, ждет окна обновления, настроек канареечного развертывания и т. д. Подробности можно увидеть с помощью команды `d8 k describe deckhouserelease $name`.
- `Deployed` — релиз применен. Это значит, что образ пода Deckhouse уже поменялся на новую версию,
 но при этом процесс обновления всех компонентов кластера идет асинхронно, так как зависит от многих настроек.
- `Superseded` — релиз устарел и больше не используется.
- `Suspended` — релиз отменен (например, в нем обнаружилась ошибка). Релиз переходит в этот статус, если его отменили и при этом он еще не был применен в кластере.

##### Процесс обновления

В момент перехода в статус `Deployed` релиз меняет версию (tag) образа Deckhouse. После запуска Deckhouse начнет проверку и обновление всех модулей, которые поменялись с предыдущего релиза. Длительность обновления зависит от настроек и размера кластера.
Например, если у вас много `NodeGroup`, они будут обновляться продолжительное время, если много `IngressNginxController` — они будут
обновляться по одному и это тоже займет некоторое время.

##### Ручное применение релизов

Если выбран [ручной режим обновления](usage.html#ручное-подтверждение-обновлений) и скопилось несколько релизов,
их можно сразу одобрить к применению. В этом случае Deckhouse будет обновляться последовательно, сохраняя порядок релизов и меняя статус каждого примененного релиза.

##### Закрепление релиза

Под *закреплением* релиза подразумевается полное или частичное отключение автоматического обновления версий Deckhouse.

Есть три варианта ограничения автоматического обновления Deckhouse:

- Установить ручной режим обновления.

  В этом случае вы остановитесь на текущей версии, сможете получать обновления в кластер, но для применения обновления необходимо будет выполнить [ручное действие](usage.html#ручное-подтверждение-обновлений). Это относится и к patch-версиям, и к минорным версиям.
  
  Для установки ручного режима обновления необходимо в ModuleConfig `deckhouse` установить параметр [settings.update.mode](configuration.html#parameters-update-mode) в `Manual`:

  ```shell
  d8 k patch mc deckhouse --type=merge -p='{"spec":{"settings":{"update":{"mode":"Manual"}}}}'
  ```
  
- Установить режим автоматического обновления для патч-версий.

  В этом случае вы остановитесь на текущем релизе, но будете получать patch-версии текущего релиза (с учетом установленных окон обновлений). Для применения обновления минорной версии релиза необходимо будет выполнить [ручное действие](usage.html#ручное-подтверждение-обновлений).
  
  Например: текущая версия Deckhouse Platform Certified Security Edition `v1.70.1`, после установки режима автоматического обновления для патч-версий, Deckhouse сможет обновиться до версии `v1.70.1`, но не будет обновляться до версии `v1.71.*` и выше.
  
  Для установки режима автоматического обновления для патч-версий необходимо в ModuleConfig `deckhouse` установить параметр [settings.update.mode](configuration.html#parameters-update-mode) в `AutoPatch`:

  ```shell
  d8 k patch mc deckhouse --type=merge -p='{"spec":{"settings":{"update":{"mode":"AutoPatch"}}}}'
  ```

- Установить конкретный тег для Deployment `deckhouse` и удалить параметр [releaseChannel](configuration.html#parameters-releasechannel) из конфигурации модуля `deckhouse`.

  В таком случае Deckhouse Platform Certified Security Edition останется на конкретной версии, никакой информации о новых доступных версиях (объекты DeckhouseRelease) в кластере появляться не будет.

  Пример установки версии `v1.66.3` для Deckhouse Platform Certified Security Edition EE и удаления параметра `releaseChannel` из конфигурации модуля `deckhouse`:

  ```shell
  d8 k -ti -n d8-system exec svc/deckhouse-leader -c deckhouse -- kubectl set image deployment/deckhouse deckhouse=registry.deckhouse.ru/deckhouse/ee:v1.66.3
  d8 k patch mc deckhouse --type=json -p='[{"op": "remove", "path": "/spec/settings/releaseChannel"}]'
  ```

#### Priority Classes

Модуль создает в кластере набор классов приоритета (PriorityClass) и назначает их компонентам, установленным Deckhouse, и приложениям в кластере.

Функциональность классов приоритета реализуется планировщиком (scheduler), который позволяет учитывать приоритет пода (определяемый его принадлежностью к классу) при планировании.

Например, при развертывании в кластере подов с `priorityClassName: production-low`, если в кластере не будет доступных ресурсов для данного пода, Kubernetes начнет вытеснять поды с наименьшим приоритетом.
То есть сначала будут вытеснены все поды с `priorityClassName: develop`, затем — с `cluster-low` и так далее.

При указании класса приоритета очень важно понимать тип приложения и окружение, в котором оно будет работать. Указание любого класса приоритета не уменьшит его фактический приоритет, так как если у пода не установлен приоритет, то планировщик считает его самым низким.

{% alert level="warning" %}
Нельзя использовать классы приоритета `system-node-critical`, `system-cluster-critical`, `cluster-medium`, `cluster-low`.
{% endalert %}

Устанавливаемые модулем классы приоритета (в порядке приоритета от высшего к низшему):

| Класс приоритета          | Описание                                                                                                                                                                                                                                                                                                                                                              | Значение   |
|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| `system-node-critical`    | Компоненты кластера, которые обязаны присутствовать на узле. Также полностью защищает от вытеснения kubelet'ом.<br>Примеры: `node-exporter`, `csi` и другие.                                                                                                                                  | 2000001000 |
| `system-cluster-critical` | Компоненты кластера, без которых его корректная работа невозможна. Этим PriorityClass'ом обязательно помечаются MutatingWebhooks и Extension API servers. Также полностью защищает от вытеснения kubelet'ом.<br>Примеры: `kube-dns`, `kube-proxy`, `cni-flannel`, `cni-cillium` и другие.     | 2000000000 |
| `production-high`         | Stateful-приложения, отсутствие которых в production-окружении приводит к полной недоступности сервиса или потере данных.<br>Примеры: `PostgreSQL`, `Memcached`, `Redis`, `MongoDB` и другие.                                                                                                                                                                         | 9000       |
| `cluster-medium`          | Компоненты кластера, влияющие на мониторинг (алерты, диагностика) и автомасштабирование. Без мониторинга невозможно оценить масштабы происшествия, без автомасштабирования — предоставить приложениям необходимые ресурсы.<br>Примеры: `deckhouse`, `node-local-dns`, `grafana`, `upmeter` и другие.                                                                  | 7000       |
| `production-medium`       | Основные stateless-приложения в production-окружении, которые отвечают за работу сервиса для посетителей.                                                                                                                                                                                                                                                             | 6000       |
| `deployment-machinery`    | Компоненты кластера, используемые для сборки и деплоя в кластер.                                                                                                                                                                                                                                                                                                      | 5000       |
| `production-low`          | Приложения в production-окружении (cron-задания, административные панели, batch-процессы), без которых можно обойтись некоторое время. Если batch или cron-задачи нельзя прерывать, их следует отнести к `production-medium`.                                                                                                                                         | 4000       |
| `staging`                 | Staging-окружения для приложений.                                                                                                                                                                                                                                                                                                                                     | 3000       |
| `cluster-low`             | Компоненты кластера, без которых эксплуатация возможна, но которые желательны. <br>Примеры: `dashboard`, `cert-manager`, `prometheus` и другие.                                                                                                                                                                                                                       | 2000       |
| `develop` (по умолчанию)  | Develop-окружения для приложений. Класс по умолчанию, если не указан иной класс.                                                                                                                                                                                                                                                                                      | 1000       |
| `standby`                 | Класс не предназначен для приложений. Используется в системных целях для резервирования узлов.                                                                                                                                                                                                                                                                        | -1         |

### Модуль descheduler

Каждые 15 минут модуль анализирует состояние кластера и выполняет вытеснение подов, соответствующих условиям, описанным в активных [стратегиях](#стратегии). Вытесненные поды вновь проходят процесс планирования с учетом текущего состояния кластера. Это позволяет перераспределить рабочие нагрузки в соответствие с выбранной стратегией.

Модуль основан на проекте descheduler.

#### Особенности работы модуля

* Модуль может учитывать класс приоритета пода (параметр [spec.priorityClassThreshold](cr.html#descheduler-v1alpha2-spec-priorityclassthreshold)), ограничивая работу только подами, у которых класс приоритета ниже заданного порога;
* Модуль не вытесняет под в следующих случаях:
  * под находится в пространстве имен `d8-*` или `kube-system`;
  * под имеет `priorityClassName` `system-cluster-critical` или `system-node-critical`;
  * под связан с локальным хранилищем;
  * под связан с DaemonSet;
  * вытеснение пода нарушит Pod Disruption Budget (PDB);
  * нет доступных узлов для запуска вытесненного пода.
* Поды с классом приоритета `Best effort` вытесняются раньше, чем поды с классами `Burstable` и `Guaranteed`.

Для фильтрации подов и узлов модуль использует механизм `labelSelector` Kubernetes:

* `podLabelSelector` — ограничивает поды по меткам;
* `namespaceLabelSelector` — фильтрует поды по пространствам имен.
* `nodeLabelSelector` — выбирает узлы по меткам.

#### Стратегии

##### HighNodeUtilization

{% alert level="info" %}
Концентрирует нагрузку на меньшем числе узлов. Требует настройку планировщика и включение автоматического масштабирования.

Чтобы использовать `HighNodeUtilization`, необходимо явно указать профиль планировщика [high-node-utilization](./control-plane-manager/faq.html#профили-планировщика) для каждого пода (этот профиль не может быть установлен как профиль по умолчанию).
{% endalert %}

Стратегия определяет *недостаточно нагруженные узлы* и вытесняет с них поды, чтобы распределить их компактнее, на меньшем числе узлов.

**Недостаточно нагруженный узел** — узел, использование ресурсов которого меньше **всех** пороговых значений, заданных в секции параметров [strategies.highNodeUtilization.thresholds](cr.html#descheduler-v1alpha2-spec-strategies-highnodeutilization-thresholds).

Стратегия включается параметром [spec.strategies.highNodeUtilization.enabled](cr.html#descheduler-v1alpha2-spec-strategies-highnodeutilization-enabled).

{% alert level="warning" %}
В GKE нельзя настроить конфигурацию планировщика по умолчанию, но можно использовать стратегию `optimize-utilization` или развернуть второй пользовательский планировщик.
{% endalert %}

{% alert level="warning" %}
Использование ресурсов узла учитывает extended-ресурсы и рассчитывается на основе запросов и лимитов подов (requests and limits), а не их фактического потребления. Такой подход обеспечивает согласованность с работой kube-scheduler, который использует аналогичный принцип при размещении подов на узлах. Это означает, что метрики использования ресурсов, отображаемые Kubelet (или командами вроде `kubectl top`), могут отличаться от расчетных показателей, так как Kubelet и связанные инструменты отображают данные о реальном потреблении ресурсов.
{% endalert %}

##### LowNodeUtilization

{% alert level="info" %}
Более равномерно нагружает узлы.
{% endalert %}

Стратегия выявляет *недостаточно нагруженные узлы* и вытесняет поды с других, *избыточно нагруженных узлов*. Стратегия предполагает, что пересоздание вытесненных подов произойдет на недостаточно нагруженных узлах (при обычном поведении планировщика).

**Недостаточно нагруженный узел** — узел, использование ресурсов которого меньше **всех** пороговых значений, заданных в секции параметров [strategies.lowNodeUtilization.thresholds](cr.html#descheduler-v1alpha2-spec-strategies-lownodeutilization-thresholds).

**Избыточно нагруженный узел** — узел, использование ресурсов которого больше **хотя бы одного** из пороговых значений, заданных в секции параметров [strategies.lowNodeUtilization.targetThresholds](cr.html#descheduler-v1alpha2-spec-strategies-lownodeutilization-targetthresholds).

Узлы с использованием ресурсов в диапазоне между `thresholds` и `targetThresholds` считаются оптимально используемыми. Поды на таких узлах вытесняться не будут.

Стратегия включается параметром [spec.strategies.lowNodeUtilization.enabled](cr.html#descheduler-v1alpha2-spec-strategies-lownodeutilization-enabled).

{% alert level="warning" %}
Использование ресурсов узла учитывает extended-ресурсы и рассчитывается на основе запросов и лимитов подов (requests and limits), а не их фактического потребления. Такой подход обеспечивает согласованность с работой kube-scheduler, который использует аналогичный принцип при размещении подов на узлах. Это означает, что метрики использования ресурсов, отображаемые Kubelet (или командами вроде `kubectl top`), могут отличаться от расчетных показателей, так как Kubelet и связанные инструменты отображают данные о реальном потреблении ресурсов.
{% endalert %}

##### RemoveDuplicates

{% alert level="info" %}
Предотвращает запуск нескольких подов одного контроллера (ReplicaSet, ReplicationController, StatefulSet) или заданий (Job) на одном узле.
{% endalert %}

Стратегия следит за тем, чтобы на одном узле не находилось больше одного пода ReplicaSet, ReplicationController, StatefulSet или подов одного задания (Job). Если таких подов два или больше, модуль вытесняет лишние поды, чтобы они лучше распределились по кластеру.

Описанная ситуация может возникнуть, если некоторые узлы кластеры вышли из строя по каким-либо причинам, и поды с них были перемещены на другие узлы. Как только вышедшие из строя узлы снова станут доступны для приема нагрузки, эту стратегию можно будет использовать для выселения дублирующих подов с других узлов.

Стратегия включается параметром [strategies.removeDuplicates.enabled](cr.html#descheduler-v1alpha2-spec-strategies-removeduplicates-enabled).

##### RemovePodsViolatingInterPodAntiAffinity

{% alert level="info" %}
Вытесняет поды, нарушающие правила inter-pod affinity и anti-affinity.
{% endalert %}

Стратегия гарантирует, что поды, нарушающие правила inter-pod affinity и anti-affinity, будут удалены с узлов.

Например, если на узле находится **Под1**, а также **Под2** и **Под3**, имеющие правила anti-affinity, которые запрещают им работать на одном узле с подом **Под1**, то **Под1** будет вытеснен с узла, чтобы **Под2** и **Под3** смогли работать. Такая ситуация может возникнуть, когда правила inter-pod affinity для **Под2** и **Под3** создаются когда поды уже запущены на узле.

Стратегия включается параметром [strategies.removePodsViolatingInterPodAntiAffinity.enabled](cr.html#descheduler-v1alpha2-spec-strategies-removepodsviolatinginterpodantiaffinity-enabled).

##### RemovePodsViolatingNodeAffinity

{% alert level="info" %}
Вытесняет поды, нарушающие правила node affinity.
{% endalert %}

Стратегия гарантирует, что все поды, которые нарушают правила node affinity, в конечном счете будут удалены с узлов.

По сути, в зависимости от настроек параметра [strategies.removePodsViolatingNodeAffinity.nodeAffinityType](cr.html#descheduler-v1alpha2-spec-strategies-removepodsviolatingnodeaffinity-nodeaffinitytype),  
стратегия превращает правило `requiredDuringSchedulingIgnoredDuringExecution` node affinity пода в правило `requiredDuringSchedulingRequiredDuringExecution`, а правило `preferredDuringSchedulingIgnoredDuringExecution` в правило `preferredDuringSchedulingPreferredDuringExecution`.

Пример для `nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution`. Есть под, который был назначен на узел, соответствующий правилу `requiredDuringSchedulingIgnoredDuringExecution` node affinity на момент размещения. Если со временем этот узел перестанет удовлетворять правилу node affinity, и если появится другой доступный узел, соответствующий этому правилу, стратегия вытеснит под с узла, на который он был изначально назначен.

Пример для `nodeAffinityType: preferredDuringSchedulingIgnoredDuringExecution`. Есть под, который был назначен на узел, т.к. на момент размещения отсутствовали другие узлы, удовлетворяющие правилу `preferredDuringSchedulingIgnoredDuringExecution` node affinity. Если со временем в кластере появится доступный узел, соответствующий этому правилу, стратегия вытеснит под с узла, на который он был изначально назначен.

Стратегия включается параметром [strategies.removePodsViolatingNodeAffinity.enabled](cr.html#descheduler-v1alpha2-spec-strategies-removepodsviolatingnodeaffinity-enabled).

### Модуль documentation

Модуль `documentation` создает веб-интерфейс с документацией, соответствующей запущенной версии Deckhouse Platform Certified Security Edition.

Это может быть полезно, когда Deckhouse работает в сети с ограничением доступа в интернет.

Для получения адреса веб-интерфейса в шаблоне [publicDomainTemplate](/reference/api/global.html#parameters-modules-publicdomaintemplate) глобального параметра конфигурации Deckhouse ключ `%s` замените на `documentation`.

Например, если `publicDomainTemplate` установлен как `%s-kube.company.my`, веб-интерфейс документации будет доступен по адресу `documentation-kube.company.my`.

### Модуль extended-monitoring

Модуль `extended-monitoring` расширяет возможности мониторинга кластера за счёт дополнительных Prometheus exporter’ов, которые позволяют выявлять потенциальные проблемы до того, как они скажутся на работе сервисов.

Возможности модуля:

- Расширенный сбор метрик — собирает дополнительные метрики, а также включает готовые алерты и дашборды, которые позволяют быстрее обнаруживать и диагностировать инциденты:
  - собирает и экспортирует метрики по свободному месту и inode на узлах, а также по объектам с лейблом `extended-monitoring.deckhouse.io/enabled=""` в пространстве имён;
  - автоматически формирует алерты при достижении пороговых значений.
- Мониторинг контейнерных образов:
  - добавляет метрики и отправляет алерты о недоступности образов контейнеров в registry для всех типов рабочей нагрузки (`Deployments`, `StatefulSets`, `DaemonSets`, `CronJobs`);
  - помогает заранее узнать о возможных проблемах с запуском или обновлением подов.
- События в кластере — собирает события Kubernetes и отображает их в виде метрик, что позволяет отслеживать динамику изменений и быстрее реагировать на инциденты.
- Контроль сертификатов:
  - сканирует Secret’ы кластера и генерирует метрики об истечении срока действия x509-сертификатов;
  - позволяет не пропускать критические моменты и вовремя обновлять сертификаты, избегая простоя приложений из-за просроченных сертификатов.

### Проверка хеш суммы образа

#### Описание

Для проверки целостности образа используется контрольная сумма расчитанная по алгоритму Стрибог (ГОСТ Р 34.11-2012)
Чтобы устанавлеваемые образы проверялись, необнодимо добавить метку ```gost-integrity-controller.deckhouse.io/gost-digest-validation-enabled: true``` в пространство имен кластера где необходимо производить контроль целостности образа.

Пример:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    gost-integrity-controller.deckhouse.io/gost-digest-validation-enabled: "true"
  name: default
```

В случае если во время проверки контрольная сумма образа будет некорректная, будет отказано в установке образа, о чем вы получите сообщение.

Если образ находится в закрытом репозитории, для авторизации необходимо указать в спецификации контейнера параметр ```imagePullSecrets```. И создать секрет с данными для авторизации.

#### Алгоритм расчета контрольной суммы

Для расчета контрольной суммы берется список контрольных сумм слоев образа.  Список сортируется в порядке возрастания и склеивается в одну строку. Затем производится расчет контрольной суммы от этой строки по алгоритму Стрибог (ГОСТ Р 34.11-2012).

Пример расчета контрольной суммы образа nginx:1.25.2:

```text
Контрольные суммы слоев отсортированные в порядке возрастания
[
    "sha256:27e923fb52d31d7e3bdade76ab9a8056f94dd4bc89179d1c242c0e58592b4d5c",
    "sha256:360eba32fa65016e0d558c6af176db31a202e9a6071666f9b629cb8ba6ccedf0",
    "sha256:72de7d1ce3a476d2652e24f098d571a6796524d64fb34602a90631ed71c4f7ce",
    "sha256:907d1bb4e9312e4bfeabf4115ef8592c77c3ddabcfddb0e6250f90ca1df414fe",
    "sha256:94f34d60e454ca21cf8e5b6ca1f401fcb2583d09281acb1b0de872dba2d36f34",
    "sha256:c5903f3678a7dec453012f84a7d04f6407129240f12a8ebc2cb7df4a06a08c4f",
    "sha256:e42dcfe1730ba17b27138ea21c0ab43785e4fdbea1ee753a1f70923a9c0cc9b8"
]

Склеенная строка из контрольных сумм
"sha256:27e923fb52d31d7e3bdade76ab9a8056f94dd4bc89179d1c242c0e58592b4d5csha256:360eba32fa65016e0d558c6af176db31a202e9a6071666f9b629cb8ba6ccedf0sha256:72de7d1ce3a476d2652e24f098d571a6796524d64fb34602a90631ed71c4f7cesha256:907d1bb4e9312e4bfeabf4115ef8592c77c3ddabcfddb0e6250f90ca1df414fesha256:94f34d60e454ca21cf8e5b6ca1f401fcb2583d09281acb1b0de872dba2d36f34sha256:c5903f3678a7dec453012f84a7d04f6407129240f12a8ebc2cb7df4a06a08c4fsha256:e42dcfe1730ba17b27138ea21c0ab43785e4fdbea1ee753a1f70923a9c0cc9b8"

Контрольная сумма образа

2f538c22adbdb2ca8749cdafc27e94baed8645c69d4f0745fc8889f0e1f5a3f9
```

Контрольную сумму в образ можно добавить используя утилиту crane

```bash
crane mutate --annotation gost-digest=1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937 <образ>
```

Для расчета, добавления и проверки контрольной суммы образа можно использовать утилиту gost-image-digest <https://github.com/deckhouse/gost-image-digest>.

Расчет контрольной суммы

```bash
imagedigest calculate nginx:1.25.2
1:14PM INF GOST Image Digest: 2f538c22adbdb2ca8749cdafc27e94baed8645c69d4f0745fc8889f0e1f5a3f9
```

Расчет контрольной суммы с последующим добавлением в метаданные образа и сохранением в репозитории.

```bash
imagedigest add alekseysu/simple-http:v0.2
1:19PM INF GOST Image Digest: 1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937
1:19PM INF Added successfully
```

Проверка контрольной суммы

```bash
imagedigest validate alekseysu/simple-http:v0.2
2:08PM INF GOST Image Digest from image 1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937
2:08PM INF Calculated GOST Image Digest 1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937
2:08PM INF Validate successfully
```

### Модуль ingress-nginx

Устанавливает и управляет NGINX Ingress controller с помощью Custom Resources. Если узлов для размещения Ingress-контроллера больше одного, он устанавливается в отказоустойчивом режиме и учитывает все особенности реализации инфраструктуры облаков и bare metal, а также кластеров Kubernetes различных типов.

Поддерживает запуск и раздельное конфигурирование одновременно нескольких NGINX Ingress controller'ов — один **основной** и неограниченное количество **дополнительных**. Например, это позволяет отделять внешние и intranet Ingress-ресурсы приложений.

#### Варианты терминирования трафика

Трафик к `ingress-nginx` может быть отправлен несколькими способами:

- напрямую без внешнего балансировщика;
- через внешний LoadBalancer.

#### Терминация HTTPS

Модуль позволяет управлять политиками безопасности HTTPS каждого NGINX Ingress controller, в том числе:

- параметрами HSTS;
- набором доступных версий SSL/TLS и протоколов шифрования.

Также модуль интегрирован с модулем [cert-manager](/modules/cert-manager/), при взаимодействии с которым, возможны автоматический заказ SSL-сертификатов и дальнейшее использование сертификатов NGINX Ingress controller.

#### Мониторинг и статистика

В нашей реализации `ingress-nginx` добавлена система сбора статистики в Prometheus с множеством метрик:

- по длительности времени всего ответа и апстрима отдельно;
- кодам ответа;
- количеству повторов запросов (retry);
- размерам запроса и ответа;
- методам запросов;
- типам `content-type`;
- географии распределения запросов и т. д.

Данные доступны в нескольких параметрах:

- по `namespace`;
- `vhost`;
- `ingress`-ресурсу;
- `location` (в nginx).

Все графики собраны в виде удобных дашбордов в Grafana, при этом есть возможность drill-down'а по графикам. Например, при просмотре статистики пространств имён, можно нажать ссылку на дашборд в Grafana, углубиться в статистику по `vhosts` в соответствующем `namespace` и т.д.

#### Статистика

##### Основные принципы сбора статистики

1. На каждый запрос на стадии `log_by_lua_block` вызывается модуль, который рассчитывает необходимые данные и складывает их в буфер (у каждого nginx worker собственный буфер).
1. На стадии `init_by_lua_block` для каждого nginx worker запускается процесс, который раз в секунду асинхронно отправляет данные в формате `protobuf` через TCP socket в `protobuf_exporter` (разработка Deckhouse Platform Certified Security Edition).
1. `protobuf_exporter` запущен sidecar-контейнером в поде с ingress-controller, принимает сообщения в формате `protobuf`, разбирает, агрегирует их по установленным правилам и экспортирует в формате для Prometheus.
1. Prometheus каждые 30 секунд собирает метрики как в ingress-controller (там есть небольшое количество нужных нам метрик), так и в protobuf_exporter, на основании этих данных все работает!

##### Какая статистика собирается и как она представлена

У всех собираемых метрик есть служебные лейблы, позволяющие идентифицировать экземпляр контроллера: `controller`, `app`, `instance` и `endpoint` (они видны в `/prometheus/targets`).

- Все метрики (кроме geo), экспортируемые protobuf_exporter, представлены в трех уровнях детализации:
  - `ingress_nginx_overall_*` — «вид с вертолета», у всех метрик есть лейблы `namespace`, `vhost` и `content_kind`;
  - `ingress_nginx_detail_*` — кроме лейблов уровня `overall`, добавляются `ingress`, `service`, `service_port` и `location`;
  - `ingress_nginx_detail_backend_*` — ограниченная часть данных, собирается в разрезе по бэкендам. У этих метрик, кроме лейблов уровня detail, добавляется лейбл `pod_ip`.

- Для уровней overall и detail собираются следующие метрики:
  - `*_requests_total` — counter количества запросов (дополнительные лейблы — `scheme`, `method`);
  - `*_responses_total` — counter количества ответов (дополнительный лейбл — `status`);
  - `*_request_seconds_{sum,count,bucket}` — histogram времени ответа;
  - `*_bytes_received_{sum,count,bucket}` — histogram размера запроса;
  - `*_bytes_sent_{sum,count,bucket}` — histogram размера ответа;
  - `*_upstream_response_seconds_{sum,count,bucket}` — histogram времени ответа upstream'а (используется сумма времен ответов всех upstream'ов, если их было несколько);
  - `*_lowres_upstream_response_seconds_{sum,count,bucket}` — то же самое, что и предыдущая метрика, только с меньшей детализацией (подходит для визуализации, но не подходит для расчета quantile);
  - `*_upstream_retries_{count,sum}` — количество запросов, при обработке которых были retry бэкендов, и сумма retry'ев.

- Для уровня overall собираются следующие метрики:
  - `*_geohash_total` — counter количества запросов с определенным geohash (дополнительные лейблы — `geohash`, `place`).

- Для уровня detail_backend собираются следующие метрики:
  - `*_lowres_upstream_response_seconds` — то же самое, что аналогичная метрика для overall и detail;
  - `*_responses_total` — counter количества ответов (дополнительный лейбл — `status_class`, а не просто `status`);
  - `*_upstream_bytes_received_sum` — counter суммы размеров ответов бэкенда.

### Модуль istio

#### Таблица совместимости поддерживаемых версий

| Версия Istio | Версии K8S, поддерживаемые Istio | Статус в текущем релизе D8 |
|:------------:|:------------------------------------------------------------------------------------------------------------------------------:|:--------------------------:|
|     1.25      |                                                1.29, 1.30, 1.31, 1.32                                                | Поддерживается |
|     1.21     |                                           1.26, 1.27, 1.28, 1.29, 1.30, 1.31                                          |  Поддерживается  |
|     1.19     |                                                     1.25<sup>*</sup>, 1.26, 1.27, 1.28, 1.28, 1.29, 1.30                                                     |       Устарела и будет удалена       |

<sup>*</sup> — версия Kubernetes **НЕ поддерживается** в текущем релизе Deckhouse Platform Certified Security Edition.

{::options parse_block_html="false" /}

#### Задачи, которые решает Istio

Istio — фреймворк централизованного управления сетевым трафиком, реализующий подход Service Mesh.

Istio решает для приложений следующие задачи:

- [Использование Mutual TLS:](#mutual-tls)
  - Взаимная достоверная аутентификация сервисов.
  - Шифрование трафика.
- [Авторизация доступа между сервисами.](#авторизация)
- [Маршрутизация запросов:](#маршрутизация-запросов)
  - Canary-deployment и A/B-тестирование — позволяют отправлять часть запросов на новую версию приложения.
- [Управление балансировкой запросов между эндпойнтами сервиса:](#управление-балансировкой-запросов-между-эндпойнтами-сервиса)
  - Circuit Breaker:
    - временное исключение эндпойнта из балансировки, если превышен лимит ошибок;
    - настройка лимитов на количество TCP-соединений и количество запросов в сторону одного эндпойнта;
    - выявление зависших запросов и обрывание их с кодом ошибки (HTTP request timeout).
  - Sticky Sessions:
    - привязка запросов от конечных пользователей к эндпойнту сервиса.
  - Locality Failover — позволяет отдавать предпочтение эндпойнтам в локальной зоне доступности.
  - Балансировка gRPC-сервисов.
- [Повышение Observability:](#observability)
  - Сбор и визуализация данных для трассировки прикладных запросов с помощью Jaeger.
  - Сбор метрик о трафике между сервисами в Prometheus и визуализация их в Grafana.
  - Визуализация состояния связей между сервисами и состояния служебных компонентов Istio с помощью Kiali.
- [Организация мульти-ЦОД кластера за счет объединения кластеров в единый Service Mesh (мультикластер).](#мультикластер)
- [Объединение разрозненных кластеров в федерацию с возможностью предоставлять стандартный (в понимании Service Mesh) доступ к избранным сервисам.](#федерация)

#### Mutual TLS

Это основной метод взаимной аутентификации сервисов. Принцип основан на том, что для всех исходящих запросов проверяется сертификат сервера, а для входящих - клиентский сертификат. После проверки sidecar-proxy получает возможность идентифицировать удаленный узел и использовать эти данные для аутентификации или для прикладных целей.

Каждый сервис получает собственный идентификатор в формате `<TrustDomain>/ns/<Namespace>/sa/<ServiceAccount>`, где `TrustDomain` в нашем случае — это домен кластера. Каждому сервису можно выделять собственный ServiceAccount или использовать стандартный «default». Полученный идентификатор сервиса можно использовать как в правилах авторизации, так и в прикладных целях. Именно этот идентификатор используется в качестве удостоверяемого имени в TLS-сертификатах.

Данные настройки можно переопределить на уровне namespace.

#### Авторизация

Управление авторизацией осуществляется с помощью ресурса [AuthorizationPolicy](istio-cr.html#authorizationpolicy). В момент, когда для сервиса создается этот ресурс, начинает работать следующий алгоритм принятия решения о судьбе запроса:

- Если запрос попадает под политику DENY — запретить запрос.
- Если для данного сервиса нет политик ALLOW — разрешить запрос.
- Если запрос попадает под политику ALLOW — разрешить запрос.
- Все остальные запросы — запретить.

Иными словами, если явно что-то запретить, работает только запрет. Если же что-то явно разрешить, будут разрешены только явно одобренные запросы (запреты при этом имеют приоритет).

Для написания правил авторизации доступны следующие аргументы:

- идентификаторы сервисов и wildcard на их основе (`mycluster.local/ns/myns/sa/myapp` или `mycluster.local/*`);
- пространства имён;
- диапазоны IP;
- HTTP-заголовки;
- JWT-токены из прикладных запросов.

#### Маршрутизация запросов

Основной ресурс для управления маршрутизацией — [VirtualService](istio-cr.html#virtualservice), он позволяет переопределять судьбу HTTP- или TCP-запроса. Доступные аргументы для принятия решения о маршрутизации:

- Host и любые другие заголовки;
- URI;
- метод (GET, POST и пр.);
- лейблы пода или namespace источника запросов;
- dst-IP или dst-порт для не-HTTP-запросов.

#### Управление балансировкой запросов между эндпойнтами сервиса

Основной ресурс для управления балансировкой запросов — [DestinationRule](istio-cr.html#destinationrule), он позволяет настроить нюансы исходящих из подов запросов:

- лимиты/таймауты для TCP;
- алгоритмы балансировки между эндпойнтами;
- правила определения проблем на стороне эндпойнта для выведения его из балансировки;
- нюансы шифрования.

{% alert level="warning" %}
Все настраиваемые лимиты работают для каждого пода клиента по отдельности! Если настроить для сервиса ограничение на одно TCP-соединение, а клиентских подов — три, то сервис получит три входящих соединения.
{% endalert %}

#### Observability

##### Трассировка

Istio позволяет осуществлять сбор трейсов с приложений и инъекцию трассировочных заголовков, если таковых нет. При этом важно понимать следующее:

- Если запрос инициирует на сервисе вторичные запросы, для них необходимо наследовать трассировочные заголовки средствами приложения.
- Jaeger для сбора и отображения трейсов потребуется устанавливать самостоятельно.

##### Grafana

В стандартной комплектации с модулем предоставлены дополнительные дашборды:

- дашборд для оценки производительности и успешности запросов/ответов между приложениями;
- дашборд для оценки работоспособности и нагрузки на control plane.

##### Kiali

Инструмент для визуализации дерева сервисов вашего приложения. Позволяет быстро оценить обстановку в сетевой связности благодаря визуализации запросов и их количественных характеристик непосредственно на схеме.

#### Архитектура кластера с включенным Istio

Компоненты кластера делятся на две категории:

- control plane — управляющие и обслуживающие сервисы. Под control plane обычно подразумевают поды istiod.
- data plane — прикладная часть Istio. Представляет собой контейнеры sidecar-proxy.

![Архитектура кластера с включенным Istio](images/istio-architecture.svg)


Все сервисы из data plane группируются в mesh. Его характеристики:

- Общее пространство имен для генерации идентификатора сервиса в формате `<TrustDomain>/ns/<Namespace>/sa/<ServiceAccount>`. Каждый mesh имеет идентификатор TrustDomain, который в нашем случае совпадает с доменом кластера. Например: `mycluster.local/ns/myns/sa/myapp`.
- Сервисы в рамках одного mesh имеют возможность аутентифицировать друг друга с помощью доверенных корневых сертификатов.

Элементы control plane:

- `istiod` — ключевой сервис, обеспечивающий решение следующих задач:
  - Непрерывная связь с API Kubernetes и сбор информации о прикладных сервисах.
  - Обработка и валидация с помощью механизма Kubernetes Validating Webhook всех Custom Resources, которые связаны с Istio.
  - Компоновка конфигурации для каждого sidecar-proxy индивидуально:
    - генерация правил авторизации, маршрутизации, балансировки и пр.;
    - распространение информации о других прикладных сервисах в кластере;
    - выпуск индивидуальных клиентских сертификатов для организации схемы Mutual TLS. Эти сертификаты не связаны с сертификатами, которые использует и контролирует сам Kubernetes для своих служебных нужд.
  - Автоматическая подстройка манифестов, определяющих прикладные поды через механизм Kubernetes Mutating Webhook:
    - внедрение дополнительного служебного контейнера sidecar-proxy;
    - внедрение дополнительного init-контейнера для адаптации сетевой подсистемы (настройка DNAT для перехвата прикладного трафика);
    - перенаправление readiness- и liveness-проб через sidecar-proxy.
- `operator` — компонент, отвечающий за установку всех ресурсов, необходимых для работы control plane определенной версии.
- `kiali` — панель управления и наблюдения за ресурсами Istio и пользовательскими сервисами под управлением Istio, позволяющая следующее:
  - Визуализировать связи между сервисами.
  - Диагностировать проблемные связи между сервисами.
  - Диагностировать состояние control plane.

Для приема пользовательского трафика необходима доработка Ingress-контроллера:

- К подам контроллера добавляется sidecar-proxy, который обслуживает только трафик от контроллера в сторону прикладных сервисов (параметр IngressNginxController [`enableIstioSidecar`](./ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-enableistiosidecar) у ресурса IngressNginxController).
- Сервисы не под управлением Istio продолжают работать как раньше, запросы в их сторону не перехватываются сайдкаром контроллера.
- Запросы в сторону сервисов под управлением Istio перехватываются сайдкаром и обрабатываются в соответствии с правилами Istio (подробнее о том, [как активировать Istio для приложения](#как-активировать-istio-для-приложения)).

Контроллер istiod и каждый контейнер sidecar-proxy экспортируют собственные метрики, которые собирает кластерный Prometheus.

#### Архитектура прикладного сервиса с включенным Istio

##### Особенности

- Каждый под сервиса получает дополнительный контейнер — sidecar-proxy. Технически этот контейнер содержит два приложения:
  - **Envoy** — проксирует прикладной трафик и реализует все функции, которые предоставляет Istio, включая маршрутизацию, аутентификацию, авторизацию и пр.
  - **pilot-agent** — часть Istio, отвечает за поддержание конфигурации Envoy в актуальном состоянии, а также содержит в себе кэширующий DNS-сервер.
- В каждом поде настраивается DNAT входящих и исходящих прикладных запросов в sidecar-proxy. Делается это с помощью дополнительного init-контейнера. Таким образом, трафик будет перехватываться прозрачно для приложений.
- Так как входящий прикладной трафик перенаправляется в sidecar-proxy, readiness/liveness-трафика это тоже касается. Подсистема Kubernetes, которая за это отвечает, не рассчитана на формирование проб в формате Mutual TLS. Для адаптации все существующие пробы автоматически перенастраиваются на специальный порт в sidecar-proxy, который перенаправляет трафик на приложение в неизменном виде.
- Для приема запросов извне кластера необходимо использовать подготовленный Ingress-контроллер:
  - Поды контроллера аналогично имеют дополнительный контейнер sidecar-proxy.
  - В отличие от подов приложения, sidecar-proxy Ingress-контроллера перехватывает только трафик от контроллера к сервисам. Входящий трафик от пользователей обрабатывает непосредственно сам контроллер.
- Ресурсы типа Ingress требуют минимальной доработки в виде добавления аннотаций:
  - `nginx.ingress.kubernetes.io/service-upstream: "true"` — Ingress-контроллер в качестве upstream будет использовать ClusterIP сервиса вместо адресов подов. Балансировкой трафика между подами теперь занимается sidecar-proxy. Используйте эту опцию, только если у вашего сервиса есть ClusterIP.
  - `nginx.ingress.kubernetes.io/upstream-vhost: "myservice.myns.svc"` — sidecar-proxy Ingress-контроллера принимает решения о маршрутизации на основе заголовка Host. Без данной аннотации контроллер оставит заголовок с адресом сайта, например `Host: example.com`.
- Ресурсы типа Service не требуют адаптации и продолжают выполнять свою функцию. Приложениям все так же доступны адреса сервисов вида servicename, servicename.myns.svc и пр.
- DNS-запросы изнутри подов прозрачно перенаправляются на обработку в sidecar-proxy:
  - Требуется для разыменования DNS-имен сервисов из соседних кластеров.

##### Жизненный цикл пользовательского запроса

###### Приложение с выключенным Istio

<div data-presentation="presentations/request_lifecycle_istio_disabled_ru.pdf"></div>


###### Приложение с включенным Istio

<div data-presentation="presentations/request_lifecycle_istio_enabled_ru.pdf"></div>


#### Как активировать Istio для приложения

Основная цель активации — добавить sidecar-контейнер к подам приложения, после чего Istio сможет управлять трафиком.

Рекомендованный способ добавления sidecar-ов — использовать sidecar-injector. Istio умеет «подселять» к вашим подам sidecar-контейнер с помощью механизма Admission Webhook. Настраивается с помощью лейблов и аннотаций:

- Лейбл к `namespace` — обозначает ваше пространство имён для компонента sidecar-injector. После применения лейбла к новым подам будут добавлены sidecar-контейнеры:
  - `istio-injection=enabled` — использует глобальную версию Istio (`spec.settings.globalVersion` в `ModuleConfig`);
  - `istio.io/rev=v1x16` — использует конкретную версию Istio для этого пространства имён.
- Аннотация к **поду** `sidecar.istio.io/inject` (`"true"` или `"false"`) позволяет локально переопределить политику `sidecarInjectorPolicy`. Эти аннотации работают только в пространствах имён, обозначенных лейблами из списка выше.

Также существует возможность добавить sidecar к индивидуальному поду в пространстве имён без установленных лейблов `istio-injection=enabled` или `istio.io/rev=vXxYZ` путем установки лейбла `sidecar.istio.io/inject=true`.

**Важно!** Istio-proxy, который работает в качестве sidecar-контейнера, тоже потребляет ресурсы и добавляет накладные расходы:

- Каждый запрос DNAT'ится в Envoy, который обрабатывает это запрос и создает еще один. На принимающей стороне — аналогично.
- Каждый Envoy хранит информацию обо всех сервисах в кластере, что требует памяти. Больше кластер — больше памяти потребляет Envoy. Решение — CustomResource [Sidecar](istio-cr.html#sidecar).

Также важно подготовить Ingress-контроллер и Ingress-ресурсы приложения:

- Включите [`enableIstioSidecar`](./ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-enableistiosidecar) у ресурса IngressNginxController.
- Добавьте аннотации на Ingress-ресурсы приложения:
  - `nginx.ingress.kubernetes.io/service-upstream: "true"` — Ingress-контроллер в качестве upstream использует ClusterIP сервиса вместо адресов подов. Балансировкой трафика между подами теперь занимается sidecar-proxy. Используйте эту опцию, только если у вашего сервиса есть ClusterIP;
  - `nginx.ingress.kubernetes.io/upstream-vhost: "myservice.myns.svc"` — sidecar-proxy Ingress-контроллера принимает решения о маршрутизации на основе заголовка Host. Без этой аннотации контроллер оставит заголовок с адресом сайта, например `Host: example.com`.

#### Федерация и мультикластер

Поддерживаются две схемы межкластерного взаимодействия:

- [федерация](#федерация);
- [мультикластер](#мультикластер).

Принципиальные отличия:

- Федерация объединяет суверенные кластеры:
  - у каждого кластера собственное пространство имен (для namespace, Service и пр.);
  - доступ к отдельным сервисам между кластерами явно обозначен.
- Мультикластер объединяет созависимые кластеры:
  - пространство имен у кластеров общее — каждый сервис доступен для соседних кластеров так, словно он работает на локальном кластере (если это не запрещают правила авторизации).

##### Федерация

###### Требования к кластерам

- У каждого кластера должен быть уникальный домен в параметре [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration). Обратите внимание, что ни один из кластеров не должен иметь домен `cluster.local`, который является значением по умолчанию.

  > `cluster.local` — неизменяемый псевдоним для домена локального кластера.
  > Указание `cluster.local` как principals в AuthorizationPolicy всегда будет указывать на локальный кластер, даже если в mesh существует кластер, у которого [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain) явно определен как `cluster.local`.
- Подсети сервисов и подов в параметрах [`serviceSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-servicesubnetcidr) и [`podSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-podsubnetcidr) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration) должны быть уникальными для каждого кластера.

  > - При анализе HTTP и HTTPS запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно по заголовкам.
  > - А при анализе TCP-запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно только по IP-адресу назначения и номеру порта.
  >
  > Если IP-адреса сервисов или подов пересекутся между кластерами, то под маршрутизирующие, запрещающие или разрешающие правила istio могут попасть запросы других подов иных кластеров.
  > Пересечение подсетей сервисов и подов жестко запрещено в single-network режиме, и допустимо, но не рекомендуется в режиме multi-networks.
  >
  > - В режиме single-network поды разных кластеров могут взаимодействовать друг с другом напрямую.
  > - В режиме multi-networks поды разных кластеров могут взаимодействовать друг с другом только при использовании istio-gateway.

###### Общие принципы федерации

- Федерация требует установления взаимного доверия между кластерами. Соответственно, для установления федерации нужно в кластере A сделать кластер Б доверенным и аналогично в кластере Б сделать кластер А доверенным. Это достигается взаимным обменом корневыми сертификатами.
- Для прикладной эксплуатации федерации необходимо также обменяться информацией о публичных сервисах. Чтобы опубликовать сервис bar из кластера Б в кластере А, необходимо в кластере А создать ресурс ServiceEntry, который описывает публичный адрес ingress-gateway кластера Б.

<div data-presentation="presentations/federation_common_principles_ru.pdf"></div>


###### Включение федерации

При включении федерации (параметр модуля `istio.federation.enabled = true`) происходит следующее:

- В кластер добавляется сервис `ingressgateway`, чья задача — проксировать mTLS-трафик извне кластера на прикладные сервисы.
- В кластер добавляется сервис, который экспортирует метаданные кластера наружу:
  - корневой сертификат Istio (доступен без аутентификации);
  - список публичных сервисов в кластере (доступен только для аутентифицированных запросов из соседних кластеров);
  - список публичных адресов сервиса `ingressgateway` (доступен только для аутентифицированных запросов из соседних кластеров).

###### Управление федерацией

<div data-presentation="presentations/federation_istio_federation_ru.pdf"></div>


Для построения федерации необходимо сделать следующее:

- В каждом кластере создать набор ресурсов `IstioFederation`, которые описывают все остальные кластеры.
  - После успешного автосогласования между кластерами, в ресурсе `IstioFederation` заполнятся разделы `status.metadataCache.public` и `status.metadataCache.private` служебными данными, необходимыми для работы федерации.
- Каждый ресурс(`service`), который считается публичным в рамках федерации, пометить лейблом `federation.istio.deckhouse.io/public-service: ""`.
  - В кластерах из состава федерации, для каждого `service` создадутся соответствующие `ServiceEntry`, ведущие на `ingressgateway` оригинального кластера.

{% alert level="warning" %}
В разделе `.spec.ports` этих `service` у каждого порта должно быть заполнено поле `name`.
{% endalert %}

##### Мультикластер

###### Требования к кластерам

- Домены кластеров в параметре [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration) должны быть одинаковыми для всех членов мультикластера. По умолчанию значение параметра — `cluster.local`.
* Подсети сервисов и подов в параметрах [`serviceSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-servicesubnetcidr) и [`podSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-podsubnetcidr) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration) должны быть уникальными для каждого кластера.

  > - При анализе HTTP и HTTPS запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно по заголовкам.
  > - А при анализе TCP запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно только по IP-адресу назначения и номеру порта.
  >
  > Если IP адреса сервисов или подов пересекутся между кластерами, то под маршрутизирующие, запрещающие или разрешающие правила istio могут попасть запросы других подов иных кластеров.
  > Пересечение подсетей сервисов и подов жестко запрещено в single-network режиме, и не рекомендуется в режиме multiple-networks.
  >
  > - В режиме single-network поды разных кластеров могут взаимодействовать друг с другом напрямую.
  > - В режиме multiple-networks поды разных кластеров могут взаимодействовать друг с другом только при использовании istio-gateway.

###### Общие принципы

<div data-presentation="presentations/multicluster_common_principles_ru.pdf"></div>


- Мультикластер требует установления взаимного доверия между кластерами. Соответственно, для построения мультикластера нужно в кластере A сделать кластер Б доверенным и в кластере Б сделать кластер А доверенным. Технически это достигается взаимным обменом корневыми сертификатами.
- Для сбора информации о соседних сервисах Istio подключается напрямую к API-серверу соседнего кластера. Данный модуль Deckhouse берет на себя организацию соответствующего канала связи.

###### Включение мультикластера

При включении мультикластера (параметр модуля `istio.multicluster.enabled = true`) происходит следующее:

- В кластер добавляется прокси для публикации доступа к API-серверу посредством стандартного Ingress-ресурса:
  - Доступ через данный публичный адрес ограничен авторизацией на основе Bearer-токенов, подписанных доверенными ключами. Обмен доверенными публичными ключами происходит автоматически средствами Deckhouse при взаимной настройке мультикластера.
  - Сам прокси имеет read-only-доступ к ограниченному набору ресурсов.
- В кластер добавляется сервис, который экспортирует метаданные кластера наружу:
  - Корневой сертификат Istio (доступен без аутентификации).
  - Публичный адрес, через который доступен API-сервер (доступен только для аутентифицированных запросов из соседних кластеров).
  - Список публичных адресов сервиса `ingressgateway` (доступен только для аутентифицированных запросов из соседних кластеров).
  - Публичные ключи сервера для аутентификации запросов к API-серверу и закрытым метаданным (см. выше).

###### Управление мультикластером

<div data-presentation="presentations/multicluster_istio_multicluster_ru.pdf"></div>


Для сборки мультикластера необходимо в каждом кластере создать набор ресурсов `IstioMulticluster`, которые описывают все остальные кластеры.

#### Накладные расходы

Внедрение Istio повлечёт за собой дополнительные расходы ресурсов, как для **control-plane** (контроллер istiod), так и для **data-plane** (istio-сайдкары приложений).

##### control-plane

Контроллер istiod непрерывно наблюдает за конфигурацией кластера, компонует настройки для istio-сайдкаров data-plane и рассылает их по сети. Соответственно, чем больше приложений и их экземпляров, чем больше сервисов и чем чаще эта конфигурация меняется, тем больше требуется вычислительных ресурсов и больше нагрузка на сеть. При этом, поддерживается два подхода для снижения нагрузки на экземпляры контроллеров:

- горизонтальное масштабирование (настройка модуля [`controlPlane.replicasManagement`](configuration.html#parameters-controlplane-replicasmanagement)) — чем больше экземпляров контроллеров, тем меньше экземпляров istio-сайдкаров обслуживать каждому из них и тем меньше нагрузка на CPU и на сеть.
- сегментация data-plane с помощью ресурса [*Sidecar*](istio-cr.html#sidecar) (рекомендуемый подход) — чем меньше область видимости у отдельного istio-сайдкара, тем меньше требуется обновлять данных в data-plane и тем меньше нагрузка на CPU и на сеть.

Примерная оценка накладных расходов для экземпляра control-plane, который обслуживает 1000 сервисов и 2000 istio-сайдкаров — 1 vCPU и 1.5 GB RAM.

##### data-plane

На потребление ресурсов data-plane (istio-сайдкары) влияет множество факторов:

- количество соединений,
- интенсивность запросов,
- размер запросов и ответов,
- протокол (HTTP/TCP),
- количество ядер CPU,
- сложность конфигурации Service Mesh.

Примерная оценка накладных расходов для экземпляра istio-сайдкара — 0.5 vCPU на 1000 запросов/сек и 50 MB RAM.

istio-сайдкары также вносят задержку в сетевые запросы — примерно 2.5мс на запрос.

### Модуль kube-dns

Модуль устанавливает компоненты CoreDNS для управления DNS в кластере Kubernetes.

> **Внимание!** Модуль удаляет ранее установленные kubeadm'ом Deployment, ConfigMap и RBAC для CoreDNS.

### Модуль kube-proxy

Модуль удаляет весь комплект kube-proxy (`DaemonSet`, `ConfigMap`, `RBAC`) от `kubeadm` и устанавливает свой.

По умолчанию в целях безопасности при использовании сервисов с типом NodePort подключения принимаются только на InternalIP узлов. Для снятия данного ограничения предусмотрена аннотация на узел — `node.deckhouse.io/nodeport-bind-internal-ip: "false"`.

Пример аннотации для NodeGroup:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: myng
spec:
  nodeTemplate:
    annotations:
      node.deckhouse.io/nodeport-bind-internal-ip: "false"
...
```

> **Внимание!** После добавления, удаления или изменения значения аннотации необходимо самостоятельно выполнить рестарт подов kube-proxy.
>
> **Внимание!** Модуль kube-proxy автоматически отключается при включении модуля [cni-cilium](/modules/cni-cilium/).

### Модуль local-path-provisioner

Позволяет пользователям Kubernetes использовать локальное хранилище на узлах.

#### Как это работает?

Для каждого custom resource [LocalPathProvisioner](cr.html) создается соответствующий `StorageClass`.

Допустимая топология для `StorageClass` вычисляется на основе списка `nodeGroup` из custom resource. Топология используется при шедулинге подов.

Когда под заказывает диск, то:
- создается `HostPath` PV;
- `Provisioner` создает на нужном узле локальную папку по пути, состоящем из параметра `path` custom resource, имени PV и имени PVC.
  
  Пример пути:

  ```shell
  /opt/local-path-provisioner/pvc-d9bd3878-f710-417b-a4b3-38811aa8aac1_d8-monitoring_prometheus-main-db-prometheus-main-0
  ```

#### Ограничения

- Ограничение на размер диска не поддерживается для локальных томов.

### Модуль log-shipper

Модуль `log-shipper` упрощает настройку сбора логов в Kubernetes. Он позволяет быстро организовать сбор логов как с приложений, запущенных в кластере, так и с самих узлов, а затем отправлять их в любую систему хранения — внутреннюю или внешнюю (например, Loki, Elasticsearch, S3 и другие).

Возможности log-shipper:

- централизованно собирает и передаёт логи из кластера;
- фильтрует, преобразовывает и обогащает логи перед отправкой;
- настраивает маршрутизацию логов между различными источниками и приемниками.

![log-shipper architecture](images/log_shipper_architecture.svg)


1. Deckhouse следит за ресурсами [ClusterLoggingConfig](cr.html#clusterloggingconfig), [ClusterLogDestination](cr.html#clusterlogdestination) и [PodLoggingConfig](cr.html#podloggingconfig).
   Комбинация конфигурации для сбора логов и направления для отправки называется `pipeline`.
2. Deckhouse генерирует конфигурационный файл и сохраняет его в `Secret` в Kubernetes.
3. `Secret` монтируется всем подам агентов `log-shipper`, конфигурация обновляется при ее изменении с помощью sidecar-контейнера `reloader`.

#### Топологии отправки

Этот модуль отвечает за агентов на каждом узле. Подразумевается, что логи из кластера отправляются согласно одной из описанных ниже топологий.

##### Распределенная

Агенты шлют логи напрямую в хранилище, например в Loki или Elasticsearch.

![log-shipper distributed](images/log_shipper_distributed.svg)


* Менее сложная схема для использования.
* Доступна из коробки без лишних зависимостей, кроме хранилища.
* Сложные трансформации потребляют больше ресурсов на узлах для приложений.

##### Централизованная

Все логи отсылаются в один из доступных агрегаторов, например, Logstash, Vector.
Агенты на узлах стараются отправить логи с узла максимально быстро с минимальным потреблением ресурсов.
Сложные преобразования применяются на стороне агрегатора.

![log-shipper centralized](images/log_shipper_centralized.svg)


* Меньшее потребление ресурсов для приложений на узлах.
* Пользователи могут настроить в агрегаторе любые трансформации и слать логи в гораздо большее количество хранилищ.
* Количество выделенных узлов под агрегаторы может увеличиваться или уменьшаться в зависимости от нагрузки.

##### Потоковая

Главная задача этой архитектуры — как можно быстрее отправить логи в очередь сообщений, из которой они в служебном порядке будут переданы в долгосрочное хранилище для дальнейшего анализа.

![log-shipper stream](images/log_shipper_stream.svg)


* Те же плюсы и минусы, что и у централизованной архитектуры, но добавляется еще одно промежуточное хранилище.
* Повышенная надежность. Подходит тем, для кого доставка логов является наиболее критичной.

#### Метаданные

При сборе логов сообщения будут обогащены метаданными в зависимости от способа их сбора. Обогащение происходит на этапе `Source`.

##### Kubernetes

Следующие поля будут экспортированы:

| Label        | Pod spec path           |
|--------------|-------------------------|
| `pod`        | metadata.name           |
| `namespace`  | metadata.namespace      |
| `pod_labels` | metadata.labels         |
| `pod_ip`     | status.podIP            |
| `image`      | spec.containers[].image |
| `container`  | spec.containers[].name  |
| `node`       | spec.nodeName           |
| `pod_owner`  | metadata.ownerRef[0]    |

| Label        | Node spec path                            |
|--------------|-------------------------------------------|
| `node_group` | metadata.labels[].node.deckhouse.io/group |

{% alert -%}
Для Splunk поля `pod_labels` не экспортируются, потому что это вложенный объект, который не поддерживается в Splunk.
{%- endalert %}

##### File

Лейбл `host` - это единственный лейбл, в котором записан hostname сервера.

#### Фильтры сообщений

Существуют два фильтра для снижения количества отправляемых сообщений в хранилище, — `log filter` и `label filter`.

![log-shipper pipeline](images/log_shipper_pipeline.svg)


Они запускаются сразу после объединения строк с помощью multiline parser.

1. `label filter` — правила запускаются для метаданных сообщения. Поля для метаданных (или лейблов) наполняются на основании источника логов, и для разных источников будет разный набор полей. Эти правила необходимы, например, чтобы отбросить сообщения от определенного контейнера или пода с/без какой-то метки.
1. `log filter` — правила запускаются для исходного сообщения. Существует возможность отбрасывать сообщение на основе поля JSON или, если сообщение не имеет формата JSON, использовать регулярное выражение для поиска в строке.

Оба фильтра имеют одинаковую структурированную конфигурацию:

* `field` — источник данных для запуска фильтрации (чаще всего это значение лейбла или параметра из JSON-документа).
* `operator` — действие для сравнения, доступные варианты — In, NotIn, Regex, NotRegex, Exists, DoesNotExist.
* `values` — определяет разные значения для разных операторов:
  - DoesNotExist, Exists — не поддерживается;
  - In, NotIn — значение поля должно равняться или не равняться одному из значений в списке values;
  - Regex, NotRegex — значение должно подходить хотя бы под одно или не подходить ни под одно регулярное выражение из списка values.

Найти больше примеров можно в разделе [Примеры](examples.html) документации.

{% alert -%}
Extra labels добавляются на этапе `Destination`, поэтому невозможно фильтровать логи на их основании.
{%- endalert %}

### Модуль loki

В Kubernetes системные логи на узлах сохраняются недолго и могут быть утеряны при перезапуске или обновлении. Этот модуль разворачивает в кластере собственное хранилище оперативных логов на базе Grafana Loki.

Возможности:

- системные логи автоматически попадают в Loki без дополнительной настройки;
- доступ к логам реализован через Grafana и веб-интерфейс Deckhouse Platform Certified Security Edition (console);
- модуль предназначен для хранения логов в течение короткого времени. Для долгосрочного хранения или архивирования рекомендуется использовать внешние системы, поддерживаемые через [log-shipper](./log-shipper/).

### Автоматическая настройка системы мониторинга для сбора метрик с пользовательских приложений

Для мониторинга пользовательских приложений, запущенных в кластере, требуется дополнительная настройка. Этот модуль упрощает процесс настройки, сводя её к указанию для нужного приложения определённого лейбла.

Чтобы организовать сбор метрик с приложений модулем `monitoring-custom`, необходимо:

- Поставить лейбл `prometheus.deckhouse.io/custom-target` на Service или под. Значение лейбла определит имя в списке target'ов Prometheus.
  - В качестве значения лейбла `prometheus.deckhouse.io/custom-target` рекомендуется использовать название приложения (маленькими буквами, разделитель `-`), которое позволяет его уникально идентифицировать в кластере.

     Если приложение ставится в кластер больше одного раза (staging, testing и т. д.) или даже ставится несколько раз в одно пространство имён, достаточно одного общего названия, так как у всех метрик в любом случае будут лейблы `namespace`, `pod` и, если доступ осуществляется через Service, лейбл `service`. Это название, уникально идентифицирующее приложение в кластере, а не его единичную инсталляцию.
- Порту, с которого нужно собирать метрики, указать имя `http-metrics` и `https-metrics` для подключения по HTTP или HTTPS соответственно.

  Если это невозможно (например, порт уже определен и назван другим именем), необходимо воспользоваться аннотациями: `prometheus.deckhouse.io/port: номер_порта` — для указания порта и `prometheus.deckhouse.io/tls: "true"` — если сбор метрик будет проходить по HTTPS.

  > При указании аннотации на Service в качестве значения порта необходимо использовать `targetPort`. То есть тот порт, что открыт и слушается приложением, а не порт Service'а.

  - Пример 1:

    ```yaml
    ports:
    - name: https-metrics
      containerPort: 443
    ```

  - Пример 2:

    ```yaml
    annotations:
      prometheus.deckhouse.io/port: "443"
      prometheus.deckhouse.io/tls: "true"  # Если метрики отдаются по HTTP, эту аннотацию указывать не нужно.
    ```

- При использовании service mesh [Istio](./istio/) в режиме STRICT mTLS указать для сбора метрик следующую аннотацию у Service или Pod: `prometheus.deckhouse.io/istio-mtls: "true"`. Важно, что метрики приложения должны экспортироваться по протоколу HTTP без TLS.

- *(Необязательно)* Укажите дополнительные аннотации для более тонкой настройки:

  * `prometheus.deckhouse.io/path` — путь для сбора метрик (по умолчанию: `/metrics`).
  * `prometheus.deckhouse.io/query-param-$name` — GET-параметры, будут преобразованы в map вида `$name=$value` (по умолчанию: ''):
    - возможно указать несколько таких аннотаций.

      Например, `prometheus.deckhouse.io/query-param-foo=bar` и `prometheus.deckhouse.io/query-param-bar=zxc` будут преобразованы в query: `http://...?foo=bar&bar=zxc`.
  * `prometheus.deckhouse.io/allow-unready-pod` — разрешает сбор метрик с подов в любом состоянии (по умолчанию метрики собираются только с подов в состоянии Ready). Эта опция полезна в редких случаях. Например, если ваше приложение запускается очень долго (при старте загружаются данные в базу или прогреваются кэши), но в процессе запуска уже отдаются полезные метрики, которые помогают следить за запуском приложения.
  * `prometheus.deckhouse.io/sample-limit` — сколько семплов разрешено собирать с пода (по умолчанию 5000). Значение по умолчанию защищает от ситуации, когда приложение внезапно начинает отдавать слишком большое количество метрик, что может нарушить работу всего мониторинга. Аннотация должна быть размещена на том же ресурсе, на котором висит лейбл  `prometheus.deckhouse.io/custom-target`.

##### Пример: Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    prometheus.deckhouse.io/custom-target: my-app
  annotations:
    prometheus.deckhouse.io/port: "8061"                      # По умолчанию будет использоваться порт сервиса с именем http-metrics или https-metrics.
    prometheus.deckhouse.io/path: "/my_app/metrics"           # По умолчанию /metrics.
    prometheus.deckhouse.io/query-param-format: "prometheus"  # По умолчанию ''.
    prometheus.deckhouse.io/allow-unready-pod: "true"         # По умолчанию поды НЕ в Ready игнорируются.
    prometheus.deckhouse.io/sample-limit: "5000"              # По умолчанию принимается не больше 5000 метрик от одного пода.
spec:
  ports:
  - name: my-app
    port: 8060
  - name: http-metrics
    port: 8061
    targetPort: 8061
  selector:
    app: my-app
```

##### Пример: Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
        prometheus.deckhouse.io/custom-target: my-app
      annotations:
        prometheus.deckhouse.io/sample-limit: "5000"  # По умолчанию принимается не больше 5000 метрик от одного пода.
    spec:
      containers:
      - name: my-app
        image: my-app:1.7.9
        ports:
        - name: https-metrics
          containerPort: 443
```

### Мониторинг control plane

Мониторинг control plane осуществляется с помощью модуля `monitoring-kubernetes-control-plane`, который организует безопасный сбор метрик и предоставляет базовый набор правил мониторинга следующих компонентов кластера:
* kube-apiserver;
* kube-controller-manager;
* kube-scheduler;
* kube-etcd.

### Модуль monitoring-kubernetes

Модуль `monitoring-kubernetes` обеспечивает прозрачный и своевременный контроль состояния всех узлов кластера и ключевых инфраструктурных компонентов.

Возможности модуля:

- предоставляет возможность планировать ресурсы инфраструктуры (Capacity planning);
- отслеживает версию container runtime (docker, containerd) на каждом узле и проверяет её на соответствие разрешенным версиям;
- контролирует работоспособность самой подсистемы мониторинга кластера (Dead man’s switch);
- снимает метрики о доступности файловых дескрипторов, сокетов, свободного места и inode на каждом узле;
- следит за корректной работой ключевых компонентов мониторинга: kube-state-metrics, node-exporter, kube-dns;
- проверяет состояние всех узлов (`NotReady`, `drain`, `cordon`) и своевременно сигнализирует о неполадках;
- следит за синхронизацией времени и уведомляет об отклонениях;
- выявляет случаи продолжительного превышения CPU steal (когда узел не получает нужного времени процессора);
- контролирует состояние таблицы Conntrack на узлах;
- показывает поды с некорректными статусами — например, если kubelet не справился со своей работой;
- позволяет экспортировать метрики во внешние системы мониторинга для единой точки контроля.

### Модуль monitoring-ping

#### Описание

Модуль мониторинга сетевого взаимодействия обеспечивает непрерывную проверку связности между всеми основными и, при необходимости, внешними узлами кластера.

Возможности и особенности модуля:

- автоматически проверяет доступность всех узлов кластера (и, опционально, внешних систем) с помощью ICMP (ping) — тестирование запускается каждые две секунды;
- все результаты экспортируются в формате метрик в систему мониторинга Prometheus;
- в комплекте — готовый дашборд для Grafana, где в реальном времени визуализируются текущая доступность, графики задержек и потенциальные проблемы с сетевой связностью;
- позволяет быстро выявлять узлы с деградировавшей связностью и ускоряет реакцию на инциденты.

#### Как работает

Модуль отслеживает любые изменения поля `.status.addresses` узла. Если они обнаружены, срабатывает хук, который собирает полный список имен узлов и их адресов и передает в daemonSet, который заново создает поды.
Таким образом, `ping` проверяет всегда актуальный список узлов.

### Модуль multitenancy-manager

#### Описание

Модуль позволяет создавать проекты в кластере Kubernetes. **Проект** — это изолированное окружение, в котором можно развернуть приложения.

#### Для чего это нужно?

Стандартный ресурс `Namespace`, который используется для логического разделения ресурсов в Kubernetes, не предоставляет необходимых функций, поэтому не является изолированным окружением:

* Потребление ресурсов подами по умолчанию не ограничено;
* Сетевое взаимодействие с другими подами по умолчанию работает из любой точки кластера;
* Неограниченный доступ к ресурсам узла: адресное пространство, сетевое пространство, смонтированные директории хоста.

Возможности настройки пространств имен `Namespace` не полностью соответствуют современным требованиям к разработке. По умолчанию для `Namespace` не включены следующие функции:

* Сборка логов;
* Аудит;
* Сканирование уязвимостей.

Функционал проектов позволяет решить эти проблемы.

{% alert level="warning" %}
Модуль [`secret-copier`](./secret-copier/) не может использоваться совместно с модулем `multitenancy-manager`.
{% endalert %}

#### Преимущества модуля

Для администраторов платформы:

* **Единообразие**: Администраторы могут создавать проекты, используя один и тот же шаблон, что обеспечивает единообразие и упрощает управление.
* **Безопасность**: Проекты обеспечивают изоляцию ресурсов и политик доступа между различными проектами, что поддерживает безопасное многотенантное окружение.
* **Потребление ресурсов**: Администраторы могут легко устанавливать квоты на ресурсы и ограничения для каждого проекта, предотвращая избыточное использование ресурсов.

Для пользователей платформы:

* **Быстрый старт**: Разработчики могут запрашивать у администраторов проекты, созданные по готовым шаблонам, что позволяет быстро начать разработку нового приложения.
* **Изоляция**: Каждый проект обеспечивает изолированное окружение, где разработчики могут развертывать и тестировать свои приложения без влияния на другие проекты.

#### Ограничения

Модуль работает только в рамках перечисленных ниже ограничений:

- Создание более одного пространства имён внутри проекта не предусматривается. Если требуется несколько пространств имён, создайте отдельный проект для каждого из них.
- Ресурсы шаблона применяются только к одному пространству имён, имя которого совпадает с именем проекта.

#### Внутренняя логика работы

##### Создание проекта

Для создания проекта используются ресурсы:

* [ProjectTemplate](./cr.html#projecttemplate) — ресурс, который описывает шаблон проекта. При помощи него задается список ресурсов, которые будут созданы в проекте, а также схема параметров, которые можно передать при создании проекта;
* [Project](./cr.html#project) — ресурс, который описывает конкретный проект.

При создании ресурса [Project](./cr.html#project) из определенного [ProjectTemplate](./cr.html#projecttemplate) происходит следующее:

1. Переданные [параметры](./cr.html#project-v1alpha2-spec-parameters) валидируются по OpenAPI-спецификации (параметр [`parametersSchema.openAPIV3Schema`
](./cr.html#projecttemplate-v1alpha1-spec-parametersschema-openapiv3schema) ресурса [ProjectTemplate](./cr.html#projecttemplate));
1. Выполняется рендеринг [шаблона для ресурсов](./cr.html#projecttemplate-v1alpha1-spec-resourcestemplate) с помощью Helm. Значения для рендеринга берутся из параметра [`parameters`](./cr.html#project-v1alpha2-spec-parameters) ресурса [Project](./cr.html#project);
1. Cоздается `Namespace` с именем, которое совпадает c именем [Project](./cr.html#project);
1. По очереди создаются все ресурсы, описанные в шаблоне.

> **Внимание!** При изменении шаблона проекта, все созданные проекты будут обновлены в соответствии с новым шаблоном.

##### Изоляция проекта

В основе проекта используется механизм изоляции ресурсов в рамках пространства имен (`Namespace`).
Пространства имен позволяют группировать поды, сервисы, секреты и другие объекты, но не обеспечивают полноценной изоляции.
Проект расширяет функциональность пространств имен, предлагая дополнительные инструменты для повышения уровня контроля и безопасности.
Для управления уровнем изоляции проекта можно использовать возможности Kubernetes, например:

* Ресурсы контроля доступа (`AuthorizationRule` / `RoleBinding`) — позволяют управлять взаимодействием объектов внутри `Namespace`. Вы можете задавать правила и назначать роли, чтобы точно контролировать, кто и что может делать в вашем проекте.
* Ресурсы контроля использования нагрузки (`ResourceQuota`) — с их помощью можно задать лимиты на использование процессорного времени (CPU), оперативной памяти (RAM), а также количества объектов внутри `Namespace`. Это помогает избежать чрезмерной нагрузки и обеспечивает мониторинг за приложениями в рамках проекта.
* Ресурсы контроля сетевой связности (`NetworkPolicy`) — управляют входящим и исходящим сетевым трафиком в `Namespace`. Таким образом, можно настроить разрешенные подключения между подами, улучшить безопасность и управляемость сетевого взаимодействия в рамках проекта.

Эти инструменты можно комбинировать, чтобы настроить проект в соответствии с требованиями вашего приложения.

### Модуль namespace-configurator

Модуль namespace-configurator позволяет автоматически управлять аннотациями и лейблами на пространствах имён.

Чтобы автоматически включать новые пространства имён в мониторинг, добавьте лейбл `extended-monitoring.deckhouse.io/enabled=true`.

#### Как работает

Модуль следит за изменениями пространства имён и своей конфигурации:

* назначает лейблы и аннотации из конфигурации всем пространствам имён, подпадающим под шаблон `includeNames` и не подпадающим под шаблон `excludeNames`;
* игнорируются пространства имён с метками `heritage` с одним из значений `upmeter`, `deckhouse` или `multitenancy-manager`;
* при изменении конфигурации модуля лейблы и аннотации пространств имён будут переназначены согласно конфигурации.

#### Что нужно настроить?

Перечислите список желаемых лейблов и аннотаций, а также список шаблонов поиска пространств имён в конфигурации модуля.

### Модуль network-policy-engine

{% alert level="warning" %}
Не используйте модуль, если включен модуль <a href="./cni-cilium/">cilium</a>, так как в нем уже есть функционал управления сетевыми политиками.
{% endalert %}

Модуль управления сетевыми политиками.

В Deckhouse выбран консервативный подход к организации сети, при котором используются простые сетевые бэкенды (*«чистый»* CNI или flannel в режиме `host-gw`). Этот подход прост, надежен и показал себя с лучшей стороны.

Имплементация сетевых политик (`NetworkPolicy`) в Deckhouse также является простой и надежной системой, основанной на базе `kube-router` в режиме *Network Policy Controller* (`--run-firewall`). В этом случае `kube-router` транслирует сетевые политики `NetworkPolicy` в правила `iptables`, которые работают в любых инсталляциях вне зависимости от облака или используемого CNI.

Модуль `network-policy-engine` разворачивает в namespace `d8-system` DaemonSet с kube-router в режиме поддержки network policies. В результате в Kubernetes-кластере включается полная поддержка Network Policies.

Поддерживаются следующие форматы описания политик:

- *networking.k8s.io/NetworkPolicy API;*
- *network policy V1/GA semantics;*
- *network policy beta semantics.*

### Управление узлами

#### Основные функции

Управление узлами осуществляется с помощью модуля `node-manager`, основные функции которого:

1. Управление несколькими узлами как связанной группой (**NodeGroup**):
    * Возможность определить метаданные, которые наследуются всеми узлами группы.
    * Мониторинг группы узлов как единой сущности (группировка узлов на графиках по группам, группировка алертов о недоступности узлов, алерты о недоступности N узлов или N% узлов группы).
2. Систематическое прерывание работы узлов — **Chaos Monkey**. Предназначено для верификации отказоустойчивости элементов кластера и запущенных приложений.
3. Установка/обновление и настройка ПО узла (containerd, kubelet и др.), подключение узла в кластер:
    * Установка операционной системы вне зависимости от типа используемой инфраструктуры (в любом облаке или на любом железе).
    * Базовая настройка операционной системы (отключение автообновления, установка необходимых пакетов, настройка параметров журналирования и т. д.).
    * Настройка nginx (и системы автоматического обновления перечня upstream’ов) для балансировки запросов от узла (kubelet) по API-серверам.
    * Установка и настройка CRI containerd и Kubernetes, включение узла в кластер.
    * Управление обновлениями узлов и их простоем (disruptions):
        * Автоматическое определение допустимой минорной версии Kubernetes группы узлов на основании ее
          настроек (указанной для группы kubernetesVersion), версии по умолчанию для всего кластера и текущей
          действительной версии control plane (не допускается обновление узлов в опережение обновления control plane).
        * Из группы одновременно производится обновление только одного узла и только если все узлы группы доступны.
        * Два варианта обновлений узлов:
            * обычные — всегда происходят автоматически;
            * требующие disruption (например, обновление ядра, смена версии containerd, значительная смена версии kubelet и пр.) — можно выбрать ручной или автоматический режим. В случае, если разрешены автоматические disruptive-обновления, перед обновлением производится drain узла (можно отключить).
    * Мониторинг состояния и прогресса обновления.
4. Масштабирование кластера.
   * Автоматическое масштабирование.

     Доступно при использовании поддерживаемых облачных провайдеров ([подробнее](#масштабирование-узлов-в-облаке)) и недоступно для статических узлов. Облачный провайдер в автоматическом режиме может создавать или удалять виртуальные машины, подключать их к кластеру или отключать.

   * Поддержание желаемого количества узлов в группе.

     Доступно как для [облачных провайдеров](#масштабирование-узлов-в-облаке), так и для статических узлов (при использовании [Cluster API Provider Static](#работа-со-статическими-узлами)).
5. Управление Linux-пользователями на узлах.
6. Поддержка GPU на узлах:
   * Автоматическое обнаружение и настройка NVIDIA GPU.
   * Различные режимы разделения ресурсов GPU на уровне NodeGroup: Exclusive, TimeSlicing, MIG.
   * Интеграция с мониторингом — в Grafana доступны готовые дашборды с ключевыми метриками GPU.

Управление узлами осуществляется через управление группой узлов (ресурс [NodeGroup](cr.html#nodegroup)), где каждая такая группа выполняет определенные для нее задачи. Примеры групп узлов по выполняемым задачам:

- группы master-узлов;
- группа узлов маршрутизации HTTP(S)-трафика (front-узлы);
- группа узлов мониторинга;
- группа узлов приложений (worker-узлы) и т. п.

Узлы в группе имеют общие параметры и настраиваются автоматически в соответствии с параметрами группы. Deckhouse масштабирует группы, добавляя, исключая и обновляя ее узлы. Допускается иметь в одной группе как развернутые в облаке узлы типа Static, так и статические узлы (серверы bare-metal, виртуальные машины). Это позволяет получать узлы на физических серверах, которые могут масштабироваться за счет облачных узлов (гибридные кластеры).

Работа в [облачной инфраструктуре](#работа-с-узлами-в-поддерживаемых-облаках) осуществляется с помощью поддерживаемых облачных провайдеров. Если поддержки необходимой облачной платформы нет, возможно использование ее ресурсов в виде статических узлов.

Работа со [статическими узлами](#работа-со-статическими-узлами) (например, серверами bare metal) выполняется с помощью в провайдера CAPS (Cluster API Provider Static).

#### Типы узлов

Типы узлов, с которыми возможна работа в группах узлов (ресурс [NodeGroup](cr.html#nodegroup)):

- `CloudEphemeral` — узлы автоматически заказываются, создаются и удаляются в настроенном облачном провайдере.
- `CloudPermanent` — узлы отличаются тем, что их конфигурация берется не из custom resource [nodeGroup](cr.html#nodegroup), а из специального ресурса `<PROVIDER>ClusterConfiguration`. Также важное отличие узлов  в том, что для применения их конфигурации необходимо выполнить `dhctl converge` (запустив инсталлятор Deckhouse). Примером CloudPermanent-узла облачного кластера является master-узел кластера.  
- `CloudStatic` — узел, созданный *вручную* (статический узел), размещенный в том же облаке, с которым настроена интеграция у одного из облачных провайдеров. На таком узле работает CSI и такой узел управляется `cloud-controller-manager'ом`. Объект `Node` кластера обогащается информацией о зоне и регионе, в котором работает узел. Также при удалении узла из облака соответствующий ему Node-объект будет удален в кластере.
- `Static` — статический узел, размещенный на сервере bare metal или виртуальной машине. В случае облака, такой узел не управляется `cloud-controller-manager'ом`, даже если включен один из облачных провайдеров. [Подробнее про работу со статическими узлами...](#работа-со-статическими-узлами)

#### Группировка узлов и управление группами

Группировка и управление узлами как связанной группой означает, что все узлы группы будут иметь одинаковые метаданные, взятые из кастомного ресурса [`NodeGroup`](cr.html#nodegroup).

Для групп узлов доступен мониторинг:

- с группировкой параметров узлов на графиках группы;
- с группировкой алертов о недоступности узлов;
- с алертами о недоступности N узлов или N% узлов группы и т. п.

#### Автоматическое развертывание, настройка и обновление узлов Kubernetes

Автоматическое развертывание (в *static/hybrid* — частично), настройка и дальнейшее обновление ПО работают на любых кластерах, независимо от его размещения в облаке или на bare metal.

##### Развертывание узлов Kubernetes

Deckhouse автоматически разворачивает узлы кластера, выполняя следующие **идемпотентные** операции:

- Настройку и оптимизацию операционной системы для работы с containerd и Kubernetes:
  - устанавливаются требуемые пакеты из репозиториев дистрибутива;
  - настраиваются параметры работы ядра, параметры журналирования, ротация журналов и другие параметры системы.
- Установку требуемых версий containerd и kubelet, включение узла в кластер Kubernetes.
- Настройку Nginx и обновление списка upstream для балансировки запросов от узла к Kubernetes API.

##### Поддержка актуального состояния узлов

Для поддержания узлов кластера в актуальном состоянии могут применяться два типа обновлений:

- **Обычные**. Такие обновления всегда применяются автоматически, и не приводят к остановке или перезагрузке узла.
- **Требующие прерывания** (disruption). Пример таких обновлений — обновление версии ядра или containerd, значительная смена версии kubelet и т. д. Для этого типа обновлений можно выбрать ручной или автоматический режим (секция параметров [disruptions](cr.html#nodegroup-v1-spec-disruptions)). В автоматическом режиме перед обновлением выполняется корректная приостановка работы узла (drain) и только после этого производится обновление.

В один момент времени производится обновление только одного узла из группы и только в том случае, когда все узлы группы доступны.

Модуль `node-manager` имеет набор встроенных метрик мониторинга, которые позволяют контролировать прогресс обновления, получать уведомления о возникающих во время обновления проблемах или о необходимости получения разрешения на обновление (ручное подтверждение обновления).

#### Работа с узлами в поддерживаемых облаках

У каждого поддерживаемого облачного провайдера существует возможность автоматического заказа узлов. Для этого необходимо указать требуемые параметры для каждого узла или группы узлов.

В зависимости от провайдера этими параметрами могут быть:

- тип узлов или количество ядер процессора и объем оперативной памяти;
- размер диска;
- настройки безопасности;
- подключаемые сети и др.

Создание, запуск и подключение виртуальных машин к кластеру выполняются автоматически.

##### Масштабирование узлов в облаке

Возможны два режима масштабирования узлов в группе:

- **Автоматическое масштабирование**.

  При дефиците ресурсов, наличии подов в состоянии `Pending`, в группу будут добавлены узлы. При отсутствии нагрузки на один или несколько узлов, они будут удалены из кластера. При работе автомасштабирования учитывается [приоритет](cr.html#nodegroup-v1-spec-cloudinstances-priority) группы (в первую очередь будет масштабироваться группа, у которой приоритет больше).
  
  Чтобы включить автоматическое масштабирование узлов, необходимо указать разные ненулевые значения [минимального](cr.html#nodegroup-v1-spec-cloudinstances-minperzone) и [максимального](cr.html#nodegroup-v1-spec-cloudinstances-maxperzone) количества узлов в группе.

- **Фиксированное количество узлов.**

  В этом случае Deckhouse будет поддерживать указанное количество узлов (например, заказывая новые в случае выхода из строя старых узлов).

  Чтобы указать фиксированное количество узлов в группе и отключить автоматическое масштабирование, необходимо указать одинаковые значения параметров [minPerZone](cr.html#nodegroup-v1-spec-cloudinstances-minperzone) и [maxPerZone](cr.html#nodegroup-v1-spec-cloudinstances-maxperzone).

#### Работа со статическими узлами

При работе со статическими узлами функции модуля `node-manager` выполняются со следующими ограничениями:

- **Отсутствует заказ узлов.** Непосредственное выделение ресурсов (серверов bare metal, виртуальных машин, связанных ресурсов) выполняется вручную. Дальнейшая настройка ресурсов  (подключение узла к кластеру, настройка мониторинга и т.п.) выполняются полностью автоматически (аналогично узлам в облаке) или частично.
- **Отсутствует автоматическое масштабирование узлов.** Доступно поддержание в группе указанного количества узлов при использовании [Cluster API Provider Static](#cluster-api-provider-static) (параметр [staticInstances.count](cr.html#nodegroup-v1-spec-staticinstances-count)). Т.е. Deckhouse будет пытаться поддерживать указанное количество узлов в группе, очищая лишние узлы и настраивая новые при необходимости (выбирая их из ресурсов [StaticInstance](cr.html#staticinstance), находящихся в состоянии *Pending*).

Настройка/очистка узла, его подключение к кластеру и отключение могут выполняться следующими способами:

- **Вручную,** с помощью подготовленных скриптов.

  Для настройки сервера (ВМ) и ввода узла в кластер нужно загрузить и выполнить специальный bootstrap-скрипт. Такой скрипт генерируется для каждой группы статических узлов (каждого ресурса `NodeGroup`). Он находится в секрете `d8-cloud-instance-manager/manual-bootstrap-for-<ИМЯ-NODEGROUP>`. Пример добавления статического узла в кластер можно найти в [FAQ](examples.html#вручную).

  Для отключения узла кластера и очистки сервера (виртуальной машины) нужно выполнить скрипт `/var/lib/bashible/cleanup_static_node.sh`, который уже находится на каждом статическом узле. Пример отключения узла кластера и очистки сервера можно найти в [FAQ](faq.html#как-вручную-очистить-статический-узел).

- **Автоматически,** с помощью [Cluster API Provider Static](#cluster-api-provider-static).

  Cluster API Provider Static (CAPS) подключается к серверу (ВМ) используя ресурсы [StaticInstance](cr.html#staticinstance) и [SSHCredentials](cr.html#sshcredentials), выполняет настройку, и вводит узел в кластер.

  При необходимости (например, если удален соответствующий серверу ресурс [StaticInstance](cr.html#staticinstance) или уменьшено [количество узлов группы](cr.html#nodegroup-v1-spec-staticinstances-count)), Cluster API Provider Static подключается к узлу кластера, очищает его и отключает от кластера.

- **Вручную с последующей передачей узла под автоматическое управление** [Cluster API Provider Static](#cluster-api-provider-static).

  > Функциональность доступна начиная с версии Deckhouse 1.63.

  Для передачи существующего узла кластера под управление CAPS необходимо подготовить для этого узла ресурсы [StaticInstance](cr.html#staticinstance) и [SSHCredentials](cr.html#sshcredentials), как при автоматическом управлении в пункте выше, однако ресурс [StaticInstance](cr.html#staticinstance) должен дополнительно быть помечен аннотацией `static.node.deckhouse.io/skip-bootstrap-phase: ""`.

##### Cluster API Provider Static

Cluster API Provider Static (CAPS), это реализация провайдера декларативного управления статическими узлами (серверами bare metal или виртуальными машинами) для проекта Cluster API Kubernetes. По сути, CAPS это дополнительный слой абстракции к уже существующему функционалу Deckhouse по автоматической настройке и очистке статических узлов с помощью скриптов, генерируемых для каждой группы узлов (см. раздел [Работа со статическими узлами](#работа-со-статическими-узлами)).

CAPS выполняет следующие функции:

- настройка сервера bare metal (или виртуальной машины) для подключения к кластеру Kubernetes;
- подключение узла в кластер Kubernetes;
- отключение узла от кластера Kubernetes;
- очистка сервера bare metal (или виртуальной машины) после отключения узла из кластера Kubernetes.

CAPS использует следующие ресурсы (CustomResource) при работе:

- **[StaticInstance](cr.html#staticinstance).** Каждый ресурс `StaticInstance` описывает конкретный хост (сервер, ВМ), который управляется с помощью CAPS.
- **[SSHCredentials](cr.html#sshcredentials)**. Содержит данные SSH, необходимые для подключения к хосту (`SSHCredentials` указывается в параметре [credentialsRef](cr.html#staticinstance-v1alpha1-spec-credentialsref) ресурса `StaticInstance`).
- **[NodeGroup](cr.html#nodegroup)**. Секция параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances) определяет необходимое количество узлов в группе и фильтр множества ресурсов `StaticInstance` которые могут использоваться в группе.

CAPS включается автоматически, если в NodeGroup заполнена секция параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances). Если в `NodeGroup` секция параметров `staticInstances` не заполнена, то настройка и очистка узлов для работы в этой группе выполняется *вручную* (см. примеры [добавления статического узла в кластер](examples.html#вручную) и [очистки узла](faq.html#как-вручную-очистить-статический-узел)), а не с помощью CAPS.

Схема работы со статичными узлами при использовании Cluster API Provider Static (CAPS) ([практический пример добавления узла](examples.html#с-помощью-cluster-api-provider-static)):

1. **Подготовка ресурсов.**

   Перед тем, как отдать bare-metal-сервер или виртуальную машину под управление CAPS, может быть необходима предварительная подготовка, например:

   - Подготовка системы хранения, добавление точек монтирования и т. п.;
   - Установка специфических пакетов ОС;
   - Настройка необходимой сетевой связности. Например, между сервером и узлами кластера;
   - Настройка доступа по SSH на сервер, создание пользователя для управления с root-доступом через `sudo`. Хорошей практикой является создание отдельного пользователя и уникальных ключей для каждого сервера.

1. **Создание ресурса [SSHCredentials](cr.html#sshcredentials).**

   В ресурсе `SSHCredentials` указываются параметры, необходимые CAPS для подключения к серверу по SSH. Один ресурс `SSHCredentials` может использоваться для подключения к нескольким серверам, но хорошей практикой является создание уникальных пользователей и ключей доступа для подключения к каждому серверу. В этом случае ресурс `SSHCredentials` также будет отдельный на каждый сервер.

1. **Создание ресурса [StaticInstance](cr.html#staticinstance).**

   На каждый сервер (ВМ) в кластере создается отдельный ресурс `StaticInstance`. В нем указан IP-адрес для подключения и ссылка на ресурс `SSHCredentials`, данные которого нужно использовать при подключении.

   Возможные состояния `StaticInstances` и связанных с ним серверов (ВМ) и узлов кластера:
   - `Pending`. Сервер не настроен, и в кластере нет соответствующего узла.
   - `Bootstrapping`. Выполняется процедура настройки сервера (ВМ) и подключения узла в кластер.
   - `Running`. Сервер настроен, и в кластер добавлен соответствующий узел.
   - `Cleaning`. Выполняется процедура очистки сервера и отключение узла из кластера.

   > Можно отдать существующий узел кластера, заранее введенный в кластер вручную, под управление CAPS, пометив его StaticInstance аннотацией `static.node.deckhouse.io/skip-bootstrap-phase: ""`.

1. **Создание ресурса [NodeGroup](cr.html#nodegroup).**

   В контексте CAPS в ресурсе `NodeGroup` нужно обратить внимание на параметр [nodeType](cr.html#nodegroup-v1-spec-nodetype) (должен быть `Static`) и секцию параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances).

   Секция параметров [staticInstances.labelSelector](cr.html#nodegroup-v1-spec-staticinstances-labelselector) определяет фильтр, по которому CAPS выбирает ресурсы `StaticInstance` для использования в группе. Фильтр позволяет использовать для разных групп узлов только определенные `StaticInstance`, а также позволяет использовать один `StaticInstance` в разных группах узлов. Фильтр можно не определять, чтобы использовать в группе узлов любой доступный `StaticInstance`.

   Параметр [staticInstances.count](cr.html#nodegroup-v1-spec-staticinstances-count) определяет желаемое количество узлов в группе.  При изменении параметра, CAPS начинает добавлять или удалять необходимое количество узлов, запуская этот процесс параллельно.

В соответствии с данными секции параметров [staticInstances](cr.html#nodegroup-v1-spec-staticinstances), CAPS будет пытаться поддерживать указанное (параметр [count](cr.html#nodegroup-v1-spec-staticinstances-count)) количество узлов в группе. При необходимости добавить узел в группу, CAPS выбирает соответствующий [фильтру](cr.html#nodegroup-v1-spec-staticinstances-labelselector) ресурс [StaticInstance](cr.html#staticinstance) находящийся в статусе `Pending`, настраивает сервер (ВМ) и добавляет узел в кластер. При необходимости удалить узел из группы, CAPS выбирает [StaticInstance](cr.html#staticinstance) находящийся в статусе `Running`, очищает сервер (ВМ) и удаляет узел из кластера (после чего, соответствующий `StaticInstance` переходит в состояние `Pending` и снова может быть использован).

#### Пользовательские настройки на узлах

Для автоматизации действий на узлах группы предусмотрен ресурс [NodeGroupConfiguration](cr.html#nodegroupconfiguration). Ресурс позволяет выполнять на узлах bash-скрипты, в которых можно пользоваться набором команд bashbooster. Это удобно для автоматизации таких операций, как:

- Установка и настройки дополнительных пакетов ОС.  

  Примеры:  
  - [установка kubectl-плагина](examples.html#установка-плагина-cert-manager-для-kubectl-на-master-узлах);
  - [настройка containerd с поддержкой Nvidia GPU](faq.html#как-использовать-containerd-с-поддержкой-nvidia-gpu).

- Обновление ядра ОС на конкретную версию.

  Примеры:
  - [обновление ядра Debian](faq.html#для-дистрибутивов-основанных-на-debian);
  - [обновление ядра CentOS](faq.html#для-дистрибутивов-основанных-на-centos).

- Изменение параметров ОС.

  Примеры:  
  - [настройка параметра sysctl](examples.html#задание-параметра-sysctl);
  - [добавление корневого сертификата](examples.html#добавление-корневого-сертификата-в-хост).

- Сбор информации на узле и выполнение других подобных действий.

Ресурс `NodeGroupConfiguration` позволяет указывать [приоритет](cr.html#nodegroupconfiguration-v1alpha1-spec-weight) выполняемым скриптам, ограничивать их выполнение определенными [группами узлов](cr.html#nodegroupconfiguration-v1alpha1-spec-nodegroups) и [типами ОС](cr.html#nodegroupconfiguration-v1alpha1-spec-bundles).

Код скрипта указывается в параметре [content](cr.html#nodegroupconfiguration-v1alpha1-spec-content) ресурса. При создании скрипта на узле содержимое параметра `content` проходит через шаблонизатор Go Template, который позволят встроить дополнительный уровень логики при генерации скрипта. При прохождении через шаблонизатор становится доступным контекст с набором динамических переменных.

Переменные, которые доступны для использования в шаблонизаторе:
<ul>
<li><code>.cloudProvider</code> (для групп узлов с nodeType <code>CloudEphemeral</code> или <code>CloudPermanent</code>) — массив данных облачного провайдера.
</li>
<li><code>.cri</code> — используемый CRI (с версии Deckhouse 1.49 используется только <code>Containerd</code>).</li>
<li><code>.kubernetesVersion</code> — используемая версия Kubernetes.</li>
<li><code>.nodeUsers</code> — массив данных о пользователях узла, добавленных через ресурс <a href="cr.html#nodeuser">NodeUser</a>.
{% offtopic title="Пример данных..." %}
```yaml
nodeUsers:
- name: user1
  spec:
    isSudoer: true
    nodeGroups:
    - '*'
    passwordHash: PASSWORD_HASH
    sshPublicKey: SSH_PUBLIC_KEY
    uid: 1050
```
{% endofftopic %}
</li>
<li><code>.nodeGroup</code> — массив данных группы узлов.
{% offtopic title="Пример данных..." %}
```yaml
nodeGroup:
  cri:
    type: Containerd
  disruptions:
    approvalMode: Automatic
  kubelet:
    containerLogMaxFiles: 4
    containerLogMaxSize: 50Mi
    resourceReservation:
      mode: "Off"
  kubernetesVersion: "1.29"
  manualRolloutID: ""
  name: master
  nodeTemplate:
    labels:
      node-role.kubernetes.io/control-plane: ""
      node-role.kubernetes.io/master: ""
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
  nodeType: CloudPermanent
  updateEpoch: "1699879470"
```
{% endofftopic %}</li>
</ul>

{% raw %}
Пример использования переменных в шаблонизаторе:

```shell
{{- range .nodeUsers }}
echo 'Tuning environment for user {{ .name }}'
### Some code for tuning user environment
{{- end }}
```

Пример использования команд bashbooster:

```shell
bb-event-on 'bb-package-installed' 'post-install'
post-install() {
  bb-log-info "Setting reboot flag due to kernel was updated"
  bb-flag-set reboot
}
```

{% endraw %}

Ход выполнения скриптов можно увидеть на узле в журнале сервиса bashible c помощью команды:

```bash
journalctl -u bashible.service
```  

Сами скрипты находятся на узле в директории `/var/lib/bashible/bundle_steps/`.  

Сервис принимает решение о повторном запуске скриптов путем сравнения единой контрольной суммы всех файлов, расположенной по пути `/var/lib/bashible/configuration_checksum` с контрольной суммой размещенной в кластере `kubernetes` в секрете `configuration-checksums` namespace `d8-cloud-instance-manager`.
Проверить контрольную сумму можно следующей командой:  

```bash
d8 k -n d8-cloud-instance-manager get secret configuration-checksums -o yaml
```  

Сравнение контрольных суммы сервис совершает каждую минуту.  

Контрольная сумма в кластере изменяется раз в 4 часа, тем самым повторно запуская скрипты на всех узлах.  
Принудительный вызов исполнения bashible на узле можно произвести путем удаления файла с контрольной суммой скриптов с помощью следующей команды:  

```bash
rm /var/lib/bashible/configuration_checksum
```  

##### Особенности написания скриптов

При написании скриптов важно учитывать следующие особенности их использования в Deckhouse:

1. Скрипты в deckhouse выполняются раз в 4 часа или на основании внешних триггеров. Поэтому важно писать скрипты таким образом, чтобы они производили проверку необходимости своих изменений в системе перед выполнением действий, а не производили изменения каждый раз при запуске.
2. Существуют предзаготовленные скрипты пользовательских скриптов. Например, если в скрипте планируется произвести перезапуск сервиса, то данный скрипт должен вызываться после скрипта установки сервиса. В противном случае он не сможет выполниться при развертывании нового узла.

Полезные особенности некоторых скриптов:

* `032_configure_containerd.sh` - производит объединение всех конфигурационных файлов сервиса `containerd` расположенных по пути `/etc/containerd/conf.d/*.toml`, а также **перезапуск** сервиса. Следует учитывать что директория `/etc/containerd/conf.d/` не создается автоматически, а также что создание файлов в этой директории следует производить в скриптах с приоритетом менее `32`

#### Chaos Monkey

Инструмент (включается у каждой из `NodeGroup` отдельно), позволяющий систематически вызывать случайные прерывания работы узлов. Предназначен для проверки элементов кластера, приложений и инфраструктурных компонентов на реальную работу отказоустойчивости.

#### Мониторинг

Для групп узлов (ресурс `NodeGroup`) Deckhouse Platform Certified Security Edition экспортирует метрики доступности группы.

##### Какую информацию собирает Prometheus?

Все метрики групп узлов имеют префикс `d8_node_group_` в названии, и метку с именем группы `node_group_name`.

Следующие метрики собираются для каждой группы узлов:

- `d8_node_group_ready` — количество узлов группы, находящихся в статусе `Ready`;
- `d8_node_group_nodes` — количество узлов в группе (в любом статусе);
- `d8_node_group_instances` — количество инстансов в группе (в любом статусе);
- `d8_node_group_desired` — желаемое (целевое) количество объектов `Machines` в группе;
- `d8_node_group_min` — минимальное количество инстансов в группе;
- `d8_node_group_max` — максимальное количество инстансов в группе;
- `d8_node_group_up_to_date` — количество узлов в группе в состоянии up-to-date;
- `d8_node_group_standby` — количество резервных узлов (см. параметр [standby](cr.html#nodegroup-v1-spec-cloudinstances-standby)) в группе;
- `d8_node_group_has_errors` — единица, если в группе узлов есть какие-либо ошибки.

### Модуль okmeter

Модуль `okmeter` устанавливает агент Okmeter как `daemonset` в пространстве имён `d8-okmeter` и удаляет ранее установленный вручную `okmeter`.

### Модуль openvpn

Дает доступ к ресурсам кластера через OpenVPN с аутентификацией по сертификатам, предоставляет простой веб-интерфейс.

В веб-интерфейсе можно:

- выпускать сертификаты;
- отзывать сертификаты;
- отменять отзыв сертификатов;
- получать готовый пользовательский конфигурационный файл.

Интеграция с модулем [user-authn](/modules/user-authn/) позволит управлять доступом пользователей в веб-интерфейс.

#### Варианты публикации VPN-сервиса

1. Выберите для подключения один или несколько внешних IP-адресов.
1. Воспользуйтесь одним из методов подключения:
   - по внешнему IP-адресу (`ExternalIP`) — если есть узлы с публичными IP-адресами;
   - с помощью `LoadBalancer` — для всех облачных провайдеров и их схем размещения с поддержкой заказа LoadBalancer;
   - `Direct` — настройте путь трафика вручную: от точки входа в кластер до пода с OpenVPN.

#### Доступные ресурсы кластера после подключения к VPN

На компьютер пользователя после подключения к VPN доставляются (push) следующие параметры:

- адрес `kube-dns` добавляется в DNS-серверы клиента для возможности прямого обращения к сервисам Kubernetes по FQDN;
- маршрут в локальную сеть;
- маршрут в сервисную сеть кластера;
- маршрут в сеть подов.

#### Аудит пользовательских соединений

Модуль позволяет включить логирование пользовательской активности через VPN в JSON-формате. Группировка трафика происходит по полям `src_ip`, `dst_ip`, `src_port`, `dst_port`, `ip_proto`.
С помощью модуля [log-shipper](/modules/log-shipper/) логи контейнера можно собрать и отправить на хранение для последующего аудита.

### Модуль operator-prometheus

Модуль устанавливает prometheus operator.



Функции устанавливаемого оператора:

- определяет следующие кастомные ресурсы:
  - `Prometheus` — определяет инсталляцию (кластер) *Prometheus*
  - `ServiceMonitor` — определяет, как собирать метрики с сервисов
  - `Alertmanager` — определяет кластер *Alertmanager*'ов
  - `PrometheusRule` — определяет список *Prometheus rules*
- следит за этими ресурсами и:
  - генерирует `StatefulSet` с самим *Prometheus* и необходимые для его работы конфигурационные файлы, сохраняя их в `Secret`;
  - следит за ресурсами `ServiceMonitor` и `PrometheusRule` и на их основании обновляет конфигурационные файлы *Prometheus* через внесение изменений в `Secret`.

#### Prometheus

##### Что делает Prometheus?

В целом, сервер Prometheus делает две ключевых вещи — **собирает метрики** и **выполняет правила**:

* Для каждого *target'а* (цель для мониторинга), каждый `scrape_interval`, делает HTTP запрос на этот *target*, получает в ответ метрики в своем формате, которые сохраняет к себе в базу
* Каждый `evaluation_interval` обрабатывает *rules*, на основании чего:
  * или шлет алерты
  * или записывает (себе же в базу) новые метрики (результат выполнения *rule'а*)

##### Как настраивается Prometheus?

* У сервера Prometheus есть *config* и есть *rule files* (файлы с правилами)
* В `config` имеются следующие секции:
  * `scrape_configs` — настройки поиска *target'ов* (целей для мониторинга, см. подробней следующий раздел).
  * `rule_files` — список директорий, в которых лежат *rule'ы*, которые необходимо загружать:

    ```yaml
    rule_files:
    - /etc/prometheus/rules/rules-0/*
    - /etc/prometheus/rules/rules-1/*
    ```

  * `alerting` — настройки поиска *Alert Manager'ов*, в которые слать алерты. Секция очень похожа на `scrape_configs`, только результатом ее работы является список *endpoint'ов*, в которые Prometheus будет слать алерты.

##### Где Prometheus берет список *target'ов*?

* В целом Prometheus работает следующим образом:

  ![Работа Prometheus](images/targets.png)

  * **(1)** Prometheus читает секцию конфигурации `scrape_configs`, согласно которой настраивает свой внутренний механизм Service Discovery
  * **(2)** Механизм Service Discovery взаимодействует с API Kubernetes (в основном — получает endpoint`ы)
  * **(3)** На основании происходящего в Kubernetes механизм Service Discovery обновляет Targets (список *target'ов*)
* В `scrape_configs` указан список *scrape job'ов* (внутреннее понятие Prometheus), каждый из которых определяется следующим образом:

  ```yaml
  scrape_configs:
    # Общие настройки
  - job_name: d8-monitoring/custom/0    # просто название scrape job'а, показывается в разделе Service Discovery
    scrape_interval: 30s                  # как часто собирать данные
    scrape_timeout: 10s                   # таймаут на запрос
    metrics_path: /metrics                # path, который запрашивать
    scheme: http                          # http или https
    # Настройки service discovery
    kubernetes_sd_configs:                # означает, что target'ы мы получаем из Kubernetes
    - api_server: null                    # означает, что адрес API-сервера использовать из переменных окружения (которые есть в каждом Pod'е)
      role: endpoints                     # target'ы брать из endpoint'ов
      namespaces:
        names:                            # искать endpoint'ы только в этих namespace'ах
        - foo
        - baz
    # Настройки "фильтрации" (какие enpoint'ы брать, а какие нет) и "релейблинга" (какие лейблы добавить или удалить, на все получаемые метрики)
    relabel_configs:
    # Фильтр по значению label'а prometheus_custom_target (полученного из связанного с endpoint'ом service'а)
    - source_labels: [__meta_kubernetes_service_label_prometheus_custom_target]
      regex: .+                           # подходит любой НЕ пустой лейбл
      action: keep
    # Фильтр по имени порта
    - source_labels: [__meta_kubernetes_endpointslice_port_name]
      regex: http-metrics                 # подходит, только если порт называется http-metrics
      action: keep
    # Добавляем label job, используем значение label'а prometheus_custom_target у service'а, к которому добавляем префикс "custom-"
    #
    # Лейбл job это служебный лейбл Prometheus:
    #    * он определяет название группы, в которой будет показываться target на странице targets
    #    * и конечно же он будет у каждой метрики, полученной у этих target'ов, чтобы можно было удобно фильтровать в rule'ах и dashboard'ах
    - source_labels: [__meta_kubernetes_service_label_prometheus_custom_target]
      regex: (.*)
      target_label: job
      replacement: custom-$1
      action: replace
    # Добавляем label namespace
    - source_labels: [__meta_kubernetes_namespace]
      regex: (.*)
      target_label: namespace
      replacement: $1
      action: replace
    # Добавляем label service
    - source_labels: [__meta_kubernetes_service_name]
      regex: (.*)
      target_label: service
      replacement: $1
      action: replace
    # Добавляем label instance (в котором будет имя Pod'а)
    - source_labels: [__meta_kubernetes_pod_name]
      regex: (.*)
      target_label: instance
      replacement: $1
      action: replace
  ```

* Таким образом, Prometheus сам отслеживает:
  * добавление и удаление Pod'ов (при добавлении/удалении Pod'ов Kubernetes изменяет endpoint'ы, а Prometheus это видит и добавляет/удаляет *target'ы*)
  * добавление и удаление сервисов (точнее endpoint'ов) в указанных namespace'ах
* Изменение конфигурации требуется в следующих случаях:
  * нужно добавить новый scrape config (обычно — новый вид сервисов, которые надо мониторить)
  * нужно изменить список namespace'ов

#### Prometheus Operator

##### Что делает Prometheus Operator?

* С помощью механизма CRD (Custom Resource Definitions) определяет четыре кастомных ресурса:
  * prometheus Prometheus
  * servicemonitor — определяет, как "мониторить" (собирать метрики) набор сервисов
  * alertmanager — определяет кластер Alertmanager'ов
  * prometheusrule — определяет список Prometheus rules
* Следит за ресурсами `prometheus` и генерирует для каждого:
  * StatefulSet (с самим Prometheus'ом)
  * Secret с `prometheus.yaml` (конфиг Prometheus'а) и `configmaps.json` (конфиг для `prometheus-config-reloader`)
* Следит за ресурсами `servicemonitor` и `prometheusrule` и на их основании обновляет конфиги (`prometheus.yaml` и `configmaps.json`, которые лежат в секрете).

##### Что в Pod'е с Prometheus'ом?

![Что в Pod Prometheus](images/pod.png)

* Три контейнера:
  * `prometheus` — сам Prometheus
  * `prometheus-config-reloader` — обвязка, которая:
    * следит за изменениями `prometheus.yaml` и, при необходимости, вызывает reload конфигурации Prometheus'у (специальным HTTP-запросом, см. [подробнее ниже](#как-обрабатываются-service-monitorы))
    * следит за PrometheusRule'ами (см. [подробнее ниже](#как-обрабатываются-кастомные-ресурсы-с-ruleами)) и по необходимости скачивает их и перезапускает Prometheus
  * `kube-rbac-proxy` — serves as an authentication and authorization proxy server based on RBAC for accessing Prometheus metrics.
* Pod использует несколько volume, из которых три — ключевые для работы Prometheus:
  * config — примонтированный secret (два файла: `prometheus.yaml` и `configmaps.json`). Подключен в оба контейнера.
  * rules — `emptyDir`, который наполняет `prometheus-config-reloader`, а читает `prometheus`. Подключен в оба контейнера, но в `prometheus` в режиме read only.
  * data — данные Prometheus. Подмонтирован только в `prometheus`.

##### Как обрабатываются Service Monitor'ы?

![Как обрабатываются Service Monitor'ы](images/servicemonitors.png)

1. Prometheus Operator читает (а также следит за добавлением/удалением/изменением) Service Monitor'ы (какие именно Service Monitor'ы — указано в самом ресурсе `prometheus`.
1. Для каждого Service Monitor'а, если в нем НЕ указан конкретный список namespace'ов (указано `any: true`), Prometheus Operator вычисляет (обращаясь к API Kubernetes) список namespace'ов, в которых есть Service'ы (подходящие под указанные в Service Monitor'е label'ы).
1. На основании прочитанных ресурсов `servicemonitor` и на основании вычисленных namespace'ов Prometheus Operator генерирует часть конфигурации (секцию `scrape_configs`) и сохраняет конфиг в соответствующий Secret.
1. Штатными средствами самого Kubernetes данные из секрета прилетают в Pod (файл `prometheus.yaml` обновляется).
1. Изменение файла замечает `prometheus-config-reloader`, который по HTTP отправляет запрос Prometheus'у на перезагрузку.
1. Prometheus перечитывает конфиг и видит изменения в scrape_configs, которые обрабатывает уже согласно своей логике работы (см. подробнее выше).

##### Как обрабатываются кастомные ресурсы с *rule'ами*?

![Как обрабатываются кастомные ресурсы с rule'ами](images/rules.png)

1. Prometheus Operator следит за PrometheusRule'ами (подходящими под указанный в ресурсе `prometheus` `ruleSelector`).
1. Если появился новый (или был удален существующий) PrometheusRule — Prometheus Operator обновляет `prometheus.yaml` (а дальше срабатывает логика в точности соответствующая обработке Service Monitor'ов, которая описана выше).
1. Как в случае добавления/удаления PrometheusRule'а, так и при изменении содержимого PrometheusRule'а, Prometheus Operator обновляет ConfigMap `prometheus-main-rulefiles-0`.
1. Штатными средствами самого Kubernetes данные из ConfigMap прилетают в Pod
1. Изменение файла замечает `prometheus-config-reloader`, который:
   - скачивает изменившиеся ConfigMap'ы в директорию rules (это `emptyDir`)
   - по HTTP отправляет запрос Prometheus'у на перезагрузку
1. Prometheus перечитывает конфиг и видит изменившиеся *rule'ы*.

### Модуль prometheus-metrics-adapter

Позволяет работать HPA- и [VPA](./vertical-pod-autoscaler/)-автоскейлерам по «любым» метрикам.

Устанавливает в кластер имплементацию Kubernetes resource metrics API, custom metrics API и external metrics API для получения метрик из Prometheus.

Это позволяет:
- `kubectl top` брать метрики из Prometheus, через адаптер;
- использовать custom resource версии autoscaling/v2 для масштабирования приложений (HPA);
- получать информацию из Prometheus средствами API Kubernetes для других модулей (Vertical Pod Autoscaler, ...).

Модуль позволяет производить масштабирование по следующим параметрам:
* CPU (пода);
* память (пода);
* rps (Ingress'а) — за 1, 5, 15 минут (`rps_Nm`);
* CPU (пода) — за 1, 5, 15 минут (`cpu_Nm`) — среднее потребление CPU за N минут;
* память (пода) — за 1, 5, 15 минут (`memory_Nm`) — среднее потребление памяти за N минут;
* любые Prometheus-метрики и любые запросы на их основе.

#### Как работает

Данный модуль регистрирует `k8s-prometheus-adapter` в качестве external API-сервиса, который расширяет возможности Kubernetes API. Когда какому-то из компонентов Kubernetes (VPA, HPA) требуется информация об используемых ресурсах, он делает запрос в Kubernetes API, а тот, в свою очередь, проксирует запрос в адаптер. Адаптер на основе своего конфигурационного файла выясняет, как посчитать метрику, и отправляет запрос в Prometheus.

### Модуль Prometheus Pushgateway

Этот модуль устанавливает в кластер Prometheus Pushgateway. Он предназначен для приема метрик от приложения и отдачи их Prometheus.

### Prometheus-мониторинг

Модуль разворачивает стек мониторинга с предустановленными параметрами для Deckhouse Platform Certified Security Edition и приложений, что упрощает начальную настройку.

Ключевые возможности:

- В комплекте — готовые триггеры и дашборды, поддерживается push и pull-модель сбора метрик. Нагрузка оптимизирована за счет использования кеширования.
- Нагрузка оптимизирована за счет использования кешей и Deckhouse Prom++.
- Есть возможность хранить исторические данные с помощью даунсемплинга.
- Модуль покрывает все основные задачи базового мониторинга платформы и приложений.

Модуль покрывает все основные задачи базового мониторинга Deckhouse Platform Certified Security Edition и приложений.

#### Мониторинг аппаратных ресурсов

Реализовано отслеживание загрузки аппаратных ресурсов кластера с графиками по утилизации:

- процессора;
- памяти;
- диска;
- сети.

Графики доступны с агрегацией:

- по подам;
- контроллерам;
- пространствам имен;
- узлам.

#### Мониторинг Kubernetes

Deckhouse настраивает мониторинг параметров «здоровья» Kubernetes и таких его компонентов, как:

- общая утилизация кластера;
- связанность узлов Kubernetes между собой (измеряется rtt между всеми узлами);
- доступность и работоспособность компонентов control plane:
  - `etcd`;
  - `coredns` и `kube-dns`;
  - `kube-apiserver` и др.
- синхронизация времени на узлах и др.

#### Мониторинг Ingress

Подробное описание [здесь](/modules/ingress-nginx/#мониторинг-и-статистика)

#### Режим расширенного мониторинга

В Deckhouse возможно использование [режима расширенного мониторинга](./extended-monitoring/), который предоставляет алерты по дополнительным метрикам:

- свободному месту и inode на дисках узлов,
- утилизации узлов,
- доступности подов и образов контейнеров,
- истечении действия сертификатов,
- другим событиям кластера.

##### Алертинг в режиме расширенного мониторинга

Deckhouse предоставляет возможность гибкой настройки алертинга для каждого пространства имён и указания различной степени критичности в зависимости от порога. Можно определить множество порогов для отправки предупреждений в различные пространства имён, например, для следующих параметров:

- значения свободного места и inodes на диске;
- утилизация CPU узлов и контейнера;
- процент ошибок с кодом `5xx` на `ingress-nginx`;
- количество возможных недоступных подов в `Deployment`, `StatefulSet`, `DaemonSet`.

#### Управление мониторингом как кодом (IaC подход)

Возможности системы мониторинга Deckhouse Platform Certified Security Edition могут быть расширены за счёт использования [модуля Observability](/modules/observability/). С его помощью реализуется:

- управление алертами;
- разграничение прав доступа к настройкам и данным мониторинга;
- централизованное управление дашбордами.

#### Алерты

Мониторинг в составе Deckhouse включает уведомления о событиях. Стандартная поставка включает набор базовых предупреждений, охватывающих состояние кластера и его компоненты. Также остается возможность добавлять кастомные алерты.

##### Отправка алертов во внешние системы

Deckhouse поддерживает отправку алертов с помощью `Alertmanager`:

- по протоколу SMTP;
- в Telegram;
- посредством Webhook;
- по любым другим каналам, поддерживаемым в Alertmanager.

#### Архитектура

![Схема взаимодействия](images/prometheus_monitoring.svg)

##### Базовые компоненты мониторинга

| Компонент                         | Описание                                                                                                                                                                                                                                                                                                                                      |
|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **prometheus-main**               | Основной Prometheus, который выполняет scrape каждые 30 секунд (с помощью параметра `scrapeInterval` можно изменить это значение). Он обрабатывает все правила, отправляет алерты и является основным источником данных.                                                                                                                      |
| **prometheus-longterm**           | Просмотр и анализ исторических трендов.                                                                                                                                                                                                                                                                                                       |
| **trickster**                     | Кеширующий прокси, снижающий нагрузку на Prometheus.                                                                                                                                                                                                                                                                                          |
| **aggregating-proxy**             | Агрегирующий и кеширующий прокси, снижающий нагрузку на Prometheus и объединяющий main и longterm в один источник.                                                                                                                                                                                                                            |
| **memcached**                     | Сервис кеширования данных в оперативной памяти.                                                                                                                                                                                                                                                                                               |
| **grafana**                       | Управляемая платформа визуализации данных. Включает подготовленные dashboard'ы для всех модулей Deckhouse Platform Certified Security Edition и некоторых популярных приложений. Grafana умеет работать в режиме высокой доступности, не хранит состояние и настраивается с помощью CRD.                                                                                          |
| **metrics-adapter**               | Компонент, соединяющий Prometheus и Kubernetes metrics API. Включает поддержку HPA в кластере Kubernetes.                                                                                                                                                                                                                                     |
| **vertical-pod-autoscaler**       | Компонент, позволяющий автоматически изменять размер запрошенных ресурсов для подов с целью оптимальной утилизации CPU и памяти.                                                                                                                                                                                                              |
| **Различные exporter'ы**          | Подготовленные и подключенные к Prometheus exporter'ы. Список включает множество exporter'ов для всех необходимых метрик: `kube-state-metrics`, `node-exporter`, `oomkill-exporter`, `image-availability-exporter` и многие другие.                                                                                                           |
| **Push/pull модель сбора метрик** | Мониторинг по умолчанию использует pull-модель — данные  собираются с приложений по инициативе системы мониторинга. Также поддерживается push-модель: метрики можно передавать через протокол Prometheus Remote Write или с помощью [Prometheus Pushgateway](/modules/prometheus-pushgateway/). |

##### Внешние компоненты

Deckhouse может интегрироваться с большим количеством разнообразных решений следующими способами:

| Название                       | Описание|
|--------------------------------|--------------------------------------------------------------------------|
| **Alertmanagers**              | Alertmanager'ы могут быть подключены к Prometheus и Grafana и находиться как в кластере Deckhouse, так и за его пределами.|
| **Long-term metrics storages** | Используя протокол `remote write`, возможно отсылать метрики из Deckhouse в большое количество хранилищ.|

#### Режим отказоустойчивости и высокой доступности мониторинга (HA)

Модуль мониторинга обеспечивает встроенную отказоустойчивость всех ключевых компонентов Deckhouse Platform Certified Security Edition. Все сервисы мониторинга (Prometheus-серверы, системы хранения, прокси и прочие важные компоненты) по умолчанию развертываются в нескольких копиях. Это гарантирует, что в случае сбоя отдельного экземпляра сервис продолжит работу без потери данных и доступности.

Prometheus — основной компонент сбора метрик — запускается минимум в двух копиях (при наличии достаточного количества узлов в кластере). Оба инстанса Prometheus используют одинаковую конфигурацию и получают одни и те же данные. Чтобы обеспечить бесшовную работу при отказе одной из копий для обращения к Prometheus используется специальный компонент — aggregation-proxy. Он позволяет объединять метрики обоих Prometheus-инстансов и всегда возвращать наиболее полные и актуальные данные, даже если одна из копий временно недоступна.

### Модуль registry

#### Описание

Модуль отвечает за управление конфигурацией registry компонентов Deckhouse и предоставляет внутреннее хранилище образов контейнеров (container registry, registry).

Внутренний registry оптимизирует загрузку и хранение образов, а также повышает высокую доступность и отказоустойчивость Deckhouse Platform Certified Security Edition.

Модуль работает в следующих режимах:

- `Direct` — использование внутреннего registry. Обращение к внутреннему registry выполняется по фиксированному адресу `registry.d8-system.svc:5001/system/deckhouse`. Фиксированный адрес, при изменении параметров registry, позволяет избежать повторного скачивания образов и перезапуска компонентов. Переключение между режимами и registry выполняется через ModuleConfig `deckhouse`. Переключение выполняется автоматически (ознакомьтесь с [примерами использования](examples.html)).
- `Unmanaged` — работа без использования внутреннего registry. Обращение внутри кластера выполняется напрямую к внешнему registry.
  Существует 2 вида режима `Unmanaged`:
  - Конфигурируемый - режим, управляемый с помощью модуля `registry`. Переключение между режимами и registry выполняется через ModuleConfig `deckhouse`. Переключение выполняется автоматически (ознакомьтесь с [примерами использования](examples.html)).
  - Неконфигурируемый (deprecated) - режим используемый по умолчанию. Параметры конфигурации задаются [при установке кластера](/reference/api/cr.html#initconfiguration-deckhouse-imagesrepo), или при изменении в развёрнутом кластере с помощью утилиты `helper change registry` (deprecated).

#### Ограничения и особенности использования модуля

Модуль `registry` имеет ряд ограничений и особенностей, касающихся установки, условий работы и переключения режимов.

##### Ограничения при установке кластера

Bootstrap кластера Deckhouse Platform Certified Security Edition поддерживается только в неконфигурируемом `Unmanaged` режиме. Настройки registry во время bootstrap задаются через [initConfiguration](/reference/api/cr.html#initconfiguration-deckhouse-imagesrepo).

Конфигурация registry через moduleConfig `deckhouse` во время bootstrap кластера Deckhouse Platform Certified Security Edition не поддерживается.

##### Ограничения по условиям работы

Модуль работает при соблюдении следующих условий:

- Если на узлах кластера используется CRI containerd или containerd v2. Для настройки CRI ознакомьтесь с конфигурацией [`ClusterConfiguration`](/reference/api/cr.html#clusterconfiguration-defaultcri).
- Кластер полностью управляется Deckhouse Platform Certified Security Edition. В Managed Kubernetes кластерах он работать не будет.

##### Ограничения по переключению режимов

Ограничения по переключению режимов следующие:

- При первом переключении необходимо выполнить миграцию пользовательских конфигураций реестра. Подробнее — в разделе [«Модуль registry: FAQ»](./faq.html).
- Переключение в неконфигурируемый `Unmanaged` режим доступно только из `Unmanaged` режима. Подробнее — в разделе [«Модуль registry: FAQ»](./faq.html).

#### Архитектура режима Direct

В режиме `Direct` запросы к registry обрабатываются напрямую, без промежуточного кэширования.

Перенаправление запросов к registry от CRI осуществляется при помощи его настроек, которые указываются в конфигурации `containerd`.

В случае таких компонентов, как `operator-trivy`, `image-availability-exporter`, `deckhouse-controller` и ряда других, обращающихся к registry напрямую, запросы будут идти через in-cluster proxy, расположенный на master-узлах.


![direct](images/direct-ru.png)

<!-- ### Proxy режим
Данный режим позволяет registry выступать в качестве промежуточного прокси-сервера между клиентом и удалённым реестром, оптимизируя доступ к часто используемым образам и уменьшая нагрузку на сеть.
Запуск кеширующего Proxy реестра осуществляется виде статических подов на узлах control plane. Для обеспечения высокой доступности к кеширующему Proxy, используется балансировщик установленный на каждый узел кластера.
Обращение к Proxy registry от CRI осуществляется через балансировщик. Настройки для обращения к балансировщику прописываются в конфигурации `containerd`.
В случае компонентов, обращающихся к реестру напрямую, таких как `operator-trivy`, `image-availability-exporter`, `deckhouse-controller` и ряда других, обращения будут идти также через кеширующий Proxy реестр. -->

<!-- ### Local режим
Данный режим позволяет создавать локальную копию registry внутри кластера. Образы из удалённого реестра полностью скопированы в локальное хранилище.
Работа локального registry идентична работы кеширующего proxy. Запуск локального registry осуществляется в виде статических подов на узлах control plane. Для обеспечения высокой доступности к локальному registry, используется балансировщик установленный на каждый узел кластера.
Обращение к локальному registry от CRI осуществляется через балансировщик. Настройки для обращения к балансировщику прописываются в конфигурации `containerd`.
В случае компонентов, обращающихся к реестру напрямую, таких как `operator-trivy`, `image-availability-exporter`, `deckhouse-controller` и ряда других, обращения будут идти также в локальный реестр.
Для наполнения локального registry образами используется инструмент d8.
-->

### Модуль runtime-audit-engine

#### Описание

Модуль предназначен для поиска угроз безопасности.

Модуль собирает события ядра Linux и итоги аудита API Kubernetes (с помощью плагина `k8saudit`), обогащает их метаданными о подах Kubernetes и генерирует события аудита безопасности по установленным правилам.

Модуль runtime-audit-engine:
* Находит угрозы в окружениях, анализируя приложения и контейнеры.
* Помогает обнаружить попытки применения уязвимостей из базы CVE и запуска криптовалютных майнеров.
* Повышает безопасность Kubernetes, выявляя:
  * оболочки командной строки, запущенные в контейнерах или подах в Kubernetes;
  * контейнеры, работающие в привилегированном режиме; монтирование небезопасных путей (например, `/proc`) в контейнеры;
  * попытки чтения секретных данных из, например, `/etc/shadow`.

#### Требования

##### Операционная система

Модуль использует драйвер eBPF для Falco при сборке событий ядра операционной системы. Этот драйвер особенно полезен в окружениях, в которых невозможна сборка модуля ядра (например, GKE, EKS и другие решения Managed Kubernetes).
У драйвера eBPF есть следующие требования:

* Версия ядра Linux >= 5.8 с поддержкой eBPF.
* Поддержка формата метаданных BPF Type Format (BTF). Проверить можно следующими способами:
  * выполнить команду `ls -lah /sys/kernel/btf/vmlinux` — наличие файла подтверждает поддержку BTF;
  * выполнить команду `grep -E "CONFIG_DEBUG_INFO_BTF=(y|m)" /boot/config-*` — если параметр включён, BTF поддерживается.

> На некоторых системах пробы (probe) eBPF могут не работать.

##### Процессор / Память

Агенты Falco работают на каждом узле. Поды агентов потребляют ресурсы в зависимости от количества применяемых правил или собираемых событий.

#### Kubernetes Audit Webhook

Режим Webhook audit mode должен быть настроен на получение событий аудита от `kube-apiserver`.
Если модуль [control-plane-manager](/modules/control-plane-manager/) включен, настройки автоматически применятся при включении модуля `runtime-audit-engine`.

В кластерах Kubernetes, в которых control plane не управляется Deckhouse, webhook необходимо настроить вручную. Для этого:

1. Создайте файл kubeconfig для webhook с адресом `https://127.0.0.1:9765/k8s-audit` и CA (ca.crt) из Secret'а `d8-runtime-audit-engine/runtime-audit-engine-webhook-tls`.

   Пример:

   ```yaml
   apiVersion: v1
   kind: Config
   clusters:
   - name: webhook
     cluster:
       certificate-authority-data: BASE64_CA
       server: "https://127.0.0.1:9765/k8s-audit"
   users:
   - name: webhook
   contexts:
   - context:
      cluster: webhook
      user: webhook
     name: webhook
   current-context: webhook
   ```

2. Добавьте к `kube-apiserver` флаг `--audit-webhook-config-file`, который будет указывать на файл, созданный на предыдущем шаге.

{{< alert level="warning" >}}
Не забудьте настроить audit policy, поскольку Deckhouse по умолчанию собирает только события аудита Kubernetes для системных пространств имен.
Пример конфигурации можно найти в документации модуля [control-plane-manager](/modules/control-plane-manager/).
{{< /alert >}}

#### Архитектура

Ядро модуля основано на системе обнаружения угроз Falco.
Deckhouse запускает агенты Falco (объединены в DaemonSet) на каждом узле, после чего те приступают к сбору событий ядра и данных, полученных в ходе аудита Kubernetes.

![Falco DaemonSet](./images/falco_daemonset.svg)
<!--- Source: https://docs.google.com/drawings/d/1NZ91z8NXNiuS50ybcMoMsZI3SbQASZXJGLANdaNNm_U --->

{{< alert >}}
Для максимальной безопасности разработчики Falco рекомендуют запускать Falco как systemd-сервис, однако в кластерах Kubernetes с поддержкой автомасштабирования это может быть затруднительно. Дополнительные средства безопасности Deckhouse (реализованные другими модулями), такие как multi-tenancy или политики контроля создаваемых ресурсов, предоставляют достаточный уровень безопасности для предотвращения атак на DaemonSet Falco.
{{< /alert >}}

Один под Falco состоит из четырех контейнеров:
![Falco Pod](images/falco_pod.svg)
<!--- Source: https://docs.google.com/drawings/d/1rxSuJFs0tumfZ56WbAJ36crtPoy_NiPBHE6Hq5lejuI --->

1. `falco` — собирает события, обогащает их метаданными и отправляет их в stdout.
2. `rules-loader` — собирает custom resourcе'ы ([FalcoAuditRules](cr.html#falcoauditrules)) из Kubernetes и сохраняет их в общую папку.
3. `falcosidekick` — принимает события от `Falco` и перенаправляет их разными способами. По умолчанию экспортирует события как метрики, по которым потом можно настроить алерты.
4. `kube-rbac-proxy` — защищает endpoint метрик `falcosidekick` (запрещает неавторизованный доступ).

#### Правила аудита

Сборка событий сама по себе не дает ничего, поскольку объем данных, собираемый с ядра Linux, слишком велик для анализа человеком.
Правила позволяют решить эту проблему: события отбираются по определенным условиям. Условия настраиваются на выявление любой подозрительной активности.

В основе каждого правила лежит выражение, содержащее определенное условие, написанное в соответствии с синтаксисом условий.

##### Встроенные правила

Существует набор встроенных правил, который нельзя отключить.
Он помогает выявить проблемы с безопасностью Deckhouse и с самим модулем `runtime-audit-engine`. Это правила для аудита Kubernetes, статично размещённые в контейнере `falco` по пути `/etc/falco/k8s_audit_rules.yaml`.

Также существуют дополнительный набор встроенных правил:

- правила, размещённые в формате custom resource [FalcoAuditRules](cr.html#falcoauditrules), `fstec` — правила аудита удовлетворяющие требованиям приказа ФСТЭК России №118 от 4 июля 2022г. (Требования по безопасности информации к средствам контейнеризации).

Чтобы вывести список всех правил аудита `falco`, выполните:

```shell
d8 k -n d8-runtime-audit-engine exec -it daemonsets/runtime-audit-engine -c falco -- falco -L
```

Настроить список встроенных правил можно с помощью параметра [settings.builtInRulesList](configuration.html#parameters-builtinruleslist).

##### Пользовательские правила

Добавить пользовательские правила можно с помощью custom resource [FalcoAuditRules](cr.html#falcoauditrules).
У каждого агента Falco есть sidecar-контейнер с экземпляром shell-operator.
Этот экземпляр считывает правила из custom resource'ов Kubernetes, конвертирует их в правила Falco и сохраняет правила Falco в директорию `/etc/falco/rules.d/` пода.
При добавлении нового правила Falco автоматически обновляет конфигурацию.

![Falco shell-operator](images/falco_shop.svg)
<!--- Source: https://docs.google.com/drawings/d/13MFYtiwH4Y66SfEPZIcS7S2wAY6vnKcoaztxsmX1hug --->

Такая схема позволяет использовать подход «Инфраструктура как код» при работе с правилами Falco.

#### Алерты

Если несколько подов `runtime-audit-engine` не назначены на узлы планировщиком, будет сгенерирован алерт `D8RuntimeAuditEngineNotScheduledInCluster`.

### Модуль secret-copier
 
Этот модуль отвечает за копирование секретов во все пространства имён.

Он полезен тем, что позволяет не копировать каждый раз в CI секреты для пуллинга образов и заказа RBD в Ceph.

{% alert level="warning" %}
Модуль `secret-copier` не может использоваться совместно с модулем `multitenancy-manager`.

`multitenancy-manager` создаёт изолированные окружения для пользователей в их проектах, а `secret-copier` автоматически раздаёт секреты во все пространства имён.
Если в закрытом окружении пользователя окажутся чувствительные данные, это может привести к утечке данных и нарушению модели безопасности.

Поэтому, если необходимо предоставить общий сертификат (например, WC-сертификат для внутреннего окружения) или общий токен доступа к registry, не используйте `secret-copier`.
Поместите такие секреты в шаблон проекта в `multitenancy-manager` – администратор кластера должен определить их в конфигурации проекта.
{% endalert %}

##### Как работает?

Модуль `secret-copier` следит за изменениями секретов в пространстве имён `default` с лейблом `secret-copier.deckhouse.io/enabled: ""`.
* Созданный секрет будет скопирован во все пространства имён.
* Изменённый секрет с его новым содержимым будет раскопирован во все пространства имён.
* При удалении секрет будет удален из всех пространств имён.
* При изменении скопированного секрета в прикладном пространстве имён, тот будет перезаписан оригинальным содержимым.
* При создании любого пространства имён в него копируются все секреты из пространства имён `default` с лейблом `secret-copier.deckhouse.io/enabled: ""`.

Кроме этого, каждую ночь секреты будут повторно синхронизированы и приведены к состоянию в пространств имён `default`.

##### Что нужно настроить?

Чтобы все заработало, достаточно создать в пространстве имён `default` секрет с лейблом `secret-copier.deckhouse.io/enabled: ""`.

> **Внимание!** Рабочим пространством имён для модуля является `default`, Секреты будут копироваться только из него. Ресурсы с лейблом `secret-copier.deckhouse.io/enabled: ""`, созданные в других пространствах имён при включенном модуле будут автоматически удалены.

##### Как ограничить список пространств имён, в которые будет производиться копирование?

Для этого нужно задать label–селектор в значении аннотации `secret-copier.deckhouse.io/target-namespace-selector`. Например: `secret-copier.deckhouse.io/target-namespace-selector: "app=custom"`. Модуль создаст копию этого секрета во всех пространствах имён, соответствующих заданному label–селектору.

### Модуль secrets-store-integration

Модуль secrets-store-integration реализует доставку секретов для приложения в Kubernetes-кластерах
путем подключения секретов, ключей и сертификатов, хранящихся во внешних хранилищах секретов.

Секреты монтируются в поды в виде тома с использованием реализации драйвера CSI.
Хранилища секретов должны быть совместимы с API-интерфейсом HashiCorp Vault.

#### Доставка секретов в приложения

Доставить секреты в приложение из vault-совместимого хранилища можно несколькими способами:

1. Пользовательское приложение само обращается в хранилище.

   > Это наиболее безопасный вариант, но требует модификации приложений.

1. В хранилище обращается приложение-прослойка, а ваше приложение получает доступ к секретам из файлов, созданных в контейнере.

   > Если нет возможности модифицировать приложение, используйте этот вариант. Он проще в реализации, но менее безопасный, так как секретные данные хранятся в файлах в контейнере.

1. В хранилище обращается приложение-прослойка, и пользовательское приложение получает доступ к секретам из переменных среды.

   > Если нет возможности читать из файлов, можно использовать этот вариант, но он небезопасен. При таком подходе секретные данные хранятся в Kubernetes (а так же в etcd) и потенциально могут быть прочитаны на любом узле кластера.

<table>
<thead>
<tr>
<th>Вариант доставки</th>
<th>Потребление ресурсов</th>
<th>Как приложение получает данные?</th>
<th>Где хранится в Kubernetes?</th>
<th>Статус</th>
</tr>
</thead>
<tbody>
<tr>
<td><a style="color: ##0066FF;" href="#вариант-1-получение-секретов-самим-приложением">Приложение</a></td>
<td>Не меняется</td>
<td>Напрямую из хранилища секретов</td>
<td>Не хранится</td>
<td>Реализовано</td>
</tr>
<tr>
<td><a style="color: ##0066FF;" href="#механизм-csi">Механизм CSI</a></td>
<td>Два пода на каждую ноду (daemonset)</td>
<td><ul><li>Из дискового тома (как файл)</li><li>Из переменной окружения</li></ul></td>
<td>Не хранится</td>
<td>Реализовано</td>
</tr>
<tr>
<td><a style="color: ##0066FF;" href="#вариант-3-инъекция-entrypoint">Инъекция entrypoint</a></td>
<td>Один под на каждую ноду (daemonset)</td>
<td>Секреты доставляются из хранилища в момент запуска приложения в виде переменных окружения</td>
<td>Не хранится</td>
<td>Реализовано</td>
</tr>
<tr>
<td><a style="color: ##0066FF;" href="#вариант-4-доставка-секретов-через-механизмы-kubernetes">Секреты Kubernetes</a></td>
<td>Одно приложение на кластер (deployment)</td>
<td><ul><li>Из дискового тома (как файл)</li><li>Из переменной окружения</li></ul></td>
<td>Хранится в Secrets</td>
<td>Планируется</td>
</tr>
<tr>
<td><a style="color: #A9A9A9; font-style: italic;" href="#справочно-инжектор-vault-agent">Инжектор vault-agent</a></td>
<td style="color: #A9A9A9; font-style: italic;">По одному агенту на каждый под (sidecar)</td>
<td style="color: #A9A9A9; font-style: italic;">Из дискового тома (как файл)</td>
<td style="color: #A9A9A9; font-style: italic;">Не хранится</td>
<td style="color: #A9A9A9; font-style: italic;"><sup><b>*</b></sup>Не будет реализовано</td>
</tr>
</tbody>
</table>

<i><sup>*</sup>Поддержка отсутствует и не планируется, поскольку этот вариант не имеет преимуществ перед использованием механизма CSI.</i>

##### Вариант №1: Получение секретов самим приложением

> *Статус:* наиболее безопасный вариант. Рекомендован к использованию, если есть возможность модификации приложений.

Приложение обращается к API Stronghold и запрашивает необходимый секрет по HTTPS-протоколу с использованием токена авторизации (токен из SA).

Плюсы:
- Секрет, полученный приложением, нигде не хранится, кроме как в самом приложении, нет опасности что он будет скомпрометирован в процессе передачи.

Минусы:

- Требует доработки приложения для возможности работы со Stronghold.
- Требует повторения реализации доступа к секретам в каждом приложении. В случае обновления библиотеки требует пересборки всех приложений.
- Приложение должно поддерживать TLS и проверку сертификатов.
- Нет кэширования. При перезапуске приложения нужно повторно запросить секрет напрямую из хранилища.

##### Вариант №2: Доставка секретов через файлы

###### Механизм CSI

> *Статус:* безопасный вариант. Рекомендован к использованию, если отсутствует возможность модификация приложений.

При создании подов, запрашивающих тома CSI, драйвер хранилища секретов CSI отправляет запрос к Vault CSI. Затем Vault CSI использует указанный SecretProviderClass и ServiceAccount пода для получения секретов из хранилища и монтирования их в том пода.

###### Инъекция переменных окружений:

Если нет возможности изменить код приложения, то можно реализовать безопасную инъекцию секрета в качестве переменной окружения для приложения.

Для этого нужно:
- прочитать все файлы, примонтированные CSI в контейнер;
- определить переменные окружения с именами, соответствующими именам файлов, и значениями, соответствующим содержимому файлов.
- запустить оригинальное приложение.

Пример на Bash:

```bash
bash -c "for file in $(ls /mnt/secrets); do export  $file=$(cat /mnt/secrets/$file); done ; exec my_original_file_to_startup"
```

Плюсы:

- Всего два контейнера с прогнозируемыми ресурсами на каждом узле для обслуживания системы доставки секретов в приложения;
- Создание ресурсов _SecretsStore/SecretProviderClass_ уменьшает количество повторяемого кода по сравнению с другими вариантами реализации vault agent;
- При необходимости есть возможность создавать копию секрета из хранилища в виде секрета Kubernetes.
- Секрет извлекается из хранилища драйвером CSI на этапе создания контейнера. Это означает, что запуск подов заблокируется до тех пор, пока секреты не будут прочитаны из хранилища и записаны в том.

##### Вариант №3: Инъекция entrypoint

###### Доставка переменных окружения через инъекцию entrypoint в контейнер

> *Статус:* безопасный вариант. В процессе реализации.

Переменные доставляются из хранилища в момент запуска приложения и находятся только в памяти. В момент первого этапа реализации метода переменные будут доставляться через entrypoint, проброшенный в контейнер. В дальнейшем планируется интеграция функционала доставки секретов в containerd.

##### Вариант №4: Доставка секретов через механизмы Kubernetes

> *Статус:* небезопасный вариант, не рекомендован к использованию. Поддержка отсутствует, но планируется в будущем.

Этот метод интеграции, который реализует оператор секретов Kubernetes с набором CRD, отвечающих за синхронизацию секретов из Vault в секреты Kubernetes.

Минусы:

- Секрет находится и в хранилище секретов, и в секрете Kubernetes (доступном через API Kubernetes). Секрет также хранится в etcd и потенциально может быть считан на любом узле кластера или извлечён из резервной копии etcd. Нет возможности не хранить данные в секретах Kubernetes.

Плюсы:

- Классический способ передачи секрета в приложение через переменные окружения — достаточно подключить секрет Kubernetes.

##### Справочно: Инжектор vault-agent

> *Статус:* не имеет плюсов в сравнении с механизмом CSI. Поддержка отсутствует и не планируется, поскольку этот вариант не имеет преимуществ перед использованием механизма CSI.

При создании пода происходит мутация, которая добавляет контейнер с vault-agent. Агент обращается к хранилищу секретов, извлекает их, и помещает в общий том на диске, к которому может обратиться приложение.

Минусы:

- Для каждого пода нужен sidecar-контейнер, который так или иначе потребляет ресурсы.

  Например, возьмем кластер в котором 50 приложений, и каждое приложение имеет от 3 до 15 реплик. Так как для каждого sidecar-контейнера с агентом нужно выделить ресурсы CPU и памяти, то даже при незначительных ресурсах для sidecar-контейнера в размере 0.05 CPU и 100 MiB памяти, на все приложения в сумме получаются десятки ядер CPU и десятки ГБ памяти.
- Так как сбор метрик осуществляется с каждого контейнера, то с таким подходом мы получим в два раза больше метрик только по контейнерам.

### Модуль snapshot-controller

Этот модуль включает поддержку снапшотов для совместимых CSI-драйверов в кластере Kubernetes.

CSI-драйверы в Deckhouse, которые поддерживают снапшоты:
- [sds-local-volume](/modules/sds-local-volume/)
- [csi-ceph](/modules/csi-ceph/)
- [csi-nfs](/modules/csi-nfs/)

### Модуль storage-volume-data-manager

Модуль storage-volume-data-manager обеспечивает механизм экспорта содержимого пользовательского тома по протоколу HTTP.

Создает namepsaced-ресурс "DataExport" в том namespace, в котором нужно создать экспорт данных.
В этом ресурсе указывается targetRef - ссылка на ресурс, который нужно экспортировать.
Поддерживаются только PersistentVolumeClaim и VolumeSnapshot.

За основу взят стандартный файловый сервер Go. Поддерживается экспорт томов в режиме файловой системы и в блочном режиме.
Обеспечены авторизации пользователя средствами k8s, механизм скачивания файлов/блочки в диапазонах байт (поддерживаются заголовки 'Range').

#### Ключевые параметры

- ttl - это время после последнего обращения к серверу: скачивания файла или листинга директории. По истечении ttl экспортер-под удаляется, пользовательская PVC возвращается к пользовательскому PV. 
 В ресурсе DataExport в Condition Ready устанавливается статус false, Reason - в Expired

- publish - значение true в publish означает, что к экспортер-поду будет открыт доступ извне кластера.
При этом в поле PublicURL появится строка для публичного доступа вида: publicURL: `https://data-exporter.<public-domain>/<namespace>/<user-pvc-name>/`

#### Быстрый старт

Включение модуля:

```bash
kubectl apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: storage-volume-data-manager
spec:
  enabled: true
  version: 1
EOF
```

Для создания и работы с ресурсами DataExport используется команда d8, структура выглядит следующим образом:

```bash
d8 data -n <namespace> create <DataExport resource name> <тип ресурса для экспорта>/<имя ресурса для экспорта> --ttl 5m  --publish (true/false)
```

Важно!
Работа с PVC ресурсами вожможна, если PVC не используется подами в данный момент

Для примера, создание ресурса DataExport для PVC с именем "data" в namespace "project" c ttl 5m:

```bash
d8 data -n project create my-export pvc/data --ttl 5m
```

Получить информацию о созданном ресурсе можно командой:

```bash
d8 k -n project get de my-export
```

Скачивание данных производится следующей командой:

```bash
d8 data -n <namespace> download <тип ресурса (pvc/vs/dataexport)>/<имя ресурса>/<путь к файлу> -o <имя файла>
```

Например:

```bash
d8 data -n project download dataexport/my-export -o test_file.txt --publish true
```

### Модуль terraform-manager

Модуль предоставляет инструменты для работы с состоянием Terraform'а в кластере Kubernetes.

* Модуль состоит из двух частей:
  * `terraform-auto-converger` — проверяет состояние Terraform'а и применяет недеструктивные изменения;
  * `terraform-state-exporter` — проверяет состояние Terraform'а и экспортирует метрики кластера.

* Модуль включен по умолчанию, если в кластере есть Secret'ы:
  * `kube-system/d8-provider-cluster-configuration`;
  * `d8-system/d8-cluster-terraform-state`.

### Мониторинг SLA кластера

Модуль тестирует доступность платформы и состояние компонентов кластера в реальном времени, выводит информацию в виде соответствующих дашбордов.

Возможности модуля:

- для всех основных компонентов кластера разработаны сценарии постоянного тестирования;
- результаты тестирования сохраняются в виде метрик;
- отображает дашборд, показывающий работоспособность компонентов;
- данные могут быть экспортированы в любую внешнюю Prometheus-совместимую систему мониторинга.

Состав модуля:

- **agent** — работает на master-узлах и делает пробы доступности, отправляет результаты на сервер.
- **upmeter** — собирает результаты и поддерживает API-сервер для их извлечения.
- **front**:
  - **status** — показывает уровень доступности за последние 10 минут (требует авторизации, но ее можно отключить);
  - **webui** — показывает дашборд со статистикой по пробам и группам доступности (требует авторизации).
- **smoke-mini** — поддерживает постоянное *smoke-тестирование* с помощью StatefulSet.

Модуль отправляет около 100 показаний метрик каждые 5 минут. Это значение зависит от количества включенных модулей Deckhouse Platform Certified Security Edition.

#### Интерфейс

Пример веб-интерфейса:
![Пример веб-интерфейса](images/image1.png)

Пример графиков по метрикам из upmeter в Grafana:
![Пример графиков по метрикам из upmeter в Grafana](images/image2.png)

### Модуль user-authn

Модуль отвечает за единую систему аутентификации, интегрированную с Kubernetes и веб-интерфейсами, используемыми в других модулях, например, Grafana и Dashboard.

Модуль состоит из следующих компонентов:

- `dex` — федеративный OpenID Connect провайдер, поддерживающий работу со статическими пользователями и с возможностью подключения к различным внешним провайдерам аутентификации.
- `kubeconfig-generator` (он же `dex-k8s-authenticator` — веб-приложение, генерирующее команды для настройки локального `kubectl` после аутентификации в Dex;
- `dex-authenticator` (он же `oauth2-proxy` и выполняющее их авторизацию с помощью сервиса Dex.

Управление статическими пользователями осуществляется с помощью ресурсов [User](cr.html#user) и [Group](cr.html#group):

В объекте User хранится информация о пользователе, включая email и зашифрованный хеш пароля (пароль в открытом виде не сохраняется);
В объекте Group задаётся список пользователей, объединённых в группы для удобства управления правами доступа.

Поддерживаются следующие внешние провайдеры/протоколы аутентификации:

- LDAP;
- OIDC.

Одновременно можно подключить более одного внешнего провайдера аутентификации.

#### Возможности интеграции

##### Базовая аутентификация в API Kubernetes

Базовая аутентификация в API Kubernetes на данный момент доступна только для провайдера Crowd (с включением параметра [`enableBasicAuth`](cr.html#dexprovider-v1-spec-crowd-enablebasicauth)).

> К API Kubernetes можно подключаться и [через другие поддерживаемые внешние провайдеры](#веб-интерфейс-для-генерации-готовых-kubeconfig-файлов).

##### Интеграция с приложениями

Чтобы обеспечить аутентификацию в любом веб-приложении, работающем в Kubernetes, можно создать ресурс [_DexAuthenticator_](cr.html#dexauthenticator) в пространстве имен (_Namespace_) приложения и добавить несколько аннотаций к ресурсу _Ingress_.
Это позволит:
* ограничить список групп, которым разрешен доступ;
* ограничить список адресов, с которых разрешена аутентификация;
* интегрировать приложение в единую систему аутентификации, если приложение поддерживает OIDC. Для этого в Kubernetes создается ресурс [_DexClient_](cr.html#dexclient) в _Namespace_ приложения. В том же _Namespace_ создается секрет с данными для подключения в Dex по OIDC.

После такой интеграции можно:
* ограничить перечень групп, которым разрешено подключаться;
* указать перечень клиентов, OIDC-токенам которых можно доверять (`trustedPeers`).

##### Веб-интерфейс для генерации готовых kubeconfig-файлов

Модуль позволяет автоматически создавать конфигурацию для kubectl или других утилит Kubernetes.

Пользователь получит набор команд для настройки kubectl после авторизации в веб-интерфейсе генератора. Эти команды можно скопировать и вставить в консоль для использования kubectl.
Механизм аутентификации для kubeconfig использует OIDC-токен. OIDC-сессия может продлеваться автоматически, если использованный в Dex провайдер аутентификации поддерживает продление сессий. Для этого в kubeconfig указывается `refresh token`.

Дополнительно можно настроить несколько адресов `kube-apiserver` и сертификаты ЦС (CA) для каждого из них. Например, это может потребоваться, если доступ к кластеру Kubernetes осуществляется через VPN или прямое подключение.

#### Публикация API kubernetes через Ingress

Компонент kube-apiserver без дополнительных настроек доступен только во внутренней сети кластера. Этот модуль решает проблему простого и безопасного доступа к API Kubernetes извне кластера. При этом API-сервер публикуется на специальном домене (подробнее см. [раздел о служебных доменах в документации](/reference/api/global.html)).

При настройке можно указать:
* перечень сетевых адресов, с которых разрешено подключение;
* перечень групп, которым разрешен доступ к API-серверу;
* Ingress-контроллер, на котором производится аутентификация.

По умолчанию будет сгенерирован специальный сертификат ЦС (CA) и автоматически настроен генератор kubeconfig.

#### Расширения от Фланта

Модуль использует модифицированную версию Dex для поддержки:
* групп для статических учетных записей пользователей и провайдера Bitbucket Cloud (параметр [`bitbucketCloud`](cr.html#dexprovider-v1-spec-bitbucketcloud));
* передачи параметра `group` клиентам;
* механизма `obsolete tokens`, который позволяет избежать состояния гонки при продлении токена OIDC-клиентом.

#### Отказоустойчивый режим

Модуль поддерживает режим высокой доступности `highAvailability`. При его включении аутентификаторы, отвечающие на `auth request`-запросы, развертываются с учетом требуемой избыточности для обеспечения непрерывной работы. В случае отказа любого из экземпляров аутентификаторов пользовательские аутентификационные сессии не прерываются.

### Модуль user-authz

Модуль отвечает за генерацию объектов ролевой модели доступа, основанной на базе стандартного механизма RBAC Kubernetes. Модуль создает набор кластерных ролей (`ClusterRole`), подходящий для большинства задач по управлению доступом пользователей и групп.

{% alert level="warning" %}
С версии Deckhouse Platform Certified Security Edition v1.64 в модуле реализована экспериментальная модель ролевого доступа.

Функциональность экспериментальной и текущей моделей ролевого доступа несовместимы. Автоматическая конвертация ресурсов невозможна.
{% endalert %}

<div style="height: 0;" id="новая-ролевая-модель"></div>

#### Экспериментальная ролевая модель

В отличие [от текущей ролевой модели](#текущая-ролевая-модель) Deckhouse Platform Certified Security Edition, экспериментальная ролевая модель не использует ресурсы `ClusterAuthorizationRule` и `AuthorizationRule`. Настройка прав доступа выполняется стандартным для RBAC Kubernetes способом: с помощью создания ресурсов `RoleBinding` или `ClusterRoleBinding`, с указанием в них одной из подготовленных модулем `user-authz` ролей.

Модуль создаёт специальные агрегированные кластерные роли (`ClusterRole`). Используя эти роли в `RoleBinding` или `ClusterRoleBinding` можно решать следующие задачи:

- Управлять доступом к модулям определённой [подсистеме](#подсистемы-ролевой-модели) применения.

  Например, чтобы дать возможность пользователю, выполняющему функции сетевого администратора, настраивать *сетевые* модули (например, `cni-cilium`, `ingress-nginx`, `istio` и т. д.), можно использовать в `ClusterRoleBinding` роль `d8:manage:networking:manager`.
- Управлять доступом к *пользовательским* ресурсам модулей в рамках пространства имён.

  Например, использование роли `d8:use:role:manager` в `RoleBinding`, позволит удалять/создавать/редактировать ресурс [PodLoggingConfig](./log-shipper/cr.html#podloggingconfig) в пространстве имён, но не даст доступа к cluster-wide-ресурсам [ClusterLoggingConfig](./log-shipper/cr.html#clusterloggingconfig) и [ClusterLogDestination](./log-shipper/cr.html#clusterlogdestination) модуля `log-shipper`, а также не даст возможность настраивать сам модуль `log-shipper`.

Роли, создаваемые модулем, делятся на два класса:

- [Use-роли](#use-роли) — для назначения прав пользователям (например, разработчикам приложений) **в конкретном пространстве имён**.
- [Manage-роли](#manage-роли) — для назначения прав администраторам.

##### Use-роли

{% alert level="warning" %}
Use-роль можно использовать только в ресурсе `RoleBinding`.
{% endalert %}

Use-роли предназначены для назначения прав пользователю **в конкретном пространстве имён**. Под пользователями понимаются, например, разработчики, которые используют настроенный администратором кластер для развёртывания своих приложений. Таким пользователям не нужно управлять модулями Deckhouse Platform Certified Security Edition или кластером, но им нужно иметь возможность, например, создавать свои Ingress-ресурсы, настраивать аутентификацию приложений и сбор логов с приложений.

Use-роль определяет права на доступ к namespaced-ресурсам модулей и стандартным namespaced-ресурсам Kubernetes (`Pod`, `Deployment`, `Secret`, `ConfigMap` и т. п.).

Модуль создаёт следующие use-роли:

- `d8:use:role:viewer` — позволяет в конкретном пространстве имён просматривать стандартные ресурсы Kubernetes, кроме секретов и ресурсов RBAC, а также выполнять аутентификацию в кластере;
- `d8:use:role:user` — дополнительно к роли `d8:use:role:viewer` позволяет в конкретном пространстве имён просматривать секреты и ресурсы RBAC, подключаться к подам, удалять поды (но не создавать или изменять их), выполнять `kubectl port-forward` и `kubectl proxy`, изменять количество реплик контроллеров;
- `d8:use:role:manager` — дополнительно к роли `d8:use:role:user` позволяет в конкретном пространстве имён управлять ресурсами модулей (например, `Certificate`, `PodLoggingConfig` и т. п.) и стандартными namespaced-ресурсами Kubernetes (`Pod`, `ConfigMap`, `CronJob` и т. п.);
- `d8:use:role:admin` — дополнительно к роли `d8:use:role:manager` позволяет в конкретном пространстве имён управлять ресурсами `ResourceQuota`, `ServiceAccount`, `Role`, `RoleBinding`, `NetworkPolicy`.

##### Manage-роли

{% alert level="warning" %}
Manage-роль не дает доступа к пространству имён пользовательских приложений.

Manage-роль определяет доступ только к системным пространствам имён (начинающимся с `d8-` или `kube-`), и только к тем из них, в которых работают модули соответствующей подсистемы роли.
{% endalert %}

Manage-роли предназначены для назначения прав на управление всей платформой или её частью ([подсистемой](#подсистемы-ролевой-модели)), но не самими приложениями пользователей. С помощью manage-роли можно, например, дать возможность администратору безопасности управлять модулями, ответственными за функции безопасности кластера. Тогда администратор безопасности сможет настраивать аутентификацию, авторизацию, политики безопасности и т. п., но не сможет управлять остальными функциями кластера (например, настройками сети и мониторинга) и изменять настройки в пространстве имён приложений пользователей.

Manage-роль определяет права на доступ:

- к cluster-wide-ресурсам Kubernetes;
- к управлению модулями Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`) в рамках [подсистемы](#подсистемы-ролевой-модели) роли, или всеми модулями Deckhouse Platform Certified Security Edition для роли `d8:manage:all:*`;
- к управлению cluster-wide-ресурсами модулей Deckhouse Platform Certified Security Edition в рамках [подсистемы](#подсистемы-ролевой-модели) роли или всеми ресурсами модулей Deckhouse Platform Certified Security Edition для роли `d8:manage:all:*`;
- к системным пространствам имён (начинающимся с `d8-` или `kube-`), в которых работают модули [подсистемы](#подсистемы-ролевой-модели) роли, или ко всем системным пространствам имён для роли `d8:manage:all:*`.
  
Формат названия manage-роли — `d8:manage:<SUBSYSTEM>:<ACCESS_LEVEL>`, где:

- `SUBSYSTEM` — подсистема роли. Может быть либо одной из подсистем [списка](#подсистемы-ролевой-модели), либо `all` для доступа в рамках всех подсистем;
- `ACCESS_LEVEL` — уровень доступа.

  Примеры manage-ролей:
  
  - `d8:manage:all:viewer` — доступ на просмотр конфигурации всех модулей Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`), их cluster-wide-ресурсов, их namespaced-ресурсов и стандартных объектов Kubernetes (кроме секретов и ресурсов RBAC) во всех системных пространствах имён (начинающихся с `d8-` или `kube-`);
  - `d8:manage:all:manager` — аналогично роли `d8:manage:all:viewer`, только доступ на уровне `admin`, т. е. просмотр/создание/изменение/удаление конфигурации всех модулей Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`), их cluster-wide-ресурсов, их namespaced-ресурсов и стандартных объектов Kubernetes во всех системных пространствах имён (начинающихся с `d8-` или `kube-`);
  - `d8:manage:observability:viewer` — доступ на просмотр конфигурации модулей Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`) из подсистемы `observability`, их cluster-wide-ресурсов, их namespaced-ресурсов и стандартных объектов Kubernetes (кроме секретов и ресурсов RBAC) в системных пространствах имён `d8-log-shipper`, `d8-monitoring`, `d8-okmeter`, `d8-operator-prometheus`, `d8-upmeter`, `kube-prometheus-pushgateway`.

Модуль предоставляет два уровня доступа для администратора:

- `viewer` — позволяет просматривать стандартные ресурсы Kubernetes, конфигурацию модулей (ресурсы `moduleConfig`), cluster-wide-ресурсы модулей и namespaced-ресурсы модулей в пространстве имен модуля;
- `manager` — дополнительно к роли `viewer` позволяет управлять стандартными ресурсами Kubernetes, конфигурацией модулей (ресурсы `moduleConfig`), cluster-wide-ресурсами модулей и namespaced-ресурсами модулей в пространстве имен модуля;

##### Подсистемы ролевой модели

Каждый модуль Deckhouse Platform Certified Security Edition принадлежит определённой подсистемы. Для каждой подсистемы существует набор ролей с разными уровнями доступа. Роли обновляются автоматически при включении или отключении модуля.

Например, для подсистемы `networking` существуют следующие manage-роли, которые можно использовать в `ClusterRoleBinding`:

- `d8:manage:networking:viewer`
- `d8:manage:networking:manager`

Подсистема роли ограничивает её действие всеми системными (начинающимися с `d8-` или `kube-`) пространствами имён кластера (подсистема `all`) или теми пространствами имён, в которых работают модули подсистемы (см. таблицу состава подсистем).

Таблица состава подсистем ролевой модели.

{% include rbac/rbac-subsystems-list.liquid %}

<div style="height: 0;" id="устаревшая-ролевая-модель"></div>

#### Текущая ролевая модель

Особенности:

- Модуль реализует role-based-подсистему сквозной авторизации, расширяя функционал стандартного механизма RBAC.
- Настройка прав доступа происходит с помощью [ресурсов](cr.html).
- Управление доступом к инструментам масштабирования (параметр `allowScale` ресурса [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule-v1-spec-allowscale) или [AuthorizationRule](cr.html#authorizationrule-v1alpha1-spec-allowscale)).
- Управление доступом к форвардингу портов (параметр `portForwarding` ресурса [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule-v1-spec-portforwarding) или [AuthorizationRule](cr.html#authorizationrule-v1alpha1-spec-portforwarding)).
- Управление списком разрешённых пространств имён в формате labelSelector (параметр `namespaceSelector` ресурса [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule-v1-spec-namespaceselector)).

В модуле, кроме использования RBAC, можно использовать удобный набор высокоуровневых ролей:

- `User` — позволяет получать информацию обо всех объектах (включая доступ к журналам подов), но не позволяет заходить в контейнеры, читать секреты и выполнять port-forward;
- `PrivilegedUser` — то же самое, что и `User`, но позволяет заходить в контейнеры, читать секреты, а также удалять поды (что обеспечивает возможность перезагрузки);
- `Editor` — то же самое, что и `PrivilegedUser`, но предоставляет возможность создавать, изменять и удалять все объекты, которые обычно нужны для прикладных задач;
- `Admin` — то же самое, что и `Editor`, но позволяет удалять служебные объекты (производные ресурсы, например `ReplicaSet`, `certmanager.k8s.io/challenges` и `certmanager.k8s.io/orders`);
- `ClusterEditor` — то же самое, что и `Editor`, но позволяет управлять ограниченным набором `cluster-wide`-объектов, которые могут понадобиться для прикладных задач (`ClusterXXXMetric`, `KeepalivedInstance`, `DaemonSet` и т. д). Роль для работы оператора кластера;
- `ClusterAdmin` — то же самое, что и `ClusterEditor` + `Admin`, но позволяет управлять служебными `cluster-wide`-объектами (производные ресурсы, например `MachineSets`, `Machines` и т. п., а также `ClusterAuthorizationRule`, `ClusterRoleBindings` и `ClusterRole`). Роль для работы администратора кластера. **Важно**, что `ClusterAdmin`, поскольку он уполномочен редактировать `ClusterRoleBindings`, может **сам себе расширить полномочия**;
- `SuperAdmin` — разрешены любые действия с любыми объектами, при этом ограничения `namespaceSelector` и `limitNamespaces` продолжат работать.

{% alert level="warning" %}
Режим multi-tenancy (авторизация по пространству имён) в данный момент реализован по временной схеме и **не гарантирует безопасность!**
{% endalert %}

В случае, если в [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule)-ресурсе используется `namespaceSelector`, параметры `limitNamespaces` и `allowAccessToSystemNamespace` не учитываются.

Если вебхук, который реализовывает систему авторизации, по какой-то причине будет недоступен, опции `allowAccessToSystemNamespaces`, `namespaceSelector` и `limitNamespaces` в custom resource перестанут применяться и пользователи будут иметь доступ во все пространства имён. После восстановления доступности вебхука опции продолжат работать.

##### Список доступа для каждой роли модуля по умолчанию

Сокращения для `verbs`:
<!-- start user-authz roles placeholder -->
* read - `get`, `list`, `watch`
* read-write - `get`, `list`, `watch`, `create`, `delete`, `deletecollection`, `patch`, `update`
* write - `create`, `delete`, `deletecollection`, `patch`, `update`

{{site.data.i18n.common.role[page.lang] | capitalize }} `User`:

```text
read:
    - apiextensions.k8s.io/customresourcedefinitions
    - apps/daemonsets
    - apps/deployments
    - apps/replicasets
    - apps/statefulsets
    - autoscaling.k8s.io/verticalpodautoscalers
    - autoscaling/horizontalpodautoscalers
    - batch/cronjobs
    - batch/jobs
    - configmaps
    - discovery.k8s.io/endpointslices
    - endpoints
    - events
    - events.k8s.io/events
    - extensions/daemonsets
    - extensions/deployments
    - extensions/ingresses
    - extensions/replicasets
    - extensions/replicationcontrollers
    - limitranges
    - metrics.k8s.io/nodes
    - metrics.k8s.io/pods
    - namespaces
    - networking.k8s.io/ingresses
    - networking.k8s.io/networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    - pods
    - pods/log
    - policy/poddisruptionbudgets
    - rbac.authorization.k8s.io/rolebindings
    - rbac.authorization.k8s.io/roles
    - replicationcontrollers
    - resourcequotas
    - serviceaccounts
    - services
    - storage.k8s.io/storageclasses
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `PrivilegedUser` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`):

```text
create:
    - pods/eviction
create,get:
    - pods/attach
    - pods/exec
delete,deletecollection:
    - pods
read:
    - secrets
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `Editor` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`):

```text
read-write:
    - apps/deployments
    - apps/statefulsets
    - autoscaling.k8s.io/verticalpodautoscalers
    - autoscaling/horizontalpodautoscalers
    - batch/cronjobs
    - batch/jobs
    - configmaps
    - discovery.k8s.io/endpointslices
    - endpoints
    - extensions/deployments
    - extensions/ingresses
    - networking.k8s.io/ingresses
    - persistentvolumeclaims
    - policy/poddisruptionbudgets
    - serviceaccounts
    - services
write:
    - secrets
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `Admin` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`, `Editor`):

```text
create,patch,update:
    - pods
delete,deletecollection:
    - apps/replicasets
    - extensions/replicasets
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `ClusterEditor` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`, `Editor`):

```text
read:
    - rbac.authorization.k8s.io/clusterrolebindings
    - rbac.authorization.k8s.io/clusterroles
write:
    - apiextensions.k8s.io/customresourcedefinitions
    - apps/daemonsets
    - extensions/daemonsets
    - storage.k8s.io/storageclasses
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `ClusterAdmin` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`, `Editor`, `Admin`, `ClusterEditor`):

```text
read-write:
    - deckhouse.io/clusterauthorizationrules
write:
    - limitranges
    - namespaces
    - networking.k8s.io/networkpolicies
    - rbac.authorization.k8s.io/clusterrolebindings
    - rbac.authorization.k8s.io/clusterroles
    - rbac.authorization.k8s.io/rolebindings
    - rbac.authorization.k8s.io/roles
    - resourcequotas
```
<!-- end user-authz roles placeholder -->

Вы можете получить дополнительный список правил доступа для роли модуля из кластера ([существующие пользовательские правила](usage.html#настройка-прав-высокоуровневых-ролей) и нестандартные правила из других модулей Deckhouse):

```bash
D8_ROLE_NAME=Editor
kubectl get clusterrole -A -o jsonpath="{range .items[?(@.metadata.annotations.user-authz\.deckhouse\.io/access-level=='$D8_ROLE_NAME')]}{.rules}{'
'}{end}" | jq -s add
```

### 
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: speaker
  namespace: d8-{{ .Chart.Name }}
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: DaemonSet
    name: speaker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: speaker
      minAllowed:
        {{- include "speaker_resources" . | nindent 8 }}
      maxAllowed:
        cpu: 20m
        memory: 60Mi
    {{- include "helm_lib_vpa_kube_rbac_proxy_resources" . | nindent 4 }}
```

Helm-функции, описанные в файле `vpa.yaml` используются так же для установки ресурсов контейнеров в случае, если модуль `vertical-pod-autoscaler` отключен.

Для проставления ресурсов для `kube-rbac-proxy` используется специальная helm-функция `helm_lib_container_kube_rbac_proxy_resources`.

Пример:

```yaml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: speaker
  namespace: d8-{{ .Chart.Name }}
spec:
  selector:
    matchLabels:
      app: speaker
  template:
    metadata:
      labels:
        app: speaker
    spec: 
    containers:
      - name: speaker
        resources:
          requests:
          {{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler") }}
            {{- include "speaker_resources" . | nindent 14 }}
          {{- end }}
      - name: kube-rbac-proxy
        resources:
          requests:
          {{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler") }}
            {{- include "helm_lib_container_kube_rbac_proxy_resources" . | nindent 12 }}
          {{- end }}
```

#### Специальные лейблы для VPA-ресурсов

Если Pod'ы присутствуют только на мастер-узлах, для VPA-ресурса добавляется label `workload-resource-policy.deckhouse.io: master`.

Если Pod'ы присутствуют на каждом узле, для VPA-ресурса добавляется label `workload-resource-policy.deckhouse.io: every-node`.

#### TODO

* В настоящий момент для проставления ресурсов контейнеров используются значения из `minAllowed`. В этом случае возможен оверпровижининг на узле. Возможно правильнее было бы использовать значения `maxAllowed`.
* Значения `minAllowed` и `maxAllowed` проставляются вручную, возможно, определять нужно что-то одно, а второе вычислять. Например, определять `minAllowed` а `maxAllowed` считать как `minAllowed` X 2.
* Возможно стоит придумать другой механизм задания значений `minAllowed`, например, отдельный файл, в котором в YAML-структуре будут собраны данные по ресурсам всех контейнеров всех модулей.
* Issue #2084.

### Модуль vertical-pod-autoscaler

Vertical Pod Autoscaler (VPA) — это инфраструктурный сервис, который позволяет не выставлять точные resource requests, если неизвестно, сколько ресурсов необходимо контейнеру для работы. При использовании VPA и включении соответствующего режима работы resource requests выставляются автоматически на основе потребления ресурсов (полученных данных из Prometheus).
Как вариант, возможно только получать рекомендации по ресурсам, без их автоматического изменения.

У VPA есть следующие режимы работы:

- `"Auto"` (default) — в данный момент режимы работы `Auto` и `Recreate` делают одно и то же. Однако, когда в Kubernetes появится Pod in-place resource update, этот режим будет делать именно его.
- `"Recreate"` — режим разрешает VPA изменять ресурсы у запущенных подов (перезапускать их при работе). В случае работы одного пода (`replicas: 1`) это приведет к недоступности сервиса на время рестарта. В данном режиме VPA не пересоздает поды, которые были созданы без контроллера.
- `"Initial"` — VPA изменяет ресурсы подов только при создании подов, но не во время работы.
- `"Off"` — VPA не изменяет автоматически никакие ресурсы. В данном случае, если есть VPA c таким режимом работы, мы можем посмотреть, какие ресурсы рекомендует поставить VPA (`d8 k describe vpa <vpa-name>`).

Ограничения VPA:

- Обновление ресурсов запущенных подов — это экспериментальная возможность VPA. Каждый раз, когда VPA обновляет `resource requests` пода, под пересоздается. Под может быть создан на другом узле.
- VPA **не должен использоваться с HPA по CPU и памяти** в данный момент. Однако VPA можно использовать с HPA на custom/external metrics.
- VPA реагирует почти на все `out-of-memory` events, но не гарантирует реакцию.
- Производительность VPA не тестировалась на огромных кластерах.
- Рекомендации VPA могут превышать доступные ресурсы в кластере, что **может приводить к подам в состоянии Pending**.
- Использование нескольких VPA-ресурсов над одним подом может привести к неопределенному поведению.
- В случае удаления VPA или его «выключения» (режим `Off`) изменения, внесенные ранее VPA, не сбрасываются, а остаются в последнем измененном значении. Из-за этого может возникнуть путаница, что в Helm будут описаны одни ресурсы, при этом в контроллере тоже будут описаны одни ресурсы, но реально у подов ресурсы будут совсем другие и может сложиться впечатление, что они взялись «непонятно откуда».

{% alert level="info" %}
При использовании VPA настоятельно рекомендуется использовать Pod Disruption Budget.
{% endalert %}

#### Grafana dashboard

Представлено на дашбордах:

- `Main / Namespace`, `Main / Namespace / Controller`, `Main / Namespace / Controller / Pod` — столбец `VPA type` показывает значение `updatePolicy.updateMode`;
- `Main / Namespaces` — столбец `VPA %` показывает процент подов с включенным VPA.

#### Архитектура Vertical Pod Autoscaler

VPA состоит из 3 компонентов:

- `Recommender` — мониторит настоящее (делая запросы в Metrics API, который реализован в модуле [`prometheus-metrics-adapter`](/modules/prometheus-metrics-adapter/)) и прошлое потребление ресурсов (делая запросы в Trickster перед Prometheus) и предоставляет рекомендации по CPU и памяти для контейнеров.
- `Updater` — проверяет, что у подов с VPA выставлены корректные ресурсы, если нет — убивает эти поды, чтобы контроллер пересоздал поды с новыми resource requests.
- `Admission Plugin` — задает resource requests при создании новых подов (контроллером или из-за активности Updater'а).

При изменении ресурсов компонентом Updater это происходит с помощью Eviction API, поэтому учитываются `Pod Disruption Budget` для обновляемых подов.

### Модуль admission-policy-engine: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль cert-manager: настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль chrony: настройки

 
<!-- SCHEMA -->

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: chrony
spec:
  enabled: true
  settings:
    ntpServers:
      - pool.ntp.org
      - ntp.ubuntu.com
      - time.google.com
  version: 1
```
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль cilium-hubble: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Если модуль `cni-cilium` выключен, параметр `ciliumHubbleEnabled:` не повлияет на включение модуля `cilium-hubble`.

{% include module-conversion.liquid %}

{% include module-settings.liquid %}

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через `externalAuthentication`.
Если эти варианты отключены, модуль включит базовую аутентификацию со сгенерированным паролем.

Чтобы просмотреть сгенерированный пароль, выполните команду:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values cilium-hubble -o json | jq '.ciliumHubble.internal.auth.password'
```

Чтобы сгенерировать новый пароль, удалите Secret:

```shell
d8 k -n d8-cni-cilium delete secret/hubble-basic-auth
```

{% alert level="info" %}
Параметр `auth.password` больше не поддерживается.
{% endalert %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Cloud provider — DVP: настройки

Модуль автоматически включается для всех облачных кластеров, развернутых в DVP.

{% include module-alerts.liquid %}

Модуль не имеет настроек.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль cni-cilium: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль cni-flannel: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Конфигурация

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Конфигурация

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Управление control plane: настройки

Некоторые параметры кластера, влияющие на управление control plane, также берутся из ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-ceph: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-scsi-generic: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-yadro-tatlin-unified: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль dashboard: настройки

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через [`externalAuthentication`](examples.html).

Если ни один из этих способов не включен, модуль `dashboard` будет отключен.

{% alert level="warning" %}
Параметры `auth.password` и `accessLevel` больше не поддерживаются.
{% endalert %}

#### Настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль deckhouse-tools: настройки

У модуля нет обязательных настроек.

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: deckhouse-tools
spec:
  enabled: true
  version: 1
```

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль deckhouse: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль descheduler: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

У модуля нет обязательных настроек.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль documentation: настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](./user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, модуль включит базовую аутентификацию со сгенерированным паролем.

Чтобы посмотреть сгенерированный пароль, выполните команду:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values documentation -o json | jq '.internal.auth.password'
```

Чтобы сгенерировать новый пароль, удалите ресурс Secret:

```shell
d8 k -n d8-system delete secret/documentation-basic-auth
```

{% alert level="info" %}
Параметр `auth.password` больше не поддерживается.
{% endalert %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль extended-monitoring: настройки

#### Как использовать `extended-monitoring-exporter`

Чтобы включить экспортирование extended-monitoring метрик, нужно указать в пространстве имён лейбл `extended-monitoring.deckhouse.io/enabled` любым удобным способом, например:
- добавить в проект соответствующий helm-чарт (рекомендуемый);
- добавить в описание CI/CD (kubectl patch/create);
- добавить вручную (`d8 k label namespace my-app-production extended-monitoring.deckhouse.io/enabled=""`);
- настроить через [namespace-configurator](./namespace-configurator/) модуль.

Сразу же после этого для всех поддерживаемых Kubernetes-объектов в данном пространстве имён в Prometheus появятся default-метрики + любые кастомные с префиксом `threshold.extended-monitoring.deckhouse.io/`. Для ряда [non-namespaced](#non-namespaced-kubernetes-объекты) Kubernetes-объектов, описанных ниже, мониторинг включается автоматически.

К Kubernetes-объектам `threshold.extended-monitoring.deckhouse.io/что-то свое` можно добавить любые другие лейблы с указанным значением. Пример: `d8 k label pod test threshold.extended-monitoring.deckhouse.io/disk-inodes-warning=30`.
В таком случае значение из лейбла заменит значение по умолчанию.

Если вы хотите переопределить значения threshold для всех объектов в определенном пространстве имен, вы можете установить лейбл `threshold.extended-monitoring.deckhouse.io/` на уровне namespace. Например: `d8 k label namespace my-app-production threshold.extended-monitoring.deckhouse.io/5xx-warning=20`.
Это заменит значение по умолчанию для всех объектов namespace, для которых еще не установлен этот лейбл.

Слежение за объектом можно отключить индивидуально, поставив на него лейбл `extended-monitoring.deckhouse.io/enabled=false`. Соответственно, отключатся и лейблы по умолчанию, а также все алерты, привязанные к лейблам.

##### Стандартные лейблы и поддерживаемые Kubernetes-объекты

Далее приведен список используемых в Prometheus Rules лейблов, а также их стандартные значения.

**Обратите внимание,** что все лейблы начинаются с префикса `threshold.extended-monitoring.deckhouse.io/`. Указанное в лейбле значение — число, которое устанавливает порог срабатывания алерта.

Например, лейбл `threshold.extended-monitoring.deckhouse.io/5xx-warning: "5"` на Ingress-ресурсе изменяет порог срабатывания алерта с 10% (по умолчанию) на 5%.

###### Non-namespaced Kubernetes-объекты

Non-namespaced Kubernetes-объекты, то есть объекты вне пространств имён, не нуждаются в лейблах на этих пространствах и мониторинг на них включается по умолчанию при включении модуля.

####### Узел

| Лейбл                          | Тип           | Значение по умолчанию |
|--------------------------------|---------------|-----------------------|
| disk-bytes-warning             | int (percent) | 70                    |
| disk-bytes-critical            | int (percent) | 80                    |
| disk-inodes-warning            | int (percent) | 90                    |
| disk-inodes-critical           | int (percent) | 95                    |
| load-average-per-core-warning  | int           | 3                     |
| load-average-per-core-critical | int           | 10                    |

> **Важно!** Эти лейблы **не** действуют для тех разделов, в которых расположены `imagefs` (по умолчанию — `/var/lib/docker`) и `nodefs` (по умолчанию — `/var/lib/kubelet`).
Для этих разделов пороги настраиваются полностью автоматически согласно eviction thresholds в kubelet.
Значения по умолчанию указаны в документации Kubernetes, в описании экспортера.

###### Namespaced Kubernetes-объекты

####### Под

| Лейбл                | Тип           | Значение по умолчанию |
|----------------------|---------------|-----------------------|
| disk-bytes-warning   | int (percent) | 85                    |
| disk-bytes-critical  | int (percent) | 95                    |
| disk-inodes-warning  | int (percent) | 85                    |
| disk-inodes-critical | int (percent) | 90                    |

####### Ingress

| Лейбл        | Тип           | Значение по умолчанию |
|--------------|---------------|-----------------------|
| 5xx-warning  | int (percent) | 10                    |
| 5xx-critical | int (percent) | 20                    |

####### Deployment

| Лейбл              | Тип         | Значение по умолчанию |
|--------------------|-------------|-----------------------|
| replicas-not-ready | int (count) | 0                     |

Порог подразумевает количество недоступных реплик **сверх** maxUnavailable. Сработает, если недоступно реплик больше на указанное значение, чем разрешено в `maxUnavailable`. То есть при нуле сработает, если недоступно больше, чем указано в `maxUnavailable`, а при единице сработает, если недоступно больше, чем указано в `maxUnavailable`, плюс 1. Таким образом, у конкретных Deployment, которые находятся в пространстве имён со включенным расширенным мониторингом и которым допустимо быть недоступными, можно установить этот параметр, чтобы не получать ненужные алерты.

####### StatefulSet

| Лейбл              | Тип         | Значение по умолчанию |
|--------------------|-------------|-----------------------|
| replicas-not-ready | int (count) | 0                     |

Порог подразумевает количество недоступных реплик **сверх** maxUnavailable (см. комментарии к [Deployment](#deployment)).

####### DaemonSet

| Лейбл              | Тип         | Значение по умолчанию |
|--------------------|-------------|-----------------------|
| replicas-not-ready | int (count) | 0                     |

Порог подразумевает количество недоступных реплик **сверх** maxUnavailable (см. комментарии к [Deployment](#deployment)).

####### CronJob

Работает только выключение через лейбл `extended-monitoring.deckhouse.io/enabled=false`.

##### Как работает

Модуль экспортирует в Prometheus специальные лейблы Kubernetes-объектов. Позволяет улучшить Prometheus-правила путем добавления порога срабатывания для алертов.
Использование метрик, экспортируемых данным модулем, позволяет, например, заменить «магические» константы в правилах.

До:

```text
(
  kube_statefulset_status_replicas - kube_statefulset_status_replicas_ready
)
> 1
```

После:

```text
(
  kube_statefulset_status_replicas - kube_statefulset_status_replicas_ready
)
> on (namespace, statefulset)
(
  max by (namespace, statefulset) (extended_monitoring_statefulset_threshold{threshold="replicas-not-ready"})
)
```

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль ingress-nginx: настройки

{% alert level="info" %}
Если модуль был выключен и необходимо его включить, обратите внимание на глобальный параметр [publicDomainTemplate](/reference/api/global.html#параметры). Укажите его, если он не указан, иначе Ingress-ресурсы для служебных компонентов Deckhouse Platform Certified Security Edition (dashboard, user-auth, grafana, upmeter  и т. п.) не будут созданы.
{% endalert %}

Конфигурация Ingress-контроллеров выполняется с помощью Custom Resource [IngressNginxController](cr.html#ingressnginxcontroller).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль istio: настройки

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](./user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, модуль включит basic auth со сгенерированным паролем.

Посмотреть сгенерированный пароль можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values istio -o json | jq '.istio.internal.auth.password'
```

Чтобы сгенерировать новый пароль, нужно удалить Secret:

```shell
d8 k -n d8-istio delete secret/kiali-basic-auth
```

> **Внимание!** Параметр `auth.password` больше не поддерживается.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль kube-dns: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль local-path-provisioner: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Модуль не требует конфигурации.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль log-shipper: настройки

Модуль начинает чтение логов, только если создан pipeline в виде связанных между собой [ClusterLoggingConfig](cr.html#clusterloggingconfig)/[PodLoggingConfig](cr.html#podloggingconfig) и [ClusterLogDestination](cr.html#clusterlogdestination).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль loki: настройки

 
<!-- SCHEMA -->

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: loki
spec:
  settings:
    storageClass: ceph-csi-rbd
    diskSizeGigabytes: 10
    retentionPeriodHours: 48
  enabled: true
  version: 1
```
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Автоматическая настройка системы мониторинга для сбора метрик с пользовательских приложений: настройки

Модуль работает, если включен модуль `prometheus`, и не имеет параметров для настройки.

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Мониторинг control plane: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль monitoring-kubernetes: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль monitoring-ping: настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль multitenancy-manager: настройки

{% include module-alerts.liquid %}

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль namespace-configurator: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль network-policy-engine: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Управление узлами: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль okmeter: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль openvpn: настройки

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](./user-authn/). Также можно настроить аутентификацию с помощью параметра [externalAuthentication](#parameters-auth-externalauthentication). Если эти варианты отключены, модуль включит базовую аутентификацию со сгенерированным паролем.

Чтобы просмотреть сгенерированный пароль, выполните команду:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values openvpn -o json | jq '.openvpn.internal.auth.password'
```

Чтобы сгенерировать новый пароль, удалите ресурс Secret:

```shell
d8 k -n d8-openvpn delete secret/basic-auth
```

{% alert level="info" %}
Параметр `auth.password` больше не поддерживается.
{% endalert %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль operator-prometheus: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль prometheus-metrics-adapter: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Модуль работает, если включен модуль `prometheus`. В общем случае конфигурации не требуется.

#### Параметры

* `highAvailability` — ручное включение/отключение режима отказоустойчивости. По умолчанию режим отказоустойчивости определяется автоматически. Смотри [подробнее](/reference/api/global.html#параметры) про режим отказоустойчивости для модулей.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль Prometheus Pushgateway: настройки

Данный модуль устанавливает в кластер Prometheus Pushgateway. Он предназначен для приема метрик от приложения и отдачи их Prometheus.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Prometheus-мониторинг: настройки

Модуль не требует обязательной конфигурации (все работает из коробки).

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, модуль включит basic auth со сгенерированным паролем и пользователем `admin`.

Посмотреть сгенерированный пароль можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values prometheus -o json | jq '.internal.auth.password'
```

Чтобы сгенерировать новый пароль, нужно удалить Secret:

```shell
d8 k -n d8-monitoring delete secret/basic-auth
```

> **Внимание!** Параметр `auth.password` больше не поддерживается.

#### Примечание

* `retentionSize` для `main` и `longterm` **рассчитывается автоматически, возможности задать значение нет!**
  * Алгоритм расчета:
    * `pvc_size * 0.85` — если PVC существует;
    * `10 GiB` — если PVC нет и StorageClass поддерживает ресайз;
    * `25 GiB` — если PVC нет и StorageClass не поддерживает ресайз.
  * Если используется `local-storage` и требуется изменить `retentionSize`, необходимо вручную изменить размер PV и PVC в нужную сторону. **Внимание!** Для расчета берется значение из `.status.capacity.storage` PVC, поскольку оно отражает реальный размер PV в случае ручного ресайза.
* `40 GiB` — размер PersistentVolumeClaim создаваемого по умолчанию.
* Размер дисков Prometheus можно изменить стандартным для Kubernetes способом (если в StorageClass это разрешено), отредактировав в PersistentVolumeClaim поле `.spec.resources.requests.storage`.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль registry: настройка

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Для управления работой с container registry используйте секцию [`registry`](./deckhouse/configuration.html#parameters-registry) конфигурации модуля `deckhouse`, в которой можно указывать параметры подключения к registry и управлять режимом работы с ним.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### The secrets-store-integration module: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль snapshot-controller: настройки

> Модуль работает только в кластерах Kubernetes, начиная с версии 1.20.

В общем случае конфигурация модуля не требуется.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль data-exporter: настройки


 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль terraform-manager: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Мониторинг SLA кластера: настройки

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, то модуль включит basic auth со сгенерированным паролем.

Посмотреть сгенерированный пароль можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values upmeter -o json | jq '.upmeter.internal.auth.webui.password'
```

Чтобы сгенерировать новый пароль, нужно удалить Secret:

```shell
d8 k -n d8-upmeter delete secret/basic-auth-webui
```

Посмотреть сгенерированный пароль для страницы статуса можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values upmeter -o json | jq '.upmeter.internal.auth.status.password'
```

Чтобы сгенерировать новый пароль для страницы статуса, нужно удалить секрет:

```shell
d8 k -n d8-upmeter delete secret/basic-auth-status
```

> **Внимание!** Параметры `auth.status.password` и `auth.webui.password` больше не поддерживаются.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль user-authn: настройки

 
<!-- SCHEMA -->

Автоматический деплой oauth2-proxy.

**Важно!** Так как использование OpenID Connect по протоколу HTTP является слишком значительной угрозой безопасности (что подтверждается, например, тем, что Kubernetes API-сервер не поддерживает работу с OIDC по HTTP), данный модуль можно установить только при включенном HTTPS (`https.mode` выставить в отличное от `Disabled` значение или на уровне кластера, или в самом модуле).

**Важно!** При включении данного модуля аутентификация во всех веб-интерфейсах перестанет использовать HTTP Basic Auth и переключится на Dex (который, в свою очередь, будет использовать настроенные вами внешние провайдеры).
Для настройки kubectl необходимо перейти по адресу `https://kubeconfig.<modules.publicDomainTemplate>/`, авторизоваться в настроенном внешнем провайдере и скопировать shell-команды к себе в консоль.

**Важно!** Для работы аутентификации в dashboard и kubectl требуется [донастройка API-сервера](faq.html#настройка-kube-apiserver). Для автоматизации этого процесса реализован модуль [control-plane-manager](/modules/control-plane-manager/), который включен по умолчанию.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль user-authz: настройки

> **Внимание!** Мы категорически не рекомендуем создавать поды и ReplicaSet'ы — эти объекты являются второстепенными и должны создаваться из других контроллеров. Доступ к созданию и изменению подов и ReplicaSet'ов полностью отсутствует.
>
> **Внимание!** Режим multi-tenancy (авторизация по пространству имён) в данный момент реализован по временной схеме и **не гарантирует безопасность**! Если вебхук, который реализовывает систему авторизации, по какой-то причине будет недоступен, авторизация по пространству имён (опции `allowAccessToSystemNamespaces`, `namespaceSelector` и `limitNamespaces` в custom resource) перестанет работать и пользователи получат доступы во все пространства имён. После восстановления доступности вебхука всё вернётся на свои места.

Вся настройка прав доступа происходит с помощью [custom resources](cr.html).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль vertical-pod-autoscaler: настройки

VPA работает не с контроллером пода, а с самим подом, измеряя и изменяя параметры его контейнеров. Вся настройка происходит с помощью custom resource'а [`VerticalPodAutoscaler`](cr.html#verticalpodautoscaler).

В общем случае конфигурация модуля не требуется. У модуля есть только настройки `nodeSelector/tolerations`.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas[''].config-values | format_module_configuration: moduleKebabName }}

### Модуль admission-policy-engine: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль admission-policy-engine: Custom Resources (от Gatekeeper)
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль cert-manager: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Cloud provider — DVP: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль cni-cilium: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Управление control plane: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль csi-ceph: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль csi-scsi-generic: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль csi-yadro-tatlin-unified: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль descheduler: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль ingress-nginx: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль istio: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль istio: Custom Resources (от istio.io)
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль local-path-provisioner: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль log-shipper: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль multitenancy-manager: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Управление узлами: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль operator-prometheus: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль prometheus-metrics-adapter: Custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Prometheus-мониторинг: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль secrets-store-integration: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль data-exporter: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Мониторинг SLA кластера: кастомные ресурсы
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль user-authn: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль user-authz: Custom Resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль vertical-pod-autoscaler: custom resources
{{ site.data.schemas.common.crds.test-crd | format_crd: "common" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.local-path-provisioner.crds.local_path_provisioner | format_crd: "local-path-provisioner" }}
{{ site.data.schemas.control-plane-manager.crds.kube_scheduler_webhook_configuration | format_crd: "control-plane-manager" }}
{{ site.data.schemas.node-manager.crds.cluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.deckhousecontrolplane | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.extension-config | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.instancetypescatalogs | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-deployment | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-drain-rule | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-health-check | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-pools | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine-sets | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.machine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.mcm | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nfd-api-crds | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.node_group | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodegroupconfiguration | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.nodeuser | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.sshcredentials | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticcluster | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticinstance | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachine | format_crd: "node-manager" }}
{{ site.data.schemas.node-manager.crds.staticmachinetemplate | format_crd: "node-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}
{{ site.data.schemas.runtime-audit-engine.crds.falco-audit-rules | format_crd: "runtime-audit-engine" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-nfs.crds.nfsstorageclass | format_crd: "csi-nfs" }}
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.observability.crds.clustermetrics | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.clusterobservabilitypropagateddashboard | format_crd: "observability" }}
{{ site.data.schemas.observability.crds.observabilitydashboard | format_crd: "observability" }}
{{ site.data.schemas.sds-local-volume.crds.localstorageclass | format_crd: "sds-local-volume" }}
{{ site.data.schemas.sds-node-configurator.crds.blockdevices | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolume | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmlogicalvolumesnapshot | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupbackup | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.sds-node-configurator.crds.lvmvolumegroupset | format_crd: "sds-node-configurator" }}
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotclasses | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshotcontents | format_crd: "snapshot-controller" }}
{{ site.data.schemas.snapshot-controller.crds.snapshotstoragek8sio_volumesnapshots | format_crd: "snapshot-controller" }}
{{ site.data.schemas.storage-volume-data-manager.crds.dataexports | format_crd: "storage-volume-data-manager" }}
{{ site.data.schemas.virtualization.crds.clustervirtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisks | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualdisksnapshots | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualimages | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineblockdeviceattachments | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineclasses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddresses | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineipaddressleases | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachineoperations | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinerestores | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachines | format_crd: "virtualization" }}
{{ site.data.schemas.virtualization.crds.virtualmachinesnapshots | format_crd: "virtualization" }}

### Модуль cni-cilium: примеры

#### Egress Gateway

{% alert level="warning" %}Доступно в следующих редакциях: SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

##### Принцип работы

Для настройки egress-шлюза необходимы два кастомных ресурса:

1. EgressGateway — описывает группу узлов, один из которых будет выбран в качестве активного egress-шлюза, а остальные останутся в резерве на случай отказа:
   - Среди группы узлов, попадающих под `spec.nodeSelector`, будут отобраны пригодные к использованию. Один из них будет назначен активным шлюзом. Выбор активного узла осуществляется в алфавитном порядке.

     Признаки пригодного узла:
     - Узел в состоянии `Ready`.
       - Узел не находится в состоянии технического обслуживания (cordon).
       - `cilium-agent` на узле в состоянии `Ready`.
     - При использовании EgressGateway в режиме `VirtualIP` на активном узле запускается агент, который эмулирует «виртуальный» IP-адрес с использованием протокола ARP. При определении пригодности узла также учитывается состояние пода данного агента.
     - Разные EgressGateway могут использовать одни и те же узлы. Выбор активного узла в каждом EgressGateway осуществляется независимо от других, что позволяет сбалансировать нагрузку между ними.
1. EgressGatewayPolicy — описывает политику перенаправления сетевых запросов от подов в кластере на конкретный egress-шлюз, определённый с помощью EgressGateway.

##### Обслуживание узла

Для проведения работ на узле, который в данный момент является активным egress-шлюзом, выполните следующие шаги:
1. Снимите метку (label) с узла, чтобы исключить его из списка кандидатов для роли egress-шлюза. Egress-label — это метка, указанная в `spec.nodeSelector` вашего EgressGateway.

    ```bash
    d8 k label node <node-name> <egress-label>-
    ```

1. Переведите узел в режим обслуживания (cordon), чтобы предотвратить запуск новых подов:

    ```bash
    d8 k cordon <node-name>
    ```

    После этого Cilium автоматически выберет новый активный узел из оставшихся кандидатов.
    Трафик продолжит направляться через новый шлюз без прерывания.

1. Для возврата узла в работу выполните:

    ```bash
    d8 k uncordon <node-name>
    d8 k label node <node-name> <egress-label>=<value>
    ```

> Важно: повторное добавление метки может привести к тому, что узел снова будет выбран активным egress-шлюзом (если он первый в алфавитном порядке среди доступных кандидатов).
Чтобы избежать немедленного возврата узла в активное состояние, временно уменьшите количество реплик в EgressGateway или настройте приоритет выбора через дополнительные метки.

##### Сравнение с CiliumEgressGatewayPolicy

`CiliumEgressGatewayPolicy` подразумевает настройку только одного узла в качестве egress-шлюза. При выходе его из строя не предусмотрено failover-механизмов и сетевая связь будет нарушена.

##### Примеры настроек

###### EgressGateway в режиме PrimaryIPFromEgressGatewayNodeInterface (базовый режим)

```yaml
apiVersion: network.deckhouse.io/v1alpha1
kind: EgressGateway
metadata:
  name: myegressgw
spec:
  nodeSelector:
    dedicated/egress: ""
  sourceIP:
    mode: PrimaryIPFromEgressGatewayNodeInterface
    primaryIPFromEgressGatewayNodeInterface:
      # На всех узлах, попадающих под nodeSelector, «публичный» интерфейс должен называться одинаково.
      # При выходе из строя активного узла, трафик будет перенаправлен через резервный и
      # IP-адрес отправителя у сетевых пакетов поменяется.
      interfaceName: eth1
```

###### EgressGateway в режиме VirtualIPAddress (режим с Virtual IP)

```yaml
apiVersion: network.deckhouse.io/v1alpha1
kind: EgressGateway
metadata:
  name: myeg
spec:
  nodeSelector:
    dedicated/egress: ""
  sourceIP:
    mode: VirtualIPAddress
    virtualIPAddress:
      # На каждом узле должны быть настроены все необходимые маршруты для доступа на все внешние публичные сервисы,
      # «публичный» интерфейс должен быть подготовлен к автоматической настройке «виртуального» IP в качестве дополнительного (secondary) IP-адреса.
      # При выходе из строя активного узла, трафик будет перенаправлен через резервный и
      # IP-адрес отправителя у сетевых пакетов не поменяется.
      ip: 172.18.18.242
      # Список сетевых интерфейсов для «виртуального» IP.
      interfaces:
      - eth1
```

###### EgressGatewayPolicy

```yaml
apiVersion: network.deckhouse.io/v1alpha1
kind: EgressGatewayPolicy
metadata:
  name: my-egressgw-policy
spec:
  destinationCIDRs:
  - 0.0.0.0/0
  egressGatewayName: my-egressgw
  selectors:
  - podSelector:
      matchLabels:
        app: backend
        io.kubernetes.pod.namespace: my-ns
```

### Управление control plane: примеры

#### Подключение внешнего плагина планировщика

Пример подключения внешнего плагина планировщика через вебхук.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: KubeSchedulerWebhookConfiguration
metadata:
  name: sds-replicated-volume
webhooks:
- weight: 5
  failurePolicy: Ignore
  clientConfig:
    service:
      name: scheduler
      namespace: d8-sds-replicated-volume
      port: 8080
      path: /scheduler
    caBundle: ABCD=
  timeoutSeconds: 5
```

### Модуль csi-ceph: примеры

#### Пример описания `CephClusterConnection`

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephClusterConnection
metadata:
  name: ceph-cluster-1
spec:
  clusterID: 0324bfe8-c36a-4829-bacd-9e28b6480de9
  monitors:
  - 172.20.1.28:6789
  - 172.20.1.34:6789
  - 172.20.1.37:6789
  userID: admin
  userKey: AQDiVXVmBJVRLxAAg65PhODrtwbwSWrjJwssUg==
```

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get cephclusterconnection <имя cephclusterconnection>
```

#### Пример описания `CephStorageClass`

##### RBD

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-rbd-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: RBD
  rbd:
    defaultFSType: ext4
    pool: ceph-rbd-pool  
```

##### CephFS

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-fs-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: CephFS
  cephFS:
    fsName: cephfs
```

##### Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get cephstorageclass <имя storage class>
```

### Модуль csi-nfs: примеры

#### Конфигурация модуля с поддержкой RPC-with-TLS

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-nfs
spec:
  enabled: true
  version: 1
  settings:
    tlsParameters:
      ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZFVENDQXZtZ...
      mtls:
        clientCert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1J...
        clientKey: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUpRd0lCQ...
```

#### Создание StorageClass с поддержкой RPC-with-TLS

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: NFSStorageClass
metadata:
  name: nfs-storage-class
spec:
  connection:
    host: nfs-server-name.io
    share: /
    nfsVersion: "4.1"
    tls: true
    mtls: true
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
```

### Модуль dashboard: примеры

#### Пример конфигурации модуля

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: dashboard
spec:
  version: 1
  enabled: true
  settings:
    nodeSelector:
      node-role/system: ""
    tolerations:
    - key: dedicated.deckhouse.io
      operator: Equal
      value: system
    externalAuthentication:
      authURL: "https://<applicationDomain>/auth"
      authSignInURL: "https://<applicationDomain>/sign-in"
      authResponseHeaders: "Authorization"
```

### Модуль deckhouse-tools: примеры


### Модуль descheduler: примеры

#### Пример стратегии LowNodeUtilization

```yaml
---
apiVersion: deckhouse.io/v1alpha2
kind: Descheduler
metadata:
  name: low-node-utilization
spec:
  strategies:
    lowNodeUtilization:
      enabled: true
      thresholds:
        cpu: 20
      targetThresholds:
        cpu: 50
```

#### Пример стратегии HighNodeUtilization

```yaml
---
apiVersion: deckhouse.io/v1alpha2
kind: Descheduler
metadata:
  name: high-node-utilization
spec:
  strategies:
    highNodeUtilization:
      enabled: true
      thresholds:
        cpu: 50
        memory: 50
```

### Модуль documentation: примеры

#### Пример конфигурации модуля

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: documentation
spec:
  version: 1
  enabled: true
  settings:
    nodeSelector:
      node-role/system: ""
    tolerations:
    - key: dedicated.deckhouse.io
      operator: Equal
      value: system
    externalAuthentication:
      authURL: "https://<applicationDomain>/auth"
      authSignInURL: "https://<applicationDomain>/sign-in"
      authResponseHeaders: "Authorization"
```

### Модуль ingress-nginx: пример

{% raw %}

#### Пример для bare metal

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: nginx
  inlet: HostWithFailover
  nodeSelector:
    node-role.deckhouse.io/frontend: ""
  tolerations:
  - effect: NoExecute
    key: dedicated.deckhouse.io
    value: frontend
```

#### Пример для bare metal (при использовании внешнего балансировщика

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: nginx
  inlet: HostPort
  hostPort:
    httpPort: 80
    httpsPort: 443
    behindL7Proxy: true
```

{% endraw %}

#### Пример для bare metal (балансировщик MetalLB в режиме BGP LoadBalancer)

{% alert level="warning" %}Доступно в следующих редакциях: EE, CSE Pro (1.67).{% endalert %}

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: nginx
  inlet: LoadBalancer
  nodeSelector:
    node-role.deckhouse.io/frontend: ""
  tolerations:
  - effect: NoExecute
    key: dedicated.deckhouse.io
    value: frontend
```

В случае использования MetalLB его speaker-поды должны быть запущены на тех же узлах, что и поды Ingress–контроллера.

Контроллер должен получать реальные IP-адреса клиентов — поэтому его Service создается с параметром `externalTrafficPolicy: Local` (запрещая межузловой SNAT), и для принятия данного параметра MetalLB speaker анонсирует этот Service только с тех узлов, в которых запущены целевые поды.

Для этого примера [конфигурация модуля `metallb`](./metallb/configuration.html) должна быть следующей:

```yaml
metallb:
 speaker:
   nodeSelector:
     node-role.deckhouse.io/frontend: ""
   tolerations:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      value: frontend
```

#### Пример для bare metal (балансировщик MetalLB в режиме L2 LoadBalancer)

{% alert level="warning" %}Доступно в следующих редакциях: SE, SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

1. Включите модуль `metallb`:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: metallb
   spec:
     enabled: true
     version: 2
   ```

1. Создайте ресурс _MetalLoadBalancerClass_:

   ```yaml
   apiVersion: network.deckhouse.io/v1alpha1
   kind: MetalLoadBalancerClass
   metadata:
     name: ingress
   spec:
     addressPool:
       - 192.168.2.100-192.168.2.150
     isDefault: false
     nodeSelector:
       node-role.kubernetes.io/loadbalancer: "" # селектор узлов-балансировщиков
     type: L2
   ```

1. Создайте ресурс _IngressNginxController_:

   ```yaml
   apiVersion: deckhouse.io/v1
   kind: IngressNginxController
   metadata:
     name: main
   spec:
     ingressClass: nginx
     inlet: LoadBalancer
     loadBalancer:
       loadBalancerClass: ingress
       annotations:
         # Количество адресов, которые будут выделены из пула, объявленного в MetalLoadBalancerClass.
         network.deckhouse.io/l2-load-balancer-external-ips-count: "3"
     # Селектор и tolerations. Поды ingress-controller должны быть размещены на тех же узлах, что и поды MetalLB speaker.
     nodeSelector:
        node-role.kubernetes.io/loadbalancer: ""
     tolerations:
     - effect: NoSchedule
       key: node-role/loadbalancer
       operator: Exists
   ```

1. Платформа создаст сервис с типом `LoadBalancer`, которому будет присвоено заданное количество адресов:

   ```shell
   $ d8 k -n d8-ingress-nginx get svc
   NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP                                 PORT(S)                      AGE
   main-load-balancer     LoadBalancer   10.222.130.11   192.168.2.100,192.168.2.101,192.168.2.102   80:30689/TCP,443:30668/TCP   11s
   ```

### Модуль istio: примеры

#### Circuit Breaker

Для выявления проблемных эндпоинтов используются настройки `outlierDetection` в кастомном ресурсе [DestinationRule](istio-cr.html#destinationrule).

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: reviews-cb-policy
spec:
  host: reviews.prod.svc.cluster.local
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100 # Максимальное число коннектов в сторону host, суммарно для всех эндпоинтов.
      http:
        maxRequestsPerConnection: 10 # Каждые 10 запросов коннект будет пересоздаваться.
    outlierDetection:
      consecutive5xxErrors: 7 # Допустимо 7 ошибок (включая пятисотые, TCP-таймауты и HTTP-таймауты)
      interval: 5m            # в течение пяти минут,
      baseEjectionTime: 15m   # после которых эндпоинт будет исключен из балансировки на 15 минут.
```

А также для настройки HTTP-таймаутов используется ресурс [VirtualService](istio-cr.html#virtualservice). Эти таймауты также учитываются при подсчете статистики ошибок на эндпоинтах.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-productpage-rule
  namespace: myns
spec:
  hosts:
  - productpage
  http:
  - timeout: 5s
    route:
    - destination:
        host: productpage
```

#### Балансировка gRPC

**Важно!** Чтобы балансировка gRPC-сервисов заработала автоматически, присвойте name с префиксом или значением `grpc` для порта в соответствующем Service.

#### Locality Failover

> При необходимости ознакомьтесь с основной документацией.

Istio позволяет настроить приоритетный географический фейловер между эндпоинтами. Для определения зоны Istio использует лейблы узлов с соответствующей иерархией:

* `topology.istio.io/subzone`;
* `topology.kubernetes.io/zone`;
* `topology.kubernetes.io/region`.

Это полезно для межкластерного failover при использовании совместно с [мультикластером](#устройство-мультикластера-из-двух-кластеров-с-помощью-ресурса-istiomulticluster).

> **Важно!** Для включения Locality Failover используется ресурс DestinationRule, в котором также необходимо настроить `outlierDetection`.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: helloworld
spec:
  host: helloworld
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true # Включили LF.
    outlierDetection: # outlierDetection включить обязательно.
      consecutive5xxErrors: 1
      interval: 1s
      baseEjectionTime: 1m
```

#### Retry

С помощью ресурса [VirtualService](istio-cr.html#virtualservice) можно настроить Retry для запросов.

**Внимание!** По умолчанию при возникновении ошибок все запросы (включая POST-запросы) выполняются повторно до трех раз.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: ratings-route
spec:
  hosts:
  - ratings.prod.svc.cluster.local
  http:
  - route:
    - destination:
        host: ratings.prod.svc.cluster.local
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: gateway-error,connect-failure,refused-stream
```

#### Canary

**Важно!** Istio отвечает лишь за гибкую маршрутизацию запросов, которая опирается на спецзаголовки запросов (например, cookie) или просто на случайность. За настройку этой маршрутизации и «переключение» между канареечными версиями отвечает CI/CD-система.

Подразумевается, что в одном пространстве имён развёрнуты два Deployment с разными версиями приложения. У подов разных версий разные лейблы (`version: v1` и `version: v2`).

Требуется настроить два кастомных ресурса:
* [DestinationRule](istio-cr.html#destinationrule) с описанием, как идентифицировать разные версии вашего приложения (subset'ы);
* [VirtualService](istio-cr.html#virtualservice) с описанием, как распределять трафик между разными версиями приложения.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: productpage-canary
spec:
  host: productpage
  # subset'ы доступны только при обращении к хосту через VirtualService из пода под управлением Istio.
  # Эти subset'ы должны быть указаны в маршрутах.
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

##### Распределение по наличию cookie

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: productpage-canary
spec:
  hosts:
  - productpage
  http:
  - match:
    - headers:
       cookie:
         regex: "^(.*;?)?(canary=yes)(;.*)?"
    route:
    - destination:
        host: productpage
        subset: v2 # Ссылка на subset из DestinationRule.
  - route:
    - destination:
        host: productpage
        subset: v1
```

##### Распределение по вероятности

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: productpage-canary
spec:
  hosts:
  - productpage
  http:
  - route:
    - destination:
        host: productpage
        subset: v1 # Ссылка на subset из DestinationRule.
      weight: 90 # Процент трафика, который получат поды с лейблом version: v1.
  - route:
    - destination:
        host: productpage
        subset: v2
      weight: 10
```

#### Ingress для публикации приложений

##### Istio Ingress Gateway

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IngressIstioController
metadata:
 name: main
spec:
  # ingressGatewayClass содержит значение селектора меток, используемое при создании ресурса Gateway.
  ingressGatewayClass: istio-hp
  inlet: HostPort
  hostPort:
    httpPort: 80
    httpsPort: 443
  nodeSelector:
    node-role.deckhouse.io/frontend: ""
  tolerations:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      operator: Equal
      value: frontend
  resourcesRequests:
    mode: VPA
```

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-tls-secert
  namespace: d8-ingress-istio # Обратите внимание, пространство имён не является app-ns.
type: kubernetes.io/tls
data:
  tls.crt: |
    <tls.crt data>
  tls.key: |
    <tls.key data>
```

```yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: gateway-app
  namespace: app-ns
spec:
  selector:
    # Селектор меток для использования Istio Ingress Gateway main-hp.
    istio.deckhouse.io/ingress-gateway-class: istio-hp
  servers:
    - port:
        # Стандартный шаблон для использования протокола HTTP.
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - app.example.com
    - port:
        # Стандартный шаблон для использования протокола HTTPS.
        number: 443
        name: https
        protocol: HTTPS
      tls:
        mode: SIMPLE
        # Ресурс Secret с сертификатом и ключом, который должен быть создан в пространстве имён d8-ingress-istio.
        credentialName: app-tls-secrets
      hosts:
        - app.example.com
```

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: vs-app
  namespace: app-ns
spec:
  gateways:
    - gateway-app
  hosts:
    - app.example.com
  http:
    - route:
        - destination:
            host: app-svc
```

##### NGINX Ingress

Для работы с NGINX Ingress требуется подготовить:
* Ingress-контроллер, добавив к нему sidecar от Istio. В нашем случае включить параметр `enableIstioSidecar` в кастомном ресурсе [IngressNginxController](/modules/ingress-nginx/cr.html#ingressnginxcontroller) модуля [ingress-nginx](/modules/ingress-nginx/).
* Ingress-ресурс, который ссылается на Service. Обязательные аннотации для Ingress-ресурса:
  * `nginx.ingress.kubernetes.io/service-upstream: "true"` — с этой аннотацией Ingress-контроллер будет отправлять запросы на ClusterIP сервиса (из диапазона Service CIDR) вместо того, чтобы слать их напрямую в поды приложения. Sidecar-контейнер `istio-proxy` перехватывает трафик только в сторону диапазона Service CIDR, остальные запросы отправляются напрямую;
  * `nginx.ingress.kubernetes.io/upstream-vhost: myservice.myns.svc` — с данной аннотацией sidecar сможет идентифицировать прикладной сервис, для которого предназначен запрос.

Примеры:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: productpage
  namespace: bookinfo
  annotations:
    # Просим nginx проксировать трафик на ClusterIP вместо собственных IP подов.
    nginx.ingress.kubernetes.io/service-upstream: "true"
    # В Istio вся маршрутизация осуществляется на основе `Host:` заголовка запросов.
    # Чтобы не сообщать Istio о существовании внешнего домена `productpage.example.com`,
    # мы просто используем внутренний домен, о котором Istio осведомлен.
    nginx.ingress.kubernetes.io/upstream-vhost: productpage.bookinfo.svc
spec:
  rules:
    - host: productpage.example.com
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: productpage
              port:
                number: 9080
```

```yaml
apiVersion: v1
kind: Service
metadata:
  name: productpage
  namespace: bookinfo
spec:
  ports:
  - name: http
    port: 9080
  selector:
    app: productpage
  type: ClusterIP
```

#### Примеры настройки авторизации

##### Алгоритм принятия решения

**Важно!** Как только для приложения создается `AuthorizationPolicy`, начинает работать следующий алгоритм принятия решения о судьбе запроса:
* Если запрос попадает под политику DENY — запретить запрос.
* Если для данного приложения нет политик ALLOW — разрешить запрос.
* Если запрос попадает под политику ALLOW — разрешить запрос.
* Все остальные запросы — запретить.

Иными словами, если вы явно что-то запретили, работает только ваш запрет. Если же вы что-то явно разрешили, теперь разрешены только явно одобренные запросы (запреты никуда не исчезают и имеют приоритет).

**Важно!** Для работы политик, основанных на высокоуровневых параметрах, таких как пространства имён или principal, необходимо, чтобы все вовлеченные сервисы работали под управлением Istio. Также между приложениями должен быть организован Mutual TLS.

Примеры:
* Запретим POST-запросы для приложения myapp. Отныне, так как для приложения появилась политика, согласно алгоритму выше будут запрещены только POST-запросы к приложению.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-post-requests
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    action: DENY
    rules:
    - to:
      - operation:
          methods: ["POST"]
  ```

* Здесь для приложения создана политика ALLOW. При ней будут разрешены только запросы из NS `bar`, остальные запрещены.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    action: ALLOW # default, можно не указывать.
    rules:
    - from:
      - source:
          namespaces: ["bar"]
  ```

* Здесь для приложения создана политика ALLOW. При этом она не имеет ни одного правила, и поэтому ни один запрос под нее не попадет, но она таки есть. Поэтому, согласно алгоритму, раз что-то разрешено, то все остальное запрещено. В данном случае все остальное — это вообще все запросы.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    action: ALLOW # default, можно не указывать.
    rules: []
  ```

* Здесь для приложения созданы политика ALLOW (это default) и одно пустое правило. Под это правило попадает любой запрос и автоматически получает добро.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: allow-all
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    rules:
    - {}
  ```

##### Запретить все действия в рамках пространства имён foo

Два способа:

* Запретить явно. Здесь мы создаем политику DENY с единственным универсальным фильтром `{}`, под который попадают все запросы:

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec:
    action: DENY
    rules:
    - {}
  ```

* Неявно. Здесь мы создаем политику ALLOW (по умолчанию), но не создаем ни одного фильтра, так что ни один запрос под нее не попадет и будет автоматически запрещен.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec: {}
  ```

##### Запретить доступ только из пространства имён foo

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: deny-from-ns-foo
 namespace: myns
spec:
 action: DENY
 rules:
 - from:
   - source:
       namespaces: ["foo"]
```

##### Разрешить запросы только в рамках нашего пространства имён foo

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-intra-namespace-only
 namespace: foo
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       namespaces: ["foo"]
```

##### Разрешить из любого места в нашем кластере

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-my-cluster
 namespace: myns
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       principals: ["mycluster.local/*"]
```

##### Разрешить любые запросы только кластеров foo или bar

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-foo-or-bar-clusters-to-ns-baz
 namespace: baz
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       principals: ["foo.local/*", "bar.local/*"]
```

##### Разрешить любые запросы только кластеров foo или bar, при этом из пространства имён baz

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-foo-or-bar-clusters-to-ns-baz
 namespace: baz
spec:
 action: ALLOW
 rules:
 - from:
   - source: # Правила ниже логически перемножаются.
       namespaces: ["baz"]
       principals: ["foo.local/*", "bar.local/*"]
```

##### Разрешить из любого кластера (по mTLS)

**Важно!** Если есть запрещающие правила, у них будет приоритет. Смотри [алгоритм](#алгоритм-принятия-решения).

Пример:

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-any-cluster-with-mtls
 namespace: myns
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       principals: ["*"] # To set mTLS mandatory.
```

##### Разрешить вообще откуда угодно (в том числе без mTLS)

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-any
 namespace: myns
spec:
 action: ALLOW
 rules: [{}]
```

#### Устройство федерации из двух кластеров с помощью кастомного ресурса IstioFederation

Cluster A:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioFederation
metadata:
  name: cluster-b
spec:
  metadataEndpoint: https://istio.k8s-b.example.com/metadata/
  trustDomain: cluster-b.local
```

Cluster B:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioFederation
metadata:
  name: cluster-a
spec:
  metadataEndpoint: https://istio.k8s-a.example.com/metadata/
  trustDomain: cluster-a.local
```

#### Устройство мультикластера из двух кластеров с помощью ресурса IstioMulticluster

Cluster A:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioMulticluster
metadata:
  name: cluster-b
spec:
  metadataEndpoint: https://istio.k8s-b.example.com/metadata/
```

Cluster B:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioMulticluster
metadata:
  name: cluster-a
spec:
  metadataEndpoint: https://istio.k8s-a.example.com/metadata/
```

#### Управление поведением data plane

##### Предотвратить завершение работы istio-proxy до завершения соединений основного приложения

По умолчанию в процессе остановки пода все контейнеры, включая istio-proxy, получают сигнал SIGTERM одновременно. Но некоторым приложениям для правильного завершения работы необходимо время и иногда дополнительная сетевая активность. Это невозможно, если istio-proxy завершился раньше.

Решение — добавить в istio-proxy preStop-хук для оценки активности прикладных контейнеров, а единственный доступный метод — это выявление сетевых сокетов приложения, и если таковых нет, тогда можно останавливать контейнер.

Аннотация ниже добавляет описанный выше preStop-хук в контейнер istio-proxy прикладного пода:

```yaml
annotations:
  inject.istio.io/templates: "sidecar,d8-hold-istio-proxy-termination-until-application-stops"
```

#### Ограничения режима перенаправления прикладного трафика `CNIPlugin`

В отличие от режима `InitContainer`, настройка перенаправления осуществляется в момент создании пода, а не в момент срабатывания init-контейнера `istio-init`. Это значит, что прикладные init-контейнеры не смогут взаимодействовать с остальными сервисами так как весь трафик будет перенаправлен на обработку в sidecar-контейнер `istio-proxy`, который ещё не запущен. Обходные пути:

* Запустить прикладной init-контейнер от пользователя с uid `1337`. Запросы данного пользователя не перехватываются под управление Istio.
* Исключить IP-адрес или порт сервиса из-под контроля Istio с помощью аннотаций `traffic.sidecar.istio.io/excludeOutboundIPRanges` или `traffic.sidecar.istio.io/excludeOutboundPorts`.

{% alert level="warning" %}Каждый из обходных вариантов выводит трафик из-под контроля Istio, что в свою очередь убирает шифрование трафика между прикладными сервисами.{% endalert %}

#### Обновление Istio

#### Обновление control plane Istio

* Deckhouse позволяет инсталлировать несколько версий control plane одновременно:
  * Одна глобальная, обслуживает пространства имён или поды без явного указания версии (лейбл у пространства имён `istio-injection: enabled`). Настраивается параметром [globalVersion](configuration.html#parameters-globalversion).
  * Остальные — дополнительные, обслуживают пространства имён или поды с явным указанием версии (лейбл у пространства имён или пода `istio.io/rev: v1x21`). Настраиваются параметром [additionalVersions](configuration.html#parameters-additionalversions).
* Istio заявляет обратную совместимость между data plane и control plane в диапазоне двух минорных версий:
* Алгоритм обновления (для примера, с версии `1.19` на версию `1.21`):
  * Добавить желаемую версию в параметр модуля [additionalVersions](configuration.html#parameters-additionalversions) (`additionalVersions: ["1.21"]`).
  * Дождаться появления соответствующего пода `istiod-v1x21-xxx-yyy` в пространства имён `d8-istio`.
  * Для каждого прикладного пространства имён, где включен istio:
    * поменять лейбл `istio-injection: enabled` на `istio.io/rev: v1x21`;
    * по очереди пересоздать поды в пространстве имён, параллельно контролируя работоспособность приложения.
  * Поменять настройку `globalVersion` на `1.21` и удалить `additionalVersions`.
  * Убедиться, что старый под `istiod` удалился.
  * Поменять лейблы прикладных пространств имён на `istio-injection: enabled`.

Чтобы найти все поды под управлением старой ревизии Istio (в примере — версия 19), выполните команду:

```shell
d8 k get pods -A -o json | jq --arg revision "v1x19" \
  '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson |
   .revision == $revision) | .metadata.namespace + "/" + .metadata.name'
```

{% alert level="warning" %}Обновление до версии Istio 1.25 возможно только с версии 1.21.{% endalert %}

##### Автоматическое обновление data plane Istio

Для автоматизации обновления istio-sidecar'ов установите лейбл `istio.deckhouse.io/auto-upgrade="true"` на `Namespace` либо на отдельный ресурс — `Deployment`, `DaemonSet` или `StatefulSet`.

#### Настройка ресурсов istio-proxy sidecar

Для переопределения глобальных ограничений ресурсов для istio-proxy sidecar в отдельных рабочих нагрузках через аннотации, поддерживаются следующие аннотации:

##### Поддерживаемые аннотации

| Аннотация                          | Описание                     | Пример значения |
|-------------------------------------|-----------------------------|---------------|
| `sidecar.istio.io/proxyCPU`         | Запрос CPU для sidecar      | `200m`        |
| `sidecar.istio.io/proxyCPULimit`    | Лимит CPU для sidecar       | `"1"`         |
| `sidecar.istio.io/proxyMemory`      | Запрос памяти для sidecar   | `128Mi`       |
| `sidecar.istio.io/proxyMemoryLimit` | Лимит памяти для sidecar    | `512Mi`       |

##### Примеры конфигурации

Для Deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
### ...
spec:
  template:
    metadata:
      annotations:
          sidecar.istio.io/proxyCPU: 200m
          sidecar.istio.io/proxyCPULimit: "1"
          sidecar.istio.io/proxyMemory: 128Mi
          sidecar.istio.io/proxyMemoryLimit: 512Mi
### ... остальная часть манифеста
```

Для ReplicaSet:

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
### ...
spec:
  template:
    metadata:
      annotations:
          sidecar.istio.io/proxyCPU: 200m
          sidecar.istio.io/proxyCPULimit: "1"
          sidecar.istio.io/proxyMemory: 128Mi
          sidecar.istio.io/proxyMemoryLimit: 512Mi
### ... остальная часть манифеста
```

Для Pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    sidecar.istio.io/proxyCPU: 200m
    sidecar.istio.io/proxyCPULimit: "1"
    sidecar.istio.io/proxyMemory: 128Mi
    sidecar.istio.io/proxyMemoryLimit: 512Mi
### ... остальная часть манифеста
```

{% alert level="warning" %}Все четыре параметра должны быть указаны вместе — `sidecar.istio.io/proxyCPU`, `sidecar.istio.io/proxyCPULimit`, `sidecar.istio.io/proxyMemory` и `sidecar.istio.io/proxyMemoryLimit`. Частичная конфигурация не поддерживается.{% endalert %}

### Модуль kube-dns: примеры

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: kube-dns
spec:
  version: 1
  enabled: true
  settings:
    upstreamNameservers:
    - 8.8.8.8
    - 8.8.4.4
    hosts:
    - domain: one.example.com
      ip: 192.168.0.1
    - domain: two.another.example.com
      ip: 10.10.0.128
    stubZones:
    - zone: consul.local
      upstreamNameservers:
      - 10.150.0.1
    enableLogs: true
    clusterDomainAliases:
    - foo.bar
    - baz.qux
```

### Модуль local-path-provisioner: примеры

#### Пример custom resource `LocalPathProvisioner`

Reclaim policy устанавливается по умолчанию в `Retain`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: LocalPathProvisioner
metadata:
  name: localpath-system
spec:
  nodeGroups:
  - system
  path: "/opt/local-path-provisioner"
```

#### Пример custom resource `LocalPathProvisioner` с установленным `reclaimPolicy`

Reclaim policy устанавливается в `Delete`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: LocalPathProvisioner
metadata:
  name: localpath-system
spec:
  nodeGroups:
  - system
  path: "/opt/local-path-provisioner"
  reclaimPolicy: "Delete"
```

### Модуль log-shipper: примеры

{% raw %}

#### Чтение логов из всех подов кластера и направление их в Loki

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: all-logs
spec:
  type: KubernetesPods
  destinationRefs:
  - loki-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

#### Чтение логов подов из указанного namespace с указанным label и перенаправление одновременно в Loki и Elasticsearch

Чтение логов подов из namespace `whispers` только с label `app=booking` и перенаправление одновременно в Loki и Elasticsearch:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: whispers-booking-logs
spec:
  type: KubernetesPods
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [whispers]
    labelSelector:
      matchLabels:
        app: booking
  destinationRefs:
  - loki-storage
  - es-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    index: logs-%F
    auth:
      strategy: Basic
      user: elastic
      password: c2VjcmV0IC1uCg==
```

#### Создание source в namespace и чтение логов всех подов в этом NS с направлением их в Loki

Следующий pipeline создает source в namespace `test-whispers`, читает логи всех подов в этом NS и пишет их в Loki:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: PodLoggingConfig
metadata:
  name: whispers-logs
  namespace: tests-whispers
spec:
  clusterDestinationRefs:
    - loki-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

#### Чтение только подов в указанном namespace и с определенным label

Пример чтения только подов, имеющих label `app=booking`, в namespace `test-whispers`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: PodLoggingConfig
metadata:
  name: whispers-logs
  namespace: tests-whispers
spec:
  labelSelector:
    matchLabels:
      app: booking
  clusterDestinationRefs:
    - loki-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

#### Переход с Promtail на Log-Shipper

В ранее используемом URL Loki требуется убрать путь `/loki/api/v1/push`.

**Vector** сам добавит этот путь при работе с Loki.

#### Работа с Grafana Cloud

Данная документация подразумевает, что у вас уже создан ключ API.

Для начала вам потребуется закодировать в base64 ваш токен доступа к Grafana Cloud.

![Grafana cloud API key](images/grafana_cloud.png)

```bash
echo -n "<YOUR-GRAFANACLOUD-TOKEN>" | base64 -w0
```

Затем нужно создать **ClusterLogDestination**

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  loki:
    auth:
      password: PFlPVVItR1JBRkFOQUNMT1VELVRPS0VOPg==
      strategy: Basic
      user: "<YOUR-GRAFANACLOUD-USER>"
    endpoint: <YOUR-GRAFANACLOUD-URL> # Например https://logs-prod-us-central1.grafana.net или https://logs-prod-eu-west-0.grafana.net
  type: Loki
```

Теперь можно создать PodLogginConfig или ClusterPodLoggingConfig и отправлять логи в **Grafana Cloud**.

#### Добавление Loki в Deckhouse Grafana

Вы можете работать с Loki из встроенной в Deckhouse Grafana. Достаточно добавить [**GrafanaAdditionalDatasource**](/modules/prometheus/cr.html#grafanaadditionaldatasource).

```yaml
apiVersion: deckhouse.io/v1
kind: GrafanaAdditionalDatasource
metadata:
  name: loki
spec:
  access: Proxy
  basicAuth: false
  jsonData:
    maxLines: 5000
    timeInterval: 30s
  type: loki
  url: http://loki.loki:3100
```

#### Поддержка Elasticsearch < 6.X

Для Elasticsearch < 6.0 нужно включить поддержку doc_type индексов.
Сделать это можно следующим образом:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    docType: "myDocType" # Укажите значение здесь. Оно не должно начинаться с '_'.
    auth:
      strategy: Basic
      user: elastic
      password: c2VjcmV0IC1uCg==
```

#### Шаблон индекса для Elasticsearch

Существует возможность отправлять сообщения в определенные индексы на основе метаданных с помощью шаблонов индексов:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    index: "k8s-{{ namespace }}-%F"
```

В приведенном выше примере для каждого пространства имен Kubernetes будет создан свой индекс в Elasticsearch.

Эта функция также хорошо работает в комбинации с `extraLabels`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    index: "k8s-{{ service }}-{{ namespace }}-%F"
  extraLabels:
    service: "{{ service_name }}"
```

1. Если сообщение имеет формат JSON, поле `service_name` этого документа JSON перемещается на уровень метаданных.
2. Новое поле метаданных `service` используется в шаблоне индекса.

#### Пример интеграции со Splunk

Существует возможность отсылать события из Deckhouse в Splunk.

1. Endpoint должен быть таким же, как имя вашего экземпляра Splunk с портом `8088` и без указания пути, например `https://prd-p-xxxxxx.splunkcloud.com:8088`.
2. Чтобы добавить token для доступа, откройте пункт меню `Setting` -> `Data inputs`, добавьте новый `HTTP Event Collector` и скопируйте token.
3. Укажите индекс Splunk для хранения логов, например `logs`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: splunk
spec:
  type: Splunk
  splunk:
    endpoint: https://prd-p-xxxxxx.splunkcloud.com:8088
    token: xxxx-xxxx-xxxx
    index: logs
    tls:
      verifyCertificate: false
      verifyHostname: false
```

{% endraw %}
{% alert -%}
`destination` не поддерживает метки пода для индексирования. Рассмотрите возможность добавления нужных меток с помощью опции `extraLabels`.
{%- endalert %}
{% raw %}

```yaml
extraLabels:
  pod_label_app: '{{ pod_labels.app }}'
```

#### Простой пример Logstash

Чтобы отправлять логи в Logstash, на стороне Logstash должен быть настроен входящий поток `tcp` и его кодек должен быть `json`.

Пример минимальной конфигурации Logstash:

```hcl
input {
  tcp {
    port => 12345
    codec => json
  }
}
output {
  stdout { codec => json }
}
```

Пример манифеста `ClusterLogDestination`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: logstash
spec:
  type: Logstash
  logstash:
    endpoint: logstash.default:12345
```

#### Syslog

Следующий пример показывает, как отправлять сообщения через сокет по протоколу TCP в формате syslog:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: rsyslog
spec:
  type: Socket
  socket:
    mode: TCP
    address: 192.168.0.1:3000
    encoding: 
      codec: Syslog
  extraLabels:
    syslog.severity: "alert"
    # поле request_id должно присутствовать в сообщении
    syslog.message_id: "{{ request_id }}"
```

#### Пример интеграции с Graylog

Убедитесь, что в Graylog настроен входящий поток для приема сообщений по протоколу TCP на указанном порту. Пример манифеста для интеграции с Graylog:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: test-socket2-dest
spec:
  type: Socket
  socket:
    address: graylog.svc.cluster.local:9200
    mode: TCP
    encoding:
      codec: GELF
```

#### Логи в CEF формате

Существует способ формировать логи в формате CEF, используя `codec: CEF`, с переопределением `cef.name` и `cef.severity` по значениям из поля `message` (лога приложения) в формате JSON.

В примере ниже `app` и `log_level` это ключи содержащие значения для переопределения:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: siem-kafka
spec:
  extraLabels:
    cef.name: '{{ app }}'
    cef.severity: '{{ log_level }}'
  type: Kafka
  kafka:
    bootstrapServers:
      - my-cluster-kafka-brokers.kafka:9092
    encoding:
      codec: CEF
    tls:
      verifyCertificate: false
      verifyHostname: true
    topic: logs
```

Так же можно вручную задать свои значения:

```yaml
extraLabels:
  cef.name: 'TestName'
  cef.severity: '1'
```

#### Сбор событий Kubernetes

События Kubernetes могут быть собраны log-shipper'ом, если `events-exporter` включен в настройках модуля [extended-monitoring](./extended-monitoring/).

Включите events-exporter, изменив параметры модуля `extended-monitoring`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: extended-monitoring
spec:
  version: 1
  settings:
    events:
      exporterEnabled: true
```

Выложите в кластер следующий `ClusterLoggingConfig`, чтобы собирать сообщения с пода `events-exporter`:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: kubernetes-events
spec:
  type: KubernetesPods
  kubernetesPods:
    labelSelector:
      matchLabels:
        app: events-exporter
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [d8-monitoring]
  destinationRefs:
  - loki-storage
```

#### Фильтрация логов

Пользователи могут фильтровать логи, используя следующие фильтры:

* `labelFilter` — применяется к метаданным, например имени контейнера (`container`), пространству имен (`namespace`) или имени пода (`pod_name`);
* `logFilter` — применяется к полям самого сообщения, если оно в JSON-формате.

##### Сборка логов только для контейнера `nginx`

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: nginx-logs
spec:
  type: KubernetesPods
  labelFilter:
  - field: container
    operator: In
    values: [nginx]
  destinationRefs:
  - loki-storage
```

##### Сборка логов без строки, содержащей `GET /status" 200`

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: all-logs
spec:
  type: KubernetesPods
  destinationRefs:
  - loki-storage
  labelFilter:
  - field: message
    operator: NotRegex
    values:
    - .*GET /status" 200$
```

##### Аудит событий kubelet'а

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: kubelet-audit-logs
spec:
  type: File
  file:
    include:
    - /var/log/kube-audit/audit.log
  logFilter:
  - field: userAgent  
    operator: Regex
    values: ["kubelet.*"]
  destinationRefs:
  - loki-storage
```

##### Системные логи Deckhouse

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: system-logs
spec:
  type: File
  file:
    include:
    - /var/log/syslog
  labelFilter:
  - field: message
    operator: Regex
    values:
    - .*d8-kubelet-forker.*
    - .*containerd.*
    - .*bashible.*
    - .*kernel.*
  destinationRefs:
  - loki-storage
```

{% endraw %}
{% alert -%}
Если вам нужны только логи одного пода или малой группы подов, постарайтесь использовать настройки `kubernetesPods`, чтобы сузить количество читаемых файлов. Фильтры необходимы только для высокогранулярной настройки.
{%- endalert %}
{% raw %}

#### Преобразование логов

##### Преобразование логов в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы преобразовать строку в поле `message` в структурированный объект.
При использовании нескольких трансформаций `ParseMessage`, преобразование строки должно выполняться последним.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: string-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: String
        string:
          targetField: msg
```

Пример изначальной записи в логе:

```text
/docker-entrypoint.sh: Configuration complete; ready for start up
```

Результат преобразования:

```json
{... "message": { 
  "msg": "/docker-entrypoint.sh: Configuration complete; ready for start up"
  }
}
```

##### Преобразование логов в формате Klog в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате Klog и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: klog-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: Klog
```

Пример изначальной записи в логе:

```text
I0505 17:59:40.692994   28133 klog.go:70] hello from klog
```

Результат преобразования:

```json
{... "message": {
  "file":"klog.go",
  "id":28133,
  "level":"info",
  "line":70,
  "message":"hello from klog",
  "timestamp":"2025-05-05T17:59:40.692994Z"
  }
}
```

##### Преобразование логов в формате Syslog в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате Syslog и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: syslog-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: Syslog
```

Пример изначальной записи в логе:

```text
<13>1 2020-03-13T20:45:38.119Z dynamicwireless.name non 2426 ID931 [exampleSDID@32473 iut="3" eventSource= "Application" eventID="1011"] Try to override the THX port, maybe it will reboot the neural interface!
```

Результат преобразования:

```json
{... "message": {
  "appname": "non",
  "exampleSDID@32473": {
    "eventID": "1011",
    "eventSource": "Application",
    "iut": "3"
  },
  "facility": "user",
  "hostname": "dynamicwireless.name",
  "message": "Try to override the THX port, maybe it will reboot the neural interface!",
  "msgid": "ID931",
  "procid": 2426,
  "severity": "notice",
  "timestamp": "2020-03-13T20:45:38.119Z",
  "version": 1
}}
```

##### Преобразование логов в формате CLF в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате CLF и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: clf-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: CLF
```

Пример изначальной записи в логе:

```text
127.0.0.1 bob frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326
```

Результат преобразования:

```json
{... "message": {
  "host": "127.0.0.1",
  "identity": "bob",
  "message": "GET /apache_pb.gif HTTP/1.0",
  "method": "GET",
  "path": "/apache_pb.gif",
  "protocol": "HTTP/1.0",
  "size": 2326,
  "status": 200,
  "timestamp": "2000-10-10T20:55:36Z",
  "user": "frank"
}}
```

##### Преобразование логов в формате LogFmt в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате Logfmt и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: logfmt-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: Logfmt
```

Пример изначальной записи в логе:

```text
@timestamp=\"Sun Jan 10 16:47:39 EST 2021\" level=info msg=\"Stopping all fetchers\" tag#production=stopping_fetchers id=ConsumerFetcherManager-1382721708341 module=kafka.consumer.ConsumerFetcherManager
```

Результат преобразования:

```json
{... "message": {
  "@timestamp": "Sun Jan 10 16:47:39 EST 2021",
  "id": "ConsumerFetcherManager-1382721708341",
  "level": "info",
  "module": "kafka.consumer.ConsumerFetcherManager",
  "msg": "Stopping all fetchers",
  "tag#production": "stopping_fetchers"
}}
```

##### Парсинг JSON и уменьшение вложенности

Вы можете использовать трансформацию `ParseMessage`, чтобы парсить записи в логах в формате JSON.
С помощью параметра `depth` можно контролировать глубину вложенности.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: parse-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: JSON
        json:
          depth: 1
```

Пример изначальной записи в логе:

```text
{"level" : { "severity": "info" },"msg" : "fetching.module.release"}
```

Результат преобразования:

```json
{... "message": {
  "level" : "{ \"severity\": \"info\" }",
  "msg" : "fetching.module.release"
  }
}
```

##### Пример парсинга записей смешанных форматов в объект

Преобразование строки должно выполняться последним.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: parse-json
spec:
  ...
  transformations:
  - action: ParseMessage
    parseMessage:
      sourseFormat: JSON
  - action: ParseMessage
    parseMessage:
      sourceFormat: Klog
  - action: ParseMessage
    parseMessage:
      sourceFormat: String
        string:
          targetField: "text"
```

Пример изначальной записи в логе:

```text
/docker-entrypoint.sh: Configuration complete; ready for start up
{"level" : { "severity": "info" },"msg" : "fetching.module.release"}
I0505 17:59:40.692994   28133 klog.go:70] hello from klog
```

Результат преобразования:

```json
{... "message": {
  "text": "/docker-entrypoint.sh: Configuration complete; ready for start up"
  }
}
{... "message": {
  "level" : "{ "severity": "info" }",
  "msg" : "fetching.module.release"
  }
}
{... "message": {
  "file":"klog.go",
  "id":28133,
  "level":"info",
  "line":70,
  "message":"hello from klog",
  "timestamp":"2025-05-05T17:59:40.692994Z"
  }
}
```

##### Замена лейблов

Вы можете использовать трансформацию `ReplaceKeys`, чтобы заменить `source` на `target` в заданных ключах лейблов.

> Перед применением трансформации `ReplaceKeys` к полю `message` или его вложенным полям
> необходимо преобразовать запись лога в структурированный объект с помощью трансформации `ParseMessage`.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: replace-dot
spec:
  ...
  transformations:
    - action: ReplaceKeys
      replaceKeys:
        source: "."
        target: "_"
        labels:
          - .pod_labels
```

Пример изначальной записи в логе:

```text
{"msg" : "fetching.module.release"} # Лейбл пода pod.app=test
```

Результат преобразования:

```json
{... "message": {
  "msg" : "fetching.module.release"
  },
  "pod_labels": {
    "pod_app": "test"
  }
}
```

##### Удаление лейблов

Вы можете использовать трансформацию `DropLabels`, чтобы удалить заданные лейблы из записей логов.

> Перед применением трансформации `DropLabels` к полю `message` или его вложенным полям
> необходимо преобразовать запись лога в структурированный объект с помощью трансформации `ParseMessage`.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: drop-label
spec:
  ...
  transformations:
    - action: DropLabels
      dropLabels:
        labels:
          - .example
```

###### Пример удаления заданного лейбла из структурированного сообщения

В этом примере показано как вы можете удалить лейбл из структурированного JSON-сообщения.
Сначала применяется трансформация `ParseMessage` для парсинга сообщения,
после чего применяется `DropLabels` для удаления указанного лейбла.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: drop-label
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: JSON
    - action: DropLabels
      dropLabels:
        labels:
          - .message.example
```

Пример изначальной записи в логе:

```text
{"msg" : "fetching.module.release", "example": "test"}
```

Результат преобразования:

```json
{... "message": {
  "msg" : "fetching.module.release"
  }
}
```

#### Настройка сборки логов с продуктовых пространств имен, используя опцию namespace label selector

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: production-logs
spec:
  type: KubernetesPods
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchLabels:
          environment: production
  destinationRefs:
  - loki-storage
```

#### Исключение подов и пространств имён, используя label

Существует преднастроенный label для исключения определенных подов и пространств имён: `log-shipper.deckhouse.io/exclude=true`.
Он помогает остановить сбор логов с подов и пространств имён без изменения глобальной конфигурации.

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: test-namespace
  labels:
    log-shipper.deckhouse.io/exclude: "true"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-deployment
spec:
  ...
  template:
    metadata:
      labels:
        log-shipper.deckhouse.io/exclude: "true"
```

#### Включение буферизации

Настройка буферизации логов необходима для улучшения надежности и производительности системы сбора логов. Буферизация может быть полезна в следующих случаях:

1. Временные перебои с подключением. Если есть временные перебои или нестабильность соединения с системой хранения логов (например, с Elasticsearch), буфер позволяет временно сохранять логи и отправить их, когда соединение восстановится.

1. Сглаживание пиков нагрузки. При внезапных всплесках объёма логов буфер позволяет сгладить пиковую нагрузку на систему хранения логов, предотвращая её перегрузку и потенциальную потерю данных.

1. Оптимизация производительности. Буферизация помогает оптимизировать производительность системы сбора логов за счёт накопления логов и отправки их группами, что снижает количество сетевых запросов и улучшает общую пропускную способность.

##### Пример включения буферизации в оперативной памяти

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  buffer:
    memory:
      maxEvents: 4096
    type: Memory
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

##### Пример включения буферизации на диске

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  buffer:
    disk:
      maxSize: 1Gi
    type: Disk
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

##### Пример определения поведения при переполнении буфера

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  buffer:
    disk:
      maxSize: 1Gi
    type: Disk
    whenFull: DropNewest
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

Более подробное описание параметров доступно [в ресурсе ClusterLogDestination](cr.html#clusterlogdestination).

{% endraw %}

### Модуль loki: примеры

{% raw %}

#### Чтение логов из всех подов из указанного namespace и направление их в Loki

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: loki
spec:
  settings:
    storageClass: ceph-csi-rbd
    diskSizeGigabytes: 30
    retentionPeriodHours: 168
  enabled: true
  version: 1
---
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: development-logs
spec:
  type: KubernetesPods
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [development]
  destinationRefs:
    - d8-loki
```

Больше примеров в описании модуля [log-shipper](./log-shipper/examples.html).

{% endraw %}

### Модуль namespace-configurator: примеры

#### Пример

Добавьте лейбл `extended-monitoring.deckhouse.io/enabled=true` и аннотацию `foo=bar` к каждому namespace, начинающемуся с `prod-` или `infra-`, за исключением `infra-test`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: namespace-configurator
spec:
  version: 1
  enabled: true
  settings:
    configurations:
    - annotations:
        foo: bar
      labels:
        extended-monitoring.deckhouse.io/enabled: "true"
      includeNames:
      - "^prod"
      - "^infra"
      excludeNames:
      - "infra-test"
```

### Модуль network-policy-engine: примеры

#### Примеры network policies

##### Запретить обращаться снаружи к подам внутри namespace, но разрешить им обращаться внутри namespace и к внешним ресурсам

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: delete
spec:
  podSelector: {}
  egress:
  - {}
  ingress:
  - from:
    - podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

### Управление узлами: примеры

Ниже представлены несколько примеров описания NodeGroup, а также установки плагина cert-manager для `kubectl` и задания параметра `sysctl`.

#### Примеры описания NodeGroup

##### Статические узлы

<span id="пример-описания-статической-nodegroup"></span>

Для виртуальных машин на гипервизорах или физических серверов используйте статические узлы, указав `nodeType: Static` в NodeGroup.

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: worker
spec:
  nodeType: Static
```

Узлы в такую группу добавляются [вручную](#вручную) с помощью подготовленных скриптов.

Также можно использовать способ [добавления статических узлов с помощью Cluster API Provider Static](#с-помощью-cluster-api-provider-static).

##### Системные узлы

<span id="пример-описания-статичной-nodegroup-для-системных-узлов"></span>

Ниже представлен пример манифеста группы системных узлов.

При описании NodeGroup c узлами типа Static в поле `nodeType` укажите значение `Static` и используйте поле [`staticInstances`](./cr.html#nodegroup-v1-spec-staticinstances) для описания параметров настройки машин статических узлов.

При описании NodeGroup c облачными узлами типа CloudEphemeral в поле `nodeType` укажите значение `CloudEphemeral` и используйте поле [`cloudInstances`](./cr.html#nodegroup-v1-spec-cloudinstances) для описания параметров заказа облачных виртуальных машин.

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: system
spec:
  nodeTemplate:
    labels:
      node-role.deckhouse.io/system: ""
    taints:
      - effect: NoExecute
        key: dedicated.deckhouse.io
        value: system
  # Пример для узлов типа Static.
  nodeType: Static
  staticInstances:
    count: 2
    labelSelector:
      matchLabels:
        role: system
  # Пример для узлов типа CloudEphemeral.
  # nodeType: CloudEphemeral
  # cloudInstances:
  #   classReference:
  #     kind: YandexInstanceClass
  #     name: large
  #   maxPerZone: 2
  #   minPerZone: 1
  #   zones:
  #   - ru-central1-d
```

##### Узлы с GPU

Для работы узлов с GPU требуются **драйвер NVIDIA** и **NVIDIA Container Toolkit**. Возможны два варианта установки драйвера:

1. **Ручная установка** — администратор устанавливает драйвер до включения узла в кластер.
1. **Автоматизация через `NodeGroupConfiguration`** (подробнее – в разделе [Порядок действий по добавлению GPU-узла в кластер](./node-manager/faq.html#порядок-действий-по-добавлению-gpu-узла-в-кластер)).

После того как драйвер установлен и в NodeGroup добавлен блок `spec.gpu`,
`node-manager` включает полноценную поддержку GPU: автоматически разворачиваются
**NFD**, **GFD**, **NVIDIA Device Plugin**, **DCGM Exporter** и, при необходимости,
**MIG Manager**.

{% alert level="info" %}
Узлы с GPU часто помечают отдельным taint-ом (например, `node-role=gpu:NoSchedule`) — тогда по умолчанию туда не попадают обычные поды.
Сервисам, которым нужен GPU, достаточно добавить `tolerations` и `nodeSelector`.
{% endalert %}

Подробная схема параметров находится в [описании кастомного ресурса `NodeGroup`](./node-manager/cr.html#nodegroup-v1-spec-gpu).

Ниже представлены примеры манифестов NodeGroup для типовых режимов работы GPU (Exclusive,
TimeSlicing, MIG).

###### Эксклюзивный режим (Exclusive)

Каждому поду выделяется целый GPU, в кластере публикуется ресурс `nvidia.com/gpu`.

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: gpu-exclusive
spec:
  nodeType: Static
  gpu:
    sharing: Exclusive
  nodeTemplate:
    labels:
      node-role/gpu: ""
    taints:
    - key: node-role
      value: gpu
      effect: NoSchedule
```

###### TimeSlicing (4 слота)

GPU распределяется по временным слотам: до 4 подов могут последовательно
использовать одну карту. Подходит для экспериментов, CI и лёгких
inference-задач.

Поды по-прежнему запрашивают ресурс `nvidia.com/gpu`.

```yaml
spec:
  gpu:
    sharing: TimeSlicing
    timeSlicing:
      partitionCount: 4
```

###### MIG (профиль `all-1g.5gb`)

Физический GPU (A100, A30 и др.) делится на аппаратные экземпляры.
Планировщик увидит ресурсы `nvidia.com/mig-1g.5gb`.

Полный список поддерживаемых GPU-устройств и их профили можно увидеть, воспользовавшись  
[инструкцией](./node-manager/faq.html#как-посмотреть-доступные-mig-профили-в-кластере).

```yaml
spec:
  gpu:
    sharing: MIG
    mig:
      partedConfig: all-1g.5gb
```

###### Проверка работы: тестовая задача (CUDA vectoradd)

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: cuda-vectoradd
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        node-role/gpu: ""
      tolerations:
      - key: node-role
        value: gpu
        effect: NoSchedule
      containers:
      - name: cuda-vectoradd
        image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04
        resources:
          limits:
            nvidia.com/gpu: 1
```

Эта задача (Job) запускает демонстрационный пример **vectoradd** из набора CUDA-samples.
Если под завершается успешно (`Succeeded`), значит GPU-устройство доступно и корректно настроено.

#### Добавление статического узла в кластер

<span id="пример-описания-статичной-nodegroup"></span>

Добавление статического узла можно выполнить вручную или с помощью Cluster API Provider Static.

##### Вручную

Чтобы добавить новый статический узел (выделенная ВМ, bare-metal-сервер и т. п.) в кластер вручную, выполните следующие шаги:

1. Используйте существующий или создайте новый ресурс [NodeGroup](cr.html#nodegroup) ([пример](#статические-узлы) NodeGroup с именем `worker`). Параметр [nodeType](cr.html#nodegroup-v1-spec-nodetype) в ресурсе NodeGroup для статических узлов должен быть `Static` или `CloudStatic`.
1. Получите код скрипта в кодировке Base64 для добавления и настройки узла.

   Пример получения кода скрипта в кодировке Base64 для добавления узла в NodeGroup `worker`:

   ```shell
   NODE_GROUP=worker
   d8 k -n d8-cloud-instance-manager get secret manual-bootstrap-for-${NODE_GROUP} -o json | jq '.data."bootstrap.sh"' -r
   ```

1. Выполните предварительную настройку нового узла в соответствии с особенностями вашего окружения. Например:
   - добавьте необходимые точки монтирования в файл `/etc/fstab` (NFS, Ceph и т. д.);
   - установите необходимые пакеты;
   - настройте сетевую связность между новым узлом и остальными узлами кластера.
1. Зайдите на новый узел по SSH и выполните следующую команду, вставив полученную в п. 3 Base64-строку:

   ```shell
   echo <Base64-КОД-СКРИПТА> | base64 -d | bash
   ```

##### С помощью Cluster API Provider Static

{% alert level="warning" %}
Если вы ранее увеличивали количество master-узлов в кластере в NodeGroup `master` (параметр [`spec.staticInstances.count`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-count)), перед добавлением обычных узлов с помощью CAPS [убедитесь](./control-plane-manager/faq.html#как-добавить-master-узел-в-статическом-или-гибридном-кластере), что не произойдет их «перехват».
{% endalert %}

Простой пример добавления статического узла в кластер с помощью [Cluster API Provider Static (CAPS)](./#cluster-api-provider-static):

1. Подготовьте необходимые ресурсы.

   * Выделите сервер (или виртуальную машину), настройте сетевую связность и т. п., при необходимости установите специфические пакеты ОС и добавьте точки монтирования которые потребуются на узле.

   * Создайте пользователя (в примере — `caps`) с возможностью выполнять `sudo`, выполнив **на сервере** следующую команду:

     ```shell
     useradd -m -s /bin/bash caps 
     usermod -aG sudo caps
     ```

   * Разрешите пользователю выполнять команды через sudo без пароля. Для этого **на сервере** внесите следующую строку в конфигурацию sudo (отредактировав файл `/etc/sudoers`, выполнив команду `sudo visudo` или другим способом):

     ```text
     caps ALL=(ALL) NOPASSWD: ALL
     ```

   * Сгенерируйте **на сервере** пару SSH-ключей с пустой парольной фразой:

     ```shell
     ssh-keygen -t rsa -f caps-id -C "" -N ""
     ```

     Публичный и приватный ключи пользователя `caps` будут сохранены в файлах `caps-id.pub` и `caps-id` в текущей директории на сервере.

   * Добавьте полученный публичный ключ в файл `/home/caps/.ssh/authorized_keys` пользователя `caps`, выполнив в директории с ключами **на сервере** следующие команды:

     ```shell
     mkdir -p /home/caps/.ssh 
     cat caps-id.pub >> /home/caps/.ssh/authorized_keys 
     chmod 700 /home/caps/.ssh 
     chmod 600 /home/caps/.ssh/authorized_keys
     chown -R caps:caps /home/caps/
     ```

   В операционных системах семейства Astra Linux, при использовании модуля мандатного контроля целостности Parsec, сконфигурируйте максимальный уровень целостности для пользователя `caps`:

     ```shell
     pdpl-user -i 63 caps
     ```

1. Создайте в кластере ресурс [SSHCredentials](cr.html#sshcredentials).

   В директории с ключами пользователя **на сервере** выполните следующую команду для получения закрытого ключа в формате Base64:

   ```shell
   base64 -w0 caps-id
   ```

   На любом компьютере с `kubectl`, настроенным на управление кластером, создайте переменную окружения со значением закрытого ключа созданного пользователя в Base64, полученным на предыдущем шаге:

   ```shell
    CAPS_PRIVATE_KEY_BASE64=<ЗАКРЫТЫЙ_КЛЮЧ_В_BASE64>
   ```

   Выполните следующую команду, для создания в кластере ресурса `SSHCredentials` (здесь и далее также используйте `kubectl`, настроенный на управление кластером):

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: SSHCredentials
   metadata:
     name: credentials
   spec:
     user: caps
     privateSSHKey: "${CAPS_PRIVATE_KEY_BASE64}"
   EOF
   ```

1. Создайте в кластере ресурс [StaticInstance](cr.html#staticinstance), указав IP-адрес сервера статического узла:

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-worker-1
     labels:
       role: worker
   spec:
     # Укажите IP-адрес сервера статического узла.
     address: "<SERVER-IP>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   EOF
   ```

1. Создайте в кластере ресурс [NodeGroup](cr.html#nodegroup). Параметр `count` обозначает количество `staticInstances`, подпадающих под `labelSelector`, которые будут добавлены в кластер, в данном случае `1`:

   > Поле `labelSelector` в ресурсе `NodeGroup` является неизменным. Чтобы обновить `labelSelector`, нужно создать новую `NodeGroup` и перенести в неё статические узлы, изменив их лейблы (labels).

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1
   kind: NodeGroup
   metadata:
     name: worker
   spec:
     nodeType: Static
     staticInstances:
       count: 1
       labelSelector:
         matchLabels:
           role: worker
   EOF
   ```

   > Если необходимо добавить узлы в уже существующую группу узлов, укажите их желаемое количество в поле `.spec.count` NodeGroup.

##### С помощью Cluster API Provider Static для нескольких групп узлов

Пример использования фильтров в [label selector](cr.html#nodegroup-v1-spec-staticinstances-labelselector) StaticInstance, для группировки статических узлов и использования их в разных NodeGroup. В примере используются две группы узлов (`front` и `worker`), предназначенные для разных задач, которые должны содержать разные по характеристикам узлы — два сервера для группы `front` и один для группы `worker`.

1. Подготовьте необходимые ресурсы (3 сервера или виртуальные машины) и создайте ресурс `SSHCredentials`, аналогично п.1 и п.2 [примера](#с-помощью-cluster-api-provider-static).

1. Создайте в кластере два ресурса [NodeGroup](cr.html#nodegroup) (здесь и далее используйте `kubectl`, настроенный на управление кластером):

   > Поле `labelSelector` в ресурсе `NodeGroup` является неизменным. Чтобы обновить labelSelector, нужно создать новую NodeGroup и перенести в неё статические узлы, изменив их лейблы (labels).

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1
   kind: NodeGroup
   metadata:
     name: front
   spec:
     nodeType: Static
     staticInstances:
       count: 2
       labelSelector:
         matchLabels:
           role: front
   ---
   apiVersion: deckhouse.io/v1
   kind: NodeGroup
   metadata:
     name: worker
   spec:
     nodeType: Static
     staticInstances:
       count: 1
       labelSelector:
         matchLabels:
           role: worker
   EOF
   ```

1. Создайте в кластере ресурсы [StaticInstance](cr.html#staticinstance), указав актуальные IP-адреса серверов:

   ```shell
   d8 k create -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-front-1
     labels:
       role: front
   spec:
     address: "<SERVER-FRONT-IP1>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   ---
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-front-2
     labels:
       role: front
   spec:
     address: "<SERVER-FRONT-IP2>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   ---
   apiVersion: deckhouse.io/v1alpha1
   kind: StaticInstance
   metadata:
     name: static-worker-1
     labels:
       role: worker
   spec:
     address: "<SERVER-WORKER-IP>"
     credentialsRef:
       kind: SSHCredentials
       name: credentials
   EOF
   ```

##### Cluster API Provider Static: перемещение узлов между NodeGroup

В данном разделе описывается процесс перемещения статических узлов между различными NodeGroup с использованием Cluster API Provider Static (CAPS). Процесс включает изменение конфигурации NodeGroup и обновление лейблов у соответствующих StaticInstance.

###### Исходная конфигурация

Предположим, что в кластере уже существует NodeGroup с именем `worker`, настроенный для управления одним статическим узлом с лейблом `role: worker`.

`NodeGroup` worker:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: worker
spec:
  nodeType: Static
  staticInstances:
    count: 1
    labelSelector:
      matchLabels:
        role: worker
```

`StaticInstance` static-0:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: StaticInstance
metadata:
  name: static-worker-1
  labels:
    role: worker
spec:
  address: "192.168.1.100"
  credentialsRef:
    kind: SSHCredentials
    name: credentials
```

###### Шаги по перемещению узла между `NodeGroup`

{% alert level="warning" %}
В процессе переноса узлов между NodeGroup будет выполнена очистка и повторный бутстрап узла, объект `Node` будет пересоздан.
{% endalert %}

####### 1. Создание новой `NodeGroup` для целевой группы узлов

Создайте новый ресурс NodeGroup, например, с именем `front`, который будет управлять статическим узлом с лейблом `role: front`.

```shell
d8 k create -f - <<EOF
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: front
spec:
  nodeType: Static
  staticInstances:
    count: 1
    labelSelector:
      matchLabels:
        role: front
EOF
```

####### 2. Обновление лейбла у `StaticInstance`

Измените лейбл `role` у существующего StaticInstance с `worker` на `front`. Это позволит новой NodeGroup `front` начать управлять этим узлом.

```shell
d8 k label staticinstance static-worker-1 role=front --overwrite
```

####### 3. Уменьшение количества статических узлов в исходной `NodeGroup`

Обновите ресурс NodeGroup `worker`, уменьшив значение параметра `count` с `1` до `0`.

```shell
d8 k patch nodegroup worker -p '{"spec": {"staticInstances": {"count": 0}}}' --type=merge
```

#### Пример описания `NodeUser`

```yaml
apiVersion: deckhouse.io/v1
kind: NodeUser
metadata:
  name: testuser
spec:
  uid: 1100
  sshPublicKeys:
  - "<SSH_PUBLIC_KEY>"
  passwordHash: <PASSWORD_HASH>
  isSudoer: true
```

#### Пример описания `NodeGroupConfiguration`

##### Задание параметра sysctl

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: sysctl-tune.sh
spec:
  weight: 100
  bundles:
  - "*"
  nodeGroups:
  - "*"
  content: |
    sysctl -w vm.max_map_count=262144
```

##### Добавление корневого сертификата в хост

{% alert level="warning" %}
Данный пример приведен для ОС Ubuntu.  
Способ добавления сертификатов в хранилище может отличаться в зависимости от ОС.
  
При адаптации скрипта под другую ОС измените параметры [bundles](cr.html#nodegroupconfiguration-v1alpha1-spec-bundles) и [content](cr.html#nodegroupconfiguration-v1alpha1-spec-content).
{% endalert %}

{% alert level="warning" %}
Для использования сертификата в `containerd` (в т.ч. pull контейнеров из приватного репозитория) после добавления сертификата требуется произвести рестарт сервиса.
{% endalert %}

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: add-custom-ca.sh
spec:
  weight: 31
  nodeGroups:
  - '*'  
  bundles:
  - 'ubuntu-lts'
  content: |-
    CERT_FILE_NAME=example_ca
    CERTS_FOLDER="/usr/local/share/ca-certificates"
    CERT_CONTENT=$(cat <<EOF
    -----BEGIN CERTIFICATE-----
    MIIDSjCCAjKgAwIBAgIRAJ4RR/WDuAym7M11JA8W7D0wDQYJKoZIhvcNAQELBQAw
    JTEjMCEGA1UEAxMabmV4dXMuNTEuMjUwLjQxLjIuc3NsaXAuaW8wHhcNMjQwODAx
    MTAzMjA4WhcNMjQxMDMwMTAzMjA4WjAlMSMwIQYDVQQDExpuZXh1cy41MS4yNTAu
    NDEuMi5zc2xpcC5pbzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAL1p
    WLPr2c4SZX/i4IS59Ly1USPjRE21G4pMYewUjkSXnYv7hUkHvbNL/P9dmGBm2Jsl
    WFlRZbzCv7+5/J+9mPVL2TdTbWuAcTUyaG5GZ/1w64AmAWxqGMFx4eyD1zo9eSmN
    G2jis8VofL9dWDfUYhRzJ90qKxgK6k7tfhL0pv7IHDbqf28fCEnkvxsA98lGkq3H
    fUfvHV6Oi8pcyPZ/c8ayIf4+JOnf7oW/TgWqI7x6R1CkdzwepJ8oU7PGc0ySUWaP
    G5bH3ofBavL0bNEsyScz4TFCJ9b4aO5GFAOmgjFMMUi9qXDH72sBSrgi08Dxmimg
    Hfs198SZr3br5GTJoAkCAwEAAaN1MHMwDgYDVR0PAQH/BAQDAgWgMAwGA1UdEwEB
    /wQCMAAwUwYDVR0RBEwwSoIPbmV4dXMuc3ZjLmxvY2FsghpuZXh1cy41MS4yNTAu
    NDEuMi5zc2xpcC5pb4IbZG9ja2VyLjUxLjI1MC40MS4yLnNzbGlwLmlvMA0GCSqG
    SIb3DQEBCwUAA4IBAQBvTjTTXWeWtfaUDrcp1YW1pKgZ7lTb27f3QCxukXpbC+wL
    dcb4EP/vDf+UqCogKl6rCEA0i23Dtn85KAE9PQZFfI5hLulptdOgUhO3Udluoy36
    D4WvUoCfgPgx12FrdanQBBja+oDsT1QeOpKwQJuwjpZcGfB2YZqhO0UcJpC8kxtU
    by3uoxJoveHPRlbM2+ACPBPlHu/yH7st24sr1CodJHNt6P8ugIBAZxi3/Hq0wj4K
    aaQzdGXeFckWaxIny7F1M3cIWEXWzhAFnoTgrwlklf7N7VWHPIvlIh1EYASsVYKn
    iATq8C7qhUOGsknDh3QSpOJeJmpcBwln11/9BGRP
    -----END CERTIFICATE-----
    EOF
    )

    # bb-event           - Creating subscription for event function. More information: http://www.bashbooster.net/#event
    ## ca-file-updated   - Event name
    ## update-certs      - The function name that the event will call
    
    bb-event-on "ca-file-updated" "update-certs"
    
    update-certs() {          # Function with commands for adding a certificate to the store
      update-ca-certificates
    }

    # bb-tmp-file - Creating temp file function. More information: http://www.bashbooster.net/#tmp
    CERT_TMP_FILE="$( bb-tmp-file )"
    echo -e "${CERT_CONTENT}" > "${CERT_TMP_FILE}"  
    
    # bb-sync-file                                - File synchronization function. More information: http://www.bashbooster.net/#sync
    ## "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt"    - Destination file
    ##  ${CERT_TMP_FILE}                          - Source file
    ##  ca-file-updated                           - Name of event that will be called if the file changes.

    bb-sync-file \
      "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt" \
      ${CERT_TMP_FILE} \
      ca-file-updated   
```

##### Добавление в containerd возможности скачивать образы из insecure container registry

Возможность скачивания образов из insecure container registry включается с помощью параметра `insecure_skip_verify` в конфигурационном файле containerd. Подробнее — в разделе [Как добавить конфигурацию для дополнительного registry](faq.html#как-добавить-конфигурацию-для-дополнительного-registry).

### Модуль okmeter: примеры

#### Пример

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: okmeter
spec:
  version: 1
  enabled: true
  settings: 
    apiKey: 5ff9z2a3-9127-1sh4-2192-06a3fc6e13e3
```

### Модуль openvpn: примеры

#### Пример для кластеров bare metal

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: openvpn
spec:
  version: 2
  enabled: true
  settings:
    inlet: ExternalIP
    externalIP: 5.4.54.4
```

#### Пример для публичного IP-адреса на внешнем балансировщике

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: openvpn
spec:
  version: 2
  enabled: true
  settings:
    externalHost: 5.4.54.4
    externalIP: 192.168.0.30 # Внутренний IP-адрес, который примет трафик от внешнего балансировщика.
    inlet: ExternalIP
    nodeSelector:
      kubernetes.io/hostname: node
```

### Модуль Prometheus Pushgateway: примеры

#### Пример настройки модуля

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: prometheus-pushgateway
spec:
  version: 1
  enabled: true
  settings:
    instances:
    - first
    - second
    - another
```

Адрес PushGateway (из контейнера пода): `http://first.kube-prometheus-pushgateway:9091`.

#### Отправка метрики

Пример отправки метрики через curl:

```shell
echo "test_metric{env="dev"} 3.14" | curl --data-binary @- http://first.kube-prometheus-pushgateway:9091/metrics/job/myapp
```

Через 30 секунд (после скрейпа данных) метрики будут доступны в Prometheus. Пример:

```text
test_metric{container="prometheus-pushgateway", env="dev", exported_job="myapp", 
    instance="10.244.1.155:9091", job="prometheus-pushgateway", pushgateway="prometheus-pushgateway", tier="cluster"} 3.14
```

{% alert %} Название job (в примере — `myapp`) будет доступно в Prometheus в лейбле `exported_job`, а не `job` (так как лейбл `job` уже занят в Prometheus, он переименовывается при приеме метрики от PushGateway).
{% endalert %}

{% alert %} Возможно, вам потребуется получить список всех имеющихся job для выбора уникального названия (чтобы не испортить существующие графики и алерты). Получить список всех имеющихся job можно следующим запросом: {% raw %}`count({__name__=~".+"}) by (job)`.{% endraw %}
{% endalert %}

#### Удаление метрик

Пример удаления всех метрик группы `{instance="10.244.1.155:9091",job="myapp"}` через curl:

```shell
curl -X DELETE http://first.kube-prometheus-pushgateway:9091/metrics/job/myapp/instance/10.244.1.155:9091
```

Так как PushGateway хранит полученные метрики в памяти, **при рестарте пода все метрики будут утеряны**.

### Модуль registry: пример использования

#### Переключение на режим `Direct`

Для переключения уже работающего кластера на режим `Direct` выполните следующие шаги:

{% alert level="danger" %}
При изменении режима registry или параметров registry, Deckhouse будет перезапущен.
{% endalert %}

1. Перед переключением выполните [миграцию на использование модуля `registry`](faq.html#как-мигрировать-на-модуль-registry).

1. Убедитесь, что модуль `registry` включен и работает. Для этого выполните следующую команду:

   ```bash
   d8 k get module registry -o wide
   ```

   Пример вывода:

   ```console
   NAME       WEIGHT ...  PHASE   ENABLED   DISABLED MESSAGE   READY
   registry   38     ...  Ready   True                         True
   ```

1. Убедитесь, что все master-узлы находятся в состоянии `Ready` и не имеют статуса `SchedulingDisabled`, используя следующую команду:

   ```bash
   d8 k get nodes
   ```

   Пример вывода:

   ```console
   NAME       STATUS   ROLES                 ...
   master-0   Ready    control-plane,master  ...
   master-1   Ready    control-plane,master  ...
   master-2   Ready    control-plane,master  ...
   ```

   Пример вывода, когда master-узел (`master-2` в примере) находится в статусе `SchedulingDisabled`:

   ```console
   NAME       STATUS                      ROLES                 ...
   master-0   Ready    control-plane,master  ...
   master-1   Ready    control-plane,master  ...
   master-2   Ready,SchedulingDisabled    control-plane,master  ...
   ```

1. Проверьте, чтобы очередь Deckhouse была пустой и без ошибок.

1. Установите настройки режима `Direct` в ModuleConfig `deckhouse`. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [`deckhouse`](./deckhouse/) для корректной настройки.

   Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Direct
         direct:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Проверьте статус переключения registry в секрете `registry-state`, используя [инструкцию](faq.html#как-посмотреть-статус-переключения-режима-registry).

   Пример вывода:

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Direct
   target_mode: Direct
   ```

#### Переключение на режим `Unmanaged`

Для переключения уже работающего кластера на режим `Unmanaged` выполните следующие шаги:

{% alert level="danger" %}
При изменении режима registry или параметров registry, Deckhouse будет перезапущен.
{% endalert %}

1. Перед переключением выполните [миграцию на использование модуля `registry`](faq.html#как-мигрировать-на-модуль-registry).

1. Убедитесь, что модуль `registry` включен и работает. Для этого выполните следующую команду:

   ```bash
   d8 k get module registry -o wide
   ```

   Пример вывода:

   ```console
   NAME       WEIGHT ...  PHASE   ENABLED   DISABLED MESSAGE   READY
   registry   38     ...  Ready   True                         True
   ```

1. Проверьте, чтобы очередь Deckhouse была пустой и без ошибок.

1. Установите настройки режима `Unmanaged` в ModuleConfig `deckhouse`. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [`deckhouse`](./deckhouse/) для корректной настройки.

   Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Проверьте статус переключения registry в секрете `registry-state`, используя [инструкцию](faq.html#как-посмотреть-статус-переключения-режима-registry).

   Пример вывода:

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. При необходимости переключения на старый метод управления registry, ознакомьтесь с [инструкцией](faq.html#как-мигрировать-обратно-с-модуля-registry).

{% alert level="warning" %}
Это устаревший (deprecated) формат управления registry.
{% endalert %}

### Примеры

#### Добавление одного правила

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: FalcoAuditRules
metadata:
  name: ownership-permissions
spec:
  rules:
  - macro:
      name: spawned_process
      condition: (evt.type in (execve, execveat) and evt.dir=<)
  - rule:
      name: Detect Ownership Change
      desc: detect file permission/ownership change
      condition: >
        spawned_process and proc.name in (chmod, chown) and proc.args contains "/tmp/"
      output: >
        The file or directory below has had its permissions or ownership changed (user=%user.name
        command=%proc.cmdline file=%fd.name parent=%proc.pname pcmdline=%proc.pcmdline gparent=%proc.aname[2])
      priority: Warning
      tags: [filesystem]
```

#### Добавление двух правил с макросом и списком

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: FalcoAuditRules
metadata:
  name: nginx-unexpected-port
spec:
  rules:
  - macro:
      name: container
      condition: (container.id != host)

  - macro:
      name: inbound
      condition: >
        (((evt.type in (accept,listen) and evt.dir=<)) or
        (fd.typechar = 4 or fd.typechar = 6) and
        (fd.ip != "0.0.0.0" and fd.net != "127.0.0.0/8") and (evt.rawres >= 0 or evt.res = EINPROGRESS))

  - macro:
      name: outbound
      condition: >
        (((evt.type = connect and evt.dir=<)) or
        (fd.typechar = 4 or fd.typechar = 6) and
        (fd.ip != "0.0.0.0" and fd.net != "127.0.0.0/8") and (evt.rawres >= 0 or evt.res = EINPROGRESS))

  - macro:
      name: app_nginx
      condition: container and container.image contains "nginx"

  - rule:
      name: Unauthorized process opened an outbound connection (nginx)
      desc: nginx process tried to open an outbound connection and is not whitelisted
      condition: outbound and evt.rawres >= 0 and app_nginx
      output: |-
        Non-whitelisted process opened an outbound connection (command=%proc.cmdline connection=%fd.name)
      priority: Warning

  - list:
      name: nginx_allowed_inbound_ports_tcp
      items: [80, 443, 8080, 8443]

  - rule:
      name: Unexpected inbound TCP connection nginx
      desc: detect inbound traffic to nginx using tcp on a port outside of expected set
      condition: |
        inbound and evt.rawres >= 0 and not fd.sport in (nginx_allowed_inbound_ports_tcp) and app_nginx
      output: |-
        Inbound network connection to nginx on unexpected port
        (command=%proc.cmdline pid=%proc.pid connection=%fd.name sport=%fd.sport user=%user.name %container.info image=%container.image)
      priority: Notice
```

#### Добавление правила для отправки уведомлений о запуске shell-оболочки в контейнере

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: FalcoAuditRules
metadata:
  name: run-shell-in-container
spec:
  rules:
  - macro: 
      name: container
      condition: container.id != host
  
  - macro: 
      name: spawned_process
      condition: evt.type = execve and evt.dir=<
  
  - rule: 
      name: run_shell_in_container
      desc: a shell was spawned by a non-shell program in a container. Container entrypoints are excluded.
      condition: container and proc.name = bash and spawned_process and proc.pname exists and not proc.pname in (bash, docker)
      output: "Shell spawned in a container other than entrypoint (user=%user.name container_id=%container.id container_name=%container.name shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)"
      priority: Warning
```

### Мониторинг SLA кластера: примеры

#### Пример конфигурации remote_write

```yaml
apiVersion: deckhouse.io/v1
kind: UpmeterRemoteWrite
metadata:
  labels:
    heritage: upmeter
    module: upmeter
  name: victoriametrics
spec:
  additionalLabels:
    cluster: cluster-name
    some: fun
  config:
    url: https://upmeter-victoriametrics.whatever/api/v1/write
    basicAuth:
      password: "Cdp#Cd.OxfZsx4*89SZ"
      username: upmeter
  intervalSeconds: 300
```

### Модуль vertical-pod-autoscaler: примеры

#### Настройка модуля

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: vertical-pod-autoscaler
spec:
  version: 1
  enabled: true
  settings:
    nodeSelector:
      node-role/system: ""
    tolerations:
    - key: dedicated.deckhouse.io
      operator: Equal
      value: system
```

#### Пример минимального custom resource `VerticalPodAutoscaler`

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: StatefulSet
    name: my-app
```

#### Пример полного custom resource `VerticalPodAutoscaler`

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: hamster
      minAllowed:
        memory: 100Mi
        cpu: 120m
      maxAllowed:
        memory: 300Mi
        cpu: 350m
      mode: Auto
```

### Модуль admission-policy-engine: FAQ

#### Как настроить альтернативные решения по управлению политиками безопасности?

Для корректной работы Deckhouse Platform Certified Security Edition необходимы расширенные привилегии на запуск и работу полезной нагрузки системных компонентов. Если вместо модуля admission-policy-engine используется альтернативное решение по управлению политиками безопасности (например, Kyverno), необходима настройка исключений для следующих пространств имен:

- `kube-system`;
- все пространства имен с префиксом `d8-*` (например, `d8-system`).

#### Как расширить политики Pod Security Standards?

{% alert level="info" %}
Pod Security Standards реагируют на label `security.deckhouse.io/pod-policy: restricted` или `security.deckhouse.io/pod-policy: baseline`.
{% endalert %}

Чтобы расширить политику Pod Security Standards, добавив к существующим проверкам политики свои собственные, необходимо:

- создать шаблон проверки (ресурс `ConstraintTemplate`);
- привязать его к политике `restricted` или `baseline`.

Пример шаблона для проверки адреса репозитория образа контейнера:

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package d8.pod_security_standards.extended

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, input.parameters.repos])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.initContainers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, input.parameters.repos])
        }
```

Пример привязки проверки к политике `restricted`:

```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: prod-repo
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaceSelector:
      matchLabels:
        security.deckhouse.io/pod-policy: restricted
  parameters:
    repos:
      - "mycompany.registry.com"
```

Пример демонстрирует настройку проверки адреса репозитория в поле `image` у всех подов, создающихся в пространстве имен, имеющих label `security.deckhouse.io/pod-policy: restricted`. Если адрес в поле `image` создаваемого пода начинается не с `mycompany.registry.com`, под создан не будет.

Больше примеров описания проверок для расширения политики можно найти в библиотеке Gatekeeper.

#### Как включить одну или несколько политик Pod Security Standards, не отключая весь набор?

Чтобы применить только нужные политики безопасности, не отключая весь предустановленный набор:

1. Добавьте в нужное пространство имён метку: `security.deckhouse.io/pod-policy: privileged`, чтобы отключить встроенный набор политик.
1. Создайте ресурс SecurityPolicy, соответствующий уровню baseline или restricted. В секции `policies` укажите только необходимые вам настройки.
1. Добавьте в пространство имён дополнительную метку, которая будет соответствовать селектору `namespaceSelector` в SecurityPolicy. В примерах ниже это `security-policy.deckhouse.io/baseline-enabled: "true"` либо `security-policy.deckhouse.io/restricted-enabled: "true"`

SecurityPolicy, соответствующая baseline:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: baseline
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowPrivilegeEscalation: true
    allowPrivileged: false
    allowedAppArmor:
      - runtime/default
      - localhost/*
    allowedCapabilities:
      - AUDIT_WRITE
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - FSETID
      - KILL
      - MKNOD
      - NET_BIND_SERVICE
      - SETFCAP
      - SETGID
      - SETPCAP
      - SETUID
      - SYS_CHROOT
    allowedHostPaths: []
    allowedHostPorts:
      - max: 0
        min: 0
    allowedProcMount: Default
    allowedUnsafeSysctls:
      - kernel.shm_rmid_forced
      - net.ipv4.ip_local_port_range
      - net.ipv4.ip_unprivileged_port_start
      - net.ipv4.tcp_syncookies
      - net.ipv4.ping_group_range
      - net.ipv4.ip_local_reserved_ports
      - net.ipv4.tcp_keepalive_time
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_keepalive_intvl
      - net.ipv4.tcp_keepalive_probes
    seLinux:
      - type: ""
      - type: container_t
      - type: container_init_t
      - type: container_kvm_t
      - type: container_engine_t
    seccompProfiles:
      allowedProfiles:
        - RuntimeDefault
        - Localhost
        - undefined
        - ''
      allowedLocalhostFiles:
        - '*'
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          security-policy.deckhouse.io/baseline-enabled: "true"
```

SecurityPolicy, соответствующая restricted:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: restricted
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowPrivilegeEscalation: false
    allowPrivileged: false
    allowedAppArmor:
      - runtime/default
      - localhost/*
    allowedCapabilities:
      - NET_BIND_SERVICE
    allowedHostPaths: []
    allowedHostPorts:
      - max: 0
        min: 0
    allowedProcMount: Default
    allowedUnsafeSysctls:
      - kernel.shm_rmid_forced
      - net.ipv4.ip_local_port_range
      - net.ipv4.ip_unprivileged_port_start
      - net.ipv4.tcp_syncookies
      - net.ipv4.ping_group_range
      - net.ipv4.ip_local_reserved_ports
      - net.ipv4.tcp_keepalive_time
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_keepalive_intvl
      - net.ipv4.tcp_keepalive_probes
    allowedVolumes:
      - configMap
      - csi
      - downwardAPI
      - emptyDir
      - ephemeral
      - persistentVolumeClaim
      - projected
      - secret
    requiredDropCapabilities:
      - ALL
    runAsUser:
      rule: MustRunAsNonRoot
    seLinux:
      - type: ""
      - type: container_t
      - type: container_init_t
      - type: container_kvm_t
      - type: container_engine_t
    seccompProfiles:
      allowedProfiles:
        - RuntimeDefault
        - Localhost
      allowedLocalhostFiles:
        - '*'
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          security-policy.deckhouse.io/restricted-enabled: "true"
```

#### Что, если несколько политик (операционных или безопасности) применяются на один объект?

В этом случае необходимо, чтобы конфигурация объекта соответствовала всем политикам, которые на него распространяются.

Например, рассмотрим две следующие политики безопасности:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: foo
spec:
  enforcementAction: Deny
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          name: test
  policies:
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
    - MKNOD
---
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: bar
spec:
  enforcementAction: Deny
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          name: test
  policies:
    requiredDropCapabilities:
    - NET_BIND_SERVICE
```

Тогда для выполнения требований приведенных политик безопасности в спецификации контейнера нужно указать:

```yaml
    securityContext:
      capabilities:
        drop:
          - MKNOD
          - NET_BIND_SERVICE
      readOnlyRootFilesystem: true
```

#### Проверка подписи образов

{% alert level="warning" %}Доступно в следующих редакциях: SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

В модуле реализована функция проверки подписи образов контейнеров, подписанных с помощью инструмента Cosign. Проверка подписи образов контейнеров позволяет убедиться в их целостности (что образ не был изменен после его создания) и подлинности (что образ был создан доверенным источником). Включить проверку подписи образов контейнеров в кластере можно с помощью параметра [policies.verifyImageSignatures](cr.html#securitypolicy-v1alpha1-spec-policies-verifyimagesignatures) ресурса SecurityPolicy.

{% offtopic title="Как подписать образ..." %}
Шаги для подписания образа:

- Сгенерируйте ключи: `cosign generate-key-pair`
- Подпишите образ: `cosign sign --key <key> <image>`

{% endofftopic %}

Пример SecurityPolicy для настройки проверки подписи образов контейнеров:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: verify-image-signatures
spec:
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          kubernetes.io/metadata.name: default
  policies:
    verifyImageSignatures:
      - reference: docker.io/myrepo/*
        publicKeys:
        - |-
          -----BEGIN PUBLIC KEY-----
          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM
          5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA==
          -----END PUBLIC KEY-----
      - reference: company.registry.com/*
        dockerCfg: zxc==
        publicKeys:
        - |-
          -----BEGIN PUBLIC KEY-----
          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM
          5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA==
          -----END PUBLIC KEY-----
```

Политика не влияет на создание подов, адреса образов контейнеров которых не подходят под описанные в параметре `reference`.  Если же адрес какого-либо образа контейнера подходит под описанные в параметре `reference` политики, и образ не подписан или подпись не соответствует указанным в политике ключам, создание пода будет запрещено.

Пример вывода ошибки при создании пода с образом контейнера, не прошедшим проверку подписи:

```console
[verify-image-signatures] Image signature verification failed: nginx:1.17.2
```

#### Как запретить удаление узла без метки

> Примечание. Операции DELETE обрабатываются Gatekeeper по умолчанию.

Можно создать собственную политику Gatekeeper, запрещающую удаление узла без специальной метки. Пример ниже использует `oldObject` для проверки меток удаляемого узла:

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: d8customnodedeleteguard
spec:
  crd:
    spec:
      names:
        kind: D8CustomNodeDeleteGuard
      validation:
        openAPIV3Schema:
          type: object
          properties:
            requiredLabelKey:
              type: string
            requiredLabelValue:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package d8.custom

        is_delete { input.review.operation == "DELETE" }
        is_node { input.review.kind.kind == "Node" }

        has_required_label {
          key := input.parameters.requiredLabelKey
          val := input.parameters.requiredLabelValue
          obj := input.review.oldObject
          obj.metadata.labels[key] == val
        }

        violation[{"msg": msg}] {
          is_delete
          is_node
          not has_required_label
          msg := sprintf("Удаление Node запрещено. Добавьте метку %q=%q.", [input.parameters.requiredLabelKey, input.parameters.requiredLabelValue])
        }
---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: D8CustomNodeDeleteGuard
metadata:
  name: require-node-delete-label
spec:
  enforcementAction: warn
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Node"]
  parameters:
    requiredLabelKey: "admission.deckhouse.io/allow-delete"
    requiredLabelValue: "true"
```

### Модуль cert-manager: FAQ


#### Какие виды сертификатов поддерживаются?

На данный момент модуль устанавливает следующие `ClusterIssuer`:
* `letsencrypt`
* `letsencrypt-staging`
* `selfsigned`
* `selfsigned-no-trust`

Если требуется поддержка других типов сертификатов, вы можете добавить их самостоятельно.

#### Как добавить дополнительный `ClusterIssuer`?

##### В каких случаях требуется дополнительный `ClusterIssuer`?

В стандартной поставке присутствуют `ClusterIssuer`, издающие либо сертификаты из доверенного публичного удостоверяющего центра Let's Encrypt, либо самоподписанные сертификаты.

Чтобы издать сертификаты на доменное имя через Let's Encrypt, сервис требует осуществить подтверждение владения доменом.
`Cert-manager` поддерживает несколько методов для такого подтверждения при использовании `ACME`(Automated Certificate Management Environment):
* `HTTP-01` — `cert-manager` создаст временный Pod в кластере, который будет слушать на определенном URL для подтверждения владения доменом. Для его работы необходимо иметь возможность направлять внешний трафик на этот Pod, обычно через `Ingress`.
* `DNS-01` —  `cert-manager` делает TXT-запись в DNS для подтверждения владения доменом. У `cert-manager` есть встроенная поддержка популярных провайдеров DNS.

{% alert level="danger" %}
Метод `HTTP-01` не поддерживает выпуск wildcard-сертификатов.
{% endalert %}

Поставляемые `ClusterIssuers`, издающие сертификаты через Let's Encrypt, делятся на два типа:
1. `ClusterIssuer,` специфичные для используемого cloud-провайдера.  
1. `ClusterIssuer` использующие метод `HTTP-01`.  
   Добавляются автоматически, если их создание не отключено в [настройках модуля](./configuration.html#parameters-disableletsencrypt).
   * `letsencrypt`
   * `letsencrypt-staging`

Таким образом, дополнительный `ClusterIssuer` может потребоваться в случаях издания сертификатов:
1. В удостоверяющем центре (УЦ), отличном от Let's Encrypt (в т.ч. в приватном).
2. Через Let's Encrypt с помощью метода `DNS-01` через сторонний провайдер.

##### Как добавить дополнительный `Issuer` и `ClusterIssuer`, использующий HashiCorp Vault для выпуска сертификатов?

После конфигурации PKI и [включения авторизации](/modules/user-authz/), нужно:
- Создать `ServiceAccount` и скопировать ссылку на его `Secret`:

  ```shell
  d8 k create serviceaccount issuer
  
  ISSUER_SECRET_REF=$(d8 k get serviceaccount issuer -o json | jq -r ".secrets[].name")
  ```

- Создать `Issuer`:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: Issuer
  metadata:
    name: vault-issuer
    namespace: default
  spec:
    vault:
      # Если Vault разворачивался по вышеуказанной инструкции, в этом месте в инструкции опечатка.
      server: http://vault.default.svc.cluster.local:8200
      # Указывается на этапе конфигурации PKI. 
      path: pki/sign/example-dot-com 
      auth:
        kubernetes:
          mountPath: /v1/auth/kubernetes
          role: issuer
          secretRef:
            name: $ISSUER_SECRET_REF
            key: token
  EOF
  ```

- Создать ресурс `Certificate` для получения TLS-сертификата, подписанного CA Vault:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: Certificate
  metadata:
    name: example-com
    namespace: default
  spec:
    secretName: example-com-tls
    issuerRef:
      name: vault-issuer
    # Домены указываются на этапе конфигурации PKI в Vault.
    commonName: www.example.com 
    dnsNames:
    - www.example.com
  EOF
  ```

##### Как добавить `ClusterIssuer`, использующий свой или промежуточный CA для заказа сертификатов?

Для использования собственного или промежуточного CA:

- Сгенерируйте сертификат (при необходимости):

  ```shell
  openssl genrsa -out rootCAKey.pem 2048
  openssl req -x509 -sha256 -new -nodes -key rootCAKey.pem -days 3650 -out rootCACert.pem
  ```

- В пространстве имён `d8-cert-manager` создайте секрет, содержащий данные файлов сертификатов.
  Пример создания секрета с помощью команды d8 k:  

  ```shell
  d8 k create secret tls internal-ca-key-pair -n d8-cert-manager --key="rootCAKey.pem" --cert="rootCACert.pem"
  ```

  Пример создания секрета из YAML-файла (содержимое файлов сертификатов должно быть закодировано в Base64):  

  ```yaml
  apiVersion: v1
  data:
    tls.crt: <результат команды `cat rootCACert.pem | base64 -w0`>
    tls.key: <результат команды `cat rootCAKey.pem | base64 -w0`>
  kind: Secret
  metadata:
    name: internal-ca-key-pair
    namespace: d8-cert-manager
  type: Opaque
  ```

  Имя секрета может быть любым.

- Создайте `ClusterIssuer` из созданного секрета:

  ```yaml
  apiVersion: cert-manager.io/v1
  kind: ClusterIssuer
  metadata:
    name: inter-ca
  spec:
    ca:
      secretName: internal-ca-key-pair    # Имя созданного секрета.
  ```

  Имя `ClusterIssuer` также может быть любым.

Теперь можно использовать созданный `ClusterIssuer` для получения сертификатов для всех компонентов Deckhouse или конкретного компонента.

Например, чтобы использовать `ClusterIssuer` для получения сертификатов для всех компонентов Deckhouse, укажите его имя в глобальном параметре [clusterIssuerName](/reference/api/global.html#parameters-modules-https-certmanager-clusterissuername) (`d8 k edit mc global`):

  ```yaml
  spec:
    settings:
      modules:
        https:
          certManager:
            clusterIssuerName: inter-ca
          mode: CertManager
        publicDomainTemplate: '%s.<public_domain_template>'
    version: 1
  ```

#### Как защитить учетные данные `cert-manager`?

Если вы не хотите хранить учетные данные конфигурации Deckhouse (например, по соображениям безопасности), можете создать
свой собственный `ClusterIssuer` / `Issuer`.

- Создайте Secret с учетными данными:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: v1
  kind: Secret
  type: Opaque
  metadata:
    name: XXX
    namespace: default
  data:
    secret-access-key: {{ "MY-ACCESS-KEY-TOKEN" | b64enc | quote }}
  EOF
  ```

- Создайте простой `ClusterIssuer` со ссылкой на этот Secret:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: ClusterIssuer
  metadata:
    name: XXX
    namespace: default
  spec:
    acme:
      server: https://acme-v02.api.letsencrypt.org/directory
      privateKeySecretRef:
        name: tls-key
      solvers:
      - dns01:
          <solver>:
            region: us-east-1
            accessKeyID: {{ "MY-ACCESS-KEY-ID" }}
            secretAccessKeySecretRef:
              name: XXX
              key: secret-access-key
  EOF
  ```

- Закажите сертификаты как обычно, используя созданный `ClusterIssuer`:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: Certificate
  metadata:
    name: example-com
    namespace: default
  spec:
    secretName: example-com-tls
    issuerRef:
      name: XXX
    commonName: www.example.com 
    dnsNames:
    - www.example.com
  EOF
  ```

#### Работает ли старая аннотация TLS-acme?

Да, работает. Специальный компонент `cert-manager-ingress-shim` видит эти аннотации и на их основании автоматически создает ресурсы `Certificate` (в тех же namespaces, что и Ingress-ресурсы с аннотациями).

> **Важно!** При использовании аннотации ресурс Certificate создается «прилинкованным» к существующему Ingress-ресурсу, и для прохождения Challenge НЕ создается отдельный Ingress, а вносятся дополнительные записи в существующий. Это означает, что если на основном Ingress'е настроена аутентификация или whitelist — ничего не выйдет. Лучше не использовать аннотацию и переходить на ресурс Certificate.
>
> **Важно!** При переходе с аннотации на ресурс Certificate нужно удалить ресурс Certificate, который был создан по аннотации. Иначе по обоим ресурсам Certificate будет обновляться один Secret, и это может привести к достижению лимита запросов Let’s Encrypt.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/tls-acme: "true"           # Аннотация.
  name: example-com
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: site
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  - host: www.example.com                    # Дополнительный домен.
    http:
      paths:
      - backend:
          service:
            name: site
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  - host: admin.example.com                  # Еще один дополнительный домен.
    http:
      paths:
      - backend:
          service:
            name: site
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - example.com
    - www.example.com                        # Дополнительный домен.
    - admin.example.com                      # Еще один дополнительный домен.
    secretName: example-com-tls              # Имя для Certificate и Secret.
```

#### Как получить список сертификатов?

```shell
d8 k get certificate --all-namespaces

NAMESPACE          NAME                            AGE
default            example-com                     13m
```

#### Что делать, если появляется ошибка: CAA record does not match issuer?

Если `cert-manager` не может заказать сертификаты с ошибкой:

```text
CAA record does not match issuer
```

то необходимо проверить `CAA (Certificate Authority Authorization)` DNS-запись у домена, для которого заказывается сертификат.
Если вы хотите использовать Let’s Encrypt-сертификаты, у домена должна быть CAA-запись: `issue "letsencrypt.org"`.

### Модуль chrony: FAQ

#### Как запретить использование chrony и использовать NTP-демоны на узлах?

1. [Выключите](configuration.html) модуль chrony.

1. Создайте `NodeGroupConfiguration` custom step, чтобы включить NTP-демоны на узлах (пример для `systemd-timesyncd`):

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: enable-ntp-on-node.sh
   spec:
     weight: 100
     nodeGroups: ["*"]
     bundles: ["*"]
     content: |
       systemctl enable systemd-timesyncd
       systemctl start systemd-timesyncd
   ```

### Управление control plane: FAQ

<div id='как-добавить-master-узел'></div>

#### Как добавить master-узел в статическом или гибридном кластере?

> Важно иметь нечетное количество master-узлов для обеспечения кворума.

В процессе установки Deckhouse Platform Certified Security Edition с настройками по умолчанию в NodeGroup `master` отсутствует секция [`spec.staticInstances.labelSelector`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-labelselector) с настройками фильтра меток (label) по ресурсам `staticInstances`. Из-за этого после изменения количества узлов `staticInstances` в NodeGroup `master` (параметр [`spec.staticInstances.count`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-count)) при добавлении обычного узла с помощью Cluster API Provider Static (CAPS) он может быть «перехвачен» и добавлен в NodeGroup `master`, даже если в соответствующем ему `StaticInstance` (в `metadata`) указан лейбл с `role`, отличающейся от `master`.
Чтобы избежать этого «перехвата», после установки Deckhouse Platform Certified Security Edition измените NodeGroup `master` — добавьте в нее секцию [`spec.staticInstances.labelSelector`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-labelselector) с настройками фильтра меток (label) по ресурсам `staticInstances`. Пример NodeGroup `master` с `spec.staticInstances.labelSelector`:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: master
spec:
  nodeType: Static
  staticInstances:
    count: 2
    labelSelector:
      matchLabels:
        role: master
```

Далее при добавлении в кластер master-узлов с помощью CAPS указывайте в соответствующих им `StaticInstance` лейбл, заданный в `spec.staticInstances.labelSelector` NodeGroup `master`. Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: StaticInstance
metadata:
  name: static-master-1
  labels:
    # Лейбл, указанный в spec.staticInstances.labelSelector NodeGroup master.
    role: master
spec:
  # Укажите IP-адрес сервера статического узла.
  address: "<SERVER-IP>"
  credentialsRef:
    kind: SSHCredentials
    name: credentials
```

{% alert level="info" %}
При добавлении новых master-узлов с помощью CAPS и изменении в NodeGroup `master` количества master-узлов (параметр [`spec.staticInstances.count`](./node-manager/cr.html#nodegroup-v1-spec-staticinstances-count)) учитывайте следующее:

При бутстрапе кластера в конфигурации указывается первый master-узел, на который происходит установка.
Если после бутстрапа нужно сделать мультимастер и добавить master-узлы с помощь CAPS, в параметре `spec.staticInstances.count` NodeGroup `master` необходимо указать количество узлов на один меньше желаемого.

Например, если нужно сделать мультимастер с тремя master-узлами в `spec.staticInstances.count` NodeGroup `master` укажите значение `2` и создайте два `staticInstances` для добавляемых узлов. После их добавления в кластер количество master-узлов будет равно трём: master-узел, на который происходила установка и два master-узла, добавленные с помощью CAPS.
{% endalert %}

В остальном добавление master-узла в статический или гибридный кластер аналогично добавлению обычного узла.
Воспользуйтесь для этого соответствующими [примерами](./node-manager/examples.html#добавление-статического-узла-в-кластер). Все необходимые действия по настройке компонентов control plane кластера на новом узле будут выполнены автоматически, дождитесь их завершения — появления master-узлов в статусе `Ready`.

<div id='как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master'></div>

#### Как добавить master-узлы в облачном кластере?

Далее описана конвертация кластера с одним master-узлом в мультимастерный кластер.

> Перед добавлением узлов убедитесь в наличии необходимых квот.
>
> Важно иметь нечетное количество master-узлов для обеспечения кворума.

1. Сделайте [резервную копию `etcd`](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать созданию новых master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. **На локальной машине** запустите контейнер установщика Deckhouse соответствующей редакции и версии (измените адрес container registry при необходимости):

   ```bash
   DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}') 
   DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]' ) 
   docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" \
     registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
   ```

1. **В контейнере с инсталлятором** выполните следующую команду, чтобы проверить состояние перед началом работы:

   ```bash
   dhctl terraform check --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

   Ответ должен сообщить, что Terraform не нашел расхождений и изменений не требуется.

1. **В контейнере с инсталлятором** выполните следующую команду и укажите требуемое количество master-узлов в параметре `masterNodeGroup.replicas`:

   ```bash
   dhctl config edit provider-cluster-configuration --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST>
   ```

   > Для **Yandex Cloud**, при использовании внешних адресов на master-узлах, количество элементов массива в параметре [masterNodeGroup.instanceClass.externalIPAddresses](/modules/cloud-provider-yandex/cluster_configuration.html#yandexclusterconfiguration-masternodegroup-instanceclass-externalipaddresses) должно равняться количеству master-узлов. При использовании значения `Auto` (автоматический заказ публичных IP-адресов), количество элементов в массиве все равно должно соответствовать количеству master-узлов.
   >
   > Например, при трех master-узлах (`masterNodeGroup.replicas: 3`) и автоматическом заказе адресов, параметр `masterNodeGroup.instanceClass.externalIPAddresses` будет выглядеть следующим образом:
   >
   > ```bash
   > externalIPAddresses:
   > - "Auto"
   > - "Auto"
   > - "Auto"
   > ```

1. **В контейнере с инсталлятором** выполните следующую команду для запуска масштабирования:

   ```bash
   dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

1. Дождитесь появления необходимого количества master-узлов в статусе `Ready` и готовности всех экземпляров `control-plane-manager`:

   ```bash
   d8 k -n kube-system wait pod --timeout=10m --for=condition=ContainersReady -l app=d8-control-plane-manager
   ```

<div id='как-удалить-master-узел'></div>
<div id='как-уменьшить-число-master-узлов-в-облачном-кластере-multi-master-в-single-master'></div>

#### Как уменьшить число master-узлов в облачном кластере?

Далее описана конвертация мультимастерного кластера в кластер с одним master-узлом.

{% alert level="warning" %}
Описанные ниже шаги необходимо выполнять с первого по порядку master-узла кластера (master-0). Это связано с тем, что кластер всегда масштабируется по порядку: например, невозможно удалить узлы master-0 и master-1, оставив master-2.
{% endalert %}

1. Сделайте [резервную копию `etcd`](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать обновлению master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. **На локальной машине** запустите контейнер установщика Deckhouse соответствующей редакции и версии (измените адрес container registry при необходимости):

   ```bash
   DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}') 
   DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]' ) 
   docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" \
     registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
   ```

1. **В контейнере с инсталлятором** выполните следующую команду и укажите `1` в параметре `masterNodeGroup.replicas`:

   ```bash
   dhctl config edit provider-cluster-configuration --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> \
     --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

   > Для **Yandex Cloud** при использовании внешних адресов на master-узлах количество элементов массива в параметре [masterNodeGroup.instanceClass.externalIPAddresses](/modules/cloud-provider-yandex/cluster_configuration.html#yandexclusterconfiguration-masternodegroup-instanceclass-externalipaddresses) должно равняться количеству master-узлов. При использовании значения `Auto` (автоматический заказ публичных IP-адресов) количество элементов в массиве все равно должно соответствовать количеству master-узлов.
   >
   > Например, при одном master-узле (`masterNodeGroup.replicas: 1`) и автоматическом заказе адресов параметр `masterNodeGroup.instanceClass.externalIPAddresses` будет выглядеть следующим образом:
   >
   > ```yaml
   > externalIPAddresses:
   > - "Auto"
   > ```

1. Снимите следующие лейблы с удаляемых master-узлов:
   * `node-role.kubernetes.io/control-plane`
   * `node-role.kubernetes.io/master`
   * `node.deckhouse.io/group`

   Команда для снятия лейблов:

   ```bash
   d8 k label node <MASTER-NODE-N-NAME> node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- node.deckhouse.io/group-
   ```

1. Убедитесь, что удаляемые master-узлы пропали из списка узлов кластера etcd:

   ```bash
   d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o name | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ member list -w table
   ```

1. Выполните `drain` для удаляемых узлов:

   ```bash
   d8 k drain <MASTER-NODE-N-NAME> --ignore-daemonsets --delete-emptydir-data
   ```

1. Выключите виртуальные машины, соответствующие удаляемым узлам, удалите инстансы соответствующих узлов из облака и подключенные к ним диски (`kubernetes-data-master-<N>`).

1. Удалите в кластере поды, оставшиеся на удаленных узлах:

   ```bash
   d8 k delete pods --all-namespaces --field-selector spec.nodeName=<MASTER-NODE-N-NAME> --force
   ```

1. Удалите в кластере объекты `Node` удаленных узлов:

   ```bash
   d8 k delete node <MASTER-NODE-N-NAME>
   ```

1. **В контейнере с инсталлятором** выполните следующую команду для запуска масштабирования:

   ```bash
   dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> --ssh-host <MASTER-NODE-0-HOST>
   ```

#### Как убрать роль master-узла, сохранив узел?

1. Сделайте [резервную копию etcd](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать обновлению master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. Снимите следующие лейблы:
   * `node-role.kubernetes.io/control-plane`
   * `node-role.kubernetes.io/master`
   * `node.deckhouse.io/group`

   Команда для снятия лейблов:

   ```bash
   d8 k label node <MASTER-NODE-N-NAME> node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- node.deckhouse.io/group-
   ```

1. Убедитесь, что удаляемый master-узел пропал из списка узлов кластера:

   ```bash
   d8 k -n kube-system exec -ti \
   $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o json | jq -r '.items[] | select( .status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | .metadata.name' | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ member list -w table
   ```

1. Зайдите на узел и выполните следующие команды:

   ```shell
   rm -f /etc/kubernetes/manifests/{etcd,kube-apiserver,kube-scheduler,kube-controller-manager}.yaml
   rm -f /etc/kubernetes/{scheduler,controller-manager}.conf
   rm -f /etc/kubernetes/authorization-webhook-config.yaml
   rm -f /etc/kubernetes/admin.conf /root/.kube/config
   rm -rf /etc/kubernetes/deckhouse
   rm -rf /etc/kubernetes/pki/{ca.key,apiserver*,etcd/,front-proxy*,sa.*}
   rm -rf /var/lib/etcd/member/
   ```

<div id='как-изменить-образ-ос-в-multi-master-кластере'></div>

#### Как изменить образ ОС в мультимастерном кластере?

1. Сделайте [резервную копию `etcd`](faq.html#резервное-копирование-и-восстановление-etcd) и папки `/etc/kubernetes`.
1. Скопируйте полученный архив за пределы кластера (например, на локальную машину).
1. Убедитесь, что в кластере нет [алертов](./prometheus/faq.html#как-получить-информацию-об-алертах-в-кластере), которые могут помешать обновлению master-узлов.
1. Убедитесь, что очередь Deckhouse пуста.
1. **На локальной машине** запустите контейнер установщика Deckhouse соответствующей редакции и версии (измените адрес container registry при необходимости):

   ```bash
   DH_VERSION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/version}') 
   DH_EDITION=$(d8 k -n d8-system get deployment deckhouse -o jsonpath='{.metadata.annotations.core\.deckhouse\.io\/edition}' | tr '[:upper:]' '[:lower:]' ) 
   docker run --pull=always -it -v "$HOME/.ssh/:/tmp/.ssh/" \
     registry.deckhouse.io/deckhouse/${DH_EDITION}/install:${DH_VERSION} bash
   ```

1. **В контейнере с инсталлятором** выполните следующую команду, чтобы проверить состояние перед началом работы:

   ```bash
   dhctl terraform check --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST> --ssh-host <MASTER-NODE-1-HOST> --ssh-host <MASTER-NODE-2-HOST>
   ```

   Ответ должен сообщить, что Terraform не нашел расхождений и изменений не требуется.

1. **В контейнере с инсталлятором** выполните следующую команду и укажите необходимый образ ОС в параметре `masterNodeGroup.instanceClass` (укажите адреса всех master-узлов в параметре `--ssh-host`):

   ```bash
   dhctl config edit provider-cluster-configuration --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST> --ssh-host <MASTER-NODE-1-HOST> --ssh-host <MASTER-NODE-2-HOST>
   ```

1. **В контейнере с инсталлятором** выполните следующую команду, чтобы провести обновление узлов:

   Внимательно изучите действия, которые планирует выполнить converge, когда запрашивает подтверждение.

   При выполнении команды узлы будут замены на новые с подтверждением на каждом узле. Замена будет выполняться по очереди в обратном порядке (2,1,0).

   ```bash
   dhctl converge --ssh-agent-private-keys=/tmp/.ssh/<SSH_KEY_FILENAME> --ssh-user=<USERNAME> \
     --ssh-host <MASTER-NODE-0-HOST> --ssh-host <MASTER-NODE-1-HOST> --ssh-host <MASTER-NODE-2-HOST>
   ```

   Следующие действия (П. 9-12) **выполняйте поочередно на каждом** master-узле, начиная с узла с наивысшим номером (с суффиксом 2) и заканчивая узлом с наименьшим номером (с суффиксом 0).

1. **На созданном узле** откройте журнал systemd-юнита `bashible.service`. Дождитесь окончания настройки узла — в журнале должно появиться сообщение `nothing to do`:

   ```bash
   journalctl -fu bashible.service
   ```

1. Проверьте, что узел etcd отобразился в списке узлов кластера:

   ```bash
   d8 k -n kube-system exec -ti \
   $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o json | jq -r '.items[] | select( .status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | .metadata.name' | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ member list -w table
   ```

1. Убедитесь, что `control-plane-manager` функционирует на узле.

   ```bash
   d8 k -n kube-system wait pod --timeout=10m --for=condition=ContainersReady \
     -l app=d8-control-plane-manager --field-selector spec.nodeName=<MASTER-NODE-N-NAME>
   ```

1. Перейдите к обновлению следующего узла.

<div id='как-изменить-образ-ос-в-single-master-кластере'></div>

#### Как изменить образ ОС в кластере с одним master-узлом?

1. Преобразуйте кластер с одним master-узлом в мультимастерный в соответствии с [инструкцией](#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master).
1. Обновите master-узлы в соответствии с [инструкцией](#как-изменить-образ-ос-в-multi-master-кластере).
1. Преобразуйте мультимастерный кластер в кластер с одним master-узлом в соответствии с [инструкцией](#как-уменьшить-число-master-узлов-в-облачном-кластере)

<div id='как-посмотреть-список-memberов-в-etcd'></div>

#### Как посмотреть список узлов кластера в etcd?

##### Вариант 1

Используйте команду `etcdctl member list`.

Пример:

```shell
d8 k -n kube-system exec -ti \
$(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o json | jq -r '.items[] | select( .status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | .metadata.name' | head -n1) -- \
etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
--endpoints https://127.0.0.1:2379/ member list -w table
```

**Внимание.** Последний параметр в таблице вывода показывает, что узел находится в состоянии `learner`, а не в состоянии `leader`.

##### Вариант 2

Используйте команду `etcdctl endpoint status`. Для этой команды, после флага `--endpoints` нужно подставить адрес каждого узла control-plane. В пятом столбце таблицы вывода будет указано значение `true` для лидера.

Пример скрипта, который автоматически передает все адреса узлов control-plane:

```shell
MASTER_NODE_IPS=($(d8 k get nodes -l \
node-role.kubernetes.io/control-plane="" \
-o 'custom-columns=IP:.status.addresses[?(@.type=="InternalIP")].address' \
--no-headers))
unset ENDPOINTS_STRING
for master_node_ip in ${MASTER_NODE_IPS[@]}
do ENDPOINTS_STRING+="--endpoints https://${master_node_ip}:2379 "
done
d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod \
-l component=etcd,tier=control-plane -o name | head -n1) \
-- etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt  --cert /etc/kubernetes/pki/etcd/ca.crt \
--key /etc/kubernetes/pki/etcd/ca.key \
$(echo -n $ENDPOINTS_STRING) endpoint status -w table
```

#### Что делать, если что-то пошло не так?

В процессе работы `control-plane-manager` автоматически создает резервные копии конфигурации и данных, которые могут пригодиться в случае возникновения проблем. Эти резервные копии сохраняются в директории `/etc/kubernetes/deckhouse/backup`. Если в процессе работы возникли ошибки или непредвиденные ситуации, вы можете использовать эти резервные копии для восстановления до предыдущего исправного состояния.

<div id='что-делать-если-кластер-etcd-развалился'></div>

#### Что делать, если кластер etcd не функционирует?

Если кластер etcd не функционирует и не удается восстановить его из резервной копии, вы можете попытаться восстановить его с нуля, следуя шагам ниже.

1. Сначала на всех узлах, которые являются частью вашего кластера etcd, кроме одного, удалите манифест `etcd.yaml`, который находится в директории `/etc/kubernetes/manifests/`. После этого только один узел останется активным, и с него будет происходить восстановление состояния мультимастерного кластера.
1. На оставшемся узле откройте файл манифеста `etcd.yaml` и укажите параметр `--force-new-cluster` в `spec.containers.command`.
1. После успешного восстановления кластера, удалите параметр `--force-new-cluster`.

 {% alert level="warning" %}
 Эта операция является деструктивной, так как она полностью уничтожает текущие данные и инициализирует кластер с состоянием, которое сохранено на узле. Все pending-записи будут утеряны.
 {% endalert %}

##### Что делать, если etcd постоянно перезапускается с ошибкой?

Этот способ может понадобиться, если использование параметра `--force-new-cluster` не восстанавливает работу etcd. Это может произойти, если converge master-узлов прошел неудачно, в результате чего новый master-узел был создан на старом диске etcd, изменил свой адрес в локальной сети, а другие master-узлы отсутствуют. Этот метод стоит использовать если контейнер etcd находится в бесконечном цикле перезапуска, а в его логах появляется ошибка: `panic: unexpected removal of unknown remote peer`.

1. Установите утилиту etcdutl.
1. С текущего локального снапшота базы etcd (`/var/lib/etcd/member/snap/db`) выполните создание нового снапшота:

   ```shell
   ./etcdutl snapshot restore /var/lib/etcd/member/snap/db --name <HOSTNAME> \
   --initial-cluster=HOSTNAME=https://<ADDRESS>:2380 --initial-advertise-peer-urls=https://ADDRESS:2380 \
   --skip-hash-check=true --data-dir /var/lib/etcdtest
   ```

   * `<HOSTNAME>` — название master-узла;
   * `<ADDRESS>` — адрес master-узла.

1. Выполните следующие команды для использования нового снапшота:

   ```shell
   cp -r /var/lib/etcd /tmp/etcd-backup
   rm -rf /var/lib/etcd
   mv /var/lib/etcdtest /var/lib/etcd
   ```

1. Найдите контейнеры `etcd` и `api-server`:

   ```shell
   crictl ps -a | egrep "etcd|apiserver"
   ```

1. Удалите найденные контейнеры `etcd` и `api-server`:

   ```shell
   crictl rm <CONTAINER-ID>
   ```

1. Перезапустите master-узел.

##### Что делать, если объем базы данных etcd достиг лимита, установленного в quota-backend-bytes?

Когда объем базы данных etcd достигает лимита, установленного параметром `quota-backend-bytes`, доступ к ней становится "read-only". Это означает, что база данных etcd перестает принимать новые записи, но при этом остается доступной для чтения данных. Вы можете понять, что столкнулись с подобной ситуацией, выполнив команду:

   ```shell
   d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o name | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ endpoint status -w table --cluster
   ```

Если в поле `ERRORS` вы видите подобное сообщение `alarm:NOSPACE`, значит вам нужно предпринять следующие шаги:

1. Найдите строку с `--quota-backend-bytes` в файле манифеста пода etcd, расположенного по пути `/etc/kubernetes/manifests/etcd.yaml` и увеличьте значение, умножив указанный параметр в этой строке на два. Если такой строки нет — добавьте, например: `- --quota-backend-bytes=8589934592`. Эта настройка задает лимит на 8 ГБ.

1. Сбросьте активное предупреждение (alarm) о нехватке места в базе данных. Для этого выполните следующую команду:

   ```shell
   d8 k -n kube-system exec -ti $(d8 k -n kube-system get pod -l component=etcd,tier=control-plane -o name | head -n1) -- \
   etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key \
   --endpoints https://127.0.0.1:2379/ alarm disarm
   ```

1. Измените параметр [maxDbSize](configuration.html#parameters-etcd-maxdbsize) в настройках `control-plane-manager` на тот, который был задан в манифесте.

#### Как выполнить дефрагментацию etcd

{% alert level="warning" %}
Перед дефрагментацией [создайте резервную копию etcd](#как-сделать-резервную-копию-etcd-вручную).
{% endalert %}

Для просмотра размера БД etcd на определенном узле перед дефрагментацией и после ее выполнения используйте команду (здесь `NODE_NAME` — имя master-узла):

```bash
d8 k -n kube-system exec -it etcd-NODE_NAME -- /usr/bin/etcdctl \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  endpoint status --cluster -w table
```

Пример вывода (размер БД etcd на узле указывается в колонке `DB SIZE`):

```console
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
|          ENDPOINT           |        ID        | VERSION | STORAGE VERSION | DB SIZE | IN USE | PERCENTAGE NOT IN USE | QUOTA  | IS LEADER  | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | DOWNGRADE TARGET VERSION | DOWNGRADE ENABLED |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| https://192.168.199.80:2379 | 489a8af1e7acd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |       true |      false |        56 |  258054684 |          258054684 |        |                          |             false |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| https://192.168.199.81:2379 | 589a8ad1e7ccd7b0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| https://192.168.199.82:2379 | 229a8cd1e7bcd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
+-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
```

##### Как выполнить дефрагментацию etcd узла в кластере с одним master-узлом

{% alert level="warning" %}
Дефрагментация etcd — ресурсоемкая операция, которая на время полностью блокирует работу etcd на данном узле.
Учитывайте это при выборе времени для проведения операции в кластере с одним master-узлом.
{% endalert %}

Чтобы выполнить дефрагментацию etcd в кластере с одним master-узлом, используйте следующую команду (здесь `NODE_NAME` — имя master-узла):

```bash
d8 k -n kube-system exec -ti etcd-NODE_NAME -- /usr/bin/etcdctl \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --cert /etc/kubernetes/pki/etcd/ca.crt \
  --key /etc/kubernetes/pki/etcd/ca.key \
  --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
```

Пример вывода при успешном выполнении операции:

```console
Finished defragmenting etcd member[https://localhost:2379]. took 848.948927ms
```

> При появлении ошибки из-за таймаута увеличивайте значение параметра `–command-timeout` из команды выше, пока дефрагментация не выполнится успешно.

##### Как выполнить дефрагментацию etcd в кластере с несколькими master-узлами

Чтобы выполнить дефрагментацию etcd в кластере с несколькими master-узлами:

1. Получите список подов etcd. Для этого используйте следующую команду:

   ```bash
   d8 k -n kube-system get pod -l component=etcd -o wide
   ```

   Пример вывода:

   ```console
   NAME           READY    STATUS    RESTARTS   AGE     IP              NODE        NOMINATED NODE   READINESS GATES
   etcd-master-0   1/1     Running   0          3d21h   192.168.199.80  master-0    <none>           <none>
   etcd-master-1   1/1     Running   0          3d21h   192.168.199.81  master-1    <none>           <none>
   etcd-master-2   1/1     Running   0          3d21h   192.168.199.82  master-2    <none>           <none>
   ```

1. Определите master-узел — лидер. Для этого обратитесь к любому поду etcd и получите список узлов — участников кластера etcd с помощью команды (где `NODE_NAME` — имя master-узла):

   ```bash
   d8 k -n kube-system exec -it etcd-NODE_NAME -- /usr/bin/etcdctl \
     --cert=/etc/kubernetes/pki/etcd/server.crt \
     --key=/etc/kubernetes/pki/etcd/server.key \
     --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     endpoint status --cluster -w table
   ```

   Пример вывода (у лидера в колонке `IS LEADER` будет значение `true`):

   ```console
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   |          ENDPOINT           |        ID        | VERSION | STORAGE VERSION | DB SIZE | IN USE | PERCENTAGE NOT IN USE | QUOTA  | IS LEADER  | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | DOWNGRADE TARGET VERSION | DOWNGRADE ENABLED |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   | https://192.168.199.80:2379 | 489a8af1e7acd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |       true |      false |        56 |  258054684 |          258054684 |        |                          |             false |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   | https://192.168.199.81:2379 | 589a8ad1e7ccd7b0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   | https://192.168.199.82:2379 | 229a8cd1e7bcd7a0 |   3.6.1 |           3.6.0 |   76 MB |  62 MB |                   20% | 2.1 GB |      false |      false |        56 |  258054685 |          258054685 |        |                          |             false |
   +-----------------------------+------------------+---------+-----------------+---------+--------+-----------------------+--------+------------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
   ```

1. Поочередно выполните дефрагментацию etcd узлов — участников etcd кластера. Для дефрагментации используйте команду (здесь `NODE_NAME` — имя master-узла):

   > Важно: дефрагментацию лидера необходимо выполнять в последнюю очередь.
   >
   > Восстановление etcd на узле после дефрагментации может занять некоторое время. Рекомендуется подождать не менее минуты прежде чем переходить к дефрагментации etcd следующего узла.

   ```bash
   d8 k -n kube-system exec -ti etcd-NODE_NAME -- /usr/bin/etcdctl \
     --cacert /etc/kubernetes/pki/etcd/ca.crt \
     --cert /etc/kubernetes/pki/etcd/ca.crt \
     --key /etc/kubernetes/pki/etcd/ca.key \
     --endpoints https://127.0.0.1:2379/ defrag --command-timeout=30s
   ```

   Пример вывода при успешном выполнении операции:

   ```console
   Finished defragmenting etcd member[https://localhost:2379]. took 848.948927ms
   ```

   > При появлении ошибки из-за таймаута увеличивайте значение параметра `–command-timeout` из команды выше, пока дефрагментация не выполнится успешно.

#### Как настроить дополнительные политики аудита?

1. Включите параметр [auditPolicyEnabled](configuration.html#parameters-apiserver-auditpolicyenabled) в настройках модуля:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: control-plane-manager
   spec:
     version: 1
     settings:
       apiserver:
         auditPolicyEnabled: true
   ```

2. Создайте Secret `kube-system/audit-policy` с YAML-файлом политик, закодированным в Base64:

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: audit-policy
     namespace: kube-system
   data:
     audit-policy.yaml: <base64>
   ```

   Минимальный рабочий пример `audit-policy.yaml` выглядит так:

   ```yaml
   apiVersion: audit.k8s.io/v1
   kind: Policy
   rules:
   - level: Metadata
     omitStages:
     - RequestReceived
   ```

##### Как исключить встроенные политики аудита?

Установите параметр [apiserver.basicAuditPolicyEnabled](configuration.html#parameters-apiserver-basicauditpolicyenabled) модуля в `false`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    apiserver:
      auditPolicyEnabled: true
      basicAuditPolicyEnabled: false
```

##### Как вывести аудит-лог в стандартный вывод вместо файлов?

Установите параметр [apiserver.auditLog.output](configuration.html#parameters-apiserver-auditlog) модуля в значение `Stdout`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    apiserver:
      auditPolicyEnabled: true
      auditLog:
        output: Stdout
```

##### Как работать с журналом аудита?

Предполагается, что на master-узлах установлен «скрейпер логов»: [log-shipper](./log-shipper/cr.html#clusterloggingconfig), `promtail`, `filebeat`,  который будет мониторить файл с логами:

```bash
/var/log/kube-audit/audit.log
```

Параметры ротации логов в файле журнала предустановлены и их изменение не предусмотрено:

* Максимальное занимаемое место на диске `1000 МБ`.
* Максимальная глубина записи `30 дней`.

В зависимости от настроек политики (`Policy`) и количества запросов к `apiserver` логов может быть очень много, соответственно глубина хранения может быть менее 30 минут.

{% alert level="warning" %}
Текущая реализация функционала не гарантирует безопасность, так как существует риск временного нарушения работы control plane.

Если в Secret'е с конфигурационным файлом окажутся неподдерживаемые опции или опечатка, `apiserver` не сможет запуститься.
{% endalert %}

В случае возникновения проблем с запуском `apiserver`, потребуется вручную отключить параметры `--audit-log-*` в манифесте `/etc/kubernetes/manifests/kube-apiserver.yaml` и перезапустить `apiserver` следующей командой:

```bash
docker stop $(docker ps | grep kube-apiserver- | awk '{print $1}')
### Или (в зависимости используемого вами CRI).
crictl stopp $(crictl pods --name=kube-apiserver -q)
```

После перезапуска будет достаточно времени исправить Secret или удалить его:

```bash
d8 k -n kube-system delete secret audit-policy
```

#### Как ускорить перезапуск подов при потере связи с узлом?

По умолчанию, если узел в течении 40 секунд не сообщает свое состояние, он помечается как недоступный. И еще через 5 минут поды узла начнут перезапускаться на других узлах.  В итоге общее время недоступности приложений составляет около 6 минут.

В специфических случаях, когда приложение не может быть запущено в нескольких экземплярах, есть способ сократить период их недоступности:

1. Уменьшить время перехода узла в состояние `Unreachable` при потере с ним связи настройкой параметра `nodeMonitorGracePeriodSeconds`.
1. Установить меньший таймаут удаления подов с недоступного узла в параметре `failedNodePodEvictionTimeoutSeconds`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: control-plane-manager
spec:
  version: 1
  settings:
    nodeMonitorGracePeriodSeconds: 10
    failedNodePodEvictionTimeoutSeconds: 50
```

В этом случае при потере связи с узлом приложения будут перезапущены примерно через 1 минуту.

Оба упомянутых параметра напрямую влияют на использование процессора и памяти control-plane'ом. Снижая таймауты, системные компоненты чаще отправляют статусы и проверяют состояние ресурсов.

При выборе оптимальных значений учитывайте графики использования ресурсов управляющих узлов. Чем меньше значения параметров, тем больше ресурсов может понадобиться для их обработки на этих узлах.

#### Резервное копирование и восстановление etcd

##### Что выполняется автоматически

Автоматически запускаются CronJob `kube-system/d8-etcd-backup-*` в 00:00 по UTC+0. Результат сохраняется в `/var/lib/etcd/etcd-backup.tar.gz` на всех узлах с `control-plane` в кластере (master-узлы).

<div id='как-сделать-бэкап-etcd-вручную'></div>

##### Как сделать резервную копию etcd вручную

###### Используя Deckhouse CLI (Deckhouse Platform Certified Security Edition v1.65+)

Начиная с релиза Deckhouse Platform Certified Security Edition v1.65, стала доступна утилита `d8 backup etcd`, которая предназначена для быстрого создания снимков состояния etcd.

```bash
d8 backup etcd ./etcd-backup.snapshot
```

###### Используя bash (Deckhouse Platform Certified Security Edition v1.64 и старше)

Войдите на любой control-plane узел под пользователем `root` и используйте следующий bash-скрипт:

```bash
###!/usr/bin/env bash
set -e

pod=etcd-`hostname`
d8 k -n kube-system exec "$pod" -- /usr/bin/etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key --endpoints https://127.0.0.1:2379/ snapshot save /var/lib/etcd/${pod##*/}.snapshot && \
mv /var/lib/etcd/"${pod##*/}.snapshot" etcd-backup.snapshot && \
cp -r /etc/kubernetes/ ./ && \
tar -cvzf kube-backup.tar.gz ./etcd-backup.snapshot ./kubernetes/
rm -r ./kubernetes ./etcd-backup.snapshot
```

В текущей директории будет создан файл `kube-backup.tar.gz` со снимком базы etcd одного из узлов кластера.
Из полученного снимка можно будет восстановить состояние кластера.

Рекомендуем сделать резервную копию директории `/etc/kubernetes`, в которой находятся:

* манифесты и конфигурация компонентов control-plane;
* PKI кластера Kubernetes.

Данная директория поможет быстро восстановить кластер при полной потере control-plane узлов без создания нового кластера и без повторного присоединения узлов в новый кластер.

Рекомендуем хранить резервные копии снимков состояния кластера etcd, а также резервную копию директории `/etc/kubernetes/` в зашифрованном виде вне кластера Deckhouse.

##### Как выполнить полное восстановление состояния кластера из резервной копии etcd?

Далее описаны шаги по восстановлению кластера до предыдущего состояния из резервной копии при полной потере данных.

<div id='восстановление-кластера-single-master'></div>

###### Восстановление кластера с одним master-узлом

Для корректного восстановления выполните следующие шаги на master-узле:

1. Найдите утилиту `etcdutl` на master-узле и скопируйте исполняемый файл в `/usr/local/bin/`:

   ```shell
   cp $(find /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/ \
   -name etcdutl -print | tail -n 1) /usr/local/bin/etcdutl
   etcdutl version
   ```

   Должен отобразиться корректный вывод `etcdutl version` без ошибок.

   Также вы можете загрузить исполняемый файл etcdutl:

   ```shell
   wget "https://github.com/etcd-io/etcd/releases/download/v3.6.1/etcd-v3.6.1-linux-amd64.tar.gz"
   tar -xzvf etcd-v3.6.1-linux-amd64.tar.gz && mv etcd-v3.6.1-linux-amd64/etcdutl /usr/local/bin/etcdutl
   ```

   Проверить версию etcd в кластере можно выполнив следующую команду (команда может не сработать, если etcd и Kubernetes API недоступны):

   ```shell
   d8 k -n kube-system exec -ti etcd-$(hostname) -- etcdutl version
   ```

1. Остановите etcd.

   ```shell
   mv /etc/kubernetes/manifests/etcd.yaml ~/etcd.yaml
   ```

1. Сохраните текущие данные etcd.

   ```shell
   cp -r /var/lib/etcd/member/ /var/lib/deckhouse-etcd-backup
   ```

1. Очистите директорию etcd.

   ```shell
   rm -rf /var/lib/etcd
   ```

1. Положите резервную копию etcd в файл `~/etcd-backup.snapshot`.

1. Восстановите базу данных etcd.

   ```shell
   ETCDCTL_API=3 etcdutl snapshot restore ~/etcd-backup.snapshot  --data-dir=/var/lib/etcd
   ```

1. Запустите etcd. Запуск может занять некоторое время.

   ```shell
   mv ~/etcd.yaml /etc/kubernetes/manifests/etcd.yaml
      ```

   Чтобы убедиться, что etcd запущена, воспользуйтесь командой:

   ```shell
   crictl ps --label io.kubernetes.pod.name=etcd-$HOSTNAME
   ```

   Пример вывода:

   ```console
   CONTAINER        IMAGE            CREATED              STATE     NAME      ATTEMPT     POD ID          POD
   4b11d6ea0338f    16d0a07aa1e26    About a minute ago   Running   etcd      0           ee3c8c7d7bba6   etcd-gs-test
   ```

1. Перезапустите master-узел.

<div id='восстановление-кластера-multi-master'></div>

###### Восстановление мультимастерного кластера

Для корректного восстановления выполните следующие шаги:

1. Включите режим High Availability (HA) с помощью глобального параметра [highAvailability](/reference/api/global.html#parameters-highavailability). Это необходимо для сохранения хотя бы одной реплики Prometheus и его PVC, поскольку в режиме кластера с одним master-узлом HA по умолчанию отключён.

1. Переведите кластер в режим с одним master-узлом в соответствии с [инструкцией](#как-уменьшить-число-master-узлов-в-облачном-кластере) для облачных кластеров, или самостоятельно выведите статические master-узлы из кластера.

1. На оставшемся единственном master-узле выполните шаги по восстановлению etcd из резервной копии в соответствии с [инструкцией](#восстановление-кластера-single-master) для кластера с одним master-узлом.

1. Когда работа etcd будет восстановлена, удалите из кластера информацию об уже удаленных в первом пункте master-узлах, воспользовавшись следующей командой (укажите название узла):

   ```shell
   d8 k delete node <MASTER_NODE_I>
   ```

1. Перезапустите все узлы кластера.

1. Дождитесь выполнения заданий из очереди Deckhouse:

   ```shell
   d8 s queue main
   ```

1. Переведите кластер обратно в режим мультимастерного в соответствии с [инструкцией](#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master) для облачных кластеров или [инструкцией](#как-добавить-master-узел-в-статическом-или-гибридном-кластере) для статических или гибридных кластеров.

##### Как восстановить объект Kubernetes из резервной копии etcd?

Чтобы получить данные определенных объектов кластера из резервной копии etcd:

1. Запустите временный экземпляр etcd.
1. Заполните его данными из [резервной копии](#как-сделать-бэкап-etcd-вручную).
1. Получите описания нужных объектов с помощью `auger`.

###### Пример шагов по восстановлению объектов из резервной копии etcd

В следующем примере `etcd-backup.snapshot` — [резервная копия](#как-сделать-бэкап-etcd-вручную) etcd (snapshot), `infra-production` — пространство имен, в котором нужно восстановить объекты.

* Для выгрузки бинарных данных из etcd потребуется утилита auger.

  ```shell
  git clone -b v1.0.1 --depth 1 https://github.com/etcd-io/auger
  cd auger
  make release
  build/auger -h
  ```
  
* Получившийся исполняемый файл `build/auger`, а также `snapshot` из резервной копии etcd нужно загрузить на master-узел, с которого будет выполняться дальнейшие действия.

Данные действия выполняются на master-узле в кластере, на который предварительно был загружен файл `snapshot` и утилита `auger`:

1. Установите корректные права доступа для файла с резервной копией:

   ```shell
   chmod 644 etcd-backup.snapshot
   ```

1. Установите полный путь до `snapshot` и до утилиты в переменных окружения:

   ```shell
   SNAPSHOT=/root/etcd-restore/etcd-backup.snapshot
   AUGER_BIN=/root/auger 
   chmod +x $AUGER_BIN
   ```

1. Запустите под с временным экземпляром etcd:

   * Создайте манифест пода. Он будет запускаться именно на текущем master-узле, выбрав его по переменной `$HOSTNAME`, и смонтирует `snapshot` по пути `$SNAPSHOT` для загрузки во временный экземпляр etcd:

     ```shell
     cat <<EOF >etcd.pod.yaml 
     apiVersion: v1
     kind: Pod
     metadata:
       name: etcdrestore
       namespace: default
     spec:
       nodeName: $HOSTNAME
       tolerations:
       - operator: Exists
       initContainers:
       - command:
         - etcdutl
         - snapshot
         - restore
         - "/tmp/etcd-snapshot"
         - --data-dir=/default.etcd
         image: $(kubectl -n kube-system get pod -l component=etcd -o jsonpath="{.items[*].spec.containers[*].image}" | cut -f 1 -d ' ')
         imagePullPolicy: IfNotPresent
         name: etcd-snapshot-restore
         # Раскоментируйте фрагмент ниже, чтобы задать лимиты для контейнера, если ресурсов узла недостаточно для его запуска.
         # resources:
         #   requests:
         #     ephemeral-storage: "200Mi"
         #   limits:
         #     ephemeral-storage: "500Mi"
         volumeMounts:
         - name: etcddir
           mountPath: /default.etcd
         - name: etcd-snapshot
           mountPath: /tmp/etcd-snapshot
           readOnly: true
       containers:
       - command:
         - etcd
         image: $(kubectl -n kube-system get pod -l component=etcd -o jsonpath="{.items[*].spec.containers[*].image}" | cut -f 1 -d ' ')
         imagePullPolicy: IfNotPresent
         name: etcd-temp
         volumeMounts:
         - name: etcddir
           mountPath: /default.etcd
       volumes:
       - name: etcddir
         emptyDir: {}
         # Используйте фрагмент ниже вместо emptyDir: {}, чтобы задать лимиты для контейнера, если ресурсов узла недостаточно для его запуска.
         # emptyDir:
         #  sizeLimit: 500Mi
       - name: etcd-snapshot
         hostPath:
           path: $SNAPSHOT
           type: File
     EOF
     ```

   * Запустите под:

     ```shell
     d8 k create -f etcd.pod.yaml
     ```

1. Установите нужные переменные. В текущем примере:

   * `infra-production` - пространство имен, в котором мы будем искать ресурсы.

   * `/root/etcd-restore/output` - каталог для восстановленных манифестов.

   * `/root/auger` - путь до исполняемого файла утилиты `auger`:

     ```shell
     FILTER=infra-production
     BACKUP_OUTPUT_DIR=/root/etcd-restore/output
     mkdir -p $BACKUP_OUTPUT_DIR && cd $BACKUP_OUTPUT_DIR
     ```

1. Следующие команды отфильтруют список нужных ресурсов по переменной `$FILTER` и выгрузят их в каталог `$BACKUP_OUTPUT_DIR`:

   ```shell
   files=($(d8 k -n default exec etcdrestore -c etcd-temp -- etcdctl  --endpoints=localhost:2379 get / --prefix --keys-only | grep "$FILTER"))
   for file in "${files[@]}"
   do
     OBJECT=$(d8 k -n default exec etcdrestore -c etcd-temp -- etcdctl  --endpoints=localhost:2379 get "$file" --print-value-only | $AUGER_BIN decode)
     FILENAME=$(echo $file | sed -e "s#/registry/##g;s#/#_#g")
     echo "$OBJECT" > "$BACKUP_OUTPUT_DIR/$FILENAME.yaml"
     echo $BACKUP_OUTPUT_DIR/$FILENAME.yaml
   done
   ```

1. Удалите из полученных описаний объектов информацию о времени создания (`creationTimestamp`), `UID`, `status` и прочие оперативные данные, после чего восстановите объекты:

   ```bash
   d8 k create -f deployments_infra-production_supercronic.yaml
   ```

1. Удалите под с временным экземпляром etcd:

   ```bash
   d8 k -n default delete pod etcdrestore
   ```

#### Как выбирается узел, на котором будет запущен под?

За распределение подов по узлам отвечает планировщик Kubernetes (компонент `scheduler`).
Он проходит через две основные фазы — `Filtering` и `Scoring` (на самом деле, фаз больше, например, `pre-filtering` и `post-filtering`, но в общем можно выделить две ключевые фазы).

##### Общее устройство планировщика Kubernetes

Планировщик состоит из плагинов, которые работают в рамках какой-либо фазы (фаз).

Примеры плагинов:

* **ImageLocality** — отдает предпочтение узлам, на которых уже есть образы контейнеров, которые используются в запускаемом поде. Фаза: `Scoring`.
* **TaintToleration** — реализует механизм taints and tolerations. Фазы: `Filtering`, `Scoring`.
* **NodePorts** — проверяет, есть ли у узла свободные порты, необходимые для запуска пода. Фаза: `Filtering`.

##### Логика работы

###### Профили планировщика

Есть два преднастроенных профиля планировщика:

* `default-scheduler` — профиль по умолчанию, который распределяет поды на узлы с наименьшей загрузкой;
* `high-node-utilization` — профиль, при котором поды размещаются на узлах с наибольшей загрузкой.

Чтобы задать профиль планировщика, укажите его параметре `spec.schedulerName` манифеста пода.

Пример использования профиля:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: scheduler-example
  labels:
    name: scheduler-example
spec:
  schedulerName: high-node-utilization
  containers:
  - name: example-pod
    image: registry.k8s.io/pause:2.0  
```

###### Этапы планирования подов

На первой фазе — `Filtering` — активируются плагины фильтрации (filter-плагины), которые из всех доступных узлов выбирают те, которые удовлетворяют определенным условиям фильтрации (например, `taints`, `nodePorts`, `nodeName`, `unschedulable` и другие). Если узлы расположены в разных зонах, планировщик чередует выбор зон, чтобы избежать размещения всех подов в одной зоне.

Предположим, что узлы распределяются по зонам следующим образом:

```text
Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
```

В этом случае они будут выбираться в следующем порядке:

```text
Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
```

Обратите внимание, что с целью оптимизации выбираются не все попадающие под условия узлы, а только их часть. По умолчанию функция выбора количества узлов линейная. Для кластера из ≤50 узлов будут выбраны 100% узлов, для кластера из 100 узлов — 50%, а для кластера из 5000 узлов — 10%. Минимальное значение — 5% при количестве узлов более 5000. Таким образом, при настройках по умолчанию узел может не попасть в список возможных узлов для запуска.

После того как были выбраны узлы, соответствующие условиям фильтрации, запускается фаза `Scoring`. Каждый плагин анализирует список отфильтрованных узлов и назначает оценку (score) каждому узлу. Оценки от разных плагинов суммируются. На этой фазе оцениваются доступные ресурсы на узлах: `pod capacity`, `affinity`, `volume provisioning` и другие. По итогам этой фазы выбирается узел с наибольшей оценкой. Если сразу несколько узлов получили максимальную оценку, узел выбирается случайным образом.

В итоге под запускается на выбранном узле.

###### Документация

##### Как изменить или расширить логику работы планировщика

Для изменения логики работы планировщика можно использовать механизм плагинов расширения.

Каждый плагин представляет собой вебхук, отвечающий следующим требованиям:

* Использование TLS.
* Доступность через сервис внутри кластера.
* Поддержка стандартных `Verbs` (`filterVerb = filter`, `prioritizeVerb = prioritize`).
* Также, предполагается что все подключаемые плагины могут кэшировать информацию об узле (`nodeCacheCapable: true`).

Подключить `extender` можно при помощи ресурса [KubeSchedulerWebhookConfiguration](cr.html#kubeschedulerwebhookconfiguration).

{% alert level="danger" %}
При использовании опции `failurePolicy: Fail`, в случае ошибки в работе вебхука планировщик Kubernetes прекратит свою работу, и новые поды не смогут быть запущены.
{% endalert %}

#### Как происходит ротация сертификатов kubelet?

В Deckhouse Platform Certified Security Edition ротация сертификатов kubelet происходит автоматически.

Kubelet использует клиентский TLS-сертификат (`/var/lib/kubelet/pki/kubelet-client-current.pem`), при помощи которого может запросить у kube-apiserver новый клиентский сертификат или новый серверный сертификат (`/var/lib/kubelet/pki/kubelet-server-current.pem`).

Когда до истечения времени жизни сертификата остается 5-10% (случайное значение из диапазона) времени, kubelet запрашивает у kube-apiserver новый сертификат.

##### Время жизни сертификатов

По умолчанию время жизни сертификатов равно 1 году (8760 часов). При необходимости это значение можно изменить с помощью аргумента `--cluster-signing-duration` в манифесте `/etc/kubernetes/manifests/kube-controller-manager.yaml`. Но чтобы kubelet успел установить сертификат до его истечения, рекомендуем устанавливать время жизни сертификатов более, чем 1 час.

{% alert level="warning" %}
Если истекло время жизни клиентского сертификата, то kubelet не сможет делать запросы к kube-apiserver и не сможет обновить сертификаты. В данном случае узел (Node) будет помечен как `NotReady` и пересоздан.
{% endalert %}

##### Особенности работы с серверными сертификатами kubelet в Deckhouse Platform Certified Security Edition

В Deckhouse Platform Certified Security Edition для запросов в kubelet API используются IP-адреса. Поэтому в конфигурации kubelet поля `tlsCertFile` и `tlsPrivateKeyFile` не указываются, а используется динамический сертификат, который kubelet генерирует самостоятельно. Также, из-за использования динамического сертификата, в Deckhouse Platform Certified Security Edition (в модуле `operator-trivy`) отключены проверки CIS benchmark `AVD-KCV-0088` и `AVD-KCV-0089`, которые отслеживают, были ли переданы аргументы `--tls-cert-file` и `--tls-private-key-file` для kubelet.

{% offtopic title="Информация о логике работы с серверными сертификатами в Kubernetes" %}

В kubelet реализована следующая логика работы с серверными сертификатами:

* Если `tlsCertFile` и `tlsPrivateKeyFile` не пустые, то kubelet будет использовать их как сертификат и ключ по умолчанию.
  * При запросе клиента в kubelet API с указанием IP-адреса (например `https://10.1.1.2:10250/`), для установления соединения по TLS-протоколу будет использован закрытый ключ по умолчанию (`tlsPrivateKeyFile`). В данном случае ротация сертификатов не будет работать.
  * При запросе клиента в kubelet API с указанием названия хоста (например `https://k8s-node:10250/`), для установления соединения по TLS-протоколу будет использован динамически сгенерированный закрытый ключ из директории `/var/lib/kubelet/pki/`. В данном случае ротация сертификатов будет работать.
* Если `tlsCertFile` и `tlsPrivateKeyFile` пустые, то для установления соединения по TLS-протоколу будет использован динамически сгенерированный закрытый ключ из директории `/var/lib/kubelet/pki/`. В данном случае ротация сертификатов будет работать.
{% endofftopic %}

#### Как вручную обновить сертификаты компонентов управляющего слоя?

Может возникнуть ситуация, когда master-узлы кластера находятся в выключенном состоянии долгое время. За это время может истечь срок действия сертификатов компонентов управляющего слоя. После включения узлов сертификаты не обновятся автоматически, поэтому это необходимо сделать вручную.

Обновление сертификатов компонентов управляющего слоя происходит с помощью утилиты `kubeadm`.
Чтобы обновить сертификаты, выполните следующие действия на каждом master-узле:

1. Найдите утилиту `kubeadm` на master-узле и создайте символьную ссылку c помощью следующей команды:

   ```shell
   ln -s  $(find /var/lib/containerd  -name kubeadm -type f -executable -print) /usr/bin/kubeadm
   ```

2. Обновите сертификаты:

   ```shell
   kubeadm certs renew all
   ```

### Модуль csi-ceph: FAQ

#### Как получить список томов RBD, разделенный по узлам?

```shell
kubectl -n d8-csi-ceph get po -l app=csi-node-rbd -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName --no-headers \
  | awk '{print "echo "$2"; kubectl -n d8-csi-ceph exec  "$1" -c node -- rbd showmapped"}' | bash
```

#### Какие версии Ceph кластеров поддерживаются

Официально сейчас поддерживаются версии >= 16.2.0. Из нашей практики текущая версия способна работать с кластерами версий >=14.2.0, но мы рекомендуем обновить версию Ceph.

#### Какие режимы работы томов поддерживаются

RBD поддерживает только ReadWriteOnce (RWO, доступ к тому в рамках одной ноды). CephFS поддерживает как ReadWriteOnce, так и ReadWriteMany (RWX, одновременный доступ к тому с нескольких нод)

### Модуль csi-nfs: FAQ

#### Как проверить работоспособность модуля?

Для этого необходимо проверить состояние подов в пространстве имён `d8-csi-nfs`. Все поды должны быть в состоянии `Running` или `Completed`, и запущены на всех узлах. Проверить можно командой:

```shell
kubectl -n d8-csi-nfs get pod -owide -w
```

#### Возможно ли изменение параметров NFS-сервера уже созданных PV?

Нет, данные для подключения к NFS-серверу сохраняются непосредственно в манифесте PV, и не подлежат изменению. Изменение StorageClass также не повлечет изменений настроек подключения в уже существующих PV.

#### Как делать снимки томов (снапшоты)?

{% alert level="warning" %}
**Предостережение про использовании снапшотов (Volume Snapshots)**

При создании снапшотов NFS-томов важно понимать схему их создания и связанные ограничения. Мы рекомендуем по возможности избегать использования snapshots в csi-nfs:

1. CSI-драйвер создает снапшот на уровне NFS-сервера.
2. Для этого используется tar, которой упаковывается содержимое тома, со всеми ограничениями, могущими возникнуть из-за этого
3. **Перед созданием снапшота обязательно остановите рабочую нагрузку** (pods), использующую NFS-том
4. NFS не обеспечивает атомарность операций на уровне файловой системы при создании снапшота

{% endalert %}

В `csi-nfs` снимки создаются путем архивирования папки тома. Архив сохраняется в корне папки NFS-сервера, указанной в параметре `spec.connection.share`.

1. Включите `snapshot-controller`:

   ```yaml
   kubectl apply -f -<<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: snapshot-controller
   spec:
     enabled: true
     version: 1
   EOF
   ```

1. Создайте снимки томов. Для этого выполните следующую команду, указав нужные параметры:

   ```yaml
   kubectl apply -f -<<EOF
   apiVersion: snapshot.storage.k8s.io/v1
   kind: VolumeSnapshot
   metadata:
     name: my-snapshot
     namespace: <имя namespace, в котором находится PVC>
   spec:
     volumeSnapshotClassName: csi-nfs-snapshot-class
     source:
       persistentVolumeClaimName: <имя PVC, для которого необходимо создать снимок>
   EOF
   ```

1. Проверьте состояние созданного снимка командой:

   ```shell
   kubectl get volumesnapshot
   ```

Эта команда покажет список всех снимков и их текущее состояние.

#### Почему не удаляются PV созданные в StorageClass с поддержкой RPC-with-TLS, а вместе с ними и каталоги `<имя PV>` на NFS сервере?

Если ресурс [NFSStorageClass](./cr.html#nfsstorageclass) был настроен с поддержкой RPC-with-TLS, может возникнуть ситуация, когда PV не удастся удалить.
Это происходит из-за удаления секрета (например, после удаления `NFSStorageClass`), который хранит параметры монтирования. В результате контроллер не может смонтировать NFS-папку для удаления папки `<имя PV>`.

#### Как в настройках ModuleConfig в параметре `tlsParameters.ca` разместить несколько CA?

- для двух CA
```shell
cat CA1.crt CA2.crt | base64 -w0
```

- для трех CA
```shell
cat CA1.crt CA2.crt CA3.crt | base64 -w0
```

- и т.д.

#### Какие требования к Linux дистрибутиву для разворачивания NFS-сервера с поддержкой RPC-with-TLS?

- Ядро должно быть собрано с включенными параметрами `CONFIG_TLS` и `CONFIG_NET_HANDSHAKE`;
- Пакет nfs-utils (в дистрибутивах основанных на Debian - nfs-common) должен быть >= 2.6.3.

### Модуль csi-scsi-generic: FAQ

#### Как проверить работоспособность модуля?

Для этого необходимо проверить состояние подов в namespace `d8-csi-scsi-generic`. Все поды должны быть в состоянии `Running` или `Completed` и запущены на всех узлах.

```shell
kubectl -n d8-csi-scsi-generic get pod -owide -w
```

### Модуль csi-yadro-tatlin-unified: FAQ

#### Как проверить работоспособность модуля?

Для этого необходимо проверить состояние подов в namespace `d8-csi-yadro-tatlin-unified`. Все поды должны быть в состоянии `Running` или `Completed` и запущены на всех узлах.

```shell
kubectl -n d8-csi-yadro-tatlin-unified get pod -owide -w
```

### Модуль deckhouse: FAQ

#### Как собрать информацию для отладки?

Мы всегда рады помочь пользователям с расследованием сложных проблем. Пожалуйста, выполните следующие шаги, чтобы мы смогли вам помочь:

1. Выполните следующую команду, чтобы собрать необходимые данные:

   ```sh
   d8 s collect-debug-info > deckhouse-debug-$(date +"%Y_%m_%d").tar.gz
   ```

2. Отправьте получившийся архив команде Deckhouse для дальнейшего расследования.

Данные, которые будут собраны:

<table>
  <thead>
    <tr>
      <th>Категория</th>
      <th>Собираемые данные</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Deckhouse</strong></td>
      <td>
        <ul>
          <li>Состояние очереди Deckhouse</li>
          <li>Deckhouse values (за исключением значений <code>kubeRBACProxyCA</code> и <code>registry.dockercfg</code>)</li>
          <li>Данные о текущей версии пода <code>deckhouse</code></li>
          <li>Все объекты DeckhouseRelease</li>
          <li>Логи подов Deckhouse</li>
          <li>Манифесты контроллеров и подов из всех пространств имен Deckhouse</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Объекты кластера</strong></td>
      <td>
        Все объекты следующих ресурсов:
        <ul>
          <li>NodeGroup</li>
          <li>NodeGroupConfiguration</li>
          <li>Node</li>
          <li>Machine</li>
          <li>Instance</li>
          <li>StaticInstance</li>
          <li>MachineDeployment</li>
          <li>ClusterAuthorizationRule</li>
          <li>AuthorizationRule</li>
          <li>ModuleConfig</li>
        </ul>
        А также Events из всех пространств имен
      </td>
    </tr>
    <tr>
      <td><strong>Модули и их состояния</strong></td>
      <td>
        <ul>
          <li>Список включенных модулей</li>
          <li>Список объектов ModuleSource в кластере</li>
          <li>Список объектов ModulePullOverride в кластере</li>
          <li>Список модулей в режиме <code>maintenance</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Логи и манифесты контроллеров</strong></td>
      <td>
        Логи следующих компонентов:
        <ul>
          <li><code>machine-controller-manager</code></li>
          <li><code>cloud-controller-manager</code></li>
          <li><code>csi-controller</code></li>
          <li><code>cluster-autoscaler</code></li>
          <li>Vertical Pod Autoscaler admission controller</li>
          <li>Vertical Pod Autoscaler recommender</li>
          <li>Vertical Pod Autoscaler updater</li>
        </ul>
        YAML-файлы следующих контроллеров:
        <ul>
          <li><code>capi-controller-manager</code></li>
          <li><code>caps-controller-manager</code></li>
          <li><code>machine-controller-manager</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Мониторинг и алерты</strong></td>
      <td>
        <ul>
          <li>Логи Prometheus</li>
          <li>Все горящие уведомления в Prometheus</li>
          <li>Список всех подов, которые не находятся в состоянии <code>Running</code>, кроме подов в состояниях <code>Completed</code> и <code>Evicted</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Сеть</strong></td>
      <td>
        <ul>
          <li>Все объекты из пространства имен <code>d8-istio</code></li>
          <li>Все кастомные ресурсы <code>istio</code></li>
          <li>Конфигурация Envoy для <code>istio</code></li>
          <li>Логи <code>istio</code></li>
          <li>Логи <code>istio</code> ingress gateway</li>
          <li>Логи <code>istio</code> users</li>
          <li>Состояние соединения Cilium (<code>cilium health status</code>)</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

#### Как отлаживать проблемы в подах с помощью ephemeral containers?

Выполните следующую команду:

```shell
d8 k -n <namespace_name> debug -it <pod_name> --image=ubuntu <container_name>
```

#### Как отлаживать проблемы на узлах с помощью ephemeral containers?

Выполните следующую команду:

```shell
d8 k debug node/mynode -it --image=ubuntu
```

### Extended monitoring модуль: FAQ

{% raw %}

#### Как переключиться на HTTP вместо HTTPS для проверки образов из собственного registry?

Чтобы изменить протокол проверки вашего registry с HTTPS на HTTP, измените параметр `settings.imageAvailability.registry.scheme` в конфигурации модуля.

Подробные инструкции смотрите в [документации по настройке модуля](./configuration.html#parameters-imageavailability-registry-scheme).

{% endraw %}

### Модуль ingress-nginx: FAQ

#### Как разрешить доступ к приложению внутри кластера только от Ingress-контроллера?

Если необходимо ограничить доступ к вашему приложению внутри кластера исключительно от подов Ingress-контроллера, необходимо в под с приложением добавить контейнер с kube-rbac-proxy, как показано в примере ниже:

{% raw %}

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: my-namespace
spec:
  selector:
    matchLabels:
      app: my-app
  replicas: 1
  template:
    metadata:
      labels:
        app: my-app
    spec:
      serviceAccountName: my-sa
      containers:
      - name: my-cool-app
        image: mycompany/my-app:v0.5.3
        args:
        - "--listen=127.0.0.1:8080"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 443
            scheme: HTTPS
      - name: kube-rbac-proxy
        image: flant/kube-rbac-proxy:v0.1.0
        # Рекомендуется использовать прокси из репозитория Deckhouse.
        args:
        - "--secure-listen-address=0.0.0.0:443"
        - "--config-file=/etc/kube-rbac-proxy/config-file.yaml"
        - "--v=2"
        - "--logtostderr=true"
        # Если kube-apiserver недоступен, аутентификация и авторизация пользователей невозможна.
        # Stale Cache хранит результаты успешной авторизации и используется лишь в случае, если apiserver недоступен.
        - "--stale-cache-interval=1h30m"
        ports:
        - containerPort: 443
          name: https
        volumeMounts:
        - name: kube-rbac-proxy
          mountPath: /etc/kube-rbac-proxy
      volumes:
      - name: kube-rbac-proxy
        configMap:
          name: kube-rbac-proxy
```

{% endraw %}

Приложение принимает запросы на адресе `127.0.0.1`, это означает, что по незащищенному соединению к нему можно подключиться только внутри пода.
Прокси прослушивает порт на адресе `0.0.0.0` и перехватывает весь внешний трафик к поду.

##### Как выдать минимальные права для ServiceAccount?

Чтобы аутентифицировать и авторизовывать пользователей с помощью kube-apiserver, у прокси должны быть права на создание `TokenReview` и `SubjectAccessReview`.

В кластерах Deckhouse Platform Certified Security Edition уже есть готовая ClusterRole — **d8-rbac-proxy**, создавать её самостоятельно не требуется! Свяжите её с ServiceAccount вашего Deployment'а, как показано в примере ниже.
{% raw %}

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
  namespace: my-namespace
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-namespace:my-sa:d8-rbac-proxy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: d8:rbac-proxy
subjects:
- kind: ServiceAccount
  name: my-sa
  namespace: my-namespace
```

##### Конфигурация Kube-RBAC-Proxy

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-rbac-proxy
data:
  config-file.yaml: |+
    excludePaths:
    - /healthz 
  # Не требуем авторизацию для liveness пробы.
    upstreams:
    - upstream: http://127.0.0.1:8081/
  # Адрес upstream-сервиса, на который будет перенаправлен входящий трафик.
      path: / 
  # Путь, обрабатываемый прокси, по которому принимаются запросы и перенаправляются на upstream.
      authorization:
        resourceAttributes:
          namespace: my-namespace
          apiGroup: apps
          apiVersion: v1
          resource: deployments
          subresource: http
          name: my-app
```

{% endraw %}

Согласно конфигурации, у пользователя должны быть права доступа к Deployment с именем `my-app`
и его дополнительному ресурсу `http` в пространстве имён `my-namespace`.

Выглядят такие права в виде RBAC следующим образом:

{% raw %}

```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kube-rbac-proxy:my-app
  namespace: my-namespace
rules:
- apiGroups: ["apps"]
  resources: ["deployments/http"]
  resourceNames: ["my-app"]
  verbs: ["get", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-rbac-proxy:my-app
  namespace: my-namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-rbac-proxy:my-app
subjects:
### Все пользовательские сертификаты ingress-контроллеров выписаны для одной конкретной группы.
- kind: Group
  name: ingress-nginx:auth
```

Для Ingress-ресурса добавьте параметры:

```yaml
nginx.ingress.kubernetes.io/backend-protocol: HTTPS
nginx.ingress.kubernetes.io/configuration-snippet: |
  proxy_ssl_certificate /etc/nginx/ssl/client.crt;
  proxy_ssl_certificate_key /etc/nginx/ssl/client.key;
  proxy_ssl_protocols TLSv1.2;
  proxy_ssl_session_reuse on;
```

{% endraw %}

#### Как сконфигурировать балансировщик нагрузки для проверки доступности IngressNginxController?

В ситуации, когда `IngressNginxController` размещен за балансировщиком нагрузки, рекомендуется сконфигурировать балансировщик для проверки доступности
узлов `IngressNginxController` с помощью HTTP-запросов или TCP-подключений. В то время как тестирование с помощью TCP-подключений представляет собой простой и универсальный механизм проверки доступности, мы рекомендуем использовать проверку на основе HTTP-запросов со следующими параметрами:

- протокол: `HTTP`;
- путь: `/healthz`;
- порт: `80` (в случае использования инлета `HostPort` нужно указать номер порта, соответствующий параметру [httpPort](cr.html#ingressnginxcontroller-v1-spec-hostport-httpport).

#### Как настроить работу через MetalLB с доступом только из внутренней сети?

Пример MetalLB с настройками доступа только из внутренней сети:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: "nginx"
  inlet: "LoadBalancer"
  loadBalancer:
    sourceRanges:
    - 192.168.0.0/24
```

{% alert level="warning" %}
Для работы необходимо включить параметр [`svcSourceRangeCheck`](/modules/cni-cilium/configuration.html#parameters-svcsourcerangecheck) в модуле cni-cilium.
{% endalert %}

#### Как добавить дополнительные поля для логирования в nginx-controller?

Пример добавления дополнительных полей:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: "nginx"
  inlet: "LoadBalancer"
  additionalLogFields:
    my-cookie: "$cookie_MY_COOKIE"
```

#### Как включить HorizontalPodAutoscaling для IngressNginxController?

{% alert level="warning" %}
Режим HPA возможен только для контроллеров с инлетом `LoadBalancer` или `LoadBalancerWithProxyProtocol`.

Режим HPA возможен только при `minReplicas` != `maxReplicas`, в противном случае deployment `hpa-scaler` не создается.
{% endalert %}

Для включения HPA используйте атрибуты `minReplicas` и `maxReplicas` в [IngressNginxController CR](cr.html#ingressnginxcontroller).

IngressNginxController разворачивается с помощью DaemonSet. DaemonSet не предоставляет возможности горизонтального масштабирования, поэтому создается дополнительный deployment `hpa-scaler` и HPA resource, который следит за предварительно созданной метрикой `prometheus-metrics-adapter-d8-ingress-nginx-cpu-utilization-for-hpa`. Если CPU utilization превысит 50%, HPA закажет новую реплику для `hpa-scaler` (с учетом minReplicas и maxReplicas).

Deployment `hpa-scaler` обладает HardPodAntiAffinity (запрет на размещение подов с одинаковыми метками на одном узле), поэтому он попытается выделить для себя новый узел (если это возможно
в рамках своей группы узлов), куда автоматически будет размещен еще один instance Ingress-контроллера.

{% alert level="info" %}

- Минимальное реальное количество реплик IngressNginxController не может быть меньше минимального количества узлов в группе узлов, куда он разворачивается.
- Максимальное реальное количество реплик IngressNginxController не может быть больше максимального количества узлов в группе узлов, куда он разворачивается.

{% endalert %}

#### Как использовать IngressClass с установленными IngressClassParameters?

Начиная с версии 1.1 IngressNginxController, Deckhouse создает объект IngressClass самостоятельно. Если вы хотите использовать свой IngressClass с установленными IngressClassParameters, достаточно добавить к нему label `ingress-class.deckhouse.io/external: "true"`:

```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    ingress-class.deckhouse.io/external: "true"
  name: my-super-ingress
spec:
  controller: ingress-nginx.deckhouse.io/my-super-ingress
  parameters:
    apiGroup: elbv2.k8s.aws
    kind: IngressClassParams
    name: awesome-class-cfg
```

В этом случае, при указании данного IngressClass в CRD IngressNginxController, Deckhouse не будет создавать объект, а использует существующий.

#### Как отключить сборку детализированной статистики Ingress-ресурсов?

По умолчанию Deckhouse собирает подробную статистику со всех Ingress-ресурсов в кластере. Этот процесс может приводить к высокой нагрузке системы мониторинга.

Для отключения сбора статистики добавьте лейбл `ingress.deckhouse.io/discard-metrics: "true"` к соответствующему пространству имён или Ingress-ресурсу.

Пример отключения сбора статистики (метрик) для всех Ingress-ресурсов в пространстве имен `review-1`:

```shell
d8 k label ns review-1 ingress.deckhouse.io/discard-metrics=true
```

Пример отключения сбора статистики (метрик) для всех Ingress-ресурсов `test-site` в пространстве имен `development`:

```shell
d8 k label ingress test-site -n development ingress.deckhouse.io/discard-metrics=true
```

#### Как корректно вывести из эксплуатации (drain) узел с запущенным IngressNginxController?

Доступно два способа корректного вывода из эксплуатации узла, на котором запущен IngressNginxController.

1. С помощью аннотации.

    Аннотация будет автоматически удалена после завершения операции.

    ```shell
    d8 k annotate node <node_name> update.node.deckhouse.io/draining=user
    ```

1. С помощью d8 k drain.

    При использовании стандартной команды d8 k drain необходимо указать флаг `--force` даже при наличии `--ignore-daemonsets`,
    поскольку IngressNginxController развёрнут с использованием Advanced DaemonSet:

    ```shell
    d8 k drain <node_name> --delete-emptydir-data --ignore-daemonsets --force
    ```

#### Как включить Web Application Firewall (WAF)?

Для защиты веб-приложений от L7-атак используется программное обеспечение известное как Web Application Firewall (WAF).
В ingress-nginx контроллер встроен WAF под названием `ModSecurity` (проект Open Worldwide Application Security).

По умолчанию ModSecurity выключен.

##### Включение ModSecurity

Для включения ModSecurity необходимо задать параметры в кастомном ресурсе IngressNginxController, в секции `config`:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: <имя_контроллера>
spec:
  config:
    enable-modsecurity: "true"
    modsecurity-snippet: |
      Include /etc/nginx/modsecurity/modsecurity.conf
```

После применения настроек ModSecurity начнет работать для всего трафика, проходящего через данный ingress-nginx контроллер.
При этом используется режим аудита (`DetectionOnly`) и базовая рекомендуемая конфигурация.

##### Настройка ModSecurity

ModSecurity можно настраивать двумя способами:
1. Для всего ingress-nginx контроллера
   - необходимые директивы описываются в секции `config.modsecurity-snippet` в кастомном ресурсе IngressNginxController, как в примере выше.
1. Для каждого кастомного ресурса Ingress по отдельности
   - необходимые директивы описываются в аннотации `nginx.ingress.kubernetes.io/modsecurity-snippet: |` непосредственно в манифестах Ingress.

Чтобы включить выполнение правил (а не только логирование), добавьте директиву `SecRuleEngine On` по примеру ниже:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: <имя_контролера>
spec:
  config:
    enable-modsecurity: "true"
    modsecurity-snippet: |
      Include /etc/nginx/modsecurity/modsecurity.conf
      SecRuleEngine On
```

На данный момент использование набора правил OWASP Core Rule Set (CRS) недоступно.

### Модуль kube-dns: FAQ

#### Как поменять домен кластера с минимальным простоем?

Добавьте новый домен и сохраните предыдущий. Для этого измените конфигурацию параметров:

1. В [controlPlaneManager.apiserver](./control-plane-manager/configuration.html):

   - [controlPlaneManager.apiserver.certSANs](./control-plane-manager/configuration.html#parameters-apiserver-certsans),
   - [apiserver.serviceAccount.additionalAPIAudiences](./control-plane-manager/configuration.html#parameters-apiserver-serviceaccount-additionalapiaudiences),
   - [apiserver.serviceAccount.additionalAPIIssuers](./control-plane-manager/configuration.html#parameters-apiserver-serviceaccount-additionalapiissuers).

   Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: control-plane-manager
   spec:
     version: 1
     enabled: true
     settings:
       apiserver:
         certSANs:
          - kubernetes.default.svc.<старый clusterDomain>
          - kubernetes.default.svc.<новый clusterDomain>
         serviceAccount:
           additionalAPIAudiences:
           - https://kubernetes.default.svc.<старый clusterDomain>
           - https://kubernetes.default.svc.<новый clusterDomain>
           additionalAPIIssuers:
           - https://kubernetes.default.svc.<старый clusterDomain>
           - https://kubernetes.default.svc.<новый clusterDomain>
   ```

1. В [kubeDns.clusterDomainAliases](configuration.html#параметры):

   Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: kube-dns
   spec:
     version: 1
     enabled: true
     settings:
       clusterDomainAliases:
         - <старый clusterDomain>
         - <новый clusterDomain>
   ```

1. Дождитесь перезапуска `kube-apiserver`:

   ```bash
   d8 k -n kube-system get pods -l component=kube-apiserver
   ```

1. Поменяйте `clusterDomain` на новый. Для этого выполните команду:

   ```bash
   d8 system edit cluster-configuration
   ```

1. Перезапустите поды deckhouse:

   ```bash
   d8 k -n d8-system rollout restart deployment deckhouse
   ```

{% alert level="warning" %}

**Важно.** В Kubernetes, контроллеры используют расширенные токены для ServiceAccount для работы с API-server. Это означает, что каждый такой токен содержит дополнительные поля `iss:` и `aud:`, которые включают в себя старый `clusterDomain` (например, `"iss": "https://kubernetes.default.svc.cluster.local"`).
При смене `clusterDomain` API-server начнет выдавать токены с новым `service-account-issuer`, но благодаря произведенной конфигурации `additionalAPIAudiences` и `additionalAPIIssuers` по-прежнему будет принимать старые токены. По истечении 48 минут (80% от 3607 секунд) Kubernetes начнет обновлять выпущенные токены, при обновлении будет использован новый `service-account-issuer`. Через 90 минут (3607 секунд плюс дополнительный буфер) после перезагрузки kube-apiserver можете удалить конфигурацию `serviceAccount` из конфигурации `control-plane-manager`.

{% endalert %}

{% alert level="warning" %}

**Важно.** Если необходимо убрать старый домен из `clusterDomainAliases` в конфигурации kube-dns, необходимо пересоздать все поды в кластере, чтобы они запустились с новым search domain в `/etc/resolv.conf`. Это приведет к недоступности сервисов кластера, пока поды не перезапустятся.

```bash
d8 k delete pods --all-namespaces --all
```

{% endalert %}

{% alert level="warning" %}

**Важно.** Если вы используете модуль [istio](/modules/istio/), после смены `clusterDomain` обязательно потребуется рестарт всех прикладных подов под управлением Istio.

{% endalert %}

#### Как увеличить количество подов kube-dns?

Deckhouse распределяет поды kube-dns по следующему принципу: выполняется поиск узлов с метками `node-role.deckhouse.io/` и `node-role.kubernetes.io/`, затем применяются следующие правила:

* Если в кластере есть узлы с ролью `kube-dns`, количество реплик вычисляется как сумма таких узлов и master-узлов, но не больше чем количество master-узлов + 2.
* Если узлы kube-dns отсутствуют, производится поиск узлов с ролью `system`, и тогда количество реплик определяется как сумма system-узлов и master-узлов, но не больше чем количество master-узлов + 2.
* Если в кластере присутствуют только мастер-узлы, количество реплик kube-dns будет равно числу мастеров.

### Модуль local-path-provisioner: FAQ

#### Как настроить Prometheus на использование локального хранилища?

Применить custom resource `LocalPathProvisioner`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: LocalPathProvisioner
metadata:
  name: localpath-system
spec:
  nodeGroups:
  - system
  path: "/opt/local-path-provisioner"
```

- `spec.nodeGroups` должен совпадать с NodeGroup, где запущен под Prometheus’а.
- `spec.path` - путь на узле, где будут лежать данные.

Добавить в конфигурацию модуля `prometheus` следующие параметры:

```yaml
longtermStorageClass: localpath-system
storageClass: localpath-system
```

Дождаться переката подов Prometheus.

### The log-shipper module: FAQ

#### Как добавить авторизацию в ресурс _ClusterLogDestination_?

Чтобы добавить параметры авторизации в ресурс [ClusterLogDestination](cr.html#clusterlogdestination), необходимо:
- изменить [протокол](cr.html#clusterlogdestination-v1alpha1-spec-loki-endpoint) подключения к Loki на HTTPS;
- добавить секцию [auth](cr.html#clusterlogdestination-v1alpha1-spec-loki-auth), в которой:
  - параметр [strategy](cr.html#clusterlogdestination-v1alpha1-spec-loki-auth-strategy) установить в `Bearer`;
  - в параметре [token](cr.html#clusterlogdestination-v1alpha1-spec-loki-auth-token) указать токен `log-shipper-token` из пространства имен `d8-log-shipper`.

Пример:

- Ресурс _ClusterLogDestination_ без авторизации:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: loki
  spec:
    type: Loki
    loki:
      endpoint: "http://loki.d8-monitoring:3100"
  ```

- Получите токен `log-shipper-token` из пространства имен `d8-log-shipper`:

  ```bash
  d8 k -n d8-log-shipper get secret log-shipper-token -o jsonpath='{.data.token}' | base64 -d
  ```

- Ресурс _ClusterLogDestination_ с авторизацией:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: loki
  spec:
    type: Loki
    loki:
      endpoint: "https://loki.d8-monitoring:3100"
      auth:
        strategy: "Bearer"
        token: <log-shipper-token>
      tls:
        verifyHostname: false
        verifyCertificate: false
  ```

### Управление узлами: FAQ

<div id='как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master'></div>

#### Как добавить master-узлы в облачном кластере?

Как конвертировать кластер с одним master-узлом в мультикластерный описано [в FAQ модуля control-plane-manager](./control-plane-manager/faq.html#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master).

<div id='как-уменьшить-число-master-узлов-в-облачном-кластере-multi-master-в-single-master'></div>

#### Как уменьшить число master-узлов в облачном кластере?

Как конвертировать мультимастерный кластер в кластер с одним master-узлом описано [в FAQ модуля control-plane-manager](./control-plane-manager/faq.html#как-уменьшить-число-master-узлов-в-облачном-кластере-multi-master-в-single-master).

#### Статические узлы

<span id='как-добавить-статический-узел-в-кластер'></span>
<span id='как-добавить-статичный-узел-в-кластер'></span>

Добавить статический узел в кластер можно вручную ([пример](examples.html#вручную)) или с помощью [Cluster API Provider Static](#как-добавить-статический-узел-в-кластер-cluster-api-provider-static).

##### Как добавить статический узел в кластер (Cluster API Provider Static)?

Чтобы добавить статический узел в кластер (сервер bare-metal или виртуальную машину), выполните следующие шаги:

1. Подготовьте необходимые ресурсы:

   - Выделите сервер или виртуальную машину и убедитесь, что узел имеет необходимую сетевую связанность с кластером.

   - При необходимости установите дополнительные пакеты ОС и настройте точки монтирования, которые будут использоваться на узле.

1. Создайте пользователя с правами `sudo`:

   - Добавьте нового пользователя (в данном примере — `caps`) с правами выполнения команд через `sudo`:

     ```shell
     useradd -m -s /bin/bash caps 
     usermod -aG sudo caps
     ```

   - Разрешите пользователю выполнять команды через `sudo` без ввода пароля. Для этого отредактируйте конфигурацию `sudo` (отредактировав файл `/etc/sudoers`, выполнив команду `sudo visudo` или другим способом):

     ```shell
     caps ALL=(ALL) NOPASSWD: ALL
     ```

1. На сервере откройте файл `/etc/ssh/sshd_config` и убедитесь, что параметр `UsePAM` установлен в значение `yes`. Затем перезапустите службу `sshd`:

   ```shell
   sudo systemctl restart sshd
   ```

1. Сгенерируйте на сервере пару SSH-ключей с пустой парольной фразой:

   ```shell
   ssh-keygen -t rsa -f caps-id -C "" -N ""
   ```

   Приватный и публичный ключи будут сохранены в файлах `caps-id` и `caps-id.pub` соответственно в текущей директории.

1. Добавьте полученный публичный ключ в файл `/home/caps/.ssh/authorized_keys` пользователя `caps`, выполнив в директории с ключами на сервере следующие команды:

   ```shell
   mkdir -p /home/caps/.ssh 
   cat caps-id.pub >> /home/caps/.ssh/authorized_keys 
   chmod 700 /home/caps/.ssh 
   chmod 600 /home/caps/.ssh/authorized_keys
   chown -R caps:caps /home/caps/
   ```

1. Создайте ресурс [SSHCredentials](cr.html#sshcredentials).
1. Создайте ресурс [StaticInstance](cr.html#staticinstance).
1. Создайте ресурс [NodeGroup](cr.html#nodegroup) с [nodeType](cr.html#nodegroup-v1-spec-nodetype) `Static`, указав [желаемое количество узлов](cr.html#nodegroup-v1-spec-staticinstances-count) в группе и, при необходимости, [фильтр](cr.html#nodegroup-v1-spec-staticinstances-labelselector) выбора `StaticInstance`.

[Пример](examples.html#с-помощью-cluster-api-provider-static) добавления статического узла.

##### Как добавить несколько статических узлов в кластер вручную?

Используйте существующий или создайте новый кастомный ресурс (Custom Resource) [NodeGroup](cr.html#nodegroup) ([пример](examples.html#пример-описания-статической-nodegroup) `NodeGroup` с именем `worker`).

Автоматизировать процесс добавления узлов можно с помощью любой платформы автоматизации. Далее приведен пример для Ansible.

1. Получите один из адресов Kubernetes API-сервера. Обратите внимание, что IP-адрес должен быть доступен с узлов, которые добавляются в кластер:

   ```shell
   d8 k -n default get ep kubernetes -o json | jq '.subsets[0].addresses[0].ip + ":" + (.subsets[0].ports[0].port | tostring)' -r
   ```

   Проверьте версию K8s. Если версия >= 1.25, создайте токен `node-group`:

   ```shell
   d8 k create token node-group --namespace d8-cloud-instance-manager --duration 1h
   ```

   Сохраните полученный токен, и добавьте в поле `token:` playbook'а Ansible на дальнейших шагах.

1. Если версия Kubernetes меньше 1.25, получите Kubernetes API-токен для специального ServiceAccount'а, которым управляет Deckhouse:

   ```shell
   d8 k -n d8-cloud-instance-manager get $(d8 k -n d8-cloud-instance-manager get secret -o name | grep node-group-token) \
     -o json | jq '.data.token' -r | base64 -d && echo ""
   ```

1. Создайте Ansible playbook с `vars`, которые заменены на полученные на предыдущих шагах значения:

{% raw %}

   ```yaml
   - hosts: all
     become: yes
     gather_facts: no
     vars:
       kube_apiserver: <KUBE_APISERVER>
       token: <TOKEN>
     tasks:
       - name: Check if node is already bootsrapped
         stat:
           path: /var/lib/bashible
         register: bootstrapped
       - name: Get bootstrap secret
         uri:
           url: "https://{{ kube_apiserver }}/api/v1/namespaces/d8-cloud-instance-manager/secrets/manual-bootstrap-for-{{ node_group }}"
           return_content: yes
           method: GET
           status_code: 200
           body_format: json
           headers:
             Authorization: "Bearer {{ token }}"
           validate_certs: no
         register: bootstrap_secret
         when: bootstrapped.stat.exists == False
       - name: Run bootstrap.sh
         shell: "{{ bootstrap_secret.json.data['bootstrap.sh'] | b64decode }}"
         args:
           executable: /bin/bash
         ignore_errors: yes
         when: bootstrapped.stat.exists == False
       - name: wait
         wait_for_connection:
           delay: 30
         when: bootstrapped.stat.exists == False
   ```

{% endraw %}

1. Определите дополнительную переменную `node_group`. Значение переменной должно совпадать с именем `NodeGroup`, которой будет принадлежать узел. Переменную можно передать различными способами, например с использованием inventory-файла:

   ```text
   [system]
   system-0
   system-1

   [system:vars]
   node_group=system

   [worker]
   worker-0
   worker-1

   [worker:vars]
   node_group=worker
   ```

1. Запустите выполнение playbook'а с использованием inventory-файла.

##### Как вручную очистить статический узел?

<span id='как-вывести-узел-из-под-управления-node-manager'></span>

{% alert level="info" %}
Инструкция справедлива как для узла, настроенного вручную (с помощью бутстрап-скрипта), так и для узла, настроенного с помощью CAPS.
{% endalert %}

Чтобы вывести из кластера узел и очистить сервер (ВМ), выполните следующую команду на узле:

```shell
bash /var/lib/bashible/cleanup_static_node.sh --yes-i-am-sane-and-i-understand-what-i-am-doing
```

##### Можно ли удалить StaticInstance?

StaticInstance, находящийся в состоянии `Pending` можно удалять без каких-либо проблем.

Чтобы удалить StaticInstance находящийся в любом состоянии, отличном от `Pending` (`Running`, `Cleaning`, `Bootstrapping`), выполните следующие шаги:

1. Добавьте лейбл `"node.deckhouse.io/allow-bootstrap": "false"` в StaticInstance.

   Пример команды для добавления лейбла:

   ```shell
   d8 k label staticinstance d8cluster-worker node.deckhouse.io/allow-bootstrap=false
   ```

1. Дождитесь, пока StaticInstance перейдет в статус `Pending`.

   Для проверки статуса StaticInstance используйте команду:

   ```shell
   d8 k get staticinstances
   ```

1. Удалите `StaticInstance`.

   Пример команды для удаления StaticInstance:

   ```shell
   d8 k delete staticinstance d8cluster-worker
   ```

1. Уменьшите значение параметра `NodeGroup.spec.staticInstances.count` на 1.

##### Как изменить IP-адрес StaticInstance?

Изменить IP-адрес в ресурсе `StaticInstance` нельзя. Если в `StaticInstance` указан ошибочный адрес, то нужно [удалить StaticInstance](#можно-ли-удалить-staticinstance) и создать новый.

##### Как мигрировать статический узел настроенный вручную под управление CAPS?

Необходимо выполнить [очистку узла](#как-вручную-очистить-статический-узел), затем [добавить](#как-добавить-статический-узел-в-кластер-cluster-api-provider-static) узел под управление CAPS.

#### Как изменить NodeGroup у статического узла?

<span id='как-изменить-nodegroup-у-статичного-узла'><span>

Если узел находится под управлением [CAPS](./#cluster-api-provider-static), то изменить принадлежность к `NodeGroup` у такого узла **нельзя**. Единственный вариант — [удалить StaticInstance](#можно-ли-удалить-staticinstance) и создать новый.

Чтобы перенести существующий статический узел созданный [вручную](./#работа-со-статическими-узлами) из одной `NodeGroup` в другую, необходимо изменить у узла лейбл группы:

```shell
d8 k label node --overwrite <node_name> node.deckhouse.io/group=<new_node_group_name>
d8 k label node <node_name> node-role.kubernetes.io/<old_node_group_name>-
```

Применение изменений потребует некоторого времени.

#### Как очистить узел для последующего ввода в кластер?

Это необходимо только в том случае, если нужно переместить статический узел из одного кластера в другой. Имейте в виду, что эти операции удаляют данные локального хранилища. Если необходимо просто изменить `NodeGroup`, следуйте [этой инструкции](#как-изменить-nodegroup-у-статического-узла).

{% alert level="warning" %}
Если на зачищаемом узле есть пулы хранения LINSTOR/DRBD, то предварительно перенесите ресурсы с узла и удалите узел LINSTOR/DRBD, следуя [инструкции](/modules/sds-replicated-volume/faq.html#как-выгнать-ресурсы-с-узла).
{% endalert %}

1. Удалите узел из кластера Kubernetes:

   ```shell
   d8 k drain <node> --ignore-daemonsets --delete-local-data
   d8 k delete node <node>
   ```

1. Запустите на узле скрипт очистки:

   ```shell
   bash /var/lib/bashible/cleanup_static_node.sh --yes-i-am-sane-and-i-understand-what-i-am-doing
   ```

1. После перезагрузки узла [запустите](#как-добавить-статический-узел-в-кластер) скрипт `bootstrap.sh`.

#### Как понять, что что-то пошло не так?

Если узел в NodeGroup не обновляется (значение `UPTODATE` при выполнении команды `d8 k get nodegroup` меньше значения `NODES`) или вы предполагаете какие-то другие проблемы, которые могут быть связаны с модулем `node-manager`, нужно проверить логи сервиса `bashible`. Сервис `bashible` запускается на каждом узле, управляемом модулем `node-manager`.

Чтобы проверить логи сервиса `bashible`, выполните на узле следующую команду:

```shell
journalctl -fu bashible
```

Пример вывода, когда все необходимые действия выполнены:

```console
May 25 04:39:16 kube-master-0 systemd[1]: Started Bashible service.
May 25 04:39:16 kube-master-0 bashible.sh[1976339]: Configuration is in sync, nothing to do.
May 25 04:39:16 kube-master-0 systemd[1]: bashible.service: Succeeded.
```

#### Как посмотреть, что в данный момент выполняется на узле при его создании?

Если необходимо узнать, что происходит на узле (например, узел долго создается), можно проверить логи `cloud-init`. Для этого выполните следующие шаги:

1. Найдите узел, который находится в стадии бутстрапа:

   ```shell
   d8 k get instances | grep Pending
   ```

   Пример:

   ```shell
   d8 k get instances | grep Pending
   dev-worker-2a6158ff-6764d-nrtbj   Pending   46s
   ```

1. Получите информацию о параметрах подключения для просмотра логов:

   ```shell
   d8 k get instances dev-worker-2a6158ff-6764d-nrtbj -o yaml | grep 'bootstrapStatus' -B0 -A2
   ```

   Пример:

   ```shell
   d8 k get instances dev-worker-2a6158ff-6764d-nrtbj -o yaml | grep 'bootstrapStatus' -B0 -A2
   bootstrapStatus:
     description: Use 'nc 192.168.199.178 8000' to get bootstrap logs.
     logsEndpoint: 192.168.199.178:8000
   ```

1. Выполните полученную команду (в примере выше — `nc 192.168.199.178 8000`), чтобы просмотреть логи `cloud-init` и определить, на каком этапе остановилась настройка узла.

Логи первоначальной настройки узла находятся в `/var/log/cloud-init-output.log`.

#### Как обновить ядро на узлах?

##### Для дистрибутивов, основанных на Debian

Создайте ресурс `NodeGroupConfiguration`, указав в переменной `desired_version` shell-скрипта (параметр `spec.content` ресурса) желаемую версию ядра:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: install-kernel.sh
spec:
  bundles:
    - '*'
  nodeGroups:
    - '*'
  weight: 32
  content: |
    # Copyright 2022 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    desired_version="5.15.0-53-generic"

    bb-event-on 'bb-package-installed' 'post-install'
    post-install() {
      bb-log-info "Setting reboot flag due to kernel was updated"
      bb-flag-set reboot
    }

    version_in_use="$(uname -r)"

    if [[ "$version_in_use" == "$desired_version" ]]; then
      exit 0
    fi

    bb-deckhouse-get-disruptive-update-approval
    bb-apt-install "linux-image-${desired_version}"
```

##### Для дистрибутивов, основанных на CentOS

Создайте ресурс `NodeGroupConfiguration`, указав в переменной `desired_version` shell-скрипта (параметр `spec.content` ресурса) желаемую версию ядра:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: install-kernel.sh
spec:
  bundles:
    - '*'
  nodeGroups:
    - '*'
  weight: 32
  content: |
    # Copyright 2022 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    desired_version="3.10.0-1160.42.2.el7.x86_64"

    bb-event-on 'bb-package-installed' 'post-install'
    post-install() {
      bb-log-info "Setting reboot flag due to kernel was updated"
      bb-flag-set reboot
    }

    version_in_use="$(uname -r)"

    if [[ "$version_in_use" == "$desired_version" ]]; then
      exit 0
    fi

    bb-deckhouse-get-disruptive-update-approval
    bb-dnf-install "kernel-${desired_version}"
```

#### Какие параметры NodeGroup к чему приводят?

| Параметр NG                           | Disruption update          | Перезаказ узлов   | Рестарт kubelet |
|---------------------------------------|----------------------------|-------------------|-----------------|
| chaos                                 | -                          | -                 | -               |
| cloudInstances.classReference         | -                          | +                 | -               |
| cloudInstances.maxSurgePerZone        | -                          | -                 | -               |
| cri.containerd.maxConcurrentDownloads | -                          | -                 | +               |
| cri.type                              | - (NotManaged) / + (other) | -                 | -               |
| disruptions                           | -                          | -                 | -               |
| kubelet.maxPods                       | -                          | -                 | +               |
| kubelet.rootDir                       | -                          | -                 | +               |
| kubernetesVersion                     | -                          | -                 | +               |
| nodeTemplate                          | -                          | -                 | -               |
| static                                | -                          | -                 | +               |
| update.maxConcurrent                  | -                          | -                 | -               |

Подробно о всех параметрах можно прочитать в описании кастомного ресурса [NodeGroup](cr.html#nodegroup).

В случае изменения параметров `InstanceClass` или `instancePrefix` в конфигурации Deckhouse не будет происходить `RollingUpdate`. Deckhouse создаст новые `MachineDeployment`, а старые удалит. Количество заказываемых одновременно `MachineDeployment` определяется параметром `cloudInstances.maxSurgePerZone`.

При обновлении, которое требует прерывания работы узла (disruption update), выполняется процесс вытеснения подов с узла. Если какой-либо под не может быть вытеснен, попытка повторяется каждые 20 секунд до достижения глобального таймаута в 5 минут. После истечения этого времени, поды, которые не удалось вытеснить, удаляются принудительно.

#### Как пересоздать эфемерные машины в облаке с новой конфигурацией?

При изменении конфигурации Deckhouse (как в модуле `node-manager`, так и в любом из облачных провайдеров) виртуальные машины не будут перезаказаны. Пересоздание происходит только после изменения ресурсов `InstanceClass` или `NodeGroup`.

Чтобы принудительно пересоздать все узлы, связанные с ресурсом `Machines`, следует добавить/изменить аннотацию `manual-rollout-id` в `NodeGroup`: `d8 k annotate NodeGroup имя_ng "manual-rollout-id=$(uuidgen)" --overwrite`.

#### Как выделить узлы под специфические нагрузки?

{% alert level="warning" %}
Запрещено использование домена `deckhouse.io` в ключах `labels` и `taints` у `NodeGroup`. Он зарезервирован для компонентов Deckhouse. Следует отдавать предпочтение в пользу ключей `dedicated` или `dedicated.client.com`.
{% endalert %}

Для решений данной задачи существуют два механизма:

1. Установка меток в `NodeGroup` `spec.nodeTemplate.labels` для последующего использования их в `Pod` spec.nodeSelector или spec.affinity.nodeAffinity. Указывает, какие именно узлы будут выбраны планировщиком для запуска целевого приложения.
1. Установка ограничений в `NodeGroup` `spec.nodeTemplate.taints` с дальнейшим снятием их в `Pod` spec.tolerations. Запрещает исполнение не разрешенных явно приложений на этих узлах.

{% alert level="info" %}
Deckhouse по умолчанию поддерживает использование taint'а с ключом `dedicated`, поэтому рекомендуется применять этот ключ с любым значением для taints на ваших выделенных узлах.

Если требуется использовать другие ключи для taints (например, `dedicated.client.com`), необходимо добавить соответствующее значение ключа в массив [`.spec.settings.modules.placement.customTolerationKeys`](/reference/api/global.html#parameters-modules-placement-customtolerationkeys). Это обеспечит разрешение системным компонентам, таким как `cni-flannel`, использовать эти узлы.
{% endalert %}

#### Как выделить узлы под системные компоненты?

##### Фронтенд

Для Ingress-контроллеров используйте `NodeGroup` со следующей конфигурацией:

```yaml
nodeTemplate:
  labels:
    node-role.deckhouse.io/frontend: ""
  taints:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      value: frontend
```

##### Системные

Для компонентов подсистем Deckhouse параметр `NodeGroup` будет настроен с параметрами:

```yaml
nodeTemplate:
  labels:
    node-role.deckhouse.io/system: ""
  taints:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      value: system
```

#### Как ускорить заказ узлов в облаке при горизонтальном масштабировании приложений?

Самое действенное — держать в кластере некоторое количество предварительно подготовленных узлов, которые позволят новым репликам ваших приложений запускаться мгновенно. Очевидным минусом данного решения будут дополнительные расходы на содержание этих узлов.

Необходимые настройки целевой `NodeGroup` будут следующие:

1. Указать абсолютное количество предварительно подготовленных узлов (или процент от максимального количества узлов в этой группе) в параметре `cloudInstances.standby`.
1. При наличии на узлах дополнительных служебных компонентов, не обслуживаемых Deckhouse (например, DaemonSet `filebeat`), задать их процентное потребление ресурсов узла можно в параметре `standbyHolder.overprovisioningRate`.
1. Для работы этой функции требуется, чтобы как минимум один узел из группы уже был запущен в кластере. Иными словами, либо должна быть доступна одна реплика приложения, либо количество узлов для этой группы `cloudInstances.minPerZone` должно быть `1`.

Пример:

```yaml
cloudInstances:
  maxPerZone: 10
  minPerZone: 1
  standby: 10%
  standbyHolder:
    overprovisioningRate: 30%
```

#### Как выключить machine-controller-manager/CAPI в случае выполнения потенциально деструктивных изменений в кластере?

{% alert level="danger" %}
Использовать эту настройку допустимо только тогда, когда вы четко понимаете, зачем это необходимо.
{% endalert %}

Для того чтобы временно отключить machine-controller-manager (MCM) и предотвратить его автоматические действия, которые могут повлиять на инфраструктуру кластера (например, удаление или пересоздание узлов), установите следующий параметр в конфигурации:

```yaml
mcmEmergencyBrake: true
```

Для отключения CAPI установите следующий параметр в конфигурации:

```yaml
capiEmergencyBrake: true
```

#### Как восстановить master-узел, если kubelet не может загрузить компоненты control plane?

Подобная ситуация может возникнуть, если в кластере с одним master-узлом на нем были удалены образы компонентов control plane (например, удалена директория `/var/lib/containerd`).
В этом случае kubelet при рестарте не сможет скачать образы компонентов `control plane`, поскольку на master-узле нет параметров авторизации в `registry.deckhouse.io`.

Далее приведена инструкция по восстановлению master-узла.

##### containerd

Для восстановления работоспособности master-узла нужно в любом рабочем кластере под управлением Deckhouse выполнить команду:

```shell
d8 k -n d8-system get secrets deckhouse-registry -o json |
jq -r '.data.".dockerconfigjson"' | base64 -d |
jq -r '.auths."registry.deckhouse.io".auth'
```

Вывод команды нужно скопировать и присвоить переменной `AUTH` на поврежденном master-узле.

Далее на поврежденном master-узле нужно загрузить образы компонентов `control-plane`:

```shell
for image in $(grep "image:" /etc/kubernetes/manifests/* | awk '{print $3}'); do
  crictl pull --auth $AUTH $image
done
```

После загрузки образов необходимо перезапустить `kubelet`.

#### Как изменить CRI для NodeGroup?

{% alert level="warning" %}
Смена CRI возможна только между `Containerd` на `NotManaged` и обратно (параметр [cri.type](cr.html#nodegroup-v1-spec-cri-type)).
{% endalert %}

Для изменения CRI для NodeGroup, установите параметр [cri.type](cr.html#nodegroup-v1-spec-cri-type) в `Containerd` или в `NotManaged`.

Пример YAML-манифеста NodeGroup:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: worker
spec:
  nodeType: Static
  cri:
    type: Containerd
```

Также эту операцию можно выполнить с помощью патча:

* Для `Containerd`:

  ```shell
  d8 k patch nodegroup <имя NodeGroup> --type merge -p '{"spec":{"cri":{"type":"Containerd"}}}'
  ```

* Для `NotManaged`:

  ```shell
  d8 k patch nodegroup <имя NodeGroup> --type merge -p '{"spec":{"cri":{"type":"NotManaged"}}}'
  ```

{% alert level="warning" %}
 При изменении `cri.type` для NodeGroup, созданных с помощью `dhctl`, необходимо обновить это значение в `dhctl config edit provider-cluster-configuration` и настройках объекта NodeGroup.
{% endalert %}

После изменения CRI для NodeGroup модуль `node-manager` будет поочередно перезагружать узлы, применяя новый CRI.  Обновление узла сопровождается простоем (disruption). В зависимости от настройки `disruption` для NodeGroup, модуль `node-manager` либо автоматически выполнит обновление узлов, либо потребует подтверждения вручную.

#### Как изменить CRI для всего кластера?

{% alert level="warning" %}
Смена CRI возможна только между `Containerd` на `NotManaged` и обратно (параметр [cri.type](cr.html#nodegroup-v1-spec-cri-type)).
{% endalert %}

Для изменения CRI для всего кластера, необходимо с помощью утилиты `dhctl` отредактировать параметр `defaultCRI` в конфигурационном файле `cluster-configuration`.

Также возможно выполнить эту операцию с помощью `d8 k patch`.

* Для `Containerd`:

  ```shell
  data="$(d8 k -n kube-system get secret d8-cluster-configuration -o json | jq -r '.data."cluster-configuration.yaml"' | base64 -d | sed "s/NotManaged/Containerd/" | base64 -w0)"
  d8 k -n kube-system patch secret d8-cluster-configuration -p "{\"data\":{\"cluster-configuration.yaml\":\"$data\"}}"
  ```

* Для `NotManaged`:

  ```shell
  data="$(d8 k -n kube-system get secret d8-cluster-configuration -o json | jq -r '.data."cluster-configuration.yaml"' | base64 -d | sed "s/Containerd/NotManaged/" | base64 -w0)"
  d8 k -n kube-system patch secret d8-cluster-configuration -p "{\"data\":{\"cluster-configuration.yaml\":\"$data\"}}"
  ```

Если необходимо, чтобы отдельные NodeGroup использовали другой CRI, перед изменением `defaultCRI` необходимо установить CRI для этой NodeGroup,
как описано [в документации](#как-изменить-cri-для-nodegroup).

{% alert level="danger" %}
Изменение `defaultCRI` влечет за собой изменение CRI на всех узлах, включая master-узлы.
Если master-узел один, данная операция является опасной и может привести к полной неработоспособности кластера.
Рекомендуется использовать multimaster-конфигурацию и менять тип CRI только после этого.
{% endalert %}

При изменении CRI в кластере для master-узлов необходимо выполнить дополнительные шаги:

1. Чтобы определить, какой узел в текущий момент обновляется в master NodeGroup, используйте следующую команду:

   ```shell
   d8 k get nodes -l node-role.kubernetes.io/control-plane="" -o json | jq '.items[] | select(.metadata.annotations."update.node.deckhouse.io/approved"=="") | .metadata.name' -r
   ```

1. Подтвердите остановку (disruption) для master-узла, полученного на предыдущем шаге:

   ```shell
   d8 k annotate node <имя master-узла> update.node.deckhouse.io/disruption-approved=
   ```

1. Дождитесь перехода обновленного master-узла в `Ready`. Выполните итерацию для следующего master-узла.

#### Как добавить шаг для конфигурации узлов?

Дополнительные шаги для конфигурации узлов задаются с помощью кастомного ресурса [NodeGroupConfiguration](cr.html#nodegroupconfiguration).

#### Как автоматически проставить на узел кастомные лейблы?

1. На узле создайте каталог `/var/lib/node_labels`.

1. Создайте в нём файл или файлы, содержащие необходимые лейблы. Количество файлов может быть любым, как и вложенность подкаталогов, их содержащих.

1. Добавьте в файлы нужные лейблы в формате `key=value`. Например:

   ```console
   example-label=test
   ```

1. Сохраните файлы.

При добавлении узла в кластер указанные в файлах лейблы будут автоматически проставлены на узел.

{% alert level="warning" %}
Обратите внимание, что добавить таким образом лейблы, использующиеся в Deckhouse Platform Certified Security Edition, невозможно. Работать такой метод будет только с кастомными лейблами, не пересекающимися с зарезервированными для Deckhouse.
{% endalert %}

#### Как развернуть кастомный конфигурационный файл containerd?

{% alert level="info" %}
Пример `NodeGroupConfiguration` основан на функциях, заложенных в скрипте [032_configure_containerd.sh](./#особенности-написания-скриптов).
{% endalert %}

{% alert level="danger" %}
Добавление кастомных настроек вызывает перезапуск сервиса containerd.
{% endalert %}

Bashible на узлах объединяет конфигурацию containerd для Deckhouse с конфигурацией из файла `/etc/containerd/conf.d/*.toml`.

{% alert level="warning" %}
Вы можете переопределять значения параметров, которые заданы в файле `/etc/containerd/deckhouse.toml`, но их работу придётся обеспечивать самостоятельно. Также, лучше изменением конфигурации не затрагивать master-узлы (nodeGroup `master`).
{% endalert %}

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-option-config.sh
spec:
  bundles:
    - '*'
  content: |
    # Copyright 2024 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_option.toml - << EOF
    oom_score = 500
    [metrics]
    address = "127.0.0.1"
    grpc_histogram = true
    EOF
  nodeGroups:
    - "worker"
  weight: 31
```

##### Как добавить конфигурацию для дополнительного registry?

В containerd существует два способа описания конфигурации registry: **устаревший** и **актуальный**.

Для проверки наличия **устаревшего** способа конфигурации выполните на узлах кластера следующие команды:

```bash
cat /etc/containerd/config.toml | grep 'plugins."io.containerd.grpc.v1.cri".registry.mirrors'
cat /etc/containerd/config.toml | grep 'plugins."io.containerd.grpc.v1.cri".registry.configs'

### Пример вывода:
### [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
###   [plugins."io.containerd.grpc.v1.cri".registry.mirrors."<REGISTRY_URL>"]
### [plugins."io.containerd.grpc.v1.cri".registry.configs]
###   [plugins."io.containerd.grpc.v1.cri".registry.configs."<REGISTRY_URL>".auth]
```

Для проверки наличия **актуального** способа конфигурации выполните на узлах кластера следующую команду:

```bash
cat /etc/containerd/config.toml | grep '/etc/containerd/registry.d'

### Пример вывода:
### config_path = "/etc/containerd/registry.d"
```

###### Устаревший способ

{% alert level="warning" %}
Этот формат конфигурации containerd устарел (deprecated).
{% endalert %}

{% alert level="info" %}
Используется в containerd v1, если Deckhouse не управляется с помощью модуля [registry](/modules/registry/).
{% endalert %}

Конфигурация описывается в основном конфигурационном файле containerd `/etc/containerd/config.toml`.

Пользовательская конфигурация добавляется через механизм `toml merge`. Конфигурационные файлы из директории `/etc/containerd/conf.d` объединяются с основным файлом `/etc/containerd/config.toml`. Применение merge происходит на этапе выполнения скрипта `032_configure_containerd.sh`, поэтому соответствующие файлы должны быть добавлены заранее.

Пример конфигурационного файла для директории `/etc/containerd/conf.d/`:

```toml
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${REGISTRY_URL}"]
          endpoint = ["https://${REGISTRY_URL}"]
      [plugins."io.containerd.grpc.v1.cri".registry.configs]
        [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".auth]
          auth = "${BASE_64_AUTH}"
          username = "${USERNAME}"
          password = "${PASSWORD}"
        [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".tls]
          ca_file = "${CERT_DIR}/${CERT_NAME}.crt"
          insecure_skip_verify = true
```

{% alert level="danger" %}
Добавление кастомных настроек через механизм `toml merge` вызывает перезапуск сервиса containerd.
{% endalert %}

####### Как добавить авторизацию в дополнительный registry (устаревший способ)?

Пример добавления авторизации в дополнительный registry при использовании **устаревшего** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-auth.sh
spec:
  # Для добавления файла перед шагом '032_configure_containerd.sh'
  weight: 31
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
    [plugins]
      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
            [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${REGISTRY_URL}"]
              endpoint = ["https://${REGISTRY_URL}"]
          [plugins."io.containerd.grpc.v1.cri".registry.configs]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".auth]
              username = "username"
              password = "password"
              # OR
              auth = "dXNlcm5hbWU6cGFzc3dvcmQ="
    EOF
```

####### Как настроить сертификат для дополнительного registry (устаревший способ)?

Пример настройки сертификата для дополнительного registry при использовании **устаревшего** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-tls.sh
spec:
  # Для добавления файла перед шагом '032_configure_containerd.sh'
  weight: 31
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example
    CERT_FILE_NAME=${REGISTRY_URL}
    CERTS_FOLDER="/var/lib/containerd/certs/"


    mkdir -p ${CERTS_FOLDER}
    bb-sync-file "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt" - << EOF
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    EOF

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
    [plugins]
      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          [plugins."io.containerd.grpc.v1.cri".registry.configs]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".tls]
              ca_file = "${CERTS_FOLDER}/${CERT_FILE_NAME}.crt"
    EOF
```

{% alert level="info" %}
Помимо containerd, сертификат можно [добавить в операционную систему](examples.html#добавление-корневого-сертификата-в-хост).
{% endalert %}

####### Как добавить TLS skip verify (устаревший способ)?

Пример добавления TLS skip verify при использовании **устаревшего** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-skip-tls.sh
spec:
  # Для добавления файла перед шагом '032_configure_containerd.sh'
  weight: 31
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p /etc/containerd/conf.d
    bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
    [plugins]
      [plugins."io.containerd.grpc.v1.cri"]
        [plugins."io.containerd.grpc.v1.cri".registry]
          [plugins."io.containerd.grpc.v1.cri".registry.configs]
            [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".tls]
              insecure_skip_verify = true
    EOF
```

После применения конфигурационного файла проверьте доступ к registry с узлов, используя команду:

```bash
### Через cri-интерфейс
crictl pull private.registry.example/image/repo:tag
```

###### Новый способ

{% alert level="info" %}
Используется в containerd v2.  

Используется в containerd v1, если управление осуществляется через модуль [`registry`](/modules/registry/) (например, в режиме [`Direct`](./deckhouse/configuration.html#parameters-registry)).
{% endalert %}

Конфигурация описывается в каталоге `/etc/containerd/registry.d` и задаётся через создание подкаталогов с именами, соответствующими адресу registry:

```bash
/etc/containerd/registry.d
├── private.registry.example:5001
│   ├── ca.crt
│   └── hosts.toml
└── registry.deckhouse.ru
    ├── ca.crt
    └── hosts.toml
```

Пример содержимого файла `hosts.toml`:

```toml
[host]
  # Mirror 1.
  [host."https://${REGISTRY_URL_1}"]
    capabilities = ["pull", "resolve"]
    ca = ["${CERT_DIR}/${CERT_NAME}.crt"]

    [host."https://${REGISTRY_URL_1}".auth]
      username = "${USERNAME}"
      password = "${PASSWORD}"

  # Mirror 2.
  [host."http://${REGISTRY_URL_2}"]
    capabilities = ["pull", "resolve"]
    skip_verify = true
```

{% alert level="info" %}
Изменения конфигураций не приводят к перезапуску сервиса containerd.
{% endalert %}

####### Как добавить авторизацию в дополнительный registry (актуальный способ)?

Пример добавления авторизации в дополнительный registry при использовании **актуального** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-auth.sh
spec:
  # Шаг может быть любой, т.к. не требуется перезапуск сервиса containerd.
  weight: 0
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"
    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
    [host]
      [host."https://${REGISTRY_URL}"]
        capabilities = ["pull", "resolve"]
        [host."https://${REGISTRY_URL}".auth]
          username = "username"
          password = "password"
    EOF
```

####### Как настроить сертификат для дополнительного registry (актуальный способ)?

Пример настройки сертификата для дополнительного registry? при использовании **актуального** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-tls.sh
spec:
  # Шаг может быть любой, тк не требуется перезапуск сервиса containerd.
  weight: 0
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"

    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/ca.crt" - << EOF
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    EOF

    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
    [host]
      [host."https://${REGISTRY_URL}"]
        capabilities = ["pull", "resolve"]
        ca = ["/etc/containerd/registry.d/${REGISTRY_URL}/ca.crt"]
    EOF
```

{% alert level="info" %}
Помимо containerd, сертификат можно [добавить в операционную систему](examples.html#добавление-корневого-сертификата-в-хост).
{% endalert %}

####### Как добавить TLS skip verify (актуальный способ)?

Пример добавления TLS skip verify при использовании **актуального** способа конфигурации:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroupConfiguration
metadata:
  name: containerd-additional-config-skip-tls.sh
spec:
  # Шаг может быть любой, тк не требуется перезапуск сервиса containerd.
  weight: 0
  bundles:
    - '*'
  nodeGroups:
    - "*"
  content: |
    # Copyright 2023 Flant JSC
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    REGISTRY_URL=private.registry.example

    mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"
    bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
    [host]
      [host."https://${REGISTRY_URL}"]
        capabilities = ["pull", "resolve"]
        skip_verify = true
    EOF
```

После применения конфигурационного файла проверьте доступ к registry с узлов, используя команды:

```bash
### Через cri интерфейс.
crictl pull private.registry.example/image/repo:tag

### Через ctr с указанием директории с конфигурациями.
ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ private.registry.example/image/repo:tag

### Через ctr для http репозитория.
ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ --plain-http private.registry.example/image/repo:tag
```

#### Как интерпретировать состояние группы узлов?

**Ready** — группа узлов содержит минимально необходимое число запланированных узлов с состоянием `Ready` для всех зон.

Пример 1. Группа узлов в состоянии `Ready`:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: ng1
spec:
  nodeType: CloudEphemeral
  cloudInstances:
    maxPerZone: 5
    minPerZone: 1
status:
  conditions:
  - status: "True"
    type: Ready
---
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    node.deckhouse.io/group: ng1
status:
  conditions:
  - status: "True"
    type: Ready
```

Пример 2. Группа узлов в состоянии `Not Ready`:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: ng1
spec:
  nodeType: CloudEphemeral
  cloudInstances:
    maxPerZone: 5
    minPerZone: 2
status:
  conditions:
  - status: "False"
    type: Ready
---
apiVersion: v1
kind: Node
metadata:
  name: node1
  labels:
    node.deckhouse.io/group: ng1
status:
  conditions:
  - status: "True"
    type: Ready
```

**Updating** — группа узлов содержит как минимум один узел, в котором присутствует аннотация с префиксом `update.node.deckhouse.io` (например, `update.node.deckhouse.io/waiting-for-approval`).

**WaitingForDisruptiveApproval** — группа узлов содержит как минимум один узел, в котором присутствует аннотация `update.node.deckhouse.io/disruption-required` и
отсутствует аннотация `update.node.deckhouse.io/disruption-approved`.

**Scaling** — рассчитывается только для групп узлов с типом `CloudEphemeral`. Состояние `True` может быть в двух случаях:

1. Когда число узлов меньше *желаемого числа узлов в группе, то есть когда нужно увеличить число узлов в группе*.
1. Когда какой-то узел помечается к удалению или число узлов больше *желаемого числа узлов*, то есть когда нужно уменьшить число узлов в группе.

*Желаемое число узлов* — это сумма всех реплик, входящих в группу узлов.

Пример. Желаемое число узлов равно 2:

```yaml
apiVersion: deckhouse.io/v1
kind: NodeGroup
metadata:
  name: ng1
spec:
  nodeType: CloudEphemeral
  cloudInstances:
    maxPerZone: 5
    minPerZone: 2
status:
...
  desired: 2
...
```

**Error** — содержит последнюю ошибку, возникшую при создании узла в группе узлов.

#### Как заставить werf игнорировать состояние Ready в группе узлов?

werf проверяет состояние `Ready` у ресурсов и в случае его наличия дожидается, пока значение станет `True`.

Создание (обновление) ресурса [nodeGroup](cr.html#nodegroup) в кластере может потребовать значительного времени на развертывание необходимого количества узлов. При развертывании такого ресурса в кластере с помощью werf (например, в рамках процесса CI/CD) развертывание может завершиться по превышении времени ожидания готовности ресурса. Чтобы заставить werf игнорировать состояние `nodeGroup`, необходимо добавить к `nodeGroup` следующие аннотации:

```yaml
metadata:
  annotations:
    werf.io/fail-mode: IgnoreAndContinueDeployProcess
    werf.io/track-termination-mode: NonBlocking
```

#### Что такое ресурс Instance?

Ресурс `Instance` в Kubernetes представляет собой описание объекта эфемерной виртуальной машины, но без конкретной реализации. Это абстракция, которая используется для управления машинами, созданными с помощью таких инструментов, как MachineControllerManager или Cluster API Provider Static.

Объект не содержит спецификации. Статус содержит:

1. Ссылку на `InstanceClass`, если он существует для данной реализации.
1. Ссылку на объект Node Kubernetes.
1. Текущий статус машины.
1. Информацию о том, как проверить [логи создания машины](#как-посмотреть-что-в-данный-момент-выполняется-на-узле-при-его-создании) (появляется на этапе создания машины).

При создании или удалении машины создается или удаляется соответствующий объект Instance.
Самостоятельно ресурс `Instance` создать нельзя, но можно удалить. В таком случае машина будет удалена из кластера (процесс удаления зависит от деталей реализации).

#### Когда требуется перезагрузка узлов?

Некоторые операции по изменению конфигурации узлов могут потребовать перезагрузки.

Перезагрузка узла может потребоваться при изменении некоторых настроек sysctl, например, при изменении параметра `kernel.yama.ptrace_scope` (изменяется при использовании команды `astra-ptrace-lock enable/disable` в Astra Linux).

#### Как мониторить GPU?

Deckhouse Platform Certified Security Edition автоматически устанавливает **DCGM Exporter**; метрики GPU попадают в Prometheus и доступны в Grafana.

#### Какие режимы работы GPU поддерживаются?

Поддерживаются следующие режимы работы GPU:

- **Exclusive** — узел публикует ресурс `nvidia.com/gpu`; каждому поду выделяется целый GPU.
- **TimeSlicing** — временное разделение одного GPU между несколькими подами (по умолчанию `partitionCount: 4`), при этом под по-прежнему запрашивает `nvidia.com/gpu`.
- **MIG (Multi-Instance GPU)** — аппаратное разделение совместимых GPU на независимые экземпляры; при профиле `all-1g.5gb` появятся ресурсы вида `nvidia.com/mig-1g.5gb`.

Примеры приведены в разделе [Управление узлами: примеры](./node-manager/examples.html#пример-gpu-nodegroup).

#### Как посмотреть доступные MIG-профили в кластере?

<span id="как-посмотреть-доступные-mig-профили-в-кластере"></span>

Предустановленные профили находятся в ConfigMap `mig-parted-config` в пространстве имен `d8-nvidia-gpu`. Для их просмотра используйте команду:

```bash
d8 k -n d8-nvidia-gpu get cm mig-parted-config -o json | jq -r '.data["config.yaml"]'
```

В разделе mig-configs вы увидите конкретные модели ускорителей (по PCI-ID) и список совместимых MIG-профилей для каждой из них.
Найдите свою видеокарту и выберите подходящий профиль — его имя указывается в `spec.gpu.mig.partedConfig` вашего NodeGroup.
Это позволит применить правильный профиль именно к вашей карте.

#### Для GPU не активируется MIG-профиль — что проверить?

1. Модель GPU: MIG поддерживают H100/A100/A30, **не** поддерживает V100/T4.
1. Конфигурация NodeGroup:

   ```yaml
   gpu:
     sharing: MIG
     mig:
       partedConfig: all-1g.5gb
   ```

1. Дождитесь, пока `nvidia-mig-manager` выполнит drain узла и переконфигурирует GPU.

    **Этот процесс может занять несколько минут**.

    Пока операция идёт, на узле стоит taint `mig-reconfigure`. После успешного окончания taint удаляется.

1. Ход процесса можно отслеживать по label `nvidia.com/mig.config.state` на узле:

    `pending`, `rebooting`, `success` (или `error`, если что-то пошло не так).

1. Если ресурсы `nvidia.com/mig-*` не появились — проверьте:

   ```bash
   d8 k -n d8-nvidia-gpu logs daemonset/nvidia-mig-manager
   nvidia-smi -L
   ```

#### Поддерживаются ли AMD или Intel GPU?

Сейчас Deckhouse Platform Certified Security Edition автоматически настраивает **только NVIDIA GPU**. Поддержка **AMD (ROCm)** и **Intel GPU** находится в проработке и планируется к добавлению в будущих релизах.

### Модуль openvpn: FAQ

#### Почему не работает автоматическая настройка DNS-сервера при подключении на macOS и Linux с помощью клиента OpenVPN

В связи с архитектурными особенностями операционных систем семейства Linux и macOS автоматическая конфигурация DNS-сервера при подключении с помощью официального клиента OpenVPN невозможна.

Для настройки DNS-сервера в таких ОС сервисом предусмотрена возможность использования сторонних скриптов, которые запускаются при подключении и отключении клиента.

В клиентских конфигурациях, генерируемых модулем, предопределены и закомментированы блоки, отвечающие за эти настройки:

```bash
### Uncomment the lines below for use with Linux
###script-security 2
### If you use resolved
###up /etc/openvpn/update-resolv-conf
###down /etc/openvpn/update-resolv-conf
### If you use systemd-resolved, first install the openvpn-systemd-resolved package
###up /etc/openvpn/update-systemd-resolved
###down /etc/openvpn/update-systemd-resolved
```

Для активации указанных блоков кода необходимо их раскомментировать (удалить начальный символ `#`), а также указать корректные пути к скриптам.

Скрипты можно подготовить самостоятельно или воспользоваться готовыми решениями.

{% alert level="warning" %}
Скрипты должны обладать правами на исполнение.
{% endalert %}

#### Как отозвать, ротировать или удалить сертификат клиента

Все действия с клиентскими сертификатами выполняются через веб-интерфейс `openvpn-admin`. Справа от имени каждого пользователя доступны кнопки для управления сертификатом:

![Действия с активным пользователем](images/active_user.png)

Чтобы ротировать (выпустить новый сертификат) или удалить клиента, необходимо сначала отозвать его текущий сертификат (Revoke):

![Действия с отозванным пользователем](images/revoked_user.png)

После отзыва становятся доступными действия Renew (ротация) и Delete (удаление).

#### Как ротировать сертификат сервера

Серверный сертификат ротируется автоматически за 1 день до окончания срока его действия.  

Если требуется выполнить ротацию вручную (например, при повреждении сертификата или внеплановой замене), выполните следующие шаги:

1. Удалите текущий секрет, содержащий сертификат и ключ сервера:

   ```shell
   d8 k -n d8-openvpn delete secrets openvpn-pki-server
   ```

1. Перезапустите поды OpenVPN, чтобы инициировать генерацию нового сертификата:

   ```shell
   d8 k -n d8-openvpn rollout restart sts openvpn
   ```

#### Как ротировать корневой сертификат (CA)

Корневой сертификат (CA) и серверный сертификат ротируется автоматически за 1 день до окончания срока действия. Автоматическая ротация сертификатов пользователя не предусмотрена.
Корневой сертификат (CA) используется для подписи всех сертификатов в OpenVPN — как серверных, так и клиентских. Поэтому при его замене необходимо перевыпустить все зависимые сертификаты.

Шаги для ротации корневого сертификата:

1. [Отзовите или удалите](#как-отозвать-ротировать-или-удалить-сертификат-клиента) все активные клиентские сертификаты. Сделать это можно через интерфейс `openvpn-admin`. Если вы воспользуетесь отзывом, то после замены CA можно будет выполнить ротацию сертификатов (Renew), не создавая клиента заново.

1. Удалите секреты `openvpn-pki-ca` и `openvpn-pki-server`  в пространстве имён `d8-openvpn`:

   ```shell
   d8 k -n d8-openvpn delete secrets openvpn-pki-ca openvpn-pki-server
   ```

1. Перезапустите поды OpenVPN:

   ```shell
   d8 k -n d8-openvpn rollout restart sts openvpn
   ```

1. Выполните [ротацию сертификатов](#как-отозвать-ротировать-или-удалить-сертификат-клиента) для отозванных клиентов или создайте новых клиентов с новыми сертификатами.

1. Удалите все отозванные сертификаты:

   ```shell
   d8 k  -n d8-openvpn delete secrets -l revokedForever=true
   ```

### Prometheus-мониторинг: FAQ

{% raw %}

#### Как собирать метрики с приложений, расположенных вне кластера?

1. Сконфигурируйте Service по аналогии с сервисом для [сбора метрик с вашего приложения](./monitoring-custom/#пример-service), но без указания параметра `spec.selector`.
1. Создайте Endpoints для этого Service, явно указав в них `IP:PORT`, по которым ваши приложения отдают метрики.

   > Имена портов в Endpoints должны совпадать с именами этих портов в Service.

##### Пример

Метрики приложения доступны без TLS, по адресу `http://10.182.10.5:9114/metrics`.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    prometheus.deckhouse.io/custom-target: my-app
spec:
  ports:
  - name: http-metrics
    port: 9114
---
apiVersion: v1
kind: Endpoints
metadata:
  name: my-app
  namespace: my-namespace
subsets:
  - addresses:
    - ip: 10.182.10.5
    ports:
    - name: http-metrics
      port: 9114
```

#### Как добавить дополнительные дашборды в вашем проекте?

Добавление пользовательских дашбордов для Grafana в Deckhouse реализовано с помощью подхода Infrastructure as a Code.
Чтобы ваш дашборд появился в Grafana, необходимо создать в кластере специальный ресурс — [`GrafanaDashboardDefinition`](cr.html#grafanadashboarddefinition).

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: GrafanaDashboardDefinition
metadata:
  name: my-dashboard
spec:
  folder: My folder # Папка, в которой в Grafana будет отображаться ваш дашборд.
  definition: |
    {
      "annotations": {
        "list": [
          {
            "builtIn": 1,
            "datasource": "-- Grafana --",
            "enable": true,
            "hide": true,
            "iconColor": "rgba(0, 211, 255, 1)",
            "limit": 100,
...
```

{% endraw %}

{% alert level="warning" %}
Системные и добавленные через [GrafanaDashboardDefinition](cr.html#grafanadashboarddefinition) дашборды нельзя изменить через интерфейс Grafana.

Алерты, настроенные в панели dashboard, не работают с шаблонами datasource — такой dashboard является невалидным и не импортируется. В версии Grafana 9.0 функционал legacy alerting был признан устаревшим и заменён на Grafana Alerting. В связи с этим, мы не рекомендуем использовать legacy alerting (оповещения панели мониторинга) в dashboards.
{% endalert %}

{% alert level="info" %}
Если после применения дашборд не появляется в Grafana, возможно, в JSON файле дашборда присутствует ошибка. Чтобы определить источник проблемы, воспользуйтесь командой `d8 k logs -n d8-monitoring deployments/grafana-v10 dashboard-provisioner` для просмотра логов компонента, который осуществляет применение дашбордов.
{% endalert %}

{% raw %}

#### Как добавить алерты и/или recording-правила для вашего проекта?

Для добавления алертов существует специальный ресурс — `CustomPrometheusRules`.

Параметры:

- `groups` — единственный параметр, в котором необходимо описать группы алертов. Структура групп полностью совпадает с аналогичной в prometheus-operator.

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: CustomPrometheusRules
metadata:
  name: my-rules
spec:
  groups:
  - name: cluster-state-alert.rules
    rules:
    - alert: CephClusterErrorState
      annotations:
        description: Storage cluster is in error state for more than 10m.
        summary: Storage cluster is in error state
        plk_markup_format: markdown
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} > 1
```

##### Как подключить дополнительные data source для Grafana?

Для подключения дополнительных data source к Grafana существует специальный ресурс — `GrafanaAdditionalDatasource`.

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: GrafanaAdditionalDatasource
metadata:
  name: another-prometheus
spec:
  type: prometheus
  access: Proxy
  url: https://another-prometheus.example.com/prometheus
  basicAuth: true
  basicAuthUser: foo
  jsonData:
    timeInterval: 30s
    httpMethod: POST
  secureJsonData:
    basicAuthPassword: bar
```

#### Как обеспечить безопасный доступ к метрикам?

Для обеспечения безопасности настоятельно рекомендуем использовать `kube-rbac-proxy`.

##### Пример безопасного сбора метрик с приложения, расположенного в кластере

Чтобы настроить защиту метрик приложения с использованием `kube-rbac-proxy` и последующего сбора метрик с помощью Prometheus, выполните следующие шаги:

1. Создайте `ServiceAccount` с указанными ниже правами:

   ```yaml
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: rbac-proxy-test
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: rbac-proxy-test
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: d8:rbac-proxy
   subjects:
   - kind: ServiceAccount
     name: rbac-proxy-test
     namespace: default
   ```

   > Обратите внимание, что используется встроенная в Deckhouse ClusterRole `d8:rbac-proxy`.

2. Создайте конфигурацию для `kube-rbac-proxy`:

   ```yaml
   ---
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: rbac-proxy-config-test
     namespace: rbac-proxy-test
   data:
     config-file.yaml: |+
       authorization:
         resourceAttributes:
           namespace: default
           apiVersion: v1
           resource: services
           subresource: proxy
           name: rbac-proxy-test
   ```

3. Создайте `Service` и `Deployment` для вашего приложения, где `kube-rbac-proxy` займет позицию sidecar-контейнера:

   ```yaml
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: rbac-proxy-test
     labels:
       prometheus.deckhouse.io/custom-target: rbac-proxy-test
   spec:
     ports:
     - name: https-metrics
       port: 8443
       targetPort: https-metrics
     selector:
       app: rbac-proxy-test
   ---
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: rbac-proxy-test
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: rbac-proxy-test
     template:
       metadata:
         labels:
           app: rbac-proxy-test
       spec:
         securityContext:
           runAsUser: 65532
         serviceAccountName: rbac-proxy-test
         containers:
         - name: kube-rbac-proxy
           image: quay.io/brancz/kube-rbac-proxy:v0.14.0
           args:
           - "--secure-listen-address=0.0.0.0:8443"
           - "--upstream=http://127.0.0.1:8081/"
           - "--config-file=/kube-rbac-proxy/config-file.yaml"
           - "--logtostderr=true"
           - "--v=10"
           ports:
           - containerPort: 8443
             name: https-metrics
           volumeMounts:
           - name: config
             mountPath: /kube-rbac-proxy
         - name: prometheus-example-app
           image: quay.io/brancz/prometheus-example-app:v0.1.0
           args:
           - "--bind=127.0.0.1:8081"
         volumes:
         - name: config
           configMap:
             name: rbac-proxy-config-test
   ```

4. Назначьте необходимые права на ресурс для Prometheus:

   ```yaml
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: rbac-proxy-test-client
   rules:
   - apiGroups: [""]
     resources: ["services/proxy"]
     verbs: ["get"]
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: rbac-proxy-test-client
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: rbac-proxy-test-client
   subjects:
   - kind: ServiceAccount
     name: prometheus
     namespace: d8-monitoring
   ```

После шага 4 метрики вашего приложения должны появиться в Prometheus.

##### Пример безопасного сбора метрик с приложения, расположенного вне кластера

Предположим, что есть доступный через интернет сервер, на котором работает `node-exporter`. По умолчанию `node-exporter` слушает на порту `9100` и доступен на всех интерфейсах. Необходимо обеспечить контроль доступа к `node-exporter` для безопасного сбора метрик. Ниже приведен пример такой настройки.

Требования:

- Из кластера должен быть доступ к сервису `kube-rbac-proxy`, который запущен на *удаленном сервере*.
- От *удаленного сервера* должен быть доступ к API-серверу кластера.

Выполните следующие шаги:

1. Создайте `ServiceAccount` с указанными ниже правами:

   ```yaml
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: prometheus-external-endpoint-server-01
     namespace: d8-service-accounts
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: prometheus-external-endpoint
   rules:
   - apiGroups: ["authentication.k8s.io"]
     resources:
     - tokenreviews
     verbs: ["create"]
   - apiGroups: ["authorization.k8s.io"]
     resources:
     - subjectaccessreviews
     verbs: ["create"]
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: prometheus-external-endpoint-server-01
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: prometheus-external-endpoint
   subjects:
   - kind: ServiceAccount
     name: prometheus-external-endpoint-server-01
     namespace: d8-service-accounts
   ```

2. Сгенерируйте `kubeconfig` для созданного `ServiceAccount` ([пример генерации kubeconfig для `ServiceAccount`](./user-authz/usage.html#создание-serviceaccount-для-сервера-и-предоставление-ему-доступа)).

3. Положите получившийся `kubeconfig` на *удаленный сервер*. Необходимо указать путь к этому `kubeconfig` в настройках `kube-rbac-proxy` (в примере используется путь `${PWD}/.kube/config`).

4. Настройте `node-exporter` на *удаленном сервере* с доступом к нему только на локальном интерфейсе (чтобы он слушал `127.0.0.1:9100`).
5. Запустите `kube-rbac-proxy` на *удаленном сервере*:

   ```shell
   docker run --network host -d -v ${PWD}/.kube/config:/config quay.io/brancz/kube-rbac-proxy:v0.14.0 --secure-listen-address=0.0.0.0:8443 \
     --upstream=http://127.0.0.1:9100 --kubeconfig=/config --logtostderr=true --v=10
   ```

6. Проверьте, что порт `8443` доступен по внешнему адресу *удаленного сервера*.

7. Создайте в кластере `Service` и `Endpoint`, указав в качестве `<server_ip_address>` внешний адрес *удаленного сервера*:

   ```yaml
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: prometheus-external-endpoint-server-01
     labels:
       prometheus.deckhouse.io/custom-target: prometheus-external-endpoint-server-01
   spec:
     ports:
     - name: https-metrics
       port: 8443
   ---
   apiVersion: v1
   kind: Endpoints
   metadata:
     name: prometheus-external-endpoint-server-01
   subsets:
     - addresses:
       - ip: <server_ip_address>
       ports:
       - name: https-metrics
         port: 8443
   ```

#### Как добавить Alertmanager?

Создайте кастомный ресурс `CustomAlertmanager` с типом `Internal`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: CustomAlertmanager
metadata:
  name: webhook
spec:
  type: Internal
  internal:
    route:
      groupBy: ['job']
      groupWait: 30s
      groupInterval: 5m
      repeatInterval: 12h
      receiver: 'webhook'
    receivers:
    - name: 'webhook'
      webhookConfigs:
      - url: 'http://webhookserver:8080/'
```

Подробно о всех параметрах можно прочитать в описании кастомного ресурса [CustomAlertmanager](cr.html#customalertmanager).

#### Как добавить внешний дополнительный Alertmanager?

Создайте кастомный ресурс `CustomAlertmanager` с типом `External`, который может указывать на Alertmanager по FQDN или через сервис в Kubernetes-кластере.

Пример FQDN Alertmanager:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: CustomAlertmanager
metadata:
  name: my-fqdn-alertmanager
spec:
  external:
    address: https://alertmanager.mycompany.com/myprefix
  type: External
```

Пример Alertmanager с Kubernetes service:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: CustomAlertmanager
metadata:
  name: my-service-alertmanager
spec:
  external:
    service:
      namespace: myns
      name: my-alertmanager
      path: /myprefix/
  type: External
```

Подробно о всех параметрах можно прочитать в описании кастомного ресурса [CustomAlertmanager](cr.html#customalertmanager).

#### Как в Alertmanager игнорировать лишние алерты?

Решение сводится к настройке маршрутизации алертов в вашем Alertmanager.

Требования:

1. Завести получателя без параметров.
1. Смаршрутизировать лишние алерты в этого получателя.

Ниже приведены примеры настройки `CustomAlertmanager`.

Чтобы получать только алерты с лейблами `service: foo|bar|baz`:

```yaml
receivers:
  # Получатель, определенный без параметров, будет работать как "/dev/null".
  - name: blackhole
  # Действующий получатель  
  - name: some-other-receiver
    # ...
route:
  # receiver по умолчанию.
  receiver: blackhole
  routes:
    # Дочерний маршрут
    - matchers:
        - matchType: =~
          name: service
          value: ^(foo|bar|baz)$
      receiver: some-other-receiver
```

Чтобы получать все алерты, кроме `DeadMansSwitch`:

```yaml
receivers:
  # Получатель, определенный без параметров, будет работать как "/dev/null".
  - name: blackhole
  # Действующий получатель.
  - name: some-other-receiver
  # ...
route:
  # receiver по умолчанию.
  receiver: some-other-receiver
  routes:
    # Дочерний маршрут.
    - matchers:
        - matchType: =
          name: alertname
          value: DeadMansSwitch
      receiver: blackhole
```

#### Почему нельзя установить разный scrapeInterval для отдельных таргетов?

Разные scrapeInterval'ы принесут следующие проблемы:

* увеличение сложности конфигурации;
* проблемы при написании запросов и создании графиков;
* короткие интервалы больше похожи на профилирование приложения, и, скорее всего, Prometheus — не самый подходящий инструмент для этого.

Наиболее подходящее значение для scrapeInterval находится в диапазоне 10–60 секунд.

#### Как ограничить потребление ресурсов Prometheus?

Чтобы предотвратить ситуации, когда Variable Policy Agent (VPA) запрашивает у Prometheus или долгосрочного Prometheus больше ресурсов, чем доступно на выделенном узле для этих целей, можно явно установить ограничения для VPA с использованием [параметров модуля](configuration.html):

- `vpa.longtermMaxCPU`;
- `vpa.longtermMaxMemory`;
- `vpa.maxCPU`;
- `vpa.maxMemory`.

#### Как настроить ServiceMonitor или PodMonitor для работы с Prometheus?

Добавьте лейбл `prometheus: main` к Pod/Service Monitor.
Добавьте в пространство имён, в котором находится Pod/Service Monitor, лейбл `prometheus.deckhouse.io/monitor-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/monitor-watcher-enabled: "true"
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: example-app
  namespace: frontend
  labels:
    prometheus: main
spec:
  selector:
    matchLabels:
      app: example-app
  endpoints:
    - port: web
```

#### Как настроить Probe для работы с Prometheus?

Добавьте лейбл `prometheus: main` к Probe.
Добавьте в пространство имён, в котором находится Probe, лейбл `prometheus.deckhouse.io/probe-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/probe-watcher-enabled: "true"
---
apiVersion: monitoring.coreos.com/v1
kind: Probe
metadata:
  labels:
    app: prometheus
    component: probes
    prometheus: main
  name: cdn-is-up
  namespace: frontend
spec:
  interval: 30s
  jobName: httpGet
  module: http_2xx
  prober:
    path: /probe
    scheme: http
    url: blackbox-exporter.blackbox-exporter.svc.cluster.local:9115
  targets:
    staticConfig:
      static:
      - https://example.com/status
```

#### Как настроить PrometheusRules для работы с Prometheus?

Добавьте в пространство имён, в котором находятся PrometheusRules, лейбл `prometheus.deckhouse.io/rules-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/rules-watcher-enabled: "true"
```

#### Как увеличить размер диска

1. Для увеличения размера отредактируйте PersistentVolumeClaim, указав новый размер в поле `spec.resources.requests.storage`.
   * Увеличение размера возможно, если в StorageClass поле `allowVolumeExpansion` установлено в `true`.
2. Если используемое хранилище не поддерживает изменение диска на лету, в статусе PersistentVolumeClaim появится сообщение `Waiting for user to (re-)start a pod to finish file system resize of volume on node.`.
3. Перезапустите под для завершения изменения размера файловой системы.

#### Как получить информацию об алертах в кластере?

Информацию об активных алертах можно получить не только в веб-интерфейсе Grafana/Prometheus, но и в CLI. Это может быть полезным, если у вас есть только доступ к API-серверу кластера и нет возможности открыть веб-интерфейс Grafana/Prometheus.

Выполните следующую команду для получения списка алертов в кластере:

```shell
d8 k get clusteralerts
```

Пример вывода:

```console
NAME               ALERT                                      SEVERITY   AGE     LAST RECEIVED   STATUS
086551aeee5b5b24   ExtendedMonitoringDeprecatatedAnnotation   4          3h25m   38s             firing
226d35c886464d6e   ExtendedMonitoringDeprecatatedAnnotation   4          3h25m   38s             firing
235d4efba7df6af4   D8SnapshotControllerPodIsNotReady          8          5d4h    44s             firing
27464763f0aa857c   D8PrometheusOperatorPodIsNotReady          7          5d4h    43s             firing
ab17837fffa5e440   DeadMansSwitch                             4          5d4h    41s             firing
```

Выполните следующую команду для просмотра конкретного алерта:

```shell
d8 k get clusteralerts <ALERT_NAME> -o yaml
```

Пример:

```shell
### d8 k get clusteralerts 235d4efba7df6af4 -o yaml
alert:
  description: |
    The recommended course of action:
    1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-controller`
    2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-controller`
  labels:
    pod: snapshot-controller-75bd776d76-xhb2c
    prometheus: deckhouse
    tier: cluster
  name: D8SnapshotControllerPodIsNotReady
  severityLevel: "8"
  summary: The snapshot-controller Pod is NOT Ready.
apiVersion: deckhouse.io/v1alpha1
kind: ClusterAlert
metadata:
  creationTimestamp: "2023-05-15T14:24:08Z"
  generation: 1
  labels:
    app: prometheus
    heritage: deckhouse
  name: 235d4efba7df6af4
  resourceVersion: "36262598"
  uid: 817f83e4-d01a-4572-8659-0c0a7b6ca9e7
status:
  alertStatus: firing
  lastUpdateTime: "2023-05-15T18:10:09Z"
  startsAt: "2023-05-10T13:43:09Z"
```

{% endraw %}

{% alert level="info" %}
Присутствие специального алерта `MissingDeadMansSwitch` в кластере говорит о проблемах в работоспособности компонентов мониторинга.
{% endalert %}

{% raw %}

#### Как добавить дополнительные эндпоинты в scrape config?

Добавьте в пространство имён, в котором находится ScrapeConfig, лейбл `prometheus.deckhouse.io/scrape-configs-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/scrape-configs-watcher-enabled: "true"
```

Добавьте ScrapeConfig, который имеет обязательный лейбл `prometheus: main`:

```yaml
apiVersion: monitoring.coreos.com/v1alpha1
kind: ScrapeConfig
metadata:
  name: example-scrape-config
  namespace: frontend
  labels:
    prometheus: main
spec:
  honorLabels: true
  staticConfigs:
    - targets: ['example-app.frontend.svc.{{ .Values.global.discovery.clusterDomain }}.:8080']
  relabelings:
    - regex: endpoint|namespace|pod|service
      action: labeldrop
    - targetLabel: scrape_endpoint
      replacement: main
    - targetLabel: job
      replacement: kube-state-metrics
  metricsPath: '/metrics'
```

{% endraw %}

### Модуль registry: FAQ

#### Как мигрировать на модуль registry?

Во время миграции, для containerd v1 будет выполнен переход на новую схему конфигурации registry.
containerd v2 использует новую схему по умолчанию. Подробнее можно ознакомиться в разделе [с описанием способов конфигурации](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry)

##### Для containerd v2

1. Выполните переключение на использование модуля `registry`. Для этого, укажите в `moduleConfig` `deckhouse` параметры `Unmanaged` режима. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [deckhouse](./deckhouse/) для корректной настройки.

   Посмотреть текущие настройки реестра можно с помощью команды:

   ```bash
   d8 k -n d8-system exec -it svc/deckhouse-leader -c deckhouse -- deckhouse-controller global values | yq e '.modulesImages.registry' -
   ```

   Данные настройки укажите при конфигурации `Unmanaged` режима:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Дождитесь завершения переключения. Пример [статуса переключения](./faq.html#как-посмотреть-статус-переключения-режима-registry):

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

##### Для containerd v1

{% alert level="danger" %}
- Во время переключения containerd v1 сервис будет перезапущен.
- Во время переключения containerd v1 будет переведен на новую схему конфигурации registry.
- Во время переключения, [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry) для containerd v1 будут временно недоступны.
{% endalert %}

1. Убедитесь, что на узлах с containerd v1 отсутствуют [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry), расположенные в директории `/etc/containerd/conf.d`.

1. Если конфигурации присутствуют, необходимо выполнить миграцию на новый формат конфигурации registry в containerd. Для этого, необходимо добавить новые конфигурации в директорию `/etc/containerd/registry.d`. Данные конфигурации вступят в силу после переключения на модуль `registry`. Для добавления конфигураций подготовьте `NodeGroupConfiguration`, подробнее в разделе [с описанием способов конфигурации](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry). Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: containerd-additional-config-auth.sh
   spec:
     # Шаг может быть любой, т.к. не требуется перезапуск сервиса containerd
     weight: 0
     bundles:
       - '*'
     nodeGroups:
       - "*"
     content: |
       # Copyright 2023 Flant JSC
       #
       # Licensed under the Apache License, Version 2.0 (the "License");
       # you may not use this file except in compliance with the License.
       # You may obtain a copy of the License at
       #
       #     http://www.apache.org/licenses/LICENSE-2.0
       #
       # Unless required by applicable law or agreed to in writing, software
       # distributed under the License is distributed on an "AS IS" BASIS,
       # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       # See the License for the specific language governing permissions and
       # limitations under the License.
       
       REGISTRY_URL=private.registry.example

       mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"
       bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
       [host]
         [host."https://${REGISTRY_URL}"]
           capabilities = ["pull", "resolve"]
           [host."https://${REGISTRY_URL}".auth]
             username = "username"
             password = "password"
       EOF
   ```

1. Примените `NodeGroupConfiguration`. Дождитесь появления конфигурационных файлов в директории `/etc/containerd/registry.d` на всех узлах.

1. Проверьте корректность работы конфигураций. Для этого воспользуйтесь командой:

   ```bash
   # Для https:
   ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ private.registry.example/registry/path:tag

   # Для http:
   ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ --plain-http private.registry.example/registry/path:tag
   ```

1. Выполните переключение на использование модуля `registry`. Для этого, укажите в `moduleConfig` `deckhouse` параметры `Unmanaged` режима. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [deckhouse](./deckhouse/) для корректной настройки.

   Посмотреть текущие настройки реестра можно с помощью команды:

   ```bash
   d8 k -n d8-system exec -it svc/deckhouse-leader -c deckhouse -- deckhouse-controller global values | yq e '.modulesImages.registry' -
   ```

   Данные настройки укажите при конфигурации `Unmanaged` режима:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. После применения, дождитесь в [статусе переключения](faq.html#как-посмотреть-статус-переключения-режима-registry) сообщение:

   Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "2025-08-13T15:22:34Z"
     message: |
       Check current nodes configuration
       2/2 node(s) Unready:
       - master-0: has custom toml merge containerd configuration
       - worker-5e389be0-578df-s5sm5: has custom toml merge containerd configuration
     reason: Processing
     status: "False"
     type: ContainerdConfigPreflightReady
   ```

   Данное сообщение означает, что на узлах имеются старые конфигурации реестров, расположенные в директории `/etc/containerd/conf.d`. И в данный момент переключение на новую конфигурацию containerd заблокировано. Для того, чтобы разрешить переключение, необходимо удалить старые конфигурационные файлы.

1. Удалите старые конфигурационные файлы, чтобы разрешить переключение на модуль `registry`. Для этого создайте `NodeGroupConfiguration`, пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: containerd-additional-config-auth.sh
   spec:
     # Шаг должен выполниться до '032_configure_containerd.sh'
     weight: 0
     bundles:
       - '*'
     nodeGroups:
       - "*"
     content: |
       # Copyright 2023 Flant JSC
       #
       # Licensed under the Apache License, Version 2.0 (the "License");
       # you may not use this file except in compliance with the License.
       # You may obtain a copy of the License at
       #
       #     http://www.apache.org/licenses/LICENSE-2.0
       #
       # Unless required by applicable law or agreed to in writing, software
       # distributed under the License is distributed on an "AS IS" BASIS,
       # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       # See the License for the specific language governing permissions and
       # limitations under the License.

       file="/etc/containerd/conf.d/old-config.toml"

       [ -f "$file" ] && rm -f "$file"
   ```
  
1. После удаления старых конфигураций, убедитесь, что переключение продолжило выполняться. Пример [статуса переключения](faq.html#как-посмотреть-статус-переключения-режима-registry):

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "2025-08-13T16:42:09Z"
     message: ""
     reason: ""
     status: "True"
     type: ContainerdConfigPreflightReady
   ```

1. Дождитесь завершения переключения. Пример [статуса переключения](faq.html#как-посмотреть-статус-переключения-режима-registry):

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

#### Как мигрировать обратно с модуля registry?

{% alert level="danger" %}
- Это устаревший (deprecated) формат управления registry.
- Во время переключения containerd v1 будет перезапущен.
- Во время переключения containerd v1 будет переведен на старую схему конфигурации registry.
- Во время переключения, [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry) для containerd v1 будут временно недоступны.
{% endalert %}

1. Переведите registry в режим `Unmanaged`. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [deckhouse](./deckhouse/) для корректной настройки.

   Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Проверьте статус переключения, используя [инструкцию](./faq.html#как-посмотреть-статус-переключения-режима-registry). Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "..."
     message: ""
     reason: ""
     status: "True"
     type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. Переведите registry в неконфигурируемый режим `Unmanaged`. Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
   ```

1. Проверьте статус переключения, используя [инструкцию](./faq.html#как-посмотреть-статус-переключения-режима-registry). Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "..."
     message: ""
     reason: ""
     status: "True"
     type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. Если используется containerd v1, и в кластере применены [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry), их необходимо заменить на старый формат. Для этого, подготовьте конфигурации registry старого формата. Данные конфигурации на данном этапе применять не нужно. Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: containerd-additional-config-auth.sh
   spec:
     # Для добавления файла перед шагом '032_configure_containerd.sh'
     weight: 31
     bundles:
       - '*'
     nodeGroups:
       - "*"
     content: |
       # Copyright 2023 Flant JSC
       #
       # Licensed under the Apache License, Version 2.0 (the "License");
       # you may not use this file except in compliance with the License.
       # You may obtain a copy of the License at
       #
       #     http://www.apache.org/licenses/LICENSE-2.0
       #
       # Unless required by applicable law or agreed to in writing, software
       # distributed under the License is distributed on an "AS IS" BASIS,
       # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       # See the License for the specific language governing permissions and
       # limitations under the License.

       REGISTRY_URL=private.registry.example

       mkdir -p /etc/containerd/conf.d
       bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
       [plugins]
         [plugins."io.containerd.grpc.v1.cri"]
           [plugins."io.containerd.grpc.v1.cri".registry]
             [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
               [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${REGISTRY_URL}"]
                 endpoint = ["https://${REGISTRY_URL}"]
             [plugins."io.containerd.grpc.v1.cri".registry.configs]
               [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".auth]
                 username = "username"
                 password = "password"
                 # OR
                 auth = "dXNlcm5hbWU6cGFzc3dvcmQ="
       EOF
   ```

1. Удалите секрет `registry-bashible-config`. Во время удаления, containerd v1 переключится на старый формат конфигурации containerd:

   ```bash
   d8 k -n d8-system delete secret registry-bashible-config
   ```

1. После удаления дождитесь завершения переключения. Для отслеживания используйте [инструкцию](faq.html#как-посмотреть-статус-переключения-режима-registry). Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "..."
     message: ""
     reason: ""
     status: "True"
     type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. Если используется containerd v1, примените заготовленные этапом ранее `NodeGroupConfiguration` с пользовательскими конфигурациями registry.

1. Отключите модуль `registry`. Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: registry
   spec:
     enabled: false
     settings: {}
     version: 1
   ```

#### Как посмотреть статус переключения режима registry?

Статус переключения режима registry можно получить с помощью следующей команды:

<!-- TODO(nabokihms): заменить на подкоманду d8, когда она будет реализована -->
```bash
d8 k -n d8-system -o yaml get secret registry-state | yq -C -P '.data | del .state | map_values(@base64d) | .conditions = (.conditions | from_yaml)'
```

Пример вывода:

```yaml
conditions:
  - lastTransitionTime: "2025-07-15T12:52:46Z"
    message: 'registry.deckhouse.ru: all 157 items are checked'
    reason: Ready
    status: "True"
    type: RegistryContainsRequiredImages
  - lastTransitionTime: "2025-07-11T11:59:03Z"
    message: ""
    reason: ""
    status: "True"
    type: ContainerdConfigPreflightReady
  - lastTransitionTime: "2025-07-15T12:47:47Z"
    message: ""
    reason: ""
    status: "True"
    type: TransitionContainerdConfigReady
  - lastTransitionTime: "2025-07-15T12:52:48Z"
    message: ""
    reason: ""
    status: "True"
    type: InClusterProxyReady
  - lastTransitionTime: "2025-07-15T12:54:53Z"
    message: ""
    reason: ""
    status: "True"
    type: DeckhouseRegistrySwitchReady
  - lastTransitionTime: "2025-07-15T12:55:48Z"
    message: ""
    reason: ""
    status: "True"
    type: FinalContainerdConfigReady
  - lastTransitionTime: "2025-07-15T12:55:48Z"
    message: ""
    reason: ""
    status: "True"
    type: Ready
mode: Direct
target_mode: Direct
```

Вывод отображает состояние процесса переключения. Каждое условие может находиться в статусе `True` или `False`, а также содержать поле `message` с пояснением.

Описание условий:

| Условие                           | Описание                                                                                                                                                                                            |
| --------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ContainerdConfigPreflightReady`  | Состояние проверки конфигурации containerd. Проверяется, что на узлах отсутствуют пользовательские auth конфигурации containerd.                                                                    |
| `TransitionContainerdConfigReady` | Состояние подготовки конфигурации containerd в новый режим. Проверяется, что конфигурация containerd успешно подготовлена и содержит одновременно конфигурации нового и старого режима.             |
| `FinalContainerdConfigReady`      | Состояние завершения переключения containerd в новый режим. Проверяется, что конфигурация containerd успешно применена и содержит конфигурацию нового режима.                                       |
| `DeckhouseRegistrySwitchReady`    | Состояние переключения Deckhouse и его компонентов на использование нового registry. Значение `True` указывает, что Deckhouse успешно переключился на сконфигурированный registry и готов к работе. |
| `InClusterProxyReady`             | Состояние готовности In-Cluster Proxy. Проверяется, что In-Cluster Proxy успешно запущен и работает.                                                                                                |
| `CleanupInClusterProxy`           | Состояние очистки In-Cluster Proxy, если прокси не нужен для работы желаемого режима. Проверяется, что все ресурсы, связанные с In-Cluster Proxy, успешно удалены.                                  |
| `Ready`                           | Общее состояние готовности registry к работе в указанном режиме. Проверяется, что все предыдущие условия выполнены и модуль готов к работе.                                                         |

### FAQ



#### Как собирать события?

Поды `runtime-audit-engine` выводят все события в стандартный вывод.
Далее агенты log-shipper могут собирать их и отправлять в хранилище логов.

Пример конфигурации [ClusterLoggingConfig](/modules/log-shipper/cr.html#clusterloggingconfig) для модуля `log-shipper`:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: falco-events
spec:
  destinationRefs:
  - xxxx
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [d8-runtime-audit-engine]
  labelFilter:
  - operator: Regex
    values: ["\{.*"] # to collect only JSON logs
    field: "message"
  type: KubernetesPods
```

#### Как оповещать о критических событиях?

Prometheus автоматически собирает метрики о событиях.
Чтобы включить оповещения, добавьте в кластер правило [CustomPrometheusRule](/modules/prometheus/cr.html#customprometheusrules).

Пример настройки такого правила:

{% raw %}

```yaml
apiVersion: deckhouse.io/v1
kind: CustomPrometheusRules
metadata:
  name: falco-critical-alerts
spec:
  groups:
  - name: falco-critical-alerts
    rules:
    - alert: FalcoCriticalAlertsAreFiring
      for: 1m
      annotations:
        description: |
          There is a suspicious activity on a node {{ $labels.node }}. 
          Check you events journal for more details.
        summary: Falco detects a critical security incident
      expr: |
        sum by (node) (rate(falcosecurity_falcosidekick_falco_events_total{priority="Critical"}[5m]) > 0)
```

{% endraw %}

{{< alert >}}
Алерты лучше всего работают в комбинации с хранилищами событий, такими как Elasticsearch или Loki. Их задача — оповестить пользователя о подозрительном поведении на узле.
После получения алерта рекомендуется «пойти» в хранилище и посмотреть на события, которые его вызвали.
{{< /alert >}}


#### Как применить правила для Falco, найденные в интернете?

Структура правил Falco отличается от схемы CRD.
Это связано со сложностями при проверке правильности ресурсов в Kubernetes.

Скрипт для конвертации правил Falco в ресурсы [FalcoAuditRules](cr.html#falcoauditrules) встроен в функционал утилиты `d8`.  
С его помощью можно применять правила Falco в Deckhouse:

```shell
d8 tools far-converter /path/to/falco/rule_example.yaml > ./my-rules-cr.yaml
```

Пример результата работы скрипта:

```yaml
### /path/to/falco/rule_example.yaml
- macro: spawned_process
  condition: (evt.type in (execve, execveat) and evt.dir=<)

- rule: Linux Cgroup Container Escape Vulnerability (CVE-2022-0492)
  desc: "This rule detects an attempt to exploit a container escape vulnerability in the Linux Kernel."
  condition: container.id != "" and proc.name = "unshare" and spawned_process and evt.args contains "mount" and evt.args contains "-o rdma" and evt.args contains "/release_agent"
  output: "Detect Linux Cgroup Container Escape Vulnerability (CVE-2022-0492) (user=%user.loginname uid=%user.loginuid command=%proc.cmdline args=%proc.args)"
  priority: CRITICAL
  tags: [process, mitre_privilege_escalation]
```

```yaml
### ./my-rules-cr.yaml
apiVersion: deckhouse.io/v1alpha1
kind: FalcoAuditRules
metadata:
  name: rule-example
spec:
  rules:
    - macro:
        name: spawned_process
        condition: (evt.type in (execve, execveat) and evt.dir=<)
    - rule:
        name: Linux Cgroup Container Escape Vulnerability (CVE-2022-0492)
        condition: container.id != "" and proc.name = "unshare" and spawned_process and evt.args contains "mount" and evt.args contains "-o rdma" and evt.args contains "/release_agent"
        desc: This rule detects an attempt to exploit a container escape vulnerability in the Linux Kernel.
        output: Detect Linux Cgroup Container Escape Vulnerability (CVE-2022-0492) (user=%user.loginname uid=%user.loginuid command=%proc.cmdline args=%proc.args)
        priority: Critical
        tags:
          - process
          - mitre_privilege_escalation
```

### Модуль storage-volume-data-manager: FAQ

#### Что если я не хочу использовать утилиту d8? Каким ещё способом можно создавать и пользоваться ресурсами DataExport?

Создать ресурс можно через yaml-манифест, в примере для удобства будем использованть переменные (замените значения на нужные вам)

```bash
export NAMESPACE="d8-storage-volume-data-manager"
export DATA_EXPORT_RESOURCE_NAME="example-dataexport"
export TARGET_TYPE="PersistentVolumeClaim"
export TARGET_NAME="fs-pvc-data-exporter-fs-0"
```

```bash
k apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: DataExport
metadata:
  name: ${DATA_EXPORT_RESOURCE_NAME}
  namespace: ${NAMESPACE}
spec:
  ttl: 10h
  targetRef:
    kind: ${TARGET_TYPE}
    name: ${TARGET_NAME}
EOF
```

После создания ресурса нужно взять из него ca-сертификат:

```bash
kubectl -n $NAMESPACE get dataexport $DATA_EXPORT_RESOURCE_NAME  -o jsonpath='{.status.ca}' | base64 -d > ca.pem
```

Проверяем сертификат:

```bash
openssl x509 -in ca.pem -noout -text | head
### Должно быть что-то вроде:
###   Issuer: CN = data-exporter-CA
###   Signature Algorithm: ecdsa-with-SHA256
```

Экспортируем URL-адрес из DataExport ресурса и проверяем экспорт:

```bash
export POD_URL=$(kubectl -n $NAMESPACE get dataexport $DATA_EXPORT_RESOURCE_NAME  -o jsonpath='{.status.url}')
echo "POD_URL: $POD_URL"
```

Далее, мы можем подключаться следующими методами.

##### 1. С указанием сертификата и ключа из локального kube config

Копируем ключи из конфига:

```bash
cat ~/.kube/config | grep "client-certificate-data" | awk '{print $2}' | base64 -d > client.crt
cat ~/.kube/config | grep "client-key-data" | awk '{print $2}' | base64 -d > client.key
```

Проверяем содержимое на целевой PVC:

```bash
curl -v --cacert ca.pem ${POD_URL}api/v1/files/ --key client.key --cert client.crt
```

Пример вывода:

```bash
..
..

< 
* TLSv1.2 (IN), TLS header, Supplemental data (23):
{"apiVersion": "v1", "items": [{"name":"4.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"hello","size":5,"modTime":"2025-03-03 10:53:06.895434814 +0000 UTC","type":"file"}
,{"name":"7.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"lost+found","modTime":"2025-03-03 10:29:31 +0000 UTC","type":"dir"}
,{"name":"8.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"10.txt","size":13,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"9.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"3.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"2.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"1.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"6.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"5.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
]}
```

##### 2. С использованием токена и ролей

Создаём ServiceAccount:

```bash
kubectl -n $NAMESPACE create serviceaccount data-exporter-test
```

Создаём ClusterRole

```bash
kubectl create -f - <<EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: data-exporter-test-role
rules:
- apiGroups: ["storage.deckhouse.io"]
  resources: ["dataexports/download"]
  verbs: ["create"]
EOF
```

Создаём токен

```bash
export TOKEN=$(kubectl create token data-exporter-test --duration=24h)
echo $TOKEN
```

Создаём ClusterRoleBinding:

```bash
kubectl create -f - <<EOF
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: data-exporter-test-role-binding
namespace: ${NAMESPACE}
subjects:
- kind: ServiceAccount
  name: data-exporter-test
  namespace: ${NAMESPACE}
  roleRef:
  kind: ClusterRole
  name: data-exporter-test-role
  apiGroup: rbac.authorization.k8s.io
  EOF
```

Проверяем содержимое на целевой PVC:

```bash
curl -H "Authorization: Bearer $TOKEN" \
-v --cacert ca.pem ${POD_URL}api/v1/files/
```

Пример вывода:

```bash
..
..

< 
* TLSv1.2 (IN), TLS header, Supplemental data (23):
{"apiVersion": "v1", "items": [{"name":"4.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"hello","size":5,"modTime":"2025-03-03 10:53:06.895434814 +0000 UTC","type":"file"}
,{"name":"7.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"lost+found","modTime":"2025-03-03 10:29:31 +0000 UTC","type":"dir"}
,{"name":"8.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"10.txt","size":13,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"9.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"3.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"2.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"1.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"6.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
,{"name":"5.txt","size":12,"modTime":"2025-04-01 08:02:00.228156524 +0000 UTC","type":"file"}
]}
```

Важные примечания:

- Файлы скачиваются при помощи стандартных GET-запросов, содержащих в URL путь к файлу: GET /api/v1/files/largeimage.iso, GET /api/v1/files/directory/largeimage.iso. Путь к файлу не должен заканчиваться символом /.
Такой метод скачивания поддерживается стандартными средствами: браузерами, curl и т.д. Поддерживается докачка файлов, при этом сжатие не поддерживается;
- Обращение к директории осуществляется аналогичным GET-запросом, при этом путь к директории должен заканчиваться символом /: GET /api/v1/files/ - путь к root, GET /api/v1/files/directory/ - путь к directory;
- При обращении к директории обеспечивается листинг файлов в этой директории: в теле ответа отправляется JSON-строка, содержащая список файлов: имя, тип и размер. Размеры файлов не кэшируются, при каждом запросе директории вычисляются заново;

### Мониторинг SLA кластера: FAQ

#### Почему периодически не могут разместиться поды `upmeter-probe-controller-manager`, почему некоторые поды постоянно удаляются?

В модуле реализованы тесты доступности функционала различных контроллеров Kubernetes.  
Тесты выполняется путем создания и удаления временных подов.

Объекты `upmeter-probe-scheduler`, отвечают за реализацию теста функционала размещения подов на узлы.
В рамках теста создается под, который размещается на узел. Затем этот под удаляется.

Объекты `upmeter-probe-controller-manager` отвечают за тестирование работоспособности `kube-controller-manager`.  
В рамках теста создается StatefulSet, и проверяется что данный объект породил под (т.к. фактическое размещение пода не требуется и проверяется в рамках другого теста, то создается под который гарантированно не может разместиться, т.е. находится в состоянии `Pending`). Затем StatefulSet удаляется и выполняется проверка, что порожденный им под удалился.

Объекты `smoke-mini` реализуют тестирование сетевой связности между узлами.
Для проверки размещаются пять StatefulSet с одной репликой. В рамках теста проверяется связность как между подами `smoke-mini`, так и сетевая связность с подами `upmeter-agent`, работающими на master-узлах.  
Раз в минуту один из подов `smoke-mini` переносится на другой узел.

### Модуль user-authn: FAQ

{% raw %}

#### Как защитить мое приложение?

Чтобы включить аутентификацию через Dex для приложения, выполните следующие шаги:

1. Создайте custom resource [DexAuthenticator](cr.html#dexauthenticator).

   Создание `DexAuthenticator` в кластере приводит к созданию экземпляра oauth2-proxy, подключенного к Dex. После появления custom resource `DexAuthenticator` в указанном namespace появятся необходимые объекты Deployment, Service, Ingress, Secret.

   Пример custom resource `DexAuthenticator`:

   ```yaml
   apiVersion: deckhouse.io/v1
   kind: DexAuthenticator
   metadata:
     # Префикс имени подов Dex authenticator.
     # Например, если префикс имени `app-name`, то поды Dex authenticator будут вида `app-name-dex-authenticator-7f698684c8-c5cjg`.
     name: app-name
     # Namespace, в котором будет развернут Dex authenticator.
     namespace: app-ns
   spec:
     # Домен вашего приложения. Запросы на него будут перенаправляться для прохождения аутентификацию в Dex.
     applicationDomain: "app-name.kube.my-domain.com"
     # Отправлять ли `Authorization: Bearer` header приложению. Полезно в связке с auth_request в NGINX.
     sendAuthorizationHeader: false
     # Имя Secret'а с SSL-сертификатом.
     applicationIngressCertificateSecretName: "ingress-tls"
     # Название Ingress-класса, которое будет использоваться в создаваемом для Dex authenticator Ingress-ресурсе.
     applicationIngressClassName: "nginx"
     # Время, на протяжении которого пользовательская сессия будет считаться активной.
     keepUsersLoggedInFor: "720h"
     # Список групп, пользователям которых разрешено проходить аутентификацию.
     allowedGroups:
     - everyone
     - admins
     # Список адресов и сетей, с которых разрешено проходить аутентификацию.
     whitelistSourceRanges:
     - 1.1.1.1/32
     - 192.168.0.0/24
   ```

2. Подключите приложение к Dex.

   Для этого добавьте в Ingress-ресурс приложения следующие аннотации:

   - `nginx.ingress.kubernetes.io/auth-signin: https://$host/dex-authenticator/sign_in`
   - `nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-User,X-Auth-Request-Email`
   - `nginx.ingress.kubernetes.io/auth-url: https://<SERVICE_NAME>.<NS>.svc.{{ C_DOMAIN }}/dex-authenticator/auth`, где:
      - `SERVICE_NAME` — имя сервиса (Service) аутентификатора. Как правило, оно соответствует формату `<NAME>-dex-authenticator` (`<NAME>` — это `metadata.name` DexAuthenticator).
      - `NS` — значение параметра `metadata.namespace` ресурса `DexAuthenticator`.
      - `C_DOMAIN` — домен кластера (параметр [clusterDomain](/reference/api/cr.html#clusterconfiguration-clusterdomain) ресурса `ClusterConfiguration`).

   > **Важно:** Если имя DexAuthenticator (`<NAME>`) слишком длинное, имя сервиса (Service) может быть сокращено. Чтобы найти корректное имя сервиса, воспользуйтесь следующей командой (укажите имя пространства имен и имя аутентификатора):
   >
   > ```shell
   > d8 k get service -n <NS> -l "deckhouse.io/dex-authenticator-for=<NAME>" -o jsonpath='{.items[0].metadata.name}'
   > ```
   >

   Ниже представлен пример аннотаций на Ingress-ресурсе приложения, для подключения его к Dex:

   ```yaml
   annotations:
     nginx.ingress.kubernetes.io/auth-signin: https://$host/dex-authenticator/sign_in
     nginx.ingress.kubernetes.io/auth-url: https://app-name-dex-authenticator.app-ns.svc.cluster.local/dex-authenticator/auth
     nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-User,X-Auth-Request-Email
   ```

##### Настройка ограничений на основе CIDR

В DexAuthenticator нет встроенной системы управления разрешением аутентификации на основе IP-адреса пользователя. Вместо этого вы можете воспользоваться аннотациями для Ingress-ресурсов:

* Если нужно ограничить доступ по IP и оставить прохождение аутентификации в Dex, добавьте аннотацию с указанием разрешенных CIDR через запятую:

  ```yaml
  nginx.ingress.kubernetes.io/whitelist-source-range: 192.168.0.0/32,1.1.1.1
  ```

* Если необходимо, чтобы пользователи из указанных сетей освобождались от прохождения аутентификации в Dex, а пользователи из остальных сетей обязательно аутентифицировались в Dex, добавьте следующую аннотацию:

  ```yaml
  nginx.ingress.kubernetes.io/satisfy: "any"
  ```

#### Как работает аутентификация с помощью DexAuthenticator

![Как работает аутентификация с помощью DexAuthenticator](images/dex_login.svg)

1. Dex в большинстве случаев перенаправляет пользователя на страницу входа провайдера и ожидает, что пользователь будет перенаправлен на его `/callback` URL. Однако такие провайдеры, как LDAP или Atlassian Crowd, не поддерживают этот вариант. Вместо этого пользователь должен ввести свои логин и пароль в форму входа в Dex, и Dex сам проверит их верность, сделав запрос к API провайдера.

2. DexAuthenticator устанавливает cookie с целым refresh token (вместо того чтобы выдать тикет, как для ID token) потому что Redis не сохраняет данные на диск.
Если по тикету в Redis не найден ID token, пользователь сможет запросить новый ID token, предоставив refresh token из cookie.

3. DexAuthenticator выставляет HTTP-заголовок `Authorization`, равный значению ID token из Redis. Это необязательно для сервисов по типу [Upmeter](./upmeter/), потому что права доступа к Upmeter не такие проработанные.

#### Как сгенерировать kubeconfig для доступа к Kubernetes API?

Сгенерировать `kubeconfig` для удаленного доступа к кластеру через `kubectl` можно через веб-интерфейс `kubeconfigurator`.

Настройте параметр [publishAPI](configuration.html#parameters-publishapi):

- Откройте настройки модуля `user-authn` (создайте ресурс moduleConfig `user-authn`, если его нет):

  ```shell
  d8 k edit mc user-authn
  ```

- Добавьте следующую секцию в блок `settings` и сохраните изменения:

  ```yaml
  publishAPI:
    enabled: true
  ```

Для доступа к веб-интерфейсу, позволяющему сгенерировать `kubeconfig`, зарезервировано имя `kubeconfig`. URL для доступа зависит от значения параметра [publicDomainTemplate](/reference/api/global.html#parameters-modules-publicdomaintemplate) (например, для `publicDomainTemplate: %s.kube.my` это будет `kubeconfig.kube.my`, а для `publicDomainTemplate: %s-kube.company.my` — `kubeconfig-kube.company.my`)  
{% endraw %}

##### Настройка kube-apiserver

С помощью функций модуля [control-plane-manager](/modules/control-plane-manager/) Deckhouse автоматически настраивает kube-apiserver, выставляя следующие флаги так, чтобы модули `dashboard` и `kubeconfig-generator` могли работать в кластере.

{% offtopic title="Аргументы kube-apiserver, которые будут настроены" %}

* `--oidc-client-id=kubernetes`
* `--oidc-groups-claim=groups`
* `--oidc-issuer-url=https://dex.%addonsPublicDomainTemplate%/`
* `--oidc-username-claim=email`

В случае использования самоподписанных сертификатов для Dex будет добавлен еще один аргумент, а также в под с apiserver будет смонтирован файл с CA:

* `--oidc-ca-file=/etc/kubernetes/oidc-ca.crt`
{% endofftopic %}

{% raw %}

##### Как работает подключение к Kubernetes API с помощью сгенерированного kubeconfig

![Схема взаимодействия при подключении к Kubernetes API с помощью сгенерированного kubeconfig](images/kubeconfig_dex.svg)

1. До начала работы kube-apiserver необходимо запросить конфигурационный endpoint OIDC провайдера (в нашем случае — Dex), чтобы получить issuer и настройки JWKS endpoint.

2. Kubeconfig generator сохраняет ID token и refresh token в файл kubeconfig.

3. После получения запроса с ID token kube-apiserver идет проверять, что token подписан провайдером, который мы настроили на первом шаге, с помощью ключей, полученных с точки доступа JWKS. В качестве следующего шага он сравнивает значения claim'ов `iss` и `aud` из token'а со значениями из конфигурации.

#### Как Dex защищен от подбора логина и пароля?

Одному пользователю разрешено только 20 попыток входа. Если указанный лимит израсходован, одна дополнительная попытка будет добавляться каждые 6 секунд.

{% endraw %}

### Модуль user-authz: FAQ

#### Как создать пользователя?

[Создание пользователя](usage.html#создание-пользователя).

<div style="height: 0;" id="как-ограничить-права-пользователю-конкретными-пространствами-имён-устаревшая-ролевая-модель"></div>

#### Как ограничить права пользователю конкретными пространствами имён?

Чтобы ограничить права пользователя конкретными пространствами имён в экспериментальной ролевой модели, используйте в `RoleBinding` [use-роль](./#use-роли) с соответствующим уровнем доступа. [Пример...](usage.html#пример-назначения-административных-прав-пользователю-в-рамках-пространства-имён).

В текущей ролевой модули используйте параметры `namespaceSelector` или `limitNamespaces` (устарел) в кастомном ресурсе [`ClusterAuthorizationRule`](/modules/user-authz/cr.html#clusterauthorizationrule).

#### Что, если два ClusterAuthorizationRules подходят для одного пользователя?

В примере пользователь `jane.doe@example.com` состоит в группе `administrators`. Созданы два ClusterAuthorizationRules:

```yaml
apiVersion: deckhouse.io/v1
kind: ClusterAuthorizationRule
metadata:
  name: jane
spec:
  subjects:
    - kind: User
      name: jane.doe@example.com
  accessLevel: User
  namespaceSelector:
    labelSelector:
      matchLabels:
        env: review
---
apiVersion: deckhouse.io/v1
kind: ClusterAuthorizationRule
metadata:
  name: admin
spec:
  subjects:
  - kind: Group
    name: administrators
  accessLevel: ClusterAdmin
  namespaceSelector:
    labelSelector:
      matchExpressions:
      - key: env
        operator: In
        values:
        - prod
        - stage
```

1. `jane.doe@example.com` имеет право запрашивать и просматривать объекты среди всех пространств имён, помеченных `env=review`.
2. `Administrators` могут запрашивать, редактировать, получать и удалять объекты на уровне кластера и из пространств имён, помеченных `env=prod` и `env=stage`.

Так как для `Jane Doe` подходят два правила, необходимо провести вычисления:

* `Jane Doe` будет иметь самый сильный accessLevel среди всех подходящих правил — `ClusterAdmin`.
* Опции `namespaceSelector` будут объединены так, что `Jane Doe` будет иметь доступ в пространства имён, помеченные меткой `env` со значением `review`, `stage` или `prod`.

{% alert level="warning" %}
Если есть правило без опции `namespaceSelector` и без опции `limitNamespaces` (устаревшая), это значит, что доступ разрешён во все пространства имён, кроме системных, что повлияет на результат вычисления доступных пространств имён для пользователя.
{% endalert %}

#### Как расширить роли или создать новую?

[Экспериментальная ролевая модель](./#экспериментальная-ролевая-модель) построена на принципе агрегации, она собирает более мелкие роли в более обширные,
тем самым предоставляя лёгкие способы расширения модели собственными ролями.

##### Создание новой роли подсистемы

Предположим, что текущие подсистемы не подходят под ролевое распределение в компании и требуется создать новую [подсистему](./#подсистемы-ролевой-модели),
которая будет включать в себя роли из подсистемы `deckhouse`, подсистемы `kubernetes` и модуля user-authn.

Для решения этой задачи создайте следующую роль:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom:manage:mycustom:manager
  labels:
    rbac.deckhouse.io/use-role: admin
    rbac.deckhouse.io/kind: manage
    rbac.deckhouse.io/level: subsystem
    rbac.deckhouse.io/subsystem: custom
    rbac.deckhouse.io/aggregate-to-all-as: manager
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.deckhouse.io/kind: manage
        rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
    - matchLabels:
        rbac.deckhouse.io/kind: manage
        rbac.deckhouse.io/aggregate-to-kubernetes-as: manager
    - matchLabels:
        rbac.deckhouse.io/kind: manage
        module: user-authn
rules: []
```

В начале указаны лейблы для новой роли:

- показывает, какую роль хук должен использовать при создании use ролей:

  ```yaml
  rbac.deckhouse.io/use-role: admin
  ```

- показывает, что роль должна обрабатываться как manage-роль:

  ```yaml
  rbac.deckhouse.io/kind: manage
  ```

  > Этот лейбл обязателен.

- показывает, что роль является ролью подсистемы, и обрабатываться будет соответственно:

  ```yaml
  rbac.deckhouse.io/level: subsystem
  ```

- указывает подсистему, за которую отвечает роль:

  ```yaml
  rbac.deckhouse.io/subsystem: custom
  ```

- позволяет `manage:all`-роли агрегировать эту роль в себя:

  ```yaml
  rbac.deckhouse.io/aggregate-to-all-as: manager
  ```

Далее указаны селекторы, именно они реализуют агрегацию:

- агрегирует роль менеджера из подсистемы `deckhouse`:

  ```yaml
  rbac.deckhouse.io/kind: manage
  rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
  ```

- агрегирует все правила от модуля user-authn:

  ```yaml
   rbac.deckhouse.io/kind: manage
   module: user-authn
  ```

Таким образом роль получает права от подсистем `deckhouse`, `kubernetes` и от модуля user-authn.

Особенности:

* ограничений на имя роли нет, но для читаемости лучше использовать этот стиль;
* use-роли будут созданы в пространстве имён агрегированных подсистем и модуля, тип роли выбран лейблом.

##### Расширение пользовательской роли

Например, в кластере появился новый кластерный (пример для manage-роли) CRD-объект — MySuperResource, и нужно дополнить собственную роль из примера выше правами на взаимодействие с этим ресурсом.

Первым делом нужно дополнить роль новым селектором:

```yaml
rbac.deckhouse.io/kind: manage
rbac.deckhouse.io/aggregate-to-custom-as: manager
```

Этот селектор позволит агрегировать роли к новой подсистеме через указание этого лейбла. После добавления нового селектора роль будет выглядеть так:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   name: custom:manage:mycustom:manager
   labels:
     rbac.deckhouse.io/use-role: admin
     rbac.deckhouse.io/kind: manage
     rbac.deckhouse.io/level: subsystem
     rbac.deckhouse.io/subsystem: custom
     rbac.deckhouse.io/aggregate-to-all-as: manager
 aggregationRule:
   clusterRoleSelectors:
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         rbac.deckhouse.io/aggregate-to-kubernetes-as: manager
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         module: user-authn
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         rbac.deckhouse.io/aggregate-to-custom-as: manager
 rules: []
 ```

 Далее нужно создать новую роль, в которой следует определить права для нового ресурса. Например, только чтение:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   labels:
     rbac.deckhouse.io/aggregate-to-custom-as: manager
     rbac.deckhouse.io/kind: manage
   name: custom:manage:permission:mycustom:superresource:view
 rules:
 - apiGroups:
   - mygroup.io
   resources:
   - mysuperresources
   verbs:
   - get
   - list
   - watch
 ```

Роль дополнит своими правами роль подсистемы, дав права на просмотр нового объекта.

Особенности:

* ограничений на имя роли нет, но для читаемости лучше использовать этот стиль.

##### Расширение существующих manage subsystem-ролей

Если необходимо расширить существующую роль, нужно выполнить те же шаги, что и в пункте выше, но изменив лейблы и название роли.

Пример для расширения роли менеджера из подсистемы `deckhouse`(`d8:manage:deckhouse:manager`):

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
    rbac.deckhouse.io/kind: manage
  name: custom:manage:permission:mycustommodule:superresource:view
rules:
- apiGroups:
  - mygroup.io
  resources:
  - mysuperresources
  verbs:
  - get
  - list
  - watch
```

Таким образом новая роль расширит роль `d8:manage:deckhouse`.

##### Расширение manage subsystem-ролей с добавлением нового пространства имён

Если необходимо добавить новое пространство имён (для создания в нём use-роли с помощью хука), потребуется добавить лишь один лейбл:

```yaml
"rbac.deckhouse.io/namespace": namespace
```

Этот лейбл сообщает хуку, что в этом пространстве имён нужно создать use-роль:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   labels:
     rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
     rbac.deckhouse.io/kind: manage
     rbac.deckhouse.io/namespace: namespace
   name: custom:manage:permission:mycustom:superresource:view
 rules:
 - apiGroups:
   - mygroup.io
   resources:
   - mysuperresources
   verbs:
   - get
   - list
   - watch
 ```

Хук мониторит `ClusterRoleBinding` и при создании биндинга ходит по всем manage-ролям, чтобы найти все объединенные в них роли с помощью проверки правила агрегации. Затем он берёт пространство имён из лейбла `rbac.deckhouse.io/namespace` и создает use-роль в этом пространстве имён.

##### Расширение существующих use-ролей

Если ресурс принадлежит пространству имён, необходимо расширить use-роль вместо manage-роли. Разница лишь в лейблах и имени:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   labels:
     rbac.deckhouse.io/aggregate-to-kubernetes-as: user
     rbac.deckhouse.io/kind: use
   name: custom:use:capability:mycustom:superresource:view
 rules:
 - apiGroups:
   - mygroup.io
   resources:
   - mysuperresources
   verbs:
   - get
   - list
   - watch
 ```

Эта роль дополнит роль `d8:use:role:user:kubernetes`.

### Модуль vertical-pod-autoscaler: FAQ

#### Как посмотреть рекомендации Vertical Pod Autoscaler?

После создания кастомного ресурса [VerticalPodAutoscaler](cr.html#verticalpodautoscaler) посмотреть рекомендации VPA можно следующим образом:

```shell
d8 k describe vpa my-app-vpa
```

В секции `status` отобразятся параметры:

- `Target` — количество ресурсов, которое будет оптимальным для пода (в пределах resourcePolicy);
- `Lower Bound` — минимальное рекомендуемое количество ресурсов для более или менее (но не гарантированно) хорошей работы приложения;
- `Upper Bound` — максимальное рекомендуемое количество ресурсов. Скорее всего, ресурсы, выделенные сверх этого значения, идут в мусорку и совсем никогда не нужны приложению;
- `Uncapped Target` — рекомендуемое количество ресурсов в самый последний момент, то есть данное значение считается на основе самых крайних метрик, не смотря на историю ресурсов за весь период.

#### Как Vertical Pod Autoscaler работает с лимитами?

##### Пример 1

В примере представлен VPA-объект:

```yaml
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: test2
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: test2
  updatePolicy:
    updateMode: "Initial"
```

В VPA-объекте представлен под с ресурсами:

```yaml
resources:
  limits:
    cpu: 2
  requests:
    cpu: 1
```

Если контейнер использует весь CPU, и VPA рекомендует этому контейнеру 1.168 CPU, то отношение между запросами и ограничениями будет равно 100%.
В этом случае при пересоздании пода VPA модифицирует его и проставит такие ресурсы:

```yaml
resources:
  limits:
    cpu: 2336m
  requests:
    cpu: 1168m
```

##### Пример 2

В примере представлен VPA-объект:

```yaml
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: test2
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: test2
  updatePolicy:
    updateMode: "Initial"
```

В VPA-объекте представлен под с ресурсами:

```yaml
resources:
  limits:
    cpu: 1
  requests:
    cpu: 750m
```

Если отношение запросов и ограничений равно 25%, и VPA рекомендует 1.168 CPU для контейнера, VPA изменит ресурсы контейнера следующим образом:

```yaml
resources:
  limits:
    cpu: 1557m
  requests:
    cpu: 1168m
```

Если необходимо ограничить максимальное количество ресурсов, которые могут быть выделены для ограничений контейнера, нужно использовать в спецификации объекта VPA `maxAllowed` или использовать Limit Range объекта Kubernetes.

### Модуль registry

#### Описание

Модуль отвечает за управление конфигурацией registry компонентов Deckhouse и предоставляет внутреннее хранилище образов контейнеров (container registry, registry).

Внутренний registry оптимизирует загрузку и хранение образов, а также повышает высокую доступность и отказоустойчивость Deckhouse Platform Certified Security Edition.

Модуль работает в следующих режимах:

- `Direct` — использование внутреннего registry. Обращение к внутреннему registry выполняется по фиксированному адресу `registry.d8-system.svc:5001/system/deckhouse`. Фиксированный адрес, при изменении параметров registry, позволяет избежать повторного скачивания образов и перезапуска компонентов. Переключение между режимами и registry выполняется через ModuleConfig `deckhouse`. Переключение выполняется автоматически (ознакомьтесь с [примерами использования](examples.html)).
- `Unmanaged` — работа без использования внутреннего registry. Обращение внутри кластера выполняется напрямую к внешнему registry.
  Существует 2 вида режима `Unmanaged`:
  - Конфигурируемый - режим, управляемый с помощью модуля `registry`. Переключение между режимами и registry выполняется через ModuleConfig `deckhouse`. Переключение выполняется автоматически (ознакомьтесь с [примерами использования](examples.html)).
  - Неконфигурируемый (deprecated) - режим используемый по умолчанию. Параметры конфигурации задаются [при установке кластера](/reference/api/cr.html#initconfiguration-deckhouse-imagesrepo), или при изменении в развёрнутом кластере с помощью утилиты `helper change registry` (deprecated).

#### Ограничения и особенности использования модуля

Модуль `registry` имеет ряд ограничений и особенностей, касающихся установки, условий работы и переключения режимов.

##### Ограничения при установке кластера

Bootstrap кластера Deckhouse Platform Certified Security Edition поддерживается только в неконфигурируемом `Unmanaged` режиме. Настройки registry во время bootstrap задаются через [initConfiguration](/reference/api/cr.html#initconfiguration-deckhouse-imagesrepo).

Конфигурация registry через moduleConfig `deckhouse` во время bootstrap кластера Deckhouse Platform Certified Security Edition не поддерживается.

##### Ограничения по условиям работы

Модуль работает при соблюдении следующих условий:

- Если на узлах кластера используется CRI containerd или containerd v2. Для настройки CRI ознакомьтесь с конфигурацией [`ClusterConfiguration`](/reference/api/cr.html#clusterconfiguration-defaultcri).
- Кластер полностью управляется Deckhouse Platform Certified Security Edition. В Managed Kubernetes кластерах он работать не будет.

##### Ограничения по переключению режимов

Ограничения по переключению режимов следующие:

- При первом переключении необходимо выполнить миграцию пользовательских конфигураций реестра. Подробнее — в разделе [«Модуль registry: FAQ»](./faq.html).
- Переключение в неконфигурируемый `Unmanaged` режим доступно только из `Unmanaged` режима. Подробнее — в разделе [«Модуль registry: FAQ»](./faq.html).

#### Архитектура режима Direct

В режиме `Direct` запросы к registry обрабатываются напрямую, без промежуточного кэширования.

Перенаправление запросов к registry от CRI осуществляется при помощи его настроек, которые указываются в конфигурации `containerd`.

В случае таких компонентов, как `operator-trivy`, `image-availability-exporter`, `deckhouse-controller` и ряда других, обращающихся к registry напрямую, запросы будут идти через in-cluster proxy, расположенный на master-узлах.


![direct](images/direct-ru.png)

<!-- ### Proxy режим
Данный режим позволяет registry выступать в качестве промежуточного прокси-сервера между клиентом и удалённым реестром, оптимизируя доступ к часто используемым образам и уменьшая нагрузку на сеть.
Запуск кеширующего Proxy реестра осуществляется виде статических подов на узлах control plane. Для обеспечения высокой доступности к кеширующему Proxy, используется балансировщик установленный на каждый узел кластера.
Обращение к Proxy registry от CRI осуществляется через балансировщик. Настройки для обращения к балансировщику прописываются в конфигурации `containerd`.
В случае компонентов, обращающихся к реестру напрямую, таких как `operator-trivy`, `image-availability-exporter`, `deckhouse-controller` и ряда других, обращения будут идти также через кеширующий Proxy реестр. -->

<!-- ### Local режим
Данный режим позволяет создавать локальную копию registry внутри кластера. Образы из удалённого реестра полностью скопированы в локальное хранилище.
Работа локального registry идентична работы кеширующего proxy. Запуск локального registry осуществляется в виде статических подов на узлах control plane. Для обеспечения высокой доступности к локальному registry, используется балансировщик установленный на каждый узел кластера.
Обращение к локальному registry от CRI осуществляется через балансировщик. Настройки для обращения к балансировщику прописываются в конфигурации `containerd`.
В случае компонентов, обращающихся к реестру напрямую, таких как `operator-trivy`, `image-availability-exporter`, `deckhouse-controller` и ряда других, обращения будут идти также в локальный реестр.
Для наполнения локального registry образами используется инструмент d8.
-->

### Модуль registry: настройка

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Для управления работой с container registry используйте секцию [`registry`](./deckhouse/configuration.html#parameters-registry) конфигурации модуля `deckhouse`, в которой можно указывать параметры подключения к registry и управлять режимом работы с ним.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['registry'].config-values | format_module_configuration: moduleKebabName }}

### Модуль registry: пример использования

#### Переключение на режим `Direct`

Для переключения уже работающего кластера на режим `Direct` выполните следующие шаги:

{% alert level="danger" %}
При изменении режима registry или параметров registry, Deckhouse будет перезапущен.
{% endalert %}

1. Перед переключением выполните [миграцию на использование модуля `registry`](faq.html#как-мигрировать-на-модуль-registry).

1. Убедитесь, что модуль `registry` включен и работает. Для этого выполните следующую команду:

   ```bash
   d8 k get module registry -o wide
   ```

   Пример вывода:

   ```console
   NAME       WEIGHT ...  PHASE   ENABLED   DISABLED MESSAGE   READY
   registry   38     ...  Ready   True                         True
   ```

1. Убедитесь, что все master-узлы находятся в состоянии `Ready` и не имеют статуса `SchedulingDisabled`, используя следующую команду:

   ```bash
   d8 k get nodes
   ```

   Пример вывода:

   ```console
   NAME       STATUS   ROLES                 ...
   master-0   Ready    control-plane,master  ...
   master-1   Ready    control-plane,master  ...
   master-2   Ready    control-plane,master  ...
   ```

   Пример вывода, когда master-узел (`master-2` в примере) находится в статусе `SchedulingDisabled`:

   ```console
   NAME       STATUS                      ROLES                 ...
   master-0   Ready    control-plane,master  ...
   master-1   Ready    control-plane,master  ...
   master-2   Ready,SchedulingDisabled    control-plane,master  ...
   ```

1. Проверьте, чтобы очередь Deckhouse была пустой и без ошибок.

1. Установите настройки режима `Direct` в ModuleConfig `deckhouse`. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [`deckhouse`](./deckhouse/) для корректной настройки.

   Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Direct
         direct:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Проверьте статус переключения registry в секрете `registry-state`, используя [инструкцию](faq.html#как-посмотреть-статус-переключения-режима-registry).

   Пример вывода:

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Direct
   target_mode: Direct
   ```

#### Переключение на режим `Unmanaged`

Для переключения уже работающего кластера на режим `Unmanaged` выполните следующие шаги:

{% alert level="danger" %}
При изменении режима registry или параметров registry, Deckhouse будет перезапущен.
{% endalert %}

1. Перед переключением выполните [миграцию на использование модуля `registry`](faq.html#как-мигрировать-на-модуль-registry).

1. Убедитесь, что модуль `registry` включен и работает. Для этого выполните следующую команду:

   ```bash
   d8 k get module registry -o wide
   ```

   Пример вывода:

   ```console
   NAME       WEIGHT ...  PHASE   ENABLED   DISABLED MESSAGE   READY
   registry   38     ...  Ready   True                         True
   ```

1. Проверьте, чтобы очередь Deckhouse была пустой и без ошибок.

1. Установите настройки режима `Unmanaged` в ModuleConfig `deckhouse`. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [`deckhouse`](./deckhouse/) для корректной настройки.

   Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Проверьте статус переключения registry в секрете `registry-state`, используя [инструкцию](faq.html#как-посмотреть-статус-переключения-режима-registry).

   Пример вывода:

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. При необходимости переключения на старый метод управления registry, ознакомьтесь с [инструкцией](faq.html#как-мигрировать-обратно-с-модуля-registry).

{% alert level="warning" %}
Это устаревший (deprecated) формат управления registry.
{% endalert %}

### Модуль registry: FAQ

#### Как мигрировать на модуль registry?

Во время миграции, для containerd v1 будет выполнен переход на новую схему конфигурации registry.
containerd v2 использует новую схему по умолчанию. Подробнее можно ознакомиться в разделе [с описанием способов конфигурации](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry)

##### Для containerd v2

1. Выполните переключение на использование модуля `registry`. Для этого, укажите в `moduleConfig` `deckhouse` параметры `Unmanaged` режима. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [deckhouse](./deckhouse/) для корректной настройки.

   Посмотреть текущие настройки реестра можно с помощью команды:

   ```bash
   d8 k -n d8-system exec -it svc/deckhouse-leader -c deckhouse -- deckhouse-controller global values | yq e '.modulesImages.registry' -
   ```

   Данные настройки укажите при конфигурации `Unmanaged` режима:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Дождитесь завершения переключения. Пример [статуса переключения](./faq.html#как-посмотреть-статус-переключения-режима-registry):

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

##### Для containerd v1

{% alert level="danger" %}
- Во время переключения containerd v1 сервис будет перезапущен.
- Во время переключения containerd v1 будет переведен на новую схему конфигурации registry.
- Во время переключения, [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry) для containerd v1 будут временно недоступны.
{% endalert %}

1. Убедитесь, что на узлах с containerd v1 отсутствуют [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry), расположенные в директории `/etc/containerd/conf.d`.

1. Если конфигурации присутствуют, необходимо выполнить миграцию на новый формат конфигурации registry в containerd. Для этого, необходимо добавить новые конфигурации в директорию `/etc/containerd/registry.d`. Данные конфигурации вступят в силу после переключения на модуль `registry`. Для добавления конфигураций подготовьте `NodeGroupConfiguration`, подробнее в разделе [с описанием способов конфигурации](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry). Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: containerd-additional-config-auth.sh
   spec:
     # Шаг может быть любой, т.к. не требуется перезапуск сервиса containerd
     weight: 0
     bundles:
       - '*'
     nodeGroups:
       - "*"
     content: |
       # Copyright 2023 Flant JSC
       #
       # Licensed under the Apache License, Version 2.0 (the "License");
       # you may not use this file except in compliance with the License.
       # You may obtain a copy of the License at
       #
       #     http://www.apache.org/licenses/LICENSE-2.0
       #
       # Unless required by applicable law or agreed to in writing, software
       # distributed under the License is distributed on an "AS IS" BASIS,
       # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       # See the License for the specific language governing permissions and
       # limitations under the License.
       
       REGISTRY_URL=private.registry.example

       mkdir -p "/etc/containerd/registry.d/${REGISTRY_URL}"
       bb-sync-file "/etc/containerd/registry.d/${REGISTRY_URL}/hosts.toml" - << EOF
       [host]
         [host."https://${REGISTRY_URL}"]
           capabilities = ["pull", "resolve"]
           [host."https://${REGISTRY_URL}".auth]
             username = "username"
             password = "password"
       EOF
   ```

1. Примените `NodeGroupConfiguration`. Дождитесь появления конфигурационных файлов в директории `/etc/containerd/registry.d` на всех узлах.

1. Проверьте корректность работы конфигураций. Для этого воспользуйтесь командой:

   ```bash
   # Для https:
   ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ private.registry.example/registry/path:tag

   # Для http:
   ctr -n k8s.io images pull --hosts-dir=/etc/containerd/registry.d/ --plain-http private.registry.example/registry/path:tag
   ```

1. Выполните переключение на использование модуля `registry`. Для этого, укажите в `moduleConfig` `deckhouse` параметры `Unmanaged` режима. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [deckhouse](./deckhouse/) для корректной настройки.

   Посмотреть текущие настройки реестра можно с помощью команды:

   ```bash
   d8 k -n d8-system exec -it svc/deckhouse-leader -c deckhouse -- deckhouse-controller global values | yq e '.modulesImages.registry' -
   ```

   Данные настройки укажите при конфигурации `Unmanaged` режима:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. После применения, дождитесь в [статусе переключения](faq.html#как-посмотреть-статус-переключения-режима-registry) сообщение:

   Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "2025-08-13T15:22:34Z"
     message: |
       Check current nodes configuration
       2/2 node(s) Unready:
       - master-0: has custom toml merge containerd configuration
       - worker-5e389be0-578df-s5sm5: has custom toml merge containerd configuration
     reason: Processing
     status: "False"
     type: ContainerdConfigPreflightReady
   ```

   Данное сообщение означает, что на узлах имеются старые конфигурации реестров, расположенные в директории `/etc/containerd/conf.d`. И в данный момент переключение на новую конфигурацию containerd заблокировано. Для того, чтобы разрешить переключение, необходимо удалить старые конфигурационные файлы.

1. Удалите старые конфигурационные файлы, чтобы разрешить переключение на модуль `registry`. Для этого создайте `NodeGroupConfiguration`, пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: containerd-additional-config-auth.sh
   spec:
     # Шаг должен выполниться до '032_configure_containerd.sh'
     weight: 0
     bundles:
       - '*'
     nodeGroups:
       - "*"
     content: |
       # Copyright 2023 Flant JSC
       #
       # Licensed under the Apache License, Version 2.0 (the "License");
       # you may not use this file except in compliance with the License.
       # You may obtain a copy of the License at
       #
       #     http://www.apache.org/licenses/LICENSE-2.0
       #
       # Unless required by applicable law or agreed to in writing, software
       # distributed under the License is distributed on an "AS IS" BASIS,
       # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       # See the License for the specific language governing permissions and
       # limitations under the License.

       file="/etc/containerd/conf.d/old-config.toml"

       [ -f "$file" ] && rm -f "$file"
   ```
  
1. После удаления старых конфигураций, убедитесь, что переключение продолжило выполняться. Пример [статуса переключения](faq.html#как-посмотреть-статус-переключения-режима-registry):

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "2025-08-13T16:42:09Z"
     message: ""
     reason: ""
     status: "True"
     type: ContainerdConfigPreflightReady
   ```

1. Дождитесь завершения переключения. Пример [статуса переключения](faq.html#как-посмотреть-статус-переключения-режима-registry):

   ```yaml
   conditions:
   # ...
     - lastTransitionTime: "..."
       message: ""
       reason: ""
       status: "True"
       type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

#### Как мигрировать обратно с модуля registry?

{% alert level="danger" %}
- Это устаревший (deprecated) формат управления registry.
- Во время переключения containerd v1 будет перезапущен.
- Во время переключения containerd v1 будет переведен на старую схему конфигурации registry.
- Во время переключения, [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry) для containerd v1 будут временно недоступны.
{% endalert %}

1. Переведите registry в режим `Unmanaged`. Если используется registry, отличный от `registry.deckhouse.ru`, ознакомьтесь с конфигурацией модуля [deckhouse](./deckhouse/) для корректной настройки.

   Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
         unmanaged:
           imagesRepo: registry.deckhouse.ru/deckhouse/ee
           scheme: HTTPS
           license: <LICENSE_KEY> # Замените на ваш лицензионный ключ
   ```

1. Проверьте статус переключения, используя [инструкцию](./faq.html#как-посмотреть-статус-переключения-режима-registry). Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "..."
     message: ""
     reason: ""
     status: "True"
     type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. Переведите registry в неконфигурируемый режим `Unmanaged`. Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: deckhouse
   spec:
     version: 1
     enabled: true
     settings:
       registry:
         mode: Unmanaged
   ```

1. Проверьте статус переключения, используя [инструкцию](./faq.html#как-посмотреть-статус-переключения-режима-registry). Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "..."
     message: ""
     reason: ""
     status: "True"
     type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. Если используется containerd v1, и в кластере применены [пользовательские конфигурации реестра](./node-manager/faq.html#как-добавить-конфигурацию-для-дополнительного-registry), их необходимо заменить на старый формат. Для этого, подготовьте конфигурации registry старого формата. Данные конфигурации на данном этапе применять не нужно. Пример конфигурации:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: NodeGroupConfiguration
   metadata:
     name: containerd-additional-config-auth.sh
   spec:
     # Для добавления файла перед шагом '032_configure_containerd.sh'
     weight: 31
     bundles:
       - '*'
     nodeGroups:
       - "*"
     content: |
       # Copyright 2023 Flant JSC
       #
       # Licensed under the Apache License, Version 2.0 (the "License");
       # you may not use this file except in compliance with the License.
       # You may obtain a copy of the License at
       #
       #     http://www.apache.org/licenses/LICENSE-2.0
       #
       # Unless required by applicable law or agreed to in writing, software
       # distributed under the License is distributed on an "AS IS" BASIS,
       # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       # See the License for the specific language governing permissions and
       # limitations under the License.

       REGISTRY_URL=private.registry.example

       mkdir -p /etc/containerd/conf.d
       bb-sync-file /etc/containerd/conf.d/additional_registry.toml - << EOF
       [plugins]
         [plugins."io.containerd.grpc.v1.cri"]
           [plugins."io.containerd.grpc.v1.cri".registry]
             [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
               [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${REGISTRY_URL}"]
                 endpoint = ["https://${REGISTRY_URL}"]
             [plugins."io.containerd.grpc.v1.cri".registry.configs]
               [plugins."io.containerd.grpc.v1.cri".registry.configs."${REGISTRY_URL}".auth]
                 username = "username"
                 password = "password"
                 # OR
                 auth = "dXNlcm5hbWU6cGFzc3dvcmQ="
       EOF
   ```

1. Удалите секрет `registry-bashible-config`. Во время удаления, containerd v1 переключится на старый формат конфигурации containerd:

   ```bash
   d8 k -n d8-system delete secret registry-bashible-config
   ```

1. После удаления дождитесь завершения переключения. Для отслеживания используйте [инструкцию](faq.html#как-посмотреть-статус-переключения-режима-registry). Пример вывода:

   ```yaml
   conditions:
   # ...
   - lastTransitionTime: "..."
     message: ""
     reason: ""
     status: "True"
     type: Ready
   hash: ..
   mode: Unmanaged
   target_mode: Unmanaged
   ```

1. Если используется containerd v1, примените заготовленные этапом ранее `NodeGroupConfiguration` с пользовательскими конфигурациями registry.

1. Отключите модуль `registry`. Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: registry
   spec:
     enabled: false
     settings: {}
     version: 1
   ```

#### Как посмотреть статус переключения режима registry?

Статус переключения режима registry можно получить с помощью следующей команды:

<!-- TODO(nabokihms): заменить на подкоманду d8, когда она будет реализована -->
```bash
d8 k -n d8-system -o yaml get secret registry-state | yq -C -P '.data | del .state | map_values(@base64d) | .conditions = (.conditions | from_yaml)'
```

Пример вывода:

```yaml
conditions:
  - lastTransitionTime: "2025-07-15T12:52:46Z"
    message: 'registry.deckhouse.ru: all 157 items are checked'
    reason: Ready
    status: "True"
    type: RegistryContainsRequiredImages
  - lastTransitionTime: "2025-07-11T11:59:03Z"
    message: ""
    reason: ""
    status: "True"
    type: ContainerdConfigPreflightReady
  - lastTransitionTime: "2025-07-15T12:47:47Z"
    message: ""
    reason: ""
    status: "True"
    type: TransitionContainerdConfigReady
  - lastTransitionTime: "2025-07-15T12:52:48Z"
    message: ""
    reason: ""
    status: "True"
    type: InClusterProxyReady
  - lastTransitionTime: "2025-07-15T12:54:53Z"
    message: ""
    reason: ""
    status: "True"
    type: DeckhouseRegistrySwitchReady
  - lastTransitionTime: "2025-07-15T12:55:48Z"
    message: ""
    reason: ""
    status: "True"
    type: FinalContainerdConfigReady
  - lastTransitionTime: "2025-07-15T12:55:48Z"
    message: ""
    reason: ""
    status: "True"
    type: Ready
mode: Direct
target_mode: Direct
```

Вывод отображает состояние процесса переключения. Каждое условие может находиться в статусе `True` или `False`, а также содержать поле `message` с пояснением.

Описание условий:

| Условие                           | Описание                                                                                                                                                                                            |
| --------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ContainerdConfigPreflightReady`  | Состояние проверки конфигурации containerd. Проверяется, что на узлах отсутствуют пользовательские auth конфигурации containerd.                                                                    |
| `TransitionContainerdConfigReady` | Состояние подготовки конфигурации containerd в новый режим. Проверяется, что конфигурация containerd успешно подготовлена и содержит одновременно конфигурации нового и старого режима.             |
| `FinalContainerdConfigReady`      | Состояние завершения переключения containerd в новый режим. Проверяется, что конфигурация containerd успешно применена и содержит конфигурацию нового режима.                                       |
| `DeckhouseRegistrySwitchReady`    | Состояние переключения Deckhouse и его компонентов на использование нового registry. Значение `True` указывает, что Deckhouse успешно переключился на сконфигурированный registry и готов к работе. |
| `InClusterProxyReady`             | Состояние готовности In-Cluster Proxy. Проверяется, что In-Cluster Proxy успешно запущен и работает.                                                                                                |
| `CleanupInClusterProxy`           | Состояние очистки In-Cluster Proxy, если прокси не нужен для работы желаемого режима. Проверяется, что все ресурсы, связанные с In-Cluster Proxy, успешно удалены.                                  |
| `Ready`                           | Общее состояние готовности registry к работе в указанном режиме. Проверяется, что все предыдущие условия выполнены и модуль готов к работе.                                                         |
## Подсистема Deckhouse

### Веб-интерфейс

Веб-интерфейс упрощает управление Deckhouse Platform Certified Security Edition и делает состояние системы наглядным.

Доступ к интерфейсу доступен всем пользователям согласно их правам в платформе.

Если шаблон публичных доменов `%s.example.com`, то в веб-приложение можно зайти по адресу
`https://console.example.com`.

![Обзор](/images/console/screenshot-sys-overview.ru.png)

#### Основные возможности

- Обзор кластера, актуальной версии, состояния системы и обновлений
- Управление модулями и их настройками
- Управление узлами: конфигурация узлов, масштабирование, параметры обновления
- Управление тенантами: проекты, созданные на основании шаблонов
- Управление доступом: провайдеры аутентификации, права групп и пользователей
- Ингресс-контроллеры: заведение трафика в кластер
- Журналирование: сбор логов с узлов и подов, отправка в различные типы хранилищ
- Мониторинг: обработка и отправка метрик, создание алертов и recording rule, дашборды и источники данных для Grafana, настройки Prometheus и список горящих алертов
- Поддержка GitOps: специально отмечены ресурсы Kubernetes, созданные автоматикой (werf, Argo CD, Helm)
- Метрики и мониторинг в узлах, группах узлов и в ингресс-контроллерах
- Состояние подов Prometheus, ингресс-контроллеров и поды на узлах
- И многое другое!

#### Как включить

Чтобы включить модуль, создайте ModuleConfig:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: console
spec:
  enabled: true
```

#### Требования к ресурсам

Потребление ресурсов подами серверной части в зависимости от количества одновременных пользователей
отображено в таблице ниже

| Пользователей | ЦП, ядра | Память, МиБ |
| ------------: | -------: | ----------: |
|             0 |   0.0005 |          18 |
|             1 |   0.0500 |          25 |
|            10 |   0.4000 |          53 |
|           100 |   0.6500 |         130 |

Ограничение на вертикальное масштабирование подов: минимальные значения CPU/памяти в 100m/100MiB и максимальные значения в 1/512MiB.
Две реплики серверной части включаются автоматически для Deckhouse Platform Certified Security Edition в режиме высокой доступности.

### Конфигурация

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['console'].config-values | format_module_configuration: moduleKebabName }}

### Модуль deckhouse

Этот модуль настраивает в Deckhouse:

- **[Уровень логирования](configuration.html#parameters-loglevel)**
- **[Набор модулей](configuration.html#parameters-bundle), включенных по умолчанию**

  Обычно используется набор модулей `Default`, который подходит в большинстве случаев.

  Независимо от используемого набора включенных по умолчанию модулей любой модуль может быть явно включен или выключен в конфигурации Deckhouse.
- **[Канал обновлений](configuration.html#parameters-releasechannel)**

  В Deckhouse реализован механизм автоматического обновления. Этот механизм использует 5 каналов обновлений, различающиеся стабильностью и частотой выхода версий.
- **[Режим обновлений](configuration.html#parameters-update-mode)** и **[окна обновлений](configuration.html#parameters-update-windows)**

  Deckhouse может использовать **ручной** или **автоматический** режим обновлений.

  В ручном режиме обновлений автоматически применяются только важные исправления (patch-релизы), и для перехода на новый релиз Deckhouse требуется [ручное подтверждение](/reference/api/cr.html#deckhouserelease-v1alpha1-approved).

  В автоматическом режиме обновлений, если в кластере **не установлены** [окна обновлений](configuration.html#parameters-update-windows), переход на новый релиз Deckhouse осуществляется сразу после его появления на соответствующем канале обновлений. Если же в кластере **установлены** окна обновлений, переход на более свежий релиз Deckhouse начнется в ближайшее доступное окно обновлений после появления новой версии на канале обновлений.
  
- **Сервис валидирования кастомных ресурсов**

  Сервис валидирования предотвращает создание кастомных ресурсов с некорректными данными или внесение таких данных в уже существующие кастомные ресурсы. Отслеживаются только ресурсы, находящиеся под управлением модулей Deckhouse.

#### Обновление релизов Deckhouse

##### Просмотр статуса релизов Deckhouse

Список последних релизов в кластере можно получить командной `d8 k get deckhousereleases`. По умолчанию хранятся 10 последних релизов и все будущие.
Каждый релиз может иметь один из следующих статусов:

- `Pending` — релиз находится в ожидании, ждет окна обновления, настроек канареечного развертывания и т. д. Подробности можно увидеть с помощью команды `d8 k describe deckhouserelease $name`.
- `Deployed` — релиз применен. Это значит, что образ пода Deckhouse уже поменялся на новую версию,
 но при этом процесс обновления всех компонентов кластера идет асинхронно, так как зависит от многих настроек.
- `Superseded` — релиз устарел и больше не используется.
- `Suspended` — релиз отменен (например, в нем обнаружилась ошибка). Релиз переходит в этот статус, если его отменили и при этом он еще не был применен в кластере.

##### Процесс обновления

В момент перехода в статус `Deployed` релиз меняет версию (tag) образа Deckhouse. После запуска Deckhouse начнет проверку и обновление всех модулей, которые поменялись с предыдущего релиза. Длительность обновления зависит от настроек и размера кластера.
Например, если у вас много `NodeGroup`, они будут обновляться продолжительное время, если много `IngressNginxController` — они будут
обновляться по одному и это тоже займет некоторое время.

##### Ручное применение релизов

Если выбран [ручной режим обновления](usage.html#ручное-подтверждение-обновлений) и скопилось несколько релизов,
их можно сразу одобрить к применению. В этом случае Deckhouse будет обновляться последовательно, сохраняя порядок релизов и меняя статус каждого примененного релиза.

##### Закрепление релиза

Под *закреплением* релиза подразумевается полное или частичное отключение автоматического обновления версий Deckhouse.

Есть три варианта ограничения автоматического обновления Deckhouse:

- Установить ручной режим обновления.

  В этом случае вы остановитесь на текущей версии, сможете получать обновления в кластер, но для применения обновления необходимо будет выполнить [ручное действие](usage.html#ручное-подтверждение-обновлений). Это относится и к patch-версиям, и к минорным версиям.
  
  Для установки ручного режима обновления необходимо в ModuleConfig `deckhouse` установить параметр [settings.update.mode](configuration.html#parameters-update-mode) в `Manual`:

  ```shell
  d8 k patch mc deckhouse --type=merge -p='{"spec":{"settings":{"update":{"mode":"Manual"}}}}'
  ```
  
- Установить режим автоматического обновления для патч-версий.

  В этом случае вы остановитесь на текущем релизе, но будете получать patch-версии текущего релиза (с учетом установленных окон обновлений). Для применения обновления минорной версии релиза необходимо будет выполнить [ручное действие](usage.html#ручное-подтверждение-обновлений).
  
  Например: текущая версия Deckhouse Platform Certified Security Edition `v1.70.1`, после установки режима автоматического обновления для патч-версий, Deckhouse сможет обновиться до версии `v1.70.1`, но не будет обновляться до версии `v1.71.*` и выше.
  
  Для установки режима автоматического обновления для патч-версий необходимо в ModuleConfig `deckhouse` установить параметр [settings.update.mode](configuration.html#parameters-update-mode) в `AutoPatch`:

  ```shell
  d8 k patch mc deckhouse --type=merge -p='{"spec":{"settings":{"update":{"mode":"AutoPatch"}}}}'
  ```

- Установить конкретный тег для Deployment `deckhouse` и удалить параметр [releaseChannel](configuration.html#parameters-releasechannel) из конфигурации модуля `deckhouse`.

  В таком случае Deckhouse Platform Certified Security Edition останется на конкретной версии, никакой информации о новых доступных версиях (объекты DeckhouseRelease) в кластере появляться не будет.

  Пример установки версии `v1.66.3` для Deckhouse Platform Certified Security Edition EE и удаления параметра `releaseChannel` из конфигурации модуля `deckhouse`:

  ```shell
  d8 k -ti -n d8-system exec svc/deckhouse-leader -c deckhouse -- kubectl set image deployment/deckhouse deckhouse=registry.deckhouse.ru/deckhouse/ee:v1.66.3
  d8 k patch mc deckhouse --type=json -p='[{"op": "remove", "path": "/spec/settings/releaseChannel"}]'
  ```

#### Priority Classes

Модуль создает в кластере набор классов приоритета (PriorityClass) и назначает их компонентам, установленным Deckhouse, и приложениям в кластере.

Функциональность классов приоритета реализуется планировщиком (scheduler), который позволяет учитывать приоритет пода (определяемый его принадлежностью к классу) при планировании.

Например, при развертывании в кластере подов с `priorityClassName: production-low`, если в кластере не будет доступных ресурсов для данного пода, Kubernetes начнет вытеснять поды с наименьшим приоритетом.
То есть сначала будут вытеснены все поды с `priorityClassName: develop`, затем — с `cluster-low` и так далее.

При указании класса приоритета очень важно понимать тип приложения и окружение, в котором оно будет работать. Указание любого класса приоритета не уменьшит его фактический приоритет, так как если у пода не установлен приоритет, то планировщик считает его самым низким.

{% alert level="warning" %}
Нельзя использовать классы приоритета `system-node-critical`, `system-cluster-critical`, `cluster-medium`, `cluster-low`.
{% endalert %}

Устанавливаемые модулем классы приоритета (в порядке приоритета от высшего к низшему):

| Класс приоритета          | Описание                                                                                                                                                                                                                                                                                                                                                              | Значение   |
|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| `system-node-critical`    | Компоненты кластера, которые обязаны присутствовать на узле. Также полностью защищает от вытеснения kubelet'ом.<br>Примеры: `node-exporter`, `csi` и другие.                                                                                                                                  | 2000001000 |
| `system-cluster-critical` | Компоненты кластера, без которых его корректная работа невозможна. Этим PriorityClass'ом обязательно помечаются MutatingWebhooks и Extension API servers. Также полностью защищает от вытеснения kubelet'ом.<br>Примеры: `kube-dns`, `kube-proxy`, `cni-flannel`, `cni-cillium` и другие.     | 2000000000 |
| `production-high`         | Stateful-приложения, отсутствие которых в production-окружении приводит к полной недоступности сервиса или потере данных.<br>Примеры: `PostgreSQL`, `Memcached`, `Redis`, `MongoDB` и другие.                                                                                                                                                                         | 9000       |
| `cluster-medium`          | Компоненты кластера, влияющие на мониторинг (алерты, диагностика) и автомасштабирование. Без мониторинга невозможно оценить масштабы происшествия, без автомасштабирования — предоставить приложениям необходимые ресурсы.<br>Примеры: `deckhouse`, `node-local-dns`, `grafana`, `upmeter` и другие.                                                                  | 7000       |
| `production-medium`       | Основные stateless-приложения в production-окружении, которые отвечают за работу сервиса для посетителей.                                                                                                                                                                                                                                                             | 6000       |
| `deployment-machinery`    | Компоненты кластера, используемые для сборки и деплоя в кластер.                                                                                                                                                                                                                                                                                                      | 5000       |
| `production-low`          | Приложения в production-окружении (cron-задания, административные панели, batch-процессы), без которых можно обойтись некоторое время. Если batch или cron-задачи нельзя прерывать, их следует отнести к `production-medium`.                                                                                                                                         | 4000       |
| `staging`                 | Staging-окружения для приложений.                                                                                                                                                                                                                                                                                                                                     | 3000       |
| `cluster-low`             | Компоненты кластера, без которых эксплуатация возможна, но которые желательны. <br>Примеры: `dashboard`, `cert-manager`, `prometheus` и другие.                                                                                                                                                                                                                       | 2000       |
| `develop` (по умолчанию)  | Develop-окружения для приложений. Класс по умолчанию, если не указан иной класс.                                                                                                                                                                                                                                                                                      | 1000       |
| `standby`                 | Класс не предназначен для приложений. Используется в системных целях для резервирования узлов.                                                                                                                                                                                                                                                                        | -1         |

### Модуль deckhouse: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['deckhouse'].config-values | format_module_configuration: moduleKebabName }}

### Модуль deckhouse: FAQ

#### Как собрать информацию для отладки?

Мы всегда рады помочь пользователям с расследованием сложных проблем. Пожалуйста, выполните следующие шаги, чтобы мы смогли вам помочь:

1. Выполните следующую команду, чтобы собрать необходимые данные:

   ```sh
   d8 s collect-debug-info > deckhouse-debug-$(date +"%Y_%m_%d").tar.gz
   ```

2. Отправьте получившийся архив команде Deckhouse для дальнейшего расследования.

Данные, которые будут собраны:

<table>
  <thead>
    <tr>
      <th>Категория</th>
      <th>Собираемые данные</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Deckhouse</strong></td>
      <td>
        <ul>
          <li>Состояние очереди Deckhouse</li>
          <li>Deckhouse values (за исключением значений <code>kubeRBACProxyCA</code> и <code>registry.dockercfg</code>)</li>
          <li>Данные о текущей версии пода <code>deckhouse</code></li>
          <li>Все объекты DeckhouseRelease</li>
          <li>Логи подов Deckhouse</li>
          <li>Манифесты контроллеров и подов из всех пространств имен Deckhouse</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Объекты кластера</strong></td>
      <td>
        Все объекты следующих ресурсов:
        <ul>
          <li>NodeGroup</li>
          <li>NodeGroupConfiguration</li>
          <li>Node</li>
          <li>Machine</li>
          <li>Instance</li>
          <li>StaticInstance</li>
          <li>MachineDeployment</li>
          <li>ClusterAuthorizationRule</li>
          <li>AuthorizationRule</li>
          <li>ModuleConfig</li>
        </ul>
        А также Events из всех пространств имен
      </td>
    </tr>
    <tr>
      <td><strong>Модули и их состояния</strong></td>
      <td>
        <ul>
          <li>Список включенных модулей</li>
          <li>Список объектов ModuleSource в кластере</li>
          <li>Список объектов ModulePullOverride в кластере</li>
          <li>Список модулей в режиме <code>maintenance</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Логи и манифесты контроллеров</strong></td>
      <td>
        Логи следующих компонентов:
        <ul>
          <li><code>machine-controller-manager</code></li>
          <li><code>cloud-controller-manager</code></li>
          <li><code>csi-controller</code></li>
          <li><code>cluster-autoscaler</code></li>
          <li>Vertical Pod Autoscaler admission controller</li>
          <li>Vertical Pod Autoscaler recommender</li>
          <li>Vertical Pod Autoscaler updater</li>
        </ul>
        YAML-файлы следующих контроллеров:
        <ul>
          <li><code>capi-controller-manager</code></li>
          <li><code>caps-controller-manager</code></li>
          <li><code>machine-controller-manager</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Мониторинг и алерты</strong></td>
      <td>
        <ul>
          <li>Логи Prometheus</li>
          <li>Все горящие уведомления в Prometheus</li>
          <li>Список всех подов, которые не находятся в состоянии <code>Running</code>, кроме подов в состояниях <code>Completed</code> и <code>Evicted</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Сеть</strong></td>
      <td>
        <ul>
          <li>Все объекты из пространства имен <code>d8-istio</code></li>
          <li>Все кастомные ресурсы <code>istio</code></li>
          <li>Конфигурация Envoy для <code>istio</code></li>
          <li>Логи <code>istio</code></li>
          <li>Логи <code>istio</code> ingress gateway</li>
          <li>Логи <code>istio</code> users</li>
          <li>Состояние соединения Cilium (<code>cilium health status</code>)</li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

#### Как отлаживать проблемы в подах с помощью ephemeral containers?

Выполните следующую команду:

```shell
d8 k -n <namespace_name> debug -it <pod_name> --image=ubuntu <container_name>
```

#### Как отлаживать проблемы на узлах с помощью ephemeral containers?

Выполните следующую команду:

```shell
d8 k debug node/mynode -it --image=ubuntu
```

### Модуль deckhouse-tools

Этот модуль создает веб-интерфейс со ссылками на скачивание утилиты [Deckhouse CLI]({% if site.mode != 'module' %}{{ site.canonical_url_prefix_documentation }}{% endif %}/cli/d8/) под различные операционные системы.

Адрес веб-интерфейса формируется в соответствии с шаблоном [publicDomainTemplate](/reference/api/global.html#parameters-modules-publicdomaintemplate) глобального параметра конфигурации Deckhouse (ключ `%s` заменяется на `tools`).

Например, если `publicDomainTemplate` установлен как `%s-kube.company.my`, веб-интерфейс будет доступен по адресу `tools-kube.company.my`.

### Модуль deckhouse-tools: настройки

У модуля нет обязательных настроек.

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: deckhouse-tools
spec:
  enabled: true
  version: 1
```

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['deckhouse-tools'].config-values | format_module_configuration: moduleKebabName }}

### Модуль deckhouse-tools: примеры


### Модуль documentation

Модуль `documentation` создает веб-интерфейс с документацией, соответствующей запущенной версии Deckhouse Platform Certified Security Edition.

Это может быть полезно, когда Deckhouse работает в сети с ограничением доступа в интернет.

Для получения адреса веб-интерфейса в шаблоне [publicDomainTemplate](/reference/api/global.html#parameters-modules-publicdomaintemplate) глобального параметра конфигурации Deckhouse ключ `%s` замените на `documentation`.

Например, если `publicDomainTemplate` установлен как `%s-kube.company.my`, веб-интерфейс документации будет доступен по адресу `documentation-kube.company.my`.

### Модуль documentation: настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](./user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, модуль включит базовую аутентификацию со сгенерированным паролем.

Чтобы посмотреть сгенерированный пароль, выполните команду:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values documentation -o json | jq '.internal.auth.password'
```

Чтобы сгенерировать новый пароль, удалите ресурс Secret:

```shell
d8 k -n d8-system delete secret/documentation-basic-auth
```

{% alert level="info" %}
Параметр `auth.password` больше не поддерживается.
{% endalert %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['documentation'].config-values | format_module_configuration: moduleKebabName }}

### Модуль documentation: примеры

#### Пример конфигурации модуля

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: documentation
spec:
  version: 1
  enabled: true
  settings:
    nodeSelector:
      node-role/system: ""
    tolerations:
    - key: dedicated.deckhouse.io
      operator: Equal
      value: system
    externalAuthentication:
      authURL: "https://<applicationDomain>/auth"
      authSignInURL: "https://<applicationDomain>/sign-in"
      authResponseHeaders: "Authorization"
```
## Подсистема Безопасность

### Модуль admission-policy-engine

Позволяет использовать в кластере политики безопасности согласно Pod Security Standards Kubernetes. Модуль для работы использует Gatekeeper.

Pod Security Standards определяют три политики, охватывающие весь спектр безопасности. Эти политики являются кумулятивными, то есть состоящими из набора политик, и варьируются по уровню ограничений от «неограничивающего» до «ограничивающего значительно».

{% alert level="info" %}
Модуль не применяет политики к системным пространствам имен.
{% endalert %}

Список политик, доступных для использования:
- `Privileged` — неограничивающая политика с максимально широким уровнем разрешений;
- `Baseline` — минимально ограничивающая политика, которая предотвращает наиболее известные и популярные способы повышения привилегий. Позволяет использовать стандартную (минимально заданную) конфигурацию пода;
- `Restricted` — политика со значительными ограничениями. Предъявляет самые жесткие требования к подам.

Политика кластера используемая по умолчанию определяется следующим образом:
- При установке Deckhouse версии **ниже v1.55**, для всех несистемных пространств имен используется политика по умолчанию `Privileged`;
- При установке Deckhouse версии **v1.55 и выше**, для всех несистемных пространств имен используется политика по умолчанию `Baseline`;

**Обратите внимание,** что обновление Deckhouse в кластере на версию v1.55 не вызывает автоматической смены политики по умолчанию.

Политику по умолчанию можно переопределить как глобально ([в настройках модуля](configuration.html#parameters-podsecuritystandards-defaultpolicy)), так и для каждого пространства имен отдельно (лейбл `security.deckhouse.io/pod-policy=<POLICY_NAME>` на соответствующем пространстве имен).

Пример установки политики `Restricted` для всех подов в пространстве имен `my-namespace`:

```bash
d8 k label ns my-namespace security.deckhouse.io/pod-policy=restricted
```

По умолчанию, политики Pod Security Standards применяются в режиме "Deny" и поды приложений, не удовлетворяющие данным политикам, не смогут быть запущены. Режим работы политик может быть задан как глобально для кластера так и для каждого namespace отдельно. Что бы задать режим работы политик глобально используйте [configuration](configuration.html#parameters-podsecuritystandards-enforcementaction). В случае если необходимо переопределить глобальный режим политик для определенного namespace, допускается использовать лейбл `security.deckhouse.io/pod-policy-action =<POLICY_ACTION>` на соответствующем namespace. Список допустимых режимом политик состоит из: "dryrun", "warn", "deny".

Пример установки "warn" режима политик PSS для всех подов в пространстве имен `my-namespace`:

```bash
d8 k label ns my-namespace security.deckhouse.io/pod-policy-action=warn
```

Предлагаемые модулем политики могут быть расширены. Примеры расширения политик можно найти в [FAQ](faq.html).

##### Операционные политики

Модуль предоставляет набор операционных политик и лучших практик для безопасной работы ваших приложений.
Операционные политики описываются с помощью кастомного ресурса [`OperationPolicy`](/modules/admission-policy-engine/cr.html#operationpolicy).

Мы рекомендуем устанавливать следующий минимальный набор операционных политик:

```yaml
---
apiVersion: deckhouse.io/v1alpha1
kind: OperationPolicy
metadata:
  name: common
spec:
  policies:
    allowedRepos:
      - myrepo.example.com
      - registry.deckhouse.io
    requiredResources:
      limits:
        - memory
      requests:
        - cpu
        - memory
    disallowedImageTags:
      - latest
    requiredProbes:
      - livenessProbe
      - readinessProbe
    maxRevisionHistoryLimit: 3
    imagePullPolicy: Always
    priorityClassNames:
    - production-high
    - production-low
    checkHostNetworkDNSPolicy: true
    checkContainerDuplicates: true
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          operation-policy.deckhouse.io/enabled: "true"
```

Для применения приведенной политики достаточно навесить лейбл `operation-policy.deckhouse.io/enabled: "true"` на желаемый namespace. Политика, приведенная в примере, рекомендована для использования командой Deckhouse. Аналогичным образом вы можете создать собственную политику с необходимыми настройками.

##### Политики безопасности

Модуль предоставляет возможность определять политики безопасности применимо к приложениям (контейнерам), запущенным в кластере.

Пример политики безопасности:

```yaml
---
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: mypolicy
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: true
    allowHostNetwork: true
    allowHostPID: false
    allowPrivileged: false
    allowPrivilegeEscalation: false
    allowedFlexVolumes:
    - driver: vmware
    allowedHostPorts:
    - max: 4000
      min: 2000
    allowedProcMount: Unmasked
    allowedAppArmor:
    - unconfined
    allowedUnsafeSysctls:
    - kernel.*
    allowedVolumes:
    - hostPath
    - projected
    fsGroup:
      ranges:
      - max: 200
        min: 100
      rule: MustRunAs
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
    - ALL
    runAsGroup:
      ranges:
      - max: 500
        min: 300
      rule: RunAsAny
    runAsUser:
      ranges:
      - max: 200
        min: 100
      rule: MustRunAs
    seccompProfiles:
      allowedLocalhostFiles:
      - my_profile.json
      allowedProfiles:
      - Localhost
    supplementalGroups:
      ranges:
      - max: 133
        min: 129
      rule: MustRunAs
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          enforce: mypolicy
```

Для применения приведенной политики достаточно навесить лейбл `enforce: "mypolicy"` на желаемое пространство имён.

> **Важно**. Параметры `allowPrivilegeEscalation` и `allowPrivileged` по умолчанию имеют значение `false` — даже если не указаны явно. Это означает, что контейнеры не смогут запускаться в привилегированном режиме или повышать привилегии. Чтобы разрешить такое поведение, задайте параметр в `true`.

##### Изменение ресурсов Kubernetes

Модуль позволяет использовать [кастомные ресурсы Gatekeeper](gatekeeper-cr.html) для модификации объектов в кластере, такие как:
- [AssignMetadata](gatekeeper-cr.html#assignmetadata) — для изменения секции `metadata` в ресурсе;
- [Assign](gatekeeper-cr.html#assign) — для изменения других полей, кроме `metadata`;
- [ModifySet](gatekeeper-cr.html#modifyset) — для добавления или удаления значений из списка, например аргументов для запуска контейнера.
- [AssignImage](gatekeeper-cr.html#assignimage) — для изменения параметра `image` ресурса.

### Модуль admission-policy-engine: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['admission-policy-engine'].config-values | format_module_configuration: moduleKebabName }}

### Модуль admission-policy-engine: custom resources
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}

### Модуль admission-policy-engine: Custom Resources (от Gatekeeper)
{{ site.data.schemas.admission-policy-engine.crds.native.assign-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignimage-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.assignmetadata-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.config-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.configpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constraintpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.constrainttemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplate-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.expansiontemplatepodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.modifyset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.mutatorpodstatus-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.provider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.native.syncset-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.operation-policy | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.certificatestore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.keymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedkeymanagementprovider-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedpolicy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedstore-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.namespacedverifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.policy-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.store-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.ratify.verifier-customresourcedefinition | format_crd: "admission-policy-engine" }}
{{ site.data.schemas.admission-policy-engine.crds.security-policy | format_crd: "admission-policy-engine" }}

### Модуль admission-policy-engine: FAQ

#### Как настроить альтернативные решения по управлению политиками безопасности?

Для корректной работы Deckhouse Platform Certified Security Edition необходимы расширенные привилегии на запуск и работу полезной нагрузки системных компонентов. Если вместо модуля admission-policy-engine используется альтернативное решение по управлению политиками безопасности (например, Kyverno), необходима настройка исключений для следующих пространств имен:

- `kube-system`;
- все пространства имен с префиксом `d8-*` (например, `d8-system`).

#### Как расширить политики Pod Security Standards?

{% alert level="info" %}
Pod Security Standards реагируют на label `security.deckhouse.io/pod-policy: restricted` или `security.deckhouse.io/pod-policy: baseline`.
{% endalert %}

Чтобы расширить политику Pod Security Standards, добавив к существующим проверкам политики свои собственные, необходимо:

- создать шаблон проверки (ресурс `ConstraintTemplate`);
- привязать его к политике `restricted` или `baseline`.

Пример шаблона для проверки адреса репозитория образа контейнера:

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package d8.pod_security_standards.extended

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, input.parameters.repos])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.initContainers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, input.parameters.repos])
        }
```

Пример привязки проверки к политике `restricted`:

```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: prod-repo
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaceSelector:
      matchLabels:
        security.deckhouse.io/pod-policy: restricted
  parameters:
    repos:
      - "mycompany.registry.com"
```

Пример демонстрирует настройку проверки адреса репозитория в поле `image` у всех подов, создающихся в пространстве имен, имеющих label `security.deckhouse.io/pod-policy: restricted`. Если адрес в поле `image` создаваемого пода начинается не с `mycompany.registry.com`, под создан не будет.

Больше примеров описания проверок для расширения политики можно найти в библиотеке Gatekeeper.

#### Как включить одну или несколько политик Pod Security Standards, не отключая весь набор?

Чтобы применить только нужные политики безопасности, не отключая весь предустановленный набор:

1. Добавьте в нужное пространство имён метку: `security.deckhouse.io/pod-policy: privileged`, чтобы отключить встроенный набор политик.
1. Создайте ресурс SecurityPolicy, соответствующий уровню baseline или restricted. В секции `policies` укажите только необходимые вам настройки.
1. Добавьте в пространство имён дополнительную метку, которая будет соответствовать селектору `namespaceSelector` в SecurityPolicy. В примерах ниже это `security-policy.deckhouse.io/baseline-enabled: "true"` либо `security-policy.deckhouse.io/restricted-enabled: "true"`

SecurityPolicy, соответствующая baseline:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: baseline
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowPrivilegeEscalation: true
    allowPrivileged: false
    allowedAppArmor:
      - runtime/default
      - localhost/*
    allowedCapabilities:
      - AUDIT_WRITE
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - FSETID
      - KILL
      - MKNOD
      - NET_BIND_SERVICE
      - SETFCAP
      - SETGID
      - SETPCAP
      - SETUID
      - SYS_CHROOT
    allowedHostPaths: []
    allowedHostPorts:
      - max: 0
        min: 0
    allowedProcMount: Default
    allowedUnsafeSysctls:
      - kernel.shm_rmid_forced
      - net.ipv4.ip_local_port_range
      - net.ipv4.ip_unprivileged_port_start
      - net.ipv4.tcp_syncookies
      - net.ipv4.ping_group_range
      - net.ipv4.ip_local_reserved_ports
      - net.ipv4.tcp_keepalive_time
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_keepalive_intvl
      - net.ipv4.tcp_keepalive_probes
    seLinux:
      - type: ""
      - type: container_t
      - type: container_init_t
      - type: container_kvm_t
      - type: container_engine_t
    seccompProfiles:
      allowedProfiles:
        - RuntimeDefault
        - Localhost
        - undefined
        - ''
      allowedLocalhostFiles:
        - '*'
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          security-policy.deckhouse.io/baseline-enabled: "true"
```

SecurityPolicy, соответствующая restricted:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: restricted
spec:
  enforcementAction: Deny
  policies:
    allowHostIPC: false
    allowHostNetwork: false
    allowHostPID: false
    allowPrivilegeEscalation: false
    allowPrivileged: false
    allowedAppArmor:
      - runtime/default
      - localhost/*
    allowedCapabilities:
      - NET_BIND_SERVICE
    allowedHostPaths: []
    allowedHostPorts:
      - max: 0
        min: 0
    allowedProcMount: Default
    allowedUnsafeSysctls:
      - kernel.shm_rmid_forced
      - net.ipv4.ip_local_port_range
      - net.ipv4.ip_unprivileged_port_start
      - net.ipv4.tcp_syncookies
      - net.ipv4.ping_group_range
      - net.ipv4.ip_local_reserved_ports
      - net.ipv4.tcp_keepalive_time
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_keepalive_intvl
      - net.ipv4.tcp_keepalive_probes
    allowedVolumes:
      - configMap
      - csi
      - downwardAPI
      - emptyDir
      - ephemeral
      - persistentVolumeClaim
      - projected
      - secret
    requiredDropCapabilities:
      - ALL
    runAsUser:
      rule: MustRunAsNonRoot
    seLinux:
      - type: ""
      - type: container_t
      - type: container_init_t
      - type: container_kvm_t
      - type: container_engine_t
    seccompProfiles:
      allowedProfiles:
        - RuntimeDefault
        - Localhost
      allowedLocalhostFiles:
        - '*'
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          security-policy.deckhouse.io/restricted-enabled: "true"
```

#### Что, если несколько политик (операционных или безопасности) применяются на один объект?

В этом случае необходимо, чтобы конфигурация объекта соответствовала всем политикам, которые на него распространяются.

Например, рассмотрим две следующие политики безопасности:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: foo
spec:
  enforcementAction: Deny
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          name: test
  policies:
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
    - MKNOD
---
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: bar
spec:
  enforcementAction: Deny
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          name: test
  policies:
    requiredDropCapabilities:
    - NET_BIND_SERVICE
```

Тогда для выполнения требований приведенных политик безопасности в спецификации контейнера нужно указать:

```yaml
    securityContext:
      capabilities:
        drop:
          - MKNOD
          - NET_BIND_SERVICE
      readOnlyRootFilesystem: true
```

#### Проверка подписи образов

{% alert level="warning" %}Доступно в следующих редакциях: SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

В модуле реализована функция проверки подписи образов контейнеров, подписанных с помощью инструмента Cosign. Проверка подписи образов контейнеров позволяет убедиться в их целостности (что образ не был изменен после его создания) и подлинности (что образ был создан доверенным источником). Включить проверку подписи образов контейнеров в кластере можно с помощью параметра [policies.verifyImageSignatures](cr.html#securitypolicy-v1alpha1-spec-policies-verifyimagesignatures) ресурса SecurityPolicy.

{% offtopic title="Как подписать образ..." %}
Шаги для подписания образа:

- Сгенерируйте ключи: `cosign generate-key-pair`
- Подпишите образ: `cosign sign --key <key> <image>`

{% endofftopic %}

Пример SecurityPolicy для настройки проверки подписи образов контейнеров:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: SecurityPolicy
metadata:
  name: verify-image-signatures
spec:
  match:
    namespaceSelector:
      labelSelector:
        matchLabels:
          kubernetes.io/metadata.name: default
  policies:
    verifyImageSignatures:
      - reference: docker.io/myrepo/*
        publicKeys:
        - |-
          -----BEGIN PUBLIC KEY-----
          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM
          5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA==
          -----END PUBLIC KEY-----
      - reference: company.registry.com/*
        dockerCfg: zxc==
        publicKeys:
        - |-
          -----BEGIN PUBLIC KEY-----
          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM
          5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA==
          -----END PUBLIC KEY-----
```

Политика не влияет на создание подов, адреса образов контейнеров которых не подходят под описанные в параметре `reference`.  Если же адрес какого-либо образа контейнера подходит под описанные в параметре `reference` политики, и образ не подписан или подпись не соответствует указанным в политике ключам, создание пода будет запрещено.

Пример вывода ошибки при создании пода с образом контейнера, не прошедшим проверку подписи:

```console
[verify-image-signatures] Image signature verification failed: nginx:1.17.2
```

#### Как запретить удаление узла без метки

> Примечание. Операции DELETE обрабатываются Gatekeeper по умолчанию.

Можно создать собственную политику Gatekeeper, запрещающую удаление узла без специальной метки. Пример ниже использует `oldObject` для проверки меток удаляемого узла:

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: d8customnodedeleteguard
spec:
  crd:
    spec:
      names:
        kind: D8CustomNodeDeleteGuard
      validation:
        openAPIV3Schema:
          type: object
          properties:
            requiredLabelKey:
              type: string
            requiredLabelValue:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package d8.custom

        is_delete { input.review.operation == "DELETE" }
        is_node { input.review.kind.kind == "Node" }

        has_required_label {
          key := input.parameters.requiredLabelKey
          val := input.parameters.requiredLabelValue
          obj := input.review.oldObject
          obj.metadata.labels[key] == val
        }

        violation[{"msg": msg}] {
          is_delete
          is_node
          not has_required_label
          msg := sprintf("Удаление Node запрещено. Добавьте метку %q=%q.", [input.parameters.requiredLabelKey, input.parameters.requiredLabelValue])
        }
---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: D8CustomNodeDeleteGuard
metadata:
  name: require-node-delete-label
spec:
  enforcementAction: warn
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Node"]
  parameters:
    requiredLabelKey: "admission.deckhouse.io/allow-delete"
    requiredLabelValue: "true"
```

### Модуль cert-manager

Устанавливает надежную и высокодоступную инсталляцию cert-manager release v1.17.1.

При установке модуля автоматически учитываются особенности кластера:

- компонент (webhook), к которому обращается `kube-apiserver`, устанавливается на master-узлы;
- в случае недоступности вебхука производится временное удаление `apiservice`, чтобы недоступность *cert-manager* не блокировала работу кластера.

Обновление самого модуля происходит в автоматическом режиме, в том числе с миграцией ресурсов cert-manager.

#### Возможности модуля cert-manager (с учетом внесенных изменений)

Модуль обеспечивает использование всех возможностей оригинального cert-manager, в том числе:

- заказ сертификатов во всех поддерживаемых источниках, таких как *Let’s Encrypt*, *HashiCorp Vault*, *Venafi*;
- выпуск самоподписанных сертификатов;
- поддержку актуальности сертификатов, автоматический перевыпуск и т. д.

Изменения в оригинальный cert-manager были внесены, чтобы поды `cm-acme-http-solver` могли выполняться на master-узлах и выделенных узлах.

#### Мониторинг

Модуль обеспечивает экспорт метрик в Prometheus для мониторинга:

- срока действия сертификатов;
- корректности перевыпуска сертификатов.

#### Роли доступа к ресурсам

В модуле предопределены несколько продуманных ролей для удобного доступа к ресурсам:

- `User` – доступ на чтение к ресурсам Certificate и Issuer в доступных ему namespace, а также к глобальным ClusterIssue;
- `Editor` – управление ресурсами Certificate и Issuer в доступных ему namespace;
- `ClusterEditor` – управление ресурсами Certificate и Issuer в любых namespace;
- `SuperAdmin` – управление внутренними служебными объектами.

### Модуль cert-manager: настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['cert-manager'].config-values | format_module_configuration: moduleKebabName }}

### Модуль cert-manager: custom resources
{{ site.data.schemas.cert-manager.crds.crd-certificaterequests | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-certificates | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-challenges | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-clusterissuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-issuers | format_crd: "cert-manager" }}
{{ site.data.schemas.cert-manager.crds.crd-orders | format_crd: "cert-manager" }}

### Модуль cert-manager: FAQ


#### Какие виды сертификатов поддерживаются?

На данный момент модуль устанавливает следующие `ClusterIssuer`:
* `letsencrypt`
* `letsencrypt-staging`
* `selfsigned`
* `selfsigned-no-trust`

Если требуется поддержка других типов сертификатов, вы можете добавить их самостоятельно.

#### Как добавить дополнительный `ClusterIssuer`?

##### В каких случаях требуется дополнительный `ClusterIssuer`?

В стандартной поставке присутствуют `ClusterIssuer`, издающие либо сертификаты из доверенного публичного удостоверяющего центра Let's Encrypt, либо самоподписанные сертификаты.

Чтобы издать сертификаты на доменное имя через Let's Encrypt, сервис требует осуществить подтверждение владения доменом.
`Cert-manager` поддерживает несколько методов для такого подтверждения при использовании `ACME`(Automated Certificate Management Environment):
* `HTTP-01` — `cert-manager` создаст временный Pod в кластере, который будет слушать на определенном URL для подтверждения владения доменом. Для его работы необходимо иметь возможность направлять внешний трафик на этот Pod, обычно через `Ingress`.
* `DNS-01` —  `cert-manager` делает TXT-запись в DNS для подтверждения владения доменом. У `cert-manager` есть встроенная поддержка популярных провайдеров DNS.

{% alert level="danger" %}
Метод `HTTP-01` не поддерживает выпуск wildcard-сертификатов.
{% endalert %}

Поставляемые `ClusterIssuers`, издающие сертификаты через Let's Encrypt, делятся на два типа:
1. `ClusterIssuer,` специфичные для используемого cloud-провайдера.  
1. `ClusterIssuer` использующие метод `HTTP-01`.  
   Добавляются автоматически, если их создание не отключено в [настройках модуля](./configuration.html#parameters-disableletsencrypt).
   * `letsencrypt`
   * `letsencrypt-staging`

Таким образом, дополнительный `ClusterIssuer` может потребоваться в случаях издания сертификатов:
1. В удостоверяющем центре (УЦ), отличном от Let's Encrypt (в т.ч. в приватном).
2. Через Let's Encrypt с помощью метода `DNS-01` через сторонний провайдер.

##### Как добавить дополнительный `Issuer` и `ClusterIssuer`, использующий HashiCorp Vault для выпуска сертификатов?

После конфигурации PKI и [включения авторизации](/modules/user-authz/), нужно:
- Создать `ServiceAccount` и скопировать ссылку на его `Secret`:

  ```shell
  d8 k create serviceaccount issuer
  
  ISSUER_SECRET_REF=$(d8 k get serviceaccount issuer -o json | jq -r ".secrets[].name")
  ```

- Создать `Issuer`:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: Issuer
  metadata:
    name: vault-issuer
    namespace: default
  spec:
    vault:
      # Если Vault разворачивался по вышеуказанной инструкции, в этом месте в инструкции опечатка.
      server: http://vault.default.svc.cluster.local:8200
      # Указывается на этапе конфигурации PKI. 
      path: pki/sign/example-dot-com 
      auth:
        kubernetes:
          mountPath: /v1/auth/kubernetes
          role: issuer
          secretRef:
            name: $ISSUER_SECRET_REF
            key: token
  EOF
  ```

- Создать ресурс `Certificate` для получения TLS-сертификата, подписанного CA Vault:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: Certificate
  metadata:
    name: example-com
    namespace: default
  spec:
    secretName: example-com-tls
    issuerRef:
      name: vault-issuer
    # Домены указываются на этапе конфигурации PKI в Vault.
    commonName: www.example.com 
    dnsNames:
    - www.example.com
  EOF
  ```

##### Как добавить `ClusterIssuer`, использующий свой или промежуточный CA для заказа сертификатов?

Для использования собственного или промежуточного CA:

- Сгенерируйте сертификат (при необходимости):

  ```shell
  openssl genrsa -out rootCAKey.pem 2048
  openssl req -x509 -sha256 -new -nodes -key rootCAKey.pem -days 3650 -out rootCACert.pem
  ```

- В пространстве имён `d8-cert-manager` создайте секрет, содержащий данные файлов сертификатов.
  Пример создания секрета с помощью команды d8 k:  

  ```shell
  d8 k create secret tls internal-ca-key-pair -n d8-cert-manager --key="rootCAKey.pem" --cert="rootCACert.pem"
  ```

  Пример создания секрета из YAML-файла (содержимое файлов сертификатов должно быть закодировано в Base64):  

  ```yaml
  apiVersion: v1
  data:
    tls.crt: <результат команды `cat rootCACert.pem | base64 -w0`>
    tls.key: <результат команды `cat rootCAKey.pem | base64 -w0`>
  kind: Secret
  metadata:
    name: internal-ca-key-pair
    namespace: d8-cert-manager
  type: Opaque
  ```

  Имя секрета может быть любым.

- Создайте `ClusterIssuer` из созданного секрета:

  ```yaml
  apiVersion: cert-manager.io/v1
  kind: ClusterIssuer
  metadata:
    name: inter-ca
  spec:
    ca:
      secretName: internal-ca-key-pair    # Имя созданного секрета.
  ```

  Имя `ClusterIssuer` также может быть любым.

Теперь можно использовать созданный `ClusterIssuer` для получения сертификатов для всех компонентов Deckhouse или конкретного компонента.

Например, чтобы использовать `ClusterIssuer` для получения сертификатов для всех компонентов Deckhouse, укажите его имя в глобальном параметре [clusterIssuerName](/reference/api/global.html#parameters-modules-https-certmanager-clusterissuername) (`d8 k edit mc global`):

  ```yaml
  spec:
    settings:
      modules:
        https:
          certManager:
            clusterIssuerName: inter-ca
          mode: CertManager
        publicDomainTemplate: '%s.<public_domain_template>'
    version: 1
  ```

#### Как защитить учетные данные `cert-manager`?

Если вы не хотите хранить учетные данные конфигурации Deckhouse (например, по соображениям безопасности), можете создать
свой собственный `ClusterIssuer` / `Issuer`.

- Создайте Secret с учетными данными:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: v1
  kind: Secret
  type: Opaque
  metadata:
    name: XXX
    namespace: default
  data:
    secret-access-key: {{ "MY-ACCESS-KEY-TOKEN" | b64enc | quote }}
  EOF
  ```

- Создайте простой `ClusterIssuer` со ссылкой на этот Secret:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: ClusterIssuer
  metadata:
    name: XXX
    namespace: default
  spec:
    acme:
      server: https://acme-v02.api.letsencrypt.org/directory
      privateKeySecretRef:
        name: tls-key
      solvers:
      - dns01:
          <solver>:
            region: us-east-1
            accessKeyID: {{ "MY-ACCESS-KEY-ID" }}
            secretAccessKeySecretRef:
              name: XXX
              key: secret-access-key
  EOF
  ```

- Закажите сертификаты как обычно, используя созданный `ClusterIssuer`:

  ```shell
  d8 k apply -f - <<EOF
  apiVersion: cert-manager.io/v1
  kind: Certificate
  metadata:
    name: example-com
    namespace: default
  spec:
    secretName: example-com-tls
    issuerRef:
      name: XXX
    commonName: www.example.com 
    dnsNames:
    - www.example.com
  EOF
  ```

#### Работает ли старая аннотация TLS-acme?

Да, работает. Специальный компонент `cert-manager-ingress-shim` видит эти аннотации и на их основании автоматически создает ресурсы `Certificate` (в тех же namespaces, что и Ingress-ресурсы с аннотациями).

> **Важно!** При использовании аннотации ресурс Certificate создается «прилинкованным» к существующему Ingress-ресурсу, и для прохождения Challenge НЕ создается отдельный Ingress, а вносятся дополнительные записи в существующий. Это означает, что если на основном Ingress'е настроена аутентификация или whitelist — ничего не выйдет. Лучше не использовать аннотацию и переходить на ресурс Certificate.
>
> **Важно!** При переходе с аннотации на ресурс Certificate нужно удалить ресурс Certificate, который был создан по аннотации. Иначе по обоим ресурсам Certificate будет обновляться один Secret, и это может привести к достижению лимита запросов Let’s Encrypt.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/tls-acme: "true"           # Аннотация.
  name: example-com
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: site
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  - host: www.example.com                    # Дополнительный домен.
    http:
      paths:
      - backend:
          service:
            name: site
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  - host: admin.example.com                  # Еще один дополнительный домен.
    http:
      paths:
      - backend:
          service:
            name: site
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - example.com
    - www.example.com                        # Дополнительный домен.
    - admin.example.com                      # Еще один дополнительный домен.
    secretName: example-com-tls              # Имя для Certificate и Secret.
```

#### Как получить список сертификатов?

```shell
d8 k get certificate --all-namespaces

NAMESPACE          NAME                            AGE
default            example-com                     13m
```

#### Что делать, если появляется ошибка: CAA record does not match issuer?

Если `cert-manager` не может заказать сертификаты с ошибкой:

```text
CAA record does not match issuer
```

то необходимо проверить `CAA (Certificate Authority Authorization)` DNS-запись у домена, для которого заказывается сертификат.
Если вы хотите использовать Let’s Encrypt-сертификаты, у домена должна быть CAA-запись: `issue "letsencrypt.org"`.

### Проверка хеш суммы образа

#### Описание

Для проверки целостности образа используется контрольная сумма расчитанная по алгоритму Стрибог (ГОСТ Р 34.11-2012)
Чтобы устанавлеваемые образы проверялись, необнодимо добавить метку ```gost-integrity-controller.deckhouse.io/gost-digest-validation-enabled: true``` в пространство имен кластера где необходимо производить контроль целостности образа.

Пример:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    gost-integrity-controller.deckhouse.io/gost-digest-validation-enabled: "true"
  name: default
```

В случае если во время проверки контрольная сумма образа будет некорректная, будет отказано в установке образа, о чем вы получите сообщение.

Если образ находится в закрытом репозитории, для авторизации необходимо указать в спецификации контейнера параметр ```imagePullSecrets```. И создать секрет с данными для авторизации.

#### Алгоритм расчета контрольной суммы

Для расчета контрольной суммы берется список контрольных сумм слоев образа.  Список сортируется в порядке возрастания и склеивается в одну строку. Затем производится расчет контрольной суммы от этой строки по алгоритму Стрибог (ГОСТ Р 34.11-2012).

Пример расчета контрольной суммы образа nginx:1.25.2:

```text
Контрольные суммы слоев отсортированные в порядке возрастания
[
    "sha256:27e923fb52d31d7e3bdade76ab9a8056f94dd4bc89179d1c242c0e58592b4d5c",
    "sha256:360eba32fa65016e0d558c6af176db31a202e9a6071666f9b629cb8ba6ccedf0",
    "sha256:72de7d1ce3a476d2652e24f098d571a6796524d64fb34602a90631ed71c4f7ce",
    "sha256:907d1bb4e9312e4bfeabf4115ef8592c77c3ddabcfddb0e6250f90ca1df414fe",
    "sha256:94f34d60e454ca21cf8e5b6ca1f401fcb2583d09281acb1b0de872dba2d36f34",
    "sha256:c5903f3678a7dec453012f84a7d04f6407129240f12a8ebc2cb7df4a06a08c4f",
    "sha256:e42dcfe1730ba17b27138ea21c0ab43785e4fdbea1ee753a1f70923a9c0cc9b8"
]

Склеенная строка из контрольных сумм
"sha256:27e923fb52d31d7e3bdade76ab9a8056f94dd4bc89179d1c242c0e58592b4d5csha256:360eba32fa65016e0d558c6af176db31a202e9a6071666f9b629cb8ba6ccedf0sha256:72de7d1ce3a476d2652e24f098d571a6796524d64fb34602a90631ed71c4f7cesha256:907d1bb4e9312e4bfeabf4115ef8592c77c3ddabcfddb0e6250f90ca1df414fesha256:94f34d60e454ca21cf8e5b6ca1f401fcb2583d09281acb1b0de872dba2d36f34sha256:c5903f3678a7dec453012f84a7d04f6407129240f12a8ebc2cb7df4a06a08c4fsha256:e42dcfe1730ba17b27138ea21c0ab43785e4fdbea1ee753a1f70923a9c0cc9b8"

Контрольная сумма образа

2f538c22adbdb2ca8749cdafc27e94baed8645c69d4f0745fc8889f0e1f5a3f9
```

Контрольную сумму в образ можно добавить используя утилиту crane

```bash
crane mutate --annotation gost-digest=1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937 <образ>
```

Для расчета, добавления и проверки контрольной суммы образа можно использовать утилиту gost-image-digest <https://github.com/deckhouse/gost-image-digest>.

Расчет контрольной суммы

```bash
imagedigest calculate nginx:1.25.2
1:14PM INF GOST Image Digest: 2f538c22adbdb2ca8749cdafc27e94baed8645c69d4f0745fc8889f0e1f5a3f9
```

Расчет контрольной суммы с последующим добавлением в метаданные образа и сохранением в репозитории.

```bash
imagedigest add alekseysu/simple-http:v0.2
1:19PM INF GOST Image Digest: 1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937
1:19PM INF Added successfully
```

Проверка контрольной суммы

```bash
imagedigest validate alekseysu/simple-http:v0.2
2:08PM INF GOST Image Digest from image 1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937
2:08PM INF Calculated GOST Image Digest 1aa84f6d91cc080fe198da7a6de03ca245aea0a8066a6b4fb5a93e40ebec2937
2:08PM INF Validate successfully
```

### Модуль multitenancy-manager

#### Описание

Модуль позволяет создавать проекты в кластере Kubernetes. **Проект** — это изолированное окружение, в котором можно развернуть приложения.

#### Для чего это нужно?

Стандартный ресурс `Namespace`, который используется для логического разделения ресурсов в Kubernetes, не предоставляет необходимых функций, поэтому не является изолированным окружением:

* Потребление ресурсов подами по умолчанию не ограничено;
* Сетевое взаимодействие с другими подами по умолчанию работает из любой точки кластера;
* Неограниченный доступ к ресурсам узла: адресное пространство, сетевое пространство, смонтированные директории хоста.

Возможности настройки пространств имен `Namespace` не полностью соответствуют современным требованиям к разработке. По умолчанию для `Namespace` не включены следующие функции:

* Сборка логов;
* Аудит;
* Сканирование уязвимостей.

Функционал проектов позволяет решить эти проблемы.

{% alert level="warning" %}
Модуль [`secret-copier`](./secret-copier/) не может использоваться совместно с модулем `multitenancy-manager`.
{% endalert %}

#### Преимущества модуля

Для администраторов платформы:

* **Единообразие**: Администраторы могут создавать проекты, используя один и тот же шаблон, что обеспечивает единообразие и упрощает управление.
* **Безопасность**: Проекты обеспечивают изоляцию ресурсов и политик доступа между различными проектами, что поддерживает безопасное многотенантное окружение.
* **Потребление ресурсов**: Администраторы могут легко устанавливать квоты на ресурсы и ограничения для каждого проекта, предотвращая избыточное использование ресурсов.

Для пользователей платформы:

* **Быстрый старт**: Разработчики могут запрашивать у администраторов проекты, созданные по готовым шаблонам, что позволяет быстро начать разработку нового приложения.
* **Изоляция**: Каждый проект обеспечивает изолированное окружение, где разработчики могут развертывать и тестировать свои приложения без влияния на другие проекты.

#### Ограничения

Модуль работает только в рамках перечисленных ниже ограничений:

- Создание более одного пространства имён внутри проекта не предусматривается. Если требуется несколько пространств имён, создайте отдельный проект для каждого из них.
- Ресурсы шаблона применяются только к одному пространству имён, имя которого совпадает с именем проекта.

#### Внутренняя логика работы

##### Создание проекта

Для создания проекта используются ресурсы:

* [ProjectTemplate](./cr.html#projecttemplate) — ресурс, который описывает шаблон проекта. При помощи него задается список ресурсов, которые будут созданы в проекте, а также схема параметров, которые можно передать при создании проекта;
* [Project](./cr.html#project) — ресурс, который описывает конкретный проект.

При создании ресурса [Project](./cr.html#project) из определенного [ProjectTemplate](./cr.html#projecttemplate) происходит следующее:

1. Переданные [параметры](./cr.html#project-v1alpha2-spec-parameters) валидируются по OpenAPI-спецификации (параметр [`parametersSchema.openAPIV3Schema`
](./cr.html#projecttemplate-v1alpha1-spec-parametersschema-openapiv3schema) ресурса [ProjectTemplate](./cr.html#projecttemplate));
1. Выполняется рендеринг [шаблона для ресурсов](./cr.html#projecttemplate-v1alpha1-spec-resourcestemplate) с помощью Helm. Значения для рендеринга берутся из параметра [`parameters`](./cr.html#project-v1alpha2-spec-parameters) ресурса [Project](./cr.html#project);
1. Cоздается `Namespace` с именем, которое совпадает c именем [Project](./cr.html#project);
1. По очереди создаются все ресурсы, описанные в шаблоне.

> **Внимание!** При изменении шаблона проекта, все созданные проекты будут обновлены в соответствии с новым шаблоном.

##### Изоляция проекта

В основе проекта используется механизм изоляции ресурсов в рамках пространства имен (`Namespace`).
Пространства имен позволяют группировать поды, сервисы, секреты и другие объекты, но не обеспечивают полноценной изоляции.
Проект расширяет функциональность пространств имен, предлагая дополнительные инструменты для повышения уровня контроля и безопасности.
Для управления уровнем изоляции проекта можно использовать возможности Kubernetes, например:

* Ресурсы контроля доступа (`AuthorizationRule` / `RoleBinding`) — позволяют управлять взаимодействием объектов внутри `Namespace`. Вы можете задавать правила и назначать роли, чтобы точно контролировать, кто и что может делать в вашем проекте.
* Ресурсы контроля использования нагрузки (`ResourceQuota`) — с их помощью можно задать лимиты на использование процессорного времени (CPU), оперативной памяти (RAM), а также количества объектов внутри `Namespace`. Это помогает избежать чрезмерной нагрузки и обеспечивает мониторинг за приложениями в рамках проекта.
* Ресурсы контроля сетевой связности (`NetworkPolicy`) — управляют входящим и исходящим сетевым трафиком в `Namespace`. Таким образом, можно настроить разрешенные подключения между подами, улучшить безопасность и управляемость сетевого взаимодействия в рамках проекта.

Эти инструменты можно комбинировать, чтобы настроить проект в соответствии с требованиями вашего приложения.

### Модуль multitenancy-manager: настройки

{% include module-alerts.liquid %}

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['multitenancy-manager'].config-values | format_module_configuration: moduleKebabName }}

### Модуль multitenancy-manager: Custom Resources
{{ site.data.schemas.multitenancy-manager.crds.projects | format_crd: "multitenancy-manager" }}
{{ site.data.schemas.multitenancy-manager.crds.projecttemplate | format_crd: "multitenancy-manager" }}

### Модуль operator-trivy

Модуль позволяет запускать регулярную проверку пользовательских образов в runtime на известные CVE, включая уязвимости Astra Linux, Redos и ALT Linux. Базируется на проекте Trivy уязвимостей, обогащаемые базами Astra Linux, ALT Linux и РЕД ОС.

Также модуль производит анализ соответствия кластера kubernetes требованиями CIS Kubernetes Benchmark.

Модуль выполняет сканирование в пространствах имён, которые содержат метку `security-scanning.deckhouse.io/enabled=""`.
Если в кластере отсутствуют пространства имён с указанной меткой, сканируется пространство имён `default`.

Как только в кластере обнаруживается пространство имён с меткой `security-scanning.deckhouse.io/enabled=""`, сканирование пространства имён `default` прекращается.
Чтобы снова включить сканирование для пространства имён `default`, необходимо установить у него метку командой:

```shell
d8 k label namespace default security-scanning.deckhouse.io/enabled=""
```

#### Условия запуска сканирования

Сканирование запускается:

- автоматически каждые 24 часа,
- при запуске компонентов с новыми образами контейнеров в пространствах имен, для которых включено сканирование (в частности, при появлении новых объектов).

#### Где просматривать результаты сканирования

В Grafana:

- `Security/Trivy Image Vulnerability Overview` — сводный обзор уязвимостей в образах и ресурсах кластера.
- `Security/CIS Kubernetes Benchmark` — результаты проверки соответствия кластера требованиям CIS Kubernetes Benchmark.

В ресурсах кластера:

- Отчеты о безопасности кластера:
  - [`ClusterComplianceReport`](cr.html#clustercompliancereport)
  - [`RbacAssessmentReport`](cr.html#rbacassessmentreport)
- Отчеты о безопасности ресурсов кластера:
  - [`VulnerabilityReport`](cr.html#vulnerabilityreport) — уязвимости в образах контейнеров;
  - [`SbomReport`](cr.html#sbomreport) — состав ПО в образах (SBOM);
  - [`ConfigAuditReport`](cr.html#configauditreport) — ошибки конфигурации Kubernetes-объектов;
  - [`ExposedSecretReport`](cr.html#exposedsecretreport) — утечки секретов в контейнерах.

### Модуль operator-trivy: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['modules'].config-values | format_module_configuration: moduleKebabName }}

### Модуль operator-trivy: Custom Resources title: 'libwebp: out-of-bounds read in WebPMuxCreateInternal' title: Stripe title: Stripe title: Do not allow role binding creation and association with privileged role/clusterrole title: Do not allow management of networking resources title: Do not allow management of secrets title: Do not allow privilege escalation from node proxy title: No wildcard verb roles title: Do not allow attaching to shell on pods title: No wildcard verb and resource roles title: Do not allow management of RBAC resources title: No wildcard resource roles title: Do not allow users in a rolebinding to add other users to their rolebindings title: Do not allow role to create ClusterRoleBindings and association with privileged title: Do not allow getting shell on pods title: Do not allow deletion of pod logs title: Do not allow management of configmaps title: Do not allow impersonation of privileged groups title: Do not allow update/create of a malicious pod
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_clustercompliancereports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_clusterconfigauditreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_clusterinfraassessmentreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_clusterrbacassessmentreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_clustersbomreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_clustervulnerabilityreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_configauditreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_exposedsecretreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_infraassessmentreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_rbacassessmentreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_sbomreports | format_crd: "modules" }}
{{ site.data.schemas.modules.500-operator-trivy.crds.native.aquasecuritygithubio_vulnerabilityreports | format_crd: "modules" }}

### Модуль operator-trivy: FAQ
{% raw %}

#### Просмотр ресурсов, которые не прошли CIS compliance-проверки

```bash
d8 k get clustercompliancereports.aquasecurity.github.io cis -ojson |
  jq '.status.detailReport.results | map(select(.checks | map(.success) | all | not))'
```

#### Просмотр ресурсов, которые не прошли конкретную CIS compliance-проверку

По `id`:

```bash
check_id="5.7.3"
d8 k get clustercompliancereports.aquasecurity.github.io cis -ojson |
  jq --arg check_id "$check_id" '.status.detailReport.results | map(select(.id == $check_id))'
```

По описанию:

```bash
check_desc="Apply Security Context to Your Pods and Containers"
d8 k get clustercompliancereports.aquasecurity.github.io cis -ojson |
  jq --arg check_desc "$check_desc" '.status.detailReport.results | map(select(.description == $check_desc))'
```

{% endraw %}

#### Ручной перезапуск сканирования ресурса

Модуль выполняет повторное сканирование ресурсов каждые 24 часа согласно следующему алгоритму:

1. В пространстве имён c каждым просканированным ресурсом создаётся объект `VulnerabilityReport`.
1. В этом объекте присутствует аннотация `trivy-operator.aquasecurity.github.io/report-ttl`, которая указывает время жизни отчёта (по умолчанию - `24h`).
1. По истечении этого времени объект удаляется, что вызывает повторное сканирование ресурса.

Принудительно запустить повторное сканирование ресурса можно одним из двух способов:

- Перезапишите аннотацию `trivy-operator.aquasecurity.github.io/report-ttl`, указав короткое время жизни отчёта.
- Удалите объект `VulnerabilityReport` из пространства имён, где находится просканированный ресурс.

Пример команды для перезаписи аннотации `trivy-operator.aquasecurity.github.io/report-ttl`:

```bash
d8 k annotate VulnerabilityReport -n <namespace> <reportName>  trivy-operator.aquasecurity.github.io/report-ttl=1s --overwrite
```

#### Кто имеет доступ к результатам сканирования

Доступ к результатам сканирования (в том числе возможность просматривать [ресурсы с результатами](cr.html)) предоставляется пользователям, обладающим следующими [ролями доступа](./user-authz/#экспериментальная-ролевая-модель):

- `d8:manage:networking:viewer` или выше;
- `d8:manage:permission:module:operator-trivy:view`.
  
#### Как ограничить список сканируемых ресурсов в пространстве имён

В текущей версии функциональности ограничения перечня ресурсов для сканирования в пространстве имён не предусмотрено.  
Оператор сканирует **все ресурсы**, находящиеся в пространстве имён, помеченном меткой `security-scanning.deckhouse.io/enabled=""`.

#### Как просмотреть отчёт по своему приложению

Для просмотра результатов сканирования вашего приложения воспользуйтесь Grafana-дашбордом `Security / Trivy Image Vulnerability Overview`.  
Вы можете отфильтровать результаты по нужному пространству имён и ресурсу.

Также вы можете напрямую просматривать [ресурсы](cr.html) с результатами сканирования, которые создаются для каждого сканируемого объекта.  
Подробности о структуре их имён и местоположении доступны в [документации](cr.html).

### Модуль user-authn

Модуль отвечает за единую систему аутентификации, интегрированную с Kubernetes и веб-интерфейсами, используемыми в других модулях, например, Grafana и Dashboard.

Модуль состоит из следующих компонентов:

- `dex` — федеративный OpenID Connect провайдер, поддерживающий работу со статическими пользователями и с возможностью подключения к различным внешним провайдерам аутентификации.
- `kubeconfig-generator` (он же `dex-k8s-authenticator` — веб-приложение, генерирующее команды для настройки локального `kubectl` после аутентификации в Dex;
- `dex-authenticator` (он же `oauth2-proxy` и выполняющее их авторизацию с помощью сервиса Dex.

Управление статическими пользователями осуществляется с помощью ресурсов [User](cr.html#user) и [Group](cr.html#group):

В объекте User хранится информация о пользователе, включая email и зашифрованный хеш пароля (пароль в открытом виде не сохраняется);
В объекте Group задаётся список пользователей, объединённых в группы для удобства управления правами доступа.

Поддерживаются следующие внешние провайдеры/протоколы аутентификации:

- LDAP;
- OIDC.

Одновременно можно подключить более одного внешнего провайдера аутентификации.

#### Возможности интеграции

##### Базовая аутентификация в API Kubernetes

Базовая аутентификация в API Kubernetes на данный момент доступна только для провайдера Crowd (с включением параметра [`enableBasicAuth`](cr.html#dexprovider-v1-spec-crowd-enablebasicauth)).

> К API Kubernetes можно подключаться и [через другие поддерживаемые внешние провайдеры](#веб-интерфейс-для-генерации-готовых-kubeconfig-файлов).

##### Интеграция с приложениями

Чтобы обеспечить аутентификацию в любом веб-приложении, работающем в Kubernetes, можно создать ресурс [_DexAuthenticator_](cr.html#dexauthenticator) в пространстве имен (_Namespace_) приложения и добавить несколько аннотаций к ресурсу _Ingress_.
Это позволит:
* ограничить список групп, которым разрешен доступ;
* ограничить список адресов, с которых разрешена аутентификация;
* интегрировать приложение в единую систему аутентификации, если приложение поддерживает OIDC. Для этого в Kubernetes создается ресурс [_DexClient_](cr.html#dexclient) в _Namespace_ приложения. В том же _Namespace_ создается секрет с данными для подключения в Dex по OIDC.

После такой интеграции можно:
* ограничить перечень групп, которым разрешено подключаться;
* указать перечень клиентов, OIDC-токенам которых можно доверять (`trustedPeers`).

##### Веб-интерфейс для генерации готовых kubeconfig-файлов

Модуль позволяет автоматически создавать конфигурацию для kubectl или других утилит Kubernetes.

Пользователь получит набор команд для настройки kubectl после авторизации в веб-интерфейсе генератора. Эти команды можно скопировать и вставить в консоль для использования kubectl.
Механизм аутентификации для kubeconfig использует OIDC-токен. OIDC-сессия может продлеваться автоматически, если использованный в Dex провайдер аутентификации поддерживает продление сессий. Для этого в kubeconfig указывается `refresh token`.

Дополнительно можно настроить несколько адресов `kube-apiserver` и сертификаты ЦС (CA) для каждого из них. Например, это может потребоваться, если доступ к кластеру Kubernetes осуществляется через VPN или прямое подключение.

#### Публикация API kubernetes через Ingress

Компонент kube-apiserver без дополнительных настроек доступен только во внутренней сети кластера. Этот модуль решает проблему простого и безопасного доступа к API Kubernetes извне кластера. При этом API-сервер публикуется на специальном домене (подробнее см. [раздел о служебных доменах в документации](/reference/api/global.html)).

При настройке можно указать:
* перечень сетевых адресов, с которых разрешено подключение;
* перечень групп, которым разрешен доступ к API-серверу;
* Ingress-контроллер, на котором производится аутентификация.

По умолчанию будет сгенерирован специальный сертификат ЦС (CA) и автоматически настроен генератор kubeconfig.

#### Расширения от Фланта

Модуль использует модифицированную версию Dex для поддержки:
* групп для статических учетных записей пользователей и провайдера Bitbucket Cloud (параметр [`bitbucketCloud`](cr.html#dexprovider-v1-spec-bitbucketcloud));
* передачи параметра `group` клиентам;
* механизма `obsolete tokens`, который позволяет избежать состояния гонки при продлении токена OIDC-клиентом.

#### Отказоустойчивый режим

Модуль поддерживает режим высокой доступности `highAvailability`. При его включении аутентификаторы, отвечающие на `auth request`-запросы, развертываются с учетом требуемой избыточности для обеспечения непрерывной работы. В случае отказа любого из экземпляров аутентификаторов пользовательские аутентификационные сессии не прерываются.

### Модуль user-authn: настройки

 
<!-- SCHEMA -->

Автоматический деплой oauth2-proxy.

**Важно!** Так как использование OpenID Connect по протоколу HTTP является слишком значительной угрозой безопасности (что подтверждается, например, тем, что Kubernetes API-сервер не поддерживает работу с OIDC по HTTP), данный модуль можно установить только при включенном HTTPS (`https.mode` выставить в отличное от `Disabled` значение или на уровне кластера, или в самом модуле).

**Важно!** При включении данного модуля аутентификация во всех веб-интерфейсах перестанет использовать HTTP Basic Auth и переключится на Dex (который, в свою очередь, будет использовать настроенные вами внешние провайдеры).
Для настройки kubectl необходимо перейти по адресу `https://kubeconfig.<modules.publicDomainTemplate>/`, авторизоваться в настроенном внешнем провайдере и скопировать shell-команды к себе в консоль.

**Важно!** Для работы аутентификации в dashboard и kubectl требуется [донастройка API-сервера](faq.html#настройка-kube-apiserver). Для автоматизации этого процесса реализован модуль [control-plane-manager](/modules/control-plane-manager/), который включен по умолчанию.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['user-authn'].config-values | format_module_configuration: moduleKebabName }}

### Модуль user-authn: Custom Resources
{{ site.data.schemas.user-authn.crds.dex-authenticator | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-client | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex-provider | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.dex | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.group | format_crd: "user-authn" }}
{{ site.data.schemas.user-authn.crds.user | format_crd: "user-authn" }}

### Модуль user-authn: FAQ

{% raw %}

#### Как защитить мое приложение?

Чтобы включить аутентификацию через Dex для приложения, выполните следующие шаги:

1. Создайте custom resource [DexAuthenticator](cr.html#dexauthenticator).

   Создание `DexAuthenticator` в кластере приводит к созданию экземпляра oauth2-proxy, подключенного к Dex. После появления custom resource `DexAuthenticator` в указанном namespace появятся необходимые объекты Deployment, Service, Ingress, Secret.

   Пример custom resource `DexAuthenticator`:

   ```yaml
   apiVersion: deckhouse.io/v1
   kind: DexAuthenticator
   metadata:
     # Префикс имени подов Dex authenticator.
     # Например, если префикс имени `app-name`, то поды Dex authenticator будут вида `app-name-dex-authenticator-7f698684c8-c5cjg`.
     name: app-name
     # Namespace, в котором будет развернут Dex authenticator.
     namespace: app-ns
   spec:
     # Домен вашего приложения. Запросы на него будут перенаправляться для прохождения аутентификацию в Dex.
     applicationDomain: "app-name.kube.my-domain.com"
     # Отправлять ли `Authorization: Bearer` header приложению. Полезно в связке с auth_request в NGINX.
     sendAuthorizationHeader: false
     # Имя Secret'а с SSL-сертификатом.
     applicationIngressCertificateSecretName: "ingress-tls"
     # Название Ingress-класса, которое будет использоваться в создаваемом для Dex authenticator Ingress-ресурсе.
     applicationIngressClassName: "nginx"
     # Время, на протяжении которого пользовательская сессия будет считаться активной.
     keepUsersLoggedInFor: "720h"
     # Список групп, пользователям которых разрешено проходить аутентификацию.
     allowedGroups:
     - everyone
     - admins
     # Список адресов и сетей, с которых разрешено проходить аутентификацию.
     whitelistSourceRanges:
     - 1.1.1.1/32
     - 192.168.0.0/24
   ```

2. Подключите приложение к Dex.

   Для этого добавьте в Ingress-ресурс приложения следующие аннотации:

   - `nginx.ingress.kubernetes.io/auth-signin: https://$host/dex-authenticator/sign_in`
   - `nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-User,X-Auth-Request-Email`
   - `nginx.ingress.kubernetes.io/auth-url: https://<SERVICE_NAME>.<NS>.svc.{{ C_DOMAIN }}/dex-authenticator/auth`, где:
      - `SERVICE_NAME` — имя сервиса (Service) аутентификатора. Как правило, оно соответствует формату `<NAME>-dex-authenticator` (`<NAME>` — это `metadata.name` DexAuthenticator).
      - `NS` — значение параметра `metadata.namespace` ресурса `DexAuthenticator`.
      - `C_DOMAIN` — домен кластера (параметр [clusterDomain](/reference/api/cr.html#clusterconfiguration-clusterdomain) ресурса `ClusterConfiguration`).

   > **Важно:** Если имя DexAuthenticator (`<NAME>`) слишком длинное, имя сервиса (Service) может быть сокращено. Чтобы найти корректное имя сервиса, воспользуйтесь следующей командой (укажите имя пространства имен и имя аутентификатора):
   >
   > ```shell
   > d8 k get service -n <NS> -l "deckhouse.io/dex-authenticator-for=<NAME>" -o jsonpath='{.items[0].metadata.name}'
   > ```
   >

   Ниже представлен пример аннотаций на Ingress-ресурсе приложения, для подключения его к Dex:

   ```yaml
   annotations:
     nginx.ingress.kubernetes.io/auth-signin: https://$host/dex-authenticator/sign_in
     nginx.ingress.kubernetes.io/auth-url: https://app-name-dex-authenticator.app-ns.svc.cluster.local/dex-authenticator/auth
     nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-User,X-Auth-Request-Email
   ```

##### Настройка ограничений на основе CIDR

В DexAuthenticator нет встроенной системы управления разрешением аутентификации на основе IP-адреса пользователя. Вместо этого вы можете воспользоваться аннотациями для Ingress-ресурсов:

* Если нужно ограничить доступ по IP и оставить прохождение аутентификации в Dex, добавьте аннотацию с указанием разрешенных CIDR через запятую:

  ```yaml
  nginx.ingress.kubernetes.io/whitelist-source-range: 192.168.0.0/32,1.1.1.1
  ```

* Если необходимо, чтобы пользователи из указанных сетей освобождались от прохождения аутентификации в Dex, а пользователи из остальных сетей обязательно аутентифицировались в Dex, добавьте следующую аннотацию:

  ```yaml
  nginx.ingress.kubernetes.io/satisfy: "any"
  ```

#### Как работает аутентификация с помощью DexAuthenticator

![Как работает аутентификация с помощью DexAuthenticator](images/dex_login.svg)

1. Dex в большинстве случаев перенаправляет пользователя на страницу входа провайдера и ожидает, что пользователь будет перенаправлен на его `/callback` URL. Однако такие провайдеры, как LDAP или Atlassian Crowd, не поддерживают этот вариант. Вместо этого пользователь должен ввести свои логин и пароль в форму входа в Dex, и Dex сам проверит их верность, сделав запрос к API провайдера.

2. DexAuthenticator устанавливает cookie с целым refresh token (вместо того чтобы выдать тикет, как для ID token) потому что Redis не сохраняет данные на диск.
Если по тикету в Redis не найден ID token, пользователь сможет запросить новый ID token, предоставив refresh token из cookie.

3. DexAuthenticator выставляет HTTP-заголовок `Authorization`, равный значению ID token из Redis. Это необязательно для сервисов по типу [Upmeter](./upmeter/), потому что права доступа к Upmeter не такие проработанные.

#### Как сгенерировать kubeconfig для доступа к Kubernetes API?

Сгенерировать `kubeconfig` для удаленного доступа к кластеру через `kubectl` можно через веб-интерфейс `kubeconfigurator`.

Настройте параметр [publishAPI](configuration.html#parameters-publishapi):

- Откройте настройки модуля `user-authn` (создайте ресурс moduleConfig `user-authn`, если его нет):

  ```shell
  d8 k edit mc user-authn
  ```

- Добавьте следующую секцию в блок `settings` и сохраните изменения:

  ```yaml
  publishAPI:
    enabled: true
  ```

Для доступа к веб-интерфейсу, позволяющему сгенерировать `kubeconfig`, зарезервировано имя `kubeconfig`. URL для доступа зависит от значения параметра [publicDomainTemplate](/reference/api/global.html#parameters-modules-publicdomaintemplate) (например, для `publicDomainTemplate: %s.kube.my` это будет `kubeconfig.kube.my`, а для `publicDomainTemplate: %s-kube.company.my` — `kubeconfig-kube.company.my`)  
{% endraw %}

##### Настройка kube-apiserver

С помощью функций модуля [control-plane-manager](/modules/control-plane-manager/) Deckhouse автоматически настраивает kube-apiserver, выставляя следующие флаги так, чтобы модули `dashboard` и `kubeconfig-generator` могли работать в кластере.

{% offtopic title="Аргументы kube-apiserver, которые будут настроены" %}

* `--oidc-client-id=kubernetes`
* `--oidc-groups-claim=groups`
* `--oidc-issuer-url=https://dex.%addonsPublicDomainTemplate%/`
* `--oidc-username-claim=email`

В случае использования самоподписанных сертификатов для Dex будет добавлен еще один аргумент, а также в под с apiserver будет смонтирован файл с CA:

* `--oidc-ca-file=/etc/kubernetes/oidc-ca.crt`
{% endofftopic %}

{% raw %}

##### Как работает подключение к Kubernetes API с помощью сгенерированного kubeconfig

![Схема взаимодействия при подключении к Kubernetes API с помощью сгенерированного kubeconfig](images/kubeconfig_dex.svg)

1. До начала работы kube-apiserver необходимо запросить конфигурационный endpoint OIDC провайдера (в нашем случае — Dex), чтобы получить issuer и настройки JWKS endpoint.

2. Kubeconfig generator сохраняет ID token и refresh token в файл kubeconfig.

3. После получения запроса с ID token kube-apiserver идет проверять, что token подписан провайдером, который мы настроили на первом шаге, с помощью ключей, полученных с точки доступа JWKS. В качестве следующего шага он сравнивает значения claim'ов `iss` и `aud` из token'а со значениями из конфигурации.

#### Как Dex защищен от подбора логина и пароля?

Одному пользователю разрешено только 20 попыток входа. Если указанный лимит израсходован, одна дополнительная попытка будет добавляться каждые 6 секунд.

{% endraw %}

### Модуль user-authz

Модуль отвечает за генерацию объектов ролевой модели доступа, основанной на базе стандартного механизма RBAC Kubernetes. Модуль создает набор кластерных ролей (`ClusterRole`), подходящий для большинства задач по управлению доступом пользователей и групп.

{% alert level="warning" %}
С версии Deckhouse Platform Certified Security Edition v1.64 в модуле реализована экспериментальная модель ролевого доступа.

Функциональность экспериментальной и текущей моделей ролевого доступа несовместимы. Автоматическая конвертация ресурсов невозможна.
{% endalert %}

<div style="height: 0;" id="новая-ролевая-модель"></div>

#### Экспериментальная ролевая модель

В отличие [от текущей ролевой модели](#текущая-ролевая-модель) Deckhouse Platform Certified Security Edition, экспериментальная ролевая модель не использует ресурсы `ClusterAuthorizationRule` и `AuthorizationRule`. Настройка прав доступа выполняется стандартным для RBAC Kubernetes способом: с помощью создания ресурсов `RoleBinding` или `ClusterRoleBinding`, с указанием в них одной из подготовленных модулем `user-authz` ролей.

Модуль создаёт специальные агрегированные кластерные роли (`ClusterRole`). Используя эти роли в `RoleBinding` или `ClusterRoleBinding` можно решать следующие задачи:

- Управлять доступом к модулям определённой [подсистеме](#подсистемы-ролевой-модели) применения.

  Например, чтобы дать возможность пользователю, выполняющему функции сетевого администратора, настраивать *сетевые* модули (например, `cni-cilium`, `ingress-nginx`, `istio` и т. д.), можно использовать в `ClusterRoleBinding` роль `d8:manage:networking:manager`.
- Управлять доступом к *пользовательским* ресурсам модулей в рамках пространства имён.

  Например, использование роли `d8:use:role:manager` в `RoleBinding`, позволит удалять/создавать/редактировать ресурс [PodLoggingConfig](./log-shipper/cr.html#podloggingconfig) в пространстве имён, но не даст доступа к cluster-wide-ресурсам [ClusterLoggingConfig](./log-shipper/cr.html#clusterloggingconfig) и [ClusterLogDestination](./log-shipper/cr.html#clusterlogdestination) модуля `log-shipper`, а также не даст возможность настраивать сам модуль `log-shipper`.

Роли, создаваемые модулем, делятся на два класса:

- [Use-роли](#use-роли) — для назначения прав пользователям (например, разработчикам приложений) **в конкретном пространстве имён**.
- [Manage-роли](#manage-роли) — для назначения прав администраторам.

##### Use-роли

{% alert level="warning" %}
Use-роль можно использовать только в ресурсе `RoleBinding`.
{% endalert %}

Use-роли предназначены для назначения прав пользователю **в конкретном пространстве имён**. Под пользователями понимаются, например, разработчики, которые используют настроенный администратором кластер для развёртывания своих приложений. Таким пользователям не нужно управлять модулями Deckhouse Platform Certified Security Edition или кластером, но им нужно иметь возможность, например, создавать свои Ingress-ресурсы, настраивать аутентификацию приложений и сбор логов с приложений.

Use-роль определяет права на доступ к namespaced-ресурсам модулей и стандартным namespaced-ресурсам Kubernetes (`Pod`, `Deployment`, `Secret`, `ConfigMap` и т. п.).

Модуль создаёт следующие use-роли:

- `d8:use:role:viewer` — позволяет в конкретном пространстве имён просматривать стандартные ресурсы Kubernetes, кроме секретов и ресурсов RBAC, а также выполнять аутентификацию в кластере;
- `d8:use:role:user` — дополнительно к роли `d8:use:role:viewer` позволяет в конкретном пространстве имён просматривать секреты и ресурсы RBAC, подключаться к подам, удалять поды (но не создавать или изменять их), выполнять `kubectl port-forward` и `kubectl proxy`, изменять количество реплик контроллеров;
- `d8:use:role:manager` — дополнительно к роли `d8:use:role:user` позволяет в конкретном пространстве имён управлять ресурсами модулей (например, `Certificate`, `PodLoggingConfig` и т. п.) и стандартными namespaced-ресурсами Kubernetes (`Pod`, `ConfigMap`, `CronJob` и т. п.);
- `d8:use:role:admin` — дополнительно к роли `d8:use:role:manager` позволяет в конкретном пространстве имён управлять ресурсами `ResourceQuota`, `ServiceAccount`, `Role`, `RoleBinding`, `NetworkPolicy`.

##### Manage-роли

{% alert level="warning" %}
Manage-роль не дает доступа к пространству имён пользовательских приложений.

Manage-роль определяет доступ только к системным пространствам имён (начинающимся с `d8-` или `kube-`), и только к тем из них, в которых работают модули соответствующей подсистемы роли.
{% endalert %}

Manage-роли предназначены для назначения прав на управление всей платформой или её частью ([подсистемой](#подсистемы-ролевой-модели)), но не самими приложениями пользователей. С помощью manage-роли можно, например, дать возможность администратору безопасности управлять модулями, ответственными за функции безопасности кластера. Тогда администратор безопасности сможет настраивать аутентификацию, авторизацию, политики безопасности и т. п., но не сможет управлять остальными функциями кластера (например, настройками сети и мониторинга) и изменять настройки в пространстве имён приложений пользователей.

Manage-роль определяет права на доступ:

- к cluster-wide-ресурсам Kubernetes;
- к управлению модулями Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`) в рамках [подсистемы](#подсистемы-ролевой-модели) роли, или всеми модулями Deckhouse Platform Certified Security Edition для роли `d8:manage:all:*`;
- к управлению cluster-wide-ресурсами модулей Deckhouse Platform Certified Security Edition в рамках [подсистемы](#подсистемы-ролевой-модели) роли или всеми ресурсами модулей Deckhouse Platform Certified Security Edition для роли `d8:manage:all:*`;
- к системным пространствам имён (начинающимся с `d8-` или `kube-`), в которых работают модули [подсистемы](#подсистемы-ролевой-модели) роли, или ко всем системным пространствам имён для роли `d8:manage:all:*`.
  
Формат названия manage-роли — `d8:manage:<SUBSYSTEM>:<ACCESS_LEVEL>`, где:

- `SUBSYSTEM` — подсистема роли. Может быть либо одной из подсистем [списка](#подсистемы-ролевой-модели), либо `all` для доступа в рамках всех подсистем;
- `ACCESS_LEVEL` — уровень доступа.

  Примеры manage-ролей:
  
  - `d8:manage:all:viewer` — доступ на просмотр конфигурации всех модулей Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`), их cluster-wide-ресурсов, их namespaced-ресурсов и стандартных объектов Kubernetes (кроме секретов и ресурсов RBAC) во всех системных пространствах имён (начинающихся с `d8-` или `kube-`);
  - `d8:manage:all:manager` — аналогично роли `d8:manage:all:viewer`, только доступ на уровне `admin`, т. е. просмотр/создание/изменение/удаление конфигурации всех модулей Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`), их cluster-wide-ресурсов, их namespaced-ресурсов и стандартных объектов Kubernetes во всех системных пространствах имён (начинающихся с `d8-` или `kube-`);
  - `d8:manage:observability:viewer` — доступ на просмотр конфигурации модулей Deckhouse Platform Certified Security Edition (ресурсы `moduleConfig`) из подсистемы `observability`, их cluster-wide-ресурсов, их namespaced-ресурсов и стандартных объектов Kubernetes (кроме секретов и ресурсов RBAC) в системных пространствах имён `d8-log-shipper`, `d8-monitoring`, `d8-okmeter`, `d8-operator-prometheus`, `d8-upmeter`, `kube-prometheus-pushgateway`.

Модуль предоставляет два уровня доступа для администратора:

- `viewer` — позволяет просматривать стандартные ресурсы Kubernetes, конфигурацию модулей (ресурсы `moduleConfig`), cluster-wide-ресурсы модулей и namespaced-ресурсы модулей в пространстве имен модуля;
- `manager` — дополнительно к роли `viewer` позволяет управлять стандартными ресурсами Kubernetes, конфигурацией модулей (ресурсы `moduleConfig`), cluster-wide-ресурсами модулей и namespaced-ресурсами модулей в пространстве имен модуля;

##### Подсистемы ролевой модели

Каждый модуль Deckhouse Platform Certified Security Edition принадлежит определённой подсистемы. Для каждой подсистемы существует набор ролей с разными уровнями доступа. Роли обновляются автоматически при включении или отключении модуля.

Например, для подсистемы `networking` существуют следующие manage-роли, которые можно использовать в `ClusterRoleBinding`:

- `d8:manage:networking:viewer`
- `d8:manage:networking:manager`

Подсистема роли ограничивает её действие всеми системными (начинающимися с `d8-` или `kube-`) пространствами имён кластера (подсистема `all`) или теми пространствами имён, в которых работают модули подсистемы (см. таблицу состава подсистем).

Таблица состава подсистем ролевой модели.

{% include rbac/rbac-subsystems-list.liquid %}

<div style="height: 0;" id="устаревшая-ролевая-модель"></div>

#### Текущая ролевая модель

Особенности:

- Модуль реализует role-based-подсистему сквозной авторизации, расширяя функционал стандартного механизма RBAC.
- Настройка прав доступа происходит с помощью [ресурсов](cr.html).
- Управление доступом к инструментам масштабирования (параметр `allowScale` ресурса [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule-v1-spec-allowscale) или [AuthorizationRule](cr.html#authorizationrule-v1alpha1-spec-allowscale)).
- Управление доступом к форвардингу портов (параметр `portForwarding` ресурса [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule-v1-spec-portforwarding) или [AuthorizationRule](cr.html#authorizationrule-v1alpha1-spec-portforwarding)).
- Управление списком разрешённых пространств имён в формате labelSelector (параметр `namespaceSelector` ресурса [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule-v1-spec-namespaceselector)).

В модуле, кроме использования RBAC, можно использовать удобный набор высокоуровневых ролей:

- `User` — позволяет получать информацию обо всех объектах (включая доступ к журналам подов), но не позволяет заходить в контейнеры, читать секреты и выполнять port-forward;
- `PrivilegedUser` — то же самое, что и `User`, но позволяет заходить в контейнеры, читать секреты, а также удалять поды (что обеспечивает возможность перезагрузки);
- `Editor` — то же самое, что и `PrivilegedUser`, но предоставляет возможность создавать, изменять и удалять все объекты, которые обычно нужны для прикладных задач;
- `Admin` — то же самое, что и `Editor`, но позволяет удалять служебные объекты (производные ресурсы, например `ReplicaSet`, `certmanager.k8s.io/challenges` и `certmanager.k8s.io/orders`);
- `ClusterEditor` — то же самое, что и `Editor`, но позволяет управлять ограниченным набором `cluster-wide`-объектов, которые могут понадобиться для прикладных задач (`ClusterXXXMetric`, `KeepalivedInstance`, `DaemonSet` и т. д). Роль для работы оператора кластера;
- `ClusterAdmin` — то же самое, что и `ClusterEditor` + `Admin`, но позволяет управлять служебными `cluster-wide`-объектами (производные ресурсы, например `MachineSets`, `Machines` и т. п., а также `ClusterAuthorizationRule`, `ClusterRoleBindings` и `ClusterRole`). Роль для работы администратора кластера. **Важно**, что `ClusterAdmin`, поскольку он уполномочен редактировать `ClusterRoleBindings`, может **сам себе расширить полномочия**;
- `SuperAdmin` — разрешены любые действия с любыми объектами, при этом ограничения `namespaceSelector` и `limitNamespaces` продолжат работать.

{% alert level="warning" %}
Режим multi-tenancy (авторизация по пространству имён) в данный момент реализован по временной схеме и **не гарантирует безопасность!**
{% endalert %}

В случае, если в [`ClusterAuthorizationRule`](cr.html#clusterauthorizationrule)-ресурсе используется `namespaceSelector`, параметры `limitNamespaces` и `allowAccessToSystemNamespace` не учитываются.

Если вебхук, который реализовывает систему авторизации, по какой-то причине будет недоступен, опции `allowAccessToSystemNamespaces`, `namespaceSelector` и `limitNamespaces` в custom resource перестанут применяться и пользователи будут иметь доступ во все пространства имён. После восстановления доступности вебхука опции продолжат работать.

##### Список доступа для каждой роли модуля по умолчанию

Сокращения для `verbs`:
<!-- start user-authz roles placeholder -->
* read - `get`, `list`, `watch`
* read-write - `get`, `list`, `watch`, `create`, `delete`, `deletecollection`, `patch`, `update`
* write - `create`, `delete`, `deletecollection`, `patch`, `update`

{{site.data.i18n.common.role[page.lang] | capitalize }} `User`:

```text
read:
    - apiextensions.k8s.io/customresourcedefinitions
    - apps/daemonsets
    - apps/deployments
    - apps/replicasets
    - apps/statefulsets
    - autoscaling.k8s.io/verticalpodautoscalers
    - autoscaling/horizontalpodautoscalers
    - batch/cronjobs
    - batch/jobs
    - configmaps
    - discovery.k8s.io/endpointslices
    - endpoints
    - events
    - events.k8s.io/events
    - extensions/daemonsets
    - extensions/deployments
    - extensions/ingresses
    - extensions/replicasets
    - extensions/replicationcontrollers
    - limitranges
    - metrics.k8s.io/nodes
    - metrics.k8s.io/pods
    - namespaces
    - networking.k8s.io/ingresses
    - networking.k8s.io/networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    - pods
    - pods/log
    - policy/poddisruptionbudgets
    - rbac.authorization.k8s.io/rolebindings
    - rbac.authorization.k8s.io/roles
    - replicationcontrollers
    - resourcequotas
    - serviceaccounts
    - services
    - storage.k8s.io/storageclasses
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `PrivilegedUser` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`):

```text
create:
    - pods/eviction
create,get:
    - pods/attach
    - pods/exec
delete,deletecollection:
    - pods
read:
    - secrets
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `Editor` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`):

```text
read-write:
    - apps/deployments
    - apps/statefulsets
    - autoscaling.k8s.io/verticalpodautoscalers
    - autoscaling/horizontalpodautoscalers
    - batch/cronjobs
    - batch/jobs
    - configmaps
    - discovery.k8s.io/endpointslices
    - endpoints
    - extensions/deployments
    - extensions/ingresses
    - networking.k8s.io/ingresses
    - persistentvolumeclaims
    - policy/poddisruptionbudgets
    - serviceaccounts
    - services
write:
    - secrets
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `Admin` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`, `Editor`):

```text
create,patch,update:
    - pods
delete,deletecollection:
    - apps/replicasets
    - extensions/replicasets
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `ClusterEditor` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`, `Editor`):

```text
read:
    - rbac.authorization.k8s.io/clusterrolebindings
    - rbac.authorization.k8s.io/clusterroles
write:
    - apiextensions.k8s.io/customresourcedefinitions
    - apps/daemonsets
    - extensions/daemonsets
    - storage.k8s.io/storageclasses
```

{{site.data.i18n.common.role[page.lang] | capitalize }} `ClusterAdmin` ({{site.data.i18n.common.includes_rules_from[page.lang]}} `User`, `PrivilegedUser`, `Editor`, `Admin`, `ClusterEditor`):

```text
read-write:
    - deckhouse.io/clusterauthorizationrules
write:
    - limitranges
    - namespaces
    - networking.k8s.io/networkpolicies
    - rbac.authorization.k8s.io/clusterrolebindings
    - rbac.authorization.k8s.io/clusterroles
    - rbac.authorization.k8s.io/rolebindings
    - rbac.authorization.k8s.io/roles
    - resourcequotas
```
<!-- end user-authz roles placeholder -->

Вы можете получить дополнительный список правил доступа для роли модуля из кластера ([существующие пользовательские правила](usage.html#настройка-прав-высокоуровневых-ролей) и нестандартные правила из других модулей Deckhouse):

```bash
D8_ROLE_NAME=Editor
kubectl get clusterrole -A -o jsonpath="{range .items[?(@.metadata.annotations.user-authz\.deckhouse\.io/access-level=='$D8_ROLE_NAME')]}{.rules}{'
'}{end}" | jq -s add
```

### Модуль user-authz: настройки

> **Внимание!** Мы категорически не рекомендуем создавать поды и ReplicaSet'ы — эти объекты являются второстепенными и должны создаваться из других контроллеров. Доступ к созданию и изменению подов и ReplicaSet'ов полностью отсутствует.
>
> **Внимание!** Режим multi-tenancy (авторизация по пространству имён) в данный момент реализован по временной схеме и **не гарантирует безопасность**! Если вебхук, который реализовывает систему авторизации, по какой-то причине будет недоступен, авторизация по пространству имён (опции `allowAccessToSystemNamespaces`, `namespaceSelector` и `limitNamespaces` в custom resource) перестанет работать и пользователи получат доступы во все пространства имён. После восстановления доступности вебхука всё вернётся на свои места.

Вся настройка прав доступа происходит с помощью [custom resources](cr.html).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['user-authz'].config-values | format_module_configuration: moduleKebabName }}

### Модуль user-authz: Custom Resources
{{ site.data.schemas.user-authz.crds.authorizationrule | format_crd: "user-authz" }}
{{ site.data.schemas.user-authz.crds.clusterauthorizationrule | format_crd: "user-authz" }}

### Модуль user-authz: FAQ

#### Как создать пользователя?

[Создание пользователя](usage.html#создание-пользователя).

<div style="height: 0;" id="как-ограничить-права-пользователю-конкретными-пространствами-имён-устаревшая-ролевая-модель"></div>

#### Как ограничить права пользователю конкретными пространствами имён?

Чтобы ограничить права пользователя конкретными пространствами имён в экспериментальной ролевой модели, используйте в `RoleBinding` [use-роль](./#use-роли) с соответствующим уровнем доступа. [Пример...](usage.html#пример-назначения-административных-прав-пользователю-в-рамках-пространства-имён).

В текущей ролевой модули используйте параметры `namespaceSelector` или `limitNamespaces` (устарел) в кастомном ресурсе [`ClusterAuthorizationRule`](/modules/user-authz/cr.html#clusterauthorizationrule).

#### Что, если два ClusterAuthorizationRules подходят для одного пользователя?

В примере пользователь `jane.doe@example.com` состоит в группе `administrators`. Созданы два ClusterAuthorizationRules:

```yaml
apiVersion: deckhouse.io/v1
kind: ClusterAuthorizationRule
metadata:
  name: jane
spec:
  subjects:
    - kind: User
      name: jane.doe@example.com
  accessLevel: User
  namespaceSelector:
    labelSelector:
      matchLabels:
        env: review
---
apiVersion: deckhouse.io/v1
kind: ClusterAuthorizationRule
metadata:
  name: admin
spec:
  subjects:
  - kind: Group
    name: administrators
  accessLevel: ClusterAdmin
  namespaceSelector:
    labelSelector:
      matchExpressions:
      - key: env
        operator: In
        values:
        - prod
        - stage
```

1. `jane.doe@example.com` имеет право запрашивать и просматривать объекты среди всех пространств имён, помеченных `env=review`.
2. `Administrators` могут запрашивать, редактировать, получать и удалять объекты на уровне кластера и из пространств имён, помеченных `env=prod` и `env=stage`.

Так как для `Jane Doe` подходят два правила, необходимо провести вычисления:

* `Jane Doe` будет иметь самый сильный accessLevel среди всех подходящих правил — `ClusterAdmin`.
* Опции `namespaceSelector` будут объединены так, что `Jane Doe` будет иметь доступ в пространства имён, помеченные меткой `env` со значением `review`, `stage` или `prod`.

{% alert level="warning" %}
Если есть правило без опции `namespaceSelector` и без опции `limitNamespaces` (устаревшая), это значит, что доступ разрешён во все пространства имён, кроме системных, что повлияет на результат вычисления доступных пространств имён для пользователя.
{% endalert %}

#### Как расширить роли или создать новую?

[Экспериментальная ролевая модель](./#экспериментальная-ролевая-модель) построена на принципе агрегации, она собирает более мелкие роли в более обширные,
тем самым предоставляя лёгкие способы расширения модели собственными ролями.

##### Создание новой роли подсистемы

Предположим, что текущие подсистемы не подходят под ролевое распределение в компании и требуется создать новую [подсистему](./#подсистемы-ролевой-модели),
которая будет включать в себя роли из подсистемы `deckhouse`, подсистемы `kubernetes` и модуля user-authn.

Для решения этой задачи создайте следующую роль:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom:manage:mycustom:manager
  labels:
    rbac.deckhouse.io/use-role: admin
    rbac.deckhouse.io/kind: manage
    rbac.deckhouse.io/level: subsystem
    rbac.deckhouse.io/subsystem: custom
    rbac.deckhouse.io/aggregate-to-all-as: manager
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.deckhouse.io/kind: manage
        rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
    - matchLabels:
        rbac.deckhouse.io/kind: manage
        rbac.deckhouse.io/aggregate-to-kubernetes-as: manager
    - matchLabels:
        rbac.deckhouse.io/kind: manage
        module: user-authn
rules: []
```

В начале указаны лейблы для новой роли:

- показывает, какую роль хук должен использовать при создании use ролей:

  ```yaml
  rbac.deckhouse.io/use-role: admin
  ```

- показывает, что роль должна обрабатываться как manage-роль:

  ```yaml
  rbac.deckhouse.io/kind: manage
  ```

  > Этот лейбл обязателен.

- показывает, что роль является ролью подсистемы, и обрабатываться будет соответственно:

  ```yaml
  rbac.deckhouse.io/level: subsystem
  ```

- указывает подсистему, за которую отвечает роль:

  ```yaml
  rbac.deckhouse.io/subsystem: custom
  ```

- позволяет `manage:all`-роли агрегировать эту роль в себя:

  ```yaml
  rbac.deckhouse.io/aggregate-to-all-as: manager
  ```

Далее указаны селекторы, именно они реализуют агрегацию:

- агрегирует роль менеджера из подсистемы `deckhouse`:

  ```yaml
  rbac.deckhouse.io/kind: manage
  rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
  ```

- агрегирует все правила от модуля user-authn:

  ```yaml
   rbac.deckhouse.io/kind: manage
   module: user-authn
  ```

Таким образом роль получает права от подсистем `deckhouse`, `kubernetes` и от модуля user-authn.

Особенности:

* ограничений на имя роли нет, но для читаемости лучше использовать этот стиль;
* use-роли будут созданы в пространстве имён агрегированных подсистем и модуля, тип роли выбран лейблом.

##### Расширение пользовательской роли

Например, в кластере появился новый кластерный (пример для manage-роли) CRD-объект — MySuperResource, и нужно дополнить собственную роль из примера выше правами на взаимодействие с этим ресурсом.

Первым делом нужно дополнить роль новым селектором:

```yaml
rbac.deckhouse.io/kind: manage
rbac.deckhouse.io/aggregate-to-custom-as: manager
```

Этот селектор позволит агрегировать роли к новой подсистеме через указание этого лейбла. После добавления нового селектора роль будет выглядеть так:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   name: custom:manage:mycustom:manager
   labels:
     rbac.deckhouse.io/use-role: admin
     rbac.deckhouse.io/kind: manage
     rbac.deckhouse.io/level: subsystem
     rbac.deckhouse.io/subsystem: custom
     rbac.deckhouse.io/aggregate-to-all-as: manager
 aggregationRule:
   clusterRoleSelectors:
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         rbac.deckhouse.io/aggregate-to-kubernetes-as: manager
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         module: user-authn
     - matchLabels:
         rbac.deckhouse.io/kind: manage
         rbac.deckhouse.io/aggregate-to-custom-as: manager
 rules: []
 ```

 Далее нужно создать новую роль, в которой следует определить права для нового ресурса. Например, только чтение:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   labels:
     rbac.deckhouse.io/aggregate-to-custom-as: manager
     rbac.deckhouse.io/kind: manage
   name: custom:manage:permission:mycustom:superresource:view
 rules:
 - apiGroups:
   - mygroup.io
   resources:
   - mysuperresources
   verbs:
   - get
   - list
   - watch
 ```

Роль дополнит своими правами роль подсистемы, дав права на просмотр нового объекта.

Особенности:

* ограничений на имя роли нет, но для читаемости лучше использовать этот стиль.

##### Расширение существующих manage subsystem-ролей

Если необходимо расширить существующую роль, нужно выполнить те же шаги, что и в пункте выше, но изменив лейблы и название роли.

Пример для расширения роли менеджера из подсистемы `deckhouse`(`d8:manage:deckhouse:manager`):

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
    rbac.deckhouse.io/kind: manage
  name: custom:manage:permission:mycustommodule:superresource:view
rules:
- apiGroups:
  - mygroup.io
  resources:
  - mysuperresources
  verbs:
  - get
  - list
  - watch
```

Таким образом новая роль расширит роль `d8:manage:deckhouse`.

##### Расширение manage subsystem-ролей с добавлением нового пространства имён

Если необходимо добавить новое пространство имён (для создания в нём use-роли с помощью хука), потребуется добавить лишь один лейбл:

```yaml
"rbac.deckhouse.io/namespace": namespace
```

Этот лейбл сообщает хуку, что в этом пространстве имён нужно создать use-роль:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   labels:
     rbac.deckhouse.io/aggregate-to-deckhouse-as: manager
     rbac.deckhouse.io/kind: manage
     rbac.deckhouse.io/namespace: namespace
   name: custom:manage:permission:mycustom:superresource:view
 rules:
 - apiGroups:
   - mygroup.io
   resources:
   - mysuperresources
   verbs:
   - get
   - list
   - watch
 ```

Хук мониторит `ClusterRoleBinding` и при создании биндинга ходит по всем manage-ролям, чтобы найти все объединенные в них роли с помощью проверки правила агрегации. Затем он берёт пространство имён из лейбла `rbac.deckhouse.io/namespace` и создает use-роль в этом пространстве имён.

##### Расширение существующих use-ролей

Если ресурс принадлежит пространству имён, необходимо расширить use-роль вместо manage-роли. Разница лишь в лейблах и имени:

 ```yaml
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   labels:
     rbac.deckhouse.io/aggregate-to-kubernetes-as: user
     rbac.deckhouse.io/kind: use
   name: custom:use:capability:mycustom:superresource:view
 rules:
 - apiGroups:
   - mygroup.io
   resources:
   - mysuperresources
   verbs:
   - get
   - list
   - watch
 ```

Эта роль дополнит роль `d8:use:role:user:kubernetes`.

### Модуль secret-copier
 
Этот модуль отвечает за копирование секретов во все пространства имён.

Он полезен тем, что позволяет не копировать каждый раз в CI секреты для пуллинга образов и заказа RBD в Ceph.

{% alert level="warning" %}
Модуль `secret-copier` не может использоваться совместно с модулем `multitenancy-manager`.

`multitenancy-manager` создаёт изолированные окружения для пользователей в их проектах, а `secret-copier` автоматически раздаёт секреты во все пространства имён.
Если в закрытом окружении пользователя окажутся чувствительные данные, это может привести к утечке данных и нарушению модели безопасности.

Поэтому, если необходимо предоставить общий сертификат (например, WC-сертификат для внутреннего окружения) или общий токен доступа к registry, не используйте `secret-copier`.
Поместите такие секреты в шаблон проекта в `multitenancy-manager` – администратор кластера должен определить их в конфигурации проекта.
{% endalert %}

##### Как работает?

Модуль `secret-copier` следит за изменениями секретов в пространстве имён `default` с лейблом `secret-copier.deckhouse.io/enabled: ""`.
* Созданный секрет будет скопирован во все пространства имён.
* Изменённый секрет с его новым содержимым будет раскопирован во все пространства имён.
* При удалении секрет будет удален из всех пространств имён.
* При изменении скопированного секрета в прикладном пространстве имён, тот будет перезаписан оригинальным содержимым.
* При создании любого пространства имён в него копируются все секреты из пространства имён `default` с лейблом `secret-copier.deckhouse.io/enabled: ""`.

Кроме этого, каждую ночь секреты будут повторно синхронизированы и приведены к состоянию в пространств имён `default`.

##### Что нужно настроить?

Чтобы все заработало, достаточно создать в пространстве имён `default` секрет с лейблом `secret-copier.deckhouse.io/enabled: ""`.

> **Внимание!** Рабочим пространством имён для модуля является `default`, Секреты будут копироваться только из него. Ресурсы с лейблом `secret-copier.deckhouse.io/enabled: ""`, созданные в других пространствах имён при включенном модуле будут автоматически удалены.

##### Как ограничить список пространств имён, в которые будет производиться копирование?

Для этого нужно задать label–селектор в значении аннотации `secret-copier.deckhouse.io/target-namespace-selector`. Например: `secret-copier.deckhouse.io/target-namespace-selector: "app=custom"`. Модуль создаст копию этого секрета во всех пространствах имён, соответствующих заданному label–селектору.

### Модуль secrets-store-integration

Модуль secrets-store-integration реализует доставку секретов для приложения в Kubernetes-кластерах
путем подключения секретов, ключей и сертификатов, хранящихся во внешних хранилищах секретов.

Секреты монтируются в поды в виде тома с использованием реализации драйвера CSI.
Хранилища секретов должны быть совместимы с API-интерфейсом HashiCorp Vault.

#### Доставка секретов в приложения

Доставить секреты в приложение из vault-совместимого хранилища можно несколькими способами:

1. Пользовательское приложение само обращается в хранилище.

   > Это наиболее безопасный вариант, но требует модификации приложений.

1. В хранилище обращается приложение-прослойка, а ваше приложение получает доступ к секретам из файлов, созданных в контейнере.

   > Если нет возможности модифицировать приложение, используйте этот вариант. Он проще в реализации, но менее безопасный, так как секретные данные хранятся в файлах в контейнере.

1. В хранилище обращается приложение-прослойка, и пользовательское приложение получает доступ к секретам из переменных среды.

   > Если нет возможности читать из файлов, можно использовать этот вариант, но он небезопасен. При таком подходе секретные данные хранятся в Kubernetes (а так же в etcd) и потенциально могут быть прочитаны на любом узле кластера.

<table>
<thead>
<tr>
<th>Вариант доставки</th>
<th>Потребление ресурсов</th>
<th>Как приложение получает данные?</th>
<th>Где хранится в Kubernetes?</th>
<th>Статус</th>
</tr>
</thead>
<tbody>
<tr>
<td><a style="color: ##0066FF;" href="#вариант-1-получение-секретов-самим-приложением">Приложение</a></td>
<td>Не меняется</td>
<td>Напрямую из хранилища секретов</td>
<td>Не хранится</td>
<td>Реализовано</td>
</tr>
<tr>
<td><a style="color: ##0066FF;" href="#механизм-csi">Механизм CSI</a></td>
<td>Два пода на каждую ноду (daemonset)</td>
<td><ul><li>Из дискового тома (как файл)</li><li>Из переменной окружения</li></ul></td>
<td>Не хранится</td>
<td>Реализовано</td>
</tr>
<tr>
<td><a style="color: ##0066FF;" href="#вариант-3-инъекция-entrypoint">Инъекция entrypoint</a></td>
<td>Один под на каждую ноду (daemonset)</td>
<td>Секреты доставляются из хранилища в момент запуска приложения в виде переменных окружения</td>
<td>Не хранится</td>
<td>Реализовано</td>
</tr>
<tr>
<td><a style="color: ##0066FF;" href="#вариант-4-доставка-секретов-через-механизмы-kubernetes">Секреты Kubernetes</a></td>
<td>Одно приложение на кластер (deployment)</td>
<td><ul><li>Из дискового тома (как файл)</li><li>Из переменной окружения</li></ul></td>
<td>Хранится в Secrets</td>
<td>Планируется</td>
</tr>
<tr>
<td><a style="color: #A9A9A9; font-style: italic;" href="#справочно-инжектор-vault-agent">Инжектор vault-agent</a></td>
<td style="color: #A9A9A9; font-style: italic;">По одному агенту на каждый под (sidecar)</td>
<td style="color: #A9A9A9; font-style: italic;">Из дискового тома (как файл)</td>
<td style="color: #A9A9A9; font-style: italic;">Не хранится</td>
<td style="color: #A9A9A9; font-style: italic;"><sup><b>*</b></sup>Не будет реализовано</td>
</tr>
</tbody>
</table>

<i><sup>*</sup>Поддержка отсутствует и не планируется, поскольку этот вариант не имеет преимуществ перед использованием механизма CSI.</i>

##### Вариант №1: Получение секретов самим приложением

> *Статус:* наиболее безопасный вариант. Рекомендован к использованию, если есть возможность модификации приложений.

Приложение обращается к API Stronghold и запрашивает необходимый секрет по HTTPS-протоколу с использованием токена авторизации (токен из SA).

Плюсы:
- Секрет, полученный приложением, нигде не хранится, кроме как в самом приложении, нет опасности что он будет скомпрометирован в процессе передачи.

Минусы:

- Требует доработки приложения для возможности работы со Stronghold.
- Требует повторения реализации доступа к секретам в каждом приложении. В случае обновления библиотеки требует пересборки всех приложений.
- Приложение должно поддерживать TLS и проверку сертификатов.
- Нет кэширования. При перезапуске приложения нужно повторно запросить секрет напрямую из хранилища.

##### Вариант №2: Доставка секретов через файлы

###### Механизм CSI

> *Статус:* безопасный вариант. Рекомендован к использованию, если отсутствует возможность модификация приложений.

При создании подов, запрашивающих тома CSI, драйвер хранилища секретов CSI отправляет запрос к Vault CSI. Затем Vault CSI использует указанный SecretProviderClass и ServiceAccount пода для получения секретов из хранилища и монтирования их в том пода.

###### Инъекция переменных окружений:

Если нет возможности изменить код приложения, то можно реализовать безопасную инъекцию секрета в качестве переменной окружения для приложения.

Для этого нужно:
- прочитать все файлы, примонтированные CSI в контейнер;
- определить переменные окружения с именами, соответствующими именам файлов, и значениями, соответствующим содержимому файлов.
- запустить оригинальное приложение.

Пример на Bash:

```bash
bash -c "for file in $(ls /mnt/secrets); do export  $file=$(cat /mnt/secrets/$file); done ; exec my_original_file_to_startup"
```

Плюсы:

- Всего два контейнера с прогнозируемыми ресурсами на каждом узле для обслуживания системы доставки секретов в приложения;
- Создание ресурсов _SecretsStore/SecretProviderClass_ уменьшает количество повторяемого кода по сравнению с другими вариантами реализации vault agent;
- При необходимости есть возможность создавать копию секрета из хранилища в виде секрета Kubernetes.
- Секрет извлекается из хранилища драйвером CSI на этапе создания контейнера. Это означает, что запуск подов заблокируется до тех пор, пока секреты не будут прочитаны из хранилища и записаны в том.

##### Вариант №3: Инъекция entrypoint

###### Доставка переменных окружения через инъекцию entrypoint в контейнер

> *Статус:* безопасный вариант. В процессе реализации.

Переменные доставляются из хранилища в момент запуска приложения и находятся только в памяти. В момент первого этапа реализации метода переменные будут доставляться через entrypoint, проброшенный в контейнер. В дальнейшем планируется интеграция функционала доставки секретов в containerd.

##### Вариант №4: Доставка секретов через механизмы Kubernetes

> *Статус:* небезопасный вариант, не рекомендован к использованию. Поддержка отсутствует, но планируется в будущем.

Этот метод интеграции, который реализует оператор секретов Kubernetes с набором CRD, отвечающих за синхронизацию секретов из Vault в секреты Kubernetes.

Минусы:

- Секрет находится и в хранилище секретов, и в секрете Kubernetes (доступном через API Kubernetes). Секрет также хранится в etcd и потенциально может быть считан на любом узле кластера или извлечён из резервной копии etcd. Нет возможности не хранить данные в секретах Kubernetes.

Плюсы:

- Классический способ передачи секрета в приложение через переменные окружения — достаточно подключить секрет Kubernetes.

##### Справочно: Инжектор vault-agent

> *Статус:* не имеет плюсов в сравнении с механизмом CSI. Поддержка отсутствует и не планируется, поскольку этот вариант не имеет преимуществ перед использованием механизма CSI.

При создании пода происходит мутация, которая добавляет контейнер с vault-agent. Агент обращается к хранилищу секретов, извлекает их, и помещает в общий том на диске, к которому может обратиться приложение.

Минусы:

- Для каждого пода нужен sidecar-контейнер, который так или иначе потребляет ресурсы.

  Например, возьмем кластер в котором 50 приложений, и каждое приложение имеет от 3 до 15 реплик. Так как для каждого sidecar-контейнера с агентом нужно выделить ресурсы CPU и памяти, то даже при незначительных ресурсах для sidecar-контейнера в размере 0.05 CPU и 100 MiB памяти, на все приложения в сумме получаются десятки ядер CPU и десятки ГБ памяти.
- Так как сбор метрик осуществляется с каждого контейнера, то с таким подходом мы получим в два раза больше метрик только по контейнерам.

### The secrets-store-integration module: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['secrets-store-integration'].config-values | format_module_configuration: moduleKebabName }}

### Модуль secrets-store-integration: Custom Resources
{{ site.data.schemas.secrets-store-integration.crds.secrets-store-import | format_crd: "secrets-store-integration" }}
## Подсистема Мониторинг

### Модуль extended-monitoring

Модуль `extended-monitoring` расширяет возможности мониторинга кластера за счёт дополнительных Prometheus exporter’ов, которые позволяют выявлять потенциальные проблемы до того, как они скажутся на работе сервисов.

Возможности модуля:

- Расширенный сбор метрик — собирает дополнительные метрики, а также включает готовые алерты и дашборды, которые позволяют быстрее обнаруживать и диагностировать инциденты:
  - собирает и экспортирует метрики по свободному месту и inode на узлах, а также по объектам с лейблом `extended-monitoring.deckhouse.io/enabled=""` в пространстве имён;
  - автоматически формирует алерты при достижении пороговых значений.
- Мониторинг контейнерных образов:
  - добавляет метрики и отправляет алерты о недоступности образов контейнеров в registry для всех типов рабочей нагрузки (`Deployments`, `StatefulSets`, `DaemonSets`, `CronJobs`);
  - помогает заранее узнать о возможных проблемах с запуском или обновлением подов.
- События в кластере — собирает события Kubernetes и отображает их в виде метрик, что позволяет отслеживать динамику изменений и быстрее реагировать на инциденты.
- Контроль сертификатов:
  - сканирует Secret’ы кластера и генерирует метрики об истечении срока действия x509-сертификатов;
  - позволяет не пропускать критические моменты и вовремя обновлять сертификаты, избегая простоя приложений из-за просроченных сертификатов.

### Модуль extended-monitoring: настройки

#### Как использовать `extended-monitoring-exporter`

Чтобы включить экспортирование extended-monitoring метрик, нужно указать в пространстве имён лейбл `extended-monitoring.deckhouse.io/enabled` любым удобным способом, например:
- добавить в проект соответствующий helm-чарт (рекомендуемый);
- добавить в описание CI/CD (kubectl patch/create);
- добавить вручную (`d8 k label namespace my-app-production extended-monitoring.deckhouse.io/enabled=""`);
- настроить через [namespace-configurator](./namespace-configurator/) модуль.

Сразу же после этого для всех поддерживаемых Kubernetes-объектов в данном пространстве имён в Prometheus появятся default-метрики + любые кастомные с префиксом `threshold.extended-monitoring.deckhouse.io/`. Для ряда [non-namespaced](#non-namespaced-kubernetes-объекты) Kubernetes-объектов, описанных ниже, мониторинг включается автоматически.

К Kubernetes-объектам `threshold.extended-monitoring.deckhouse.io/что-то свое` можно добавить любые другие лейблы с указанным значением. Пример: `d8 k label pod test threshold.extended-monitoring.deckhouse.io/disk-inodes-warning=30`.
В таком случае значение из лейбла заменит значение по умолчанию.

Если вы хотите переопределить значения threshold для всех объектов в определенном пространстве имен, вы можете установить лейбл `threshold.extended-monitoring.deckhouse.io/` на уровне namespace. Например: `d8 k label namespace my-app-production threshold.extended-monitoring.deckhouse.io/5xx-warning=20`.
Это заменит значение по умолчанию для всех объектов namespace, для которых еще не установлен этот лейбл.

Слежение за объектом можно отключить индивидуально, поставив на него лейбл `extended-monitoring.deckhouse.io/enabled=false`. Соответственно, отключатся и лейблы по умолчанию, а также все алерты, привязанные к лейблам.

##### Стандартные лейблы и поддерживаемые Kubernetes-объекты

Далее приведен список используемых в Prometheus Rules лейблов, а также их стандартные значения.

**Обратите внимание,** что все лейблы начинаются с префикса `threshold.extended-monitoring.deckhouse.io/`. Указанное в лейбле значение — число, которое устанавливает порог срабатывания алерта.

Например, лейбл `threshold.extended-monitoring.deckhouse.io/5xx-warning: "5"` на Ingress-ресурсе изменяет порог срабатывания алерта с 10% (по умолчанию) на 5%.

###### Non-namespaced Kubernetes-объекты

Non-namespaced Kubernetes-объекты, то есть объекты вне пространств имён, не нуждаются в лейблах на этих пространствах и мониторинг на них включается по умолчанию при включении модуля.

####### Узел

| Лейбл                          | Тип           | Значение по умолчанию |
|--------------------------------|---------------|-----------------------|
| disk-bytes-warning             | int (percent) | 70                    |
| disk-bytes-critical            | int (percent) | 80                    |
| disk-inodes-warning            | int (percent) | 90                    |
| disk-inodes-critical           | int (percent) | 95                    |
| load-average-per-core-warning  | int           | 3                     |
| load-average-per-core-critical | int           | 10                    |

> **Важно!** Эти лейблы **не** действуют для тех разделов, в которых расположены `imagefs` (по умолчанию — `/var/lib/docker`) и `nodefs` (по умолчанию — `/var/lib/kubelet`).
Для этих разделов пороги настраиваются полностью автоматически согласно eviction thresholds в kubelet.
Значения по умолчанию указаны в документации Kubernetes, в описании экспортера.

###### Namespaced Kubernetes-объекты

####### Под

| Лейбл                | Тип           | Значение по умолчанию |
|----------------------|---------------|-----------------------|
| disk-bytes-warning   | int (percent) | 85                    |
| disk-bytes-critical  | int (percent) | 95                    |
| disk-inodes-warning  | int (percent) | 85                    |
| disk-inodes-critical | int (percent) | 90                    |

####### Ingress

| Лейбл        | Тип           | Значение по умолчанию |
|--------------|---------------|-----------------------|
| 5xx-warning  | int (percent) | 10                    |
| 5xx-critical | int (percent) | 20                    |

####### Deployment

| Лейбл              | Тип         | Значение по умолчанию |
|--------------------|-------------|-----------------------|
| replicas-not-ready | int (count) | 0                     |

Порог подразумевает количество недоступных реплик **сверх** maxUnavailable. Сработает, если недоступно реплик больше на указанное значение, чем разрешено в `maxUnavailable`. То есть при нуле сработает, если недоступно больше, чем указано в `maxUnavailable`, а при единице сработает, если недоступно больше, чем указано в `maxUnavailable`, плюс 1. Таким образом, у конкретных Deployment, которые находятся в пространстве имён со включенным расширенным мониторингом и которым допустимо быть недоступными, можно установить этот параметр, чтобы не получать ненужные алерты.

####### StatefulSet

| Лейбл              | Тип         | Значение по умолчанию |
|--------------------|-------------|-----------------------|
| replicas-not-ready | int (count) | 0                     |

Порог подразумевает количество недоступных реплик **сверх** maxUnavailable (см. комментарии к [Deployment](#deployment)).

####### DaemonSet

| Лейбл              | Тип         | Значение по умолчанию |
|--------------------|-------------|-----------------------|
| replicas-not-ready | int (count) | 0                     |

Порог подразумевает количество недоступных реплик **сверх** maxUnavailable (см. комментарии к [Deployment](#deployment)).

####### CronJob

Работает только выключение через лейбл `extended-monitoring.deckhouse.io/enabled=false`.

##### Как работает

Модуль экспортирует в Prometheus специальные лейблы Kubernetes-объектов. Позволяет улучшить Prometheus-правила путем добавления порога срабатывания для алертов.
Использование метрик, экспортируемых данным модулем, позволяет, например, заменить «магические» константы в правилах.

До:

```text
(
  kube_statefulset_status_replicas - kube_statefulset_status_replicas_ready
)
> 1
```

После:

```text
(
  kube_statefulset_status_replicas - kube_statefulset_status_replicas_ready
)
> on (namespace, statefulset)
(
  max by (namespace, statefulset) (extended_monitoring_statefulset_threshold{threshold="replicas-not-ready"})
)
```

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['extended-monitoring'].config-values | format_module_configuration: moduleKebabName }}

### Extended monitoring модуль: FAQ

{% raw %}

#### Как переключиться на HTTP вместо HTTPS для проверки образов из собственного registry?

Чтобы изменить протокол проверки вашего registry с HTTPS на HTTP, измените параметр `settings.imageAvailability.registry.scheme` в конфигурации модуля.

Подробные инструкции смотрите в [документации по настройке модуля](./configuration.html#parameters-imageavailability-registry-scheme).

{% endraw %}

### Модуль loki

В Kubernetes системные логи на узлах сохраняются недолго и могут быть утеряны при перезапуске или обновлении. Этот модуль разворачивает в кластере собственное хранилище оперативных логов на базе Grafana Loki.

Возможности:

- системные логи автоматически попадают в Loki без дополнительной настройки;
- доступ к логам реализован через Grafana и веб-интерфейс Deckhouse Platform Certified Security Edition (console);
- модуль предназначен для хранения логов в течение короткого времени. Для долгосрочного хранения или архивирования рекомендуется использовать внешние системы, поддерживаемые через [log-shipper](./log-shipper/).

### Модуль loki: настройки

 
<!-- SCHEMA -->

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: loki
spec:
  settings:
    storageClass: ceph-csi-rbd
    diskSizeGigabytes: 10
    retentionPeriodHours: 48
  enabled: true
  version: 1
```
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['loki'].config-values | format_module_configuration: moduleKebabName }}

### Модуль loki: примеры

{% raw %}

#### Чтение логов из всех подов из указанного namespace и направление их в Loki

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: loki
spec:
  settings:
    storageClass: ceph-csi-rbd
    diskSizeGigabytes: 30
    retentionPeriodHours: 168
  enabled: true
  version: 1
---
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: development-logs
spec:
  type: KubernetesPods
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [development]
  destinationRefs:
    - d8-loki
```

Больше примеров в описании модуля [log-shipper](./log-shipper/examples.html).

{% endraw %}

### Модуль log-shipper

Модуль `log-shipper` упрощает настройку сбора логов в Kubernetes. Он позволяет быстро организовать сбор логов как с приложений, запущенных в кластере, так и с самих узлов, а затем отправлять их в любую систему хранения — внутреннюю или внешнюю (например, Loki, Elasticsearch, S3 и другие).

Возможности log-shipper:

- централизованно собирает и передаёт логи из кластера;
- фильтрует, преобразовывает и обогащает логи перед отправкой;
- настраивает маршрутизацию логов между различными источниками и приемниками.

![log-shipper architecture](images/log_shipper_architecture.svg)


1. Deckhouse следит за ресурсами [ClusterLoggingConfig](cr.html#clusterloggingconfig), [ClusterLogDestination](cr.html#clusterlogdestination) и [PodLoggingConfig](cr.html#podloggingconfig).
   Комбинация конфигурации для сбора логов и направления для отправки называется `pipeline`.
2. Deckhouse генерирует конфигурационный файл и сохраняет его в `Secret` в Kubernetes.
3. `Secret` монтируется всем подам агентов `log-shipper`, конфигурация обновляется при ее изменении с помощью sidecar-контейнера `reloader`.

#### Топологии отправки

Этот модуль отвечает за агентов на каждом узле. Подразумевается, что логи из кластера отправляются согласно одной из описанных ниже топологий.

##### Распределенная

Агенты шлют логи напрямую в хранилище, например в Loki или Elasticsearch.

![log-shipper distributed](images/log_shipper_distributed.svg)


* Менее сложная схема для использования.
* Доступна из коробки без лишних зависимостей, кроме хранилища.
* Сложные трансформации потребляют больше ресурсов на узлах для приложений.

##### Централизованная

Все логи отсылаются в один из доступных агрегаторов, например, Logstash, Vector.
Агенты на узлах стараются отправить логи с узла максимально быстро с минимальным потреблением ресурсов.
Сложные преобразования применяются на стороне агрегатора.

![log-shipper centralized](images/log_shipper_centralized.svg)


* Меньшее потребление ресурсов для приложений на узлах.
* Пользователи могут настроить в агрегаторе любые трансформации и слать логи в гораздо большее количество хранилищ.
* Количество выделенных узлов под агрегаторы может увеличиваться или уменьшаться в зависимости от нагрузки.

##### Потоковая

Главная задача этой архитектуры — как можно быстрее отправить логи в очередь сообщений, из которой они в служебном порядке будут переданы в долгосрочное хранилище для дальнейшего анализа.

![log-shipper stream](images/log_shipper_stream.svg)


* Те же плюсы и минусы, что и у централизованной архитектуры, но добавляется еще одно промежуточное хранилище.
* Повышенная надежность. Подходит тем, для кого доставка логов является наиболее критичной.

#### Метаданные

При сборе логов сообщения будут обогащены метаданными в зависимости от способа их сбора. Обогащение происходит на этапе `Source`.

##### Kubernetes

Следующие поля будут экспортированы:

| Label        | Pod spec path           |
|--------------|-------------------------|
| `pod`        | metadata.name           |
| `namespace`  | metadata.namespace      |
| `pod_labels` | metadata.labels         |
| `pod_ip`     | status.podIP            |
| `image`      | spec.containers[].image |
| `container`  | spec.containers[].name  |
| `node`       | spec.nodeName           |
| `pod_owner`  | metadata.ownerRef[0]    |

| Label        | Node spec path                            |
|--------------|-------------------------------------------|
| `node_group` | metadata.labels[].node.deckhouse.io/group |

{% alert -%}
Для Splunk поля `pod_labels` не экспортируются, потому что это вложенный объект, который не поддерживается в Splunk.
{%- endalert %}

##### File

Лейбл `host` - это единственный лейбл, в котором записан hostname сервера.

#### Фильтры сообщений

Существуют два фильтра для снижения количества отправляемых сообщений в хранилище, — `log filter` и `label filter`.

![log-shipper pipeline](images/log_shipper_pipeline.svg)


Они запускаются сразу после объединения строк с помощью multiline parser.

1. `label filter` — правила запускаются для метаданных сообщения. Поля для метаданных (или лейблов) наполняются на основании источника логов, и для разных источников будет разный набор полей. Эти правила необходимы, например, чтобы отбросить сообщения от определенного контейнера или пода с/без какой-то метки.
1. `log filter` — правила запускаются для исходного сообщения. Существует возможность отбрасывать сообщение на основе поля JSON или, если сообщение не имеет формата JSON, использовать регулярное выражение для поиска в строке.

Оба фильтра имеют одинаковую структурированную конфигурацию:

* `field` — источник данных для запуска фильтрации (чаще всего это значение лейбла или параметра из JSON-документа).
* `operator` — действие для сравнения, доступные варианты — In, NotIn, Regex, NotRegex, Exists, DoesNotExist.
* `values` — определяет разные значения для разных операторов:
  - DoesNotExist, Exists — не поддерживается;
  - In, NotIn — значение поля должно равняться или не равняться одному из значений в списке values;
  - Regex, NotRegex — значение должно подходить хотя бы под одно или не подходить ни под одно регулярное выражение из списка values.

Найти больше примеров можно в разделе [Примеры](examples.html) документации.

{% alert -%}
Extra labels добавляются на этапе `Destination`, поэтому невозможно фильтровать логи на их основании.
{%- endalert %}

### Модуль log-shipper: настройки

Модуль начинает чтение логов, только если создан pipeline в виде связанных между собой [ClusterLoggingConfig](cr.html#clusterloggingconfig)/[PodLoggingConfig](cr.html#podloggingconfig) и [ClusterLogDestination](cr.html#clusterlogdestination).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['log-shipper'].config-values | format_module_configuration: moduleKebabName }}

### Модуль log-shipper: Custom Resources
{{ site.data.schemas.log-shipper.crds.cluster-log-destination | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.cluster-logging-config | format_crd: "log-shipper" }}
{{ site.data.schemas.log-shipper.crds.pod-logging-config | format_crd: "log-shipper" }}

### Модуль log-shipper: примеры

{% raw %}

#### Чтение логов из всех подов кластера и направление их в Loki

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: all-logs
spec:
  type: KubernetesPods
  destinationRefs:
  - loki-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

#### Чтение логов подов из указанного namespace с указанным label и перенаправление одновременно в Loki и Elasticsearch

Чтение логов подов из namespace `whispers` только с label `app=booking` и перенаправление одновременно в Loki и Elasticsearch:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: whispers-booking-logs
spec:
  type: KubernetesPods
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [whispers]
    labelSelector:
      matchLabels:
        app: booking
  destinationRefs:
  - loki-storage
  - es-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    index: logs-%F
    auth:
      strategy: Basic
      user: elastic
      password: c2VjcmV0IC1uCg==
```

#### Создание source в namespace и чтение логов всех подов в этом NS с направлением их в Loki

Следующий pipeline создает source в namespace `test-whispers`, читает логи всех подов в этом NS и пишет их в Loki:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: PodLoggingConfig
metadata:
  name: whispers-logs
  namespace: tests-whispers
spec:
  clusterDestinationRefs:
    - loki-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

#### Чтение только подов в указанном namespace и с определенным label

Пример чтения только подов, имеющих label `app=booking`, в namespace `test-whispers`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: PodLoggingConfig
metadata:
  name: whispers-logs
  namespace: tests-whispers
spec:
  labelSelector:
    matchLabels:
      app: booking
  clusterDestinationRefs:
    - loki-storage
---
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

#### Переход с Promtail на Log-Shipper

В ранее используемом URL Loki требуется убрать путь `/loki/api/v1/push`.

**Vector** сам добавит этот путь при работе с Loki.

#### Работа с Grafana Cloud

Данная документация подразумевает, что у вас уже создан ключ API.

Для начала вам потребуется закодировать в base64 ваш токен доступа к Grafana Cloud.

![Grafana cloud API key](images/grafana_cloud.png)

```bash
echo -n "<YOUR-GRAFANACLOUD-TOKEN>" | base64 -w0
```

Затем нужно создать **ClusterLogDestination**

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  loki:
    auth:
      password: PFlPVVItR1JBRkFOQUNMT1VELVRPS0VOPg==
      strategy: Basic
      user: "<YOUR-GRAFANACLOUD-USER>"
    endpoint: <YOUR-GRAFANACLOUD-URL> # Например https://logs-prod-us-central1.grafana.net или https://logs-prod-eu-west-0.grafana.net
  type: Loki
```

Теперь можно создать PodLogginConfig или ClusterPodLoggingConfig и отправлять логи в **Grafana Cloud**.

#### Добавление Loki в Deckhouse Grafana

Вы можете работать с Loki из встроенной в Deckhouse Grafana. Достаточно добавить [**GrafanaAdditionalDatasource**](/modules/prometheus/cr.html#grafanaadditionaldatasource).

```yaml
apiVersion: deckhouse.io/v1
kind: GrafanaAdditionalDatasource
metadata:
  name: loki
spec:
  access: Proxy
  basicAuth: false
  jsonData:
    maxLines: 5000
    timeInterval: 30s
  type: loki
  url: http://loki.loki:3100
```

#### Поддержка Elasticsearch < 6.X

Для Elasticsearch < 6.0 нужно включить поддержку doc_type индексов.
Сделать это можно следующим образом:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    docType: "myDocType" # Укажите значение здесь. Оно не должно начинаться с '_'.
    auth:
      strategy: Basic
      user: elastic
      password: c2VjcmV0IC1uCg==
```

#### Шаблон индекса для Elasticsearch

Существует возможность отправлять сообщения в определенные индексы на основе метаданных с помощью шаблонов индексов:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    index: "k8s-{{ namespace }}-%F"
```

В приведенном выше примере для каждого пространства имен Kubernetes будет создан свой индекс в Elasticsearch.

Эта функция также хорошо работает в комбинации с `extraLabels`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: es-storage
spec:
  type: Elasticsearch
  elasticsearch:
    endpoint: http://192.168.1.1:9200
    index: "k8s-{{ service }}-{{ namespace }}-%F"
  extraLabels:
    service: "{{ service_name }}"
```

1. Если сообщение имеет формат JSON, поле `service_name` этого документа JSON перемещается на уровень метаданных.
2. Новое поле метаданных `service` используется в шаблоне индекса.

#### Пример интеграции со Splunk

Существует возможность отсылать события из Deckhouse в Splunk.

1. Endpoint должен быть таким же, как имя вашего экземпляра Splunk с портом `8088` и без указания пути, например `https://prd-p-xxxxxx.splunkcloud.com:8088`.
2. Чтобы добавить token для доступа, откройте пункт меню `Setting` -> `Data inputs`, добавьте новый `HTTP Event Collector` и скопируйте token.
3. Укажите индекс Splunk для хранения логов, например `logs`.

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: splunk
spec:
  type: Splunk
  splunk:
    endpoint: https://prd-p-xxxxxx.splunkcloud.com:8088
    token: xxxx-xxxx-xxxx
    index: logs
    tls:
      verifyCertificate: false
      verifyHostname: false
```

{% endraw %}
{% alert -%}
`destination` не поддерживает метки пода для индексирования. Рассмотрите возможность добавления нужных меток с помощью опции `extraLabels`.
{%- endalert %}
{% raw %}

```yaml
extraLabels:
  pod_label_app: '{{ pod_labels.app }}'
```

#### Простой пример Logstash

Чтобы отправлять логи в Logstash, на стороне Logstash должен быть настроен входящий поток `tcp` и его кодек должен быть `json`.

Пример минимальной конфигурации Logstash:

```hcl
input {
  tcp {
    port => 12345
    codec => json
  }
}
output {
  stdout { codec => json }
}
```

Пример манифеста `ClusterLogDestination`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: logstash
spec:
  type: Logstash
  logstash:
    endpoint: logstash.default:12345
```

#### Syslog

Следующий пример показывает, как отправлять сообщения через сокет по протоколу TCP в формате syslog:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: rsyslog
spec:
  type: Socket
  socket:
    mode: TCP
    address: 192.168.0.1:3000
    encoding: 
      codec: Syslog
  extraLabels:
    syslog.severity: "alert"
    # поле request_id должно присутствовать в сообщении
    syslog.message_id: "{{ request_id }}"
```

#### Пример интеграции с Graylog

Убедитесь, что в Graylog настроен входящий поток для приема сообщений по протоколу TCP на указанном порту. Пример манифеста для интеграции с Graylog:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: test-socket2-dest
spec:
  type: Socket
  socket:
    address: graylog.svc.cluster.local:9200
    mode: TCP
    encoding:
      codec: GELF
```

#### Логи в CEF формате

Существует способ формировать логи в формате CEF, используя `codec: CEF`, с переопределением `cef.name` и `cef.severity` по значениям из поля `message` (лога приложения) в формате JSON.

В примере ниже `app` и `log_level` это ключи содержащие значения для переопределения:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: siem-kafka
spec:
  extraLabels:
    cef.name: '{{ app }}'
    cef.severity: '{{ log_level }}'
  type: Kafka
  kafka:
    bootstrapServers:
      - my-cluster-kafka-brokers.kafka:9092
    encoding:
      codec: CEF
    tls:
      verifyCertificate: false
      verifyHostname: true
    topic: logs
```

Так же можно вручную задать свои значения:

```yaml
extraLabels:
  cef.name: 'TestName'
  cef.severity: '1'
```

#### Сбор событий Kubernetes

События Kubernetes могут быть собраны log-shipper'ом, если `events-exporter` включен в настройках модуля [extended-monitoring](./extended-monitoring/).

Включите events-exporter, изменив параметры модуля `extended-monitoring`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: extended-monitoring
spec:
  version: 1
  settings:
    events:
      exporterEnabled: true
```

Выложите в кластер следующий `ClusterLoggingConfig`, чтобы собирать сообщения с пода `events-exporter`:

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: kubernetes-events
spec:
  type: KubernetesPods
  kubernetesPods:
    labelSelector:
      matchLabels:
        app: events-exporter
    namespaceSelector:
      labelSelector:
        matchExpressions:
        - key: "kubernetes.io/metadata.name"
          operator: In
          values: [d8-monitoring]
  destinationRefs:
  - loki-storage
```

#### Фильтрация логов

Пользователи могут фильтровать логи, используя следующие фильтры:

* `labelFilter` — применяется к метаданным, например имени контейнера (`container`), пространству имен (`namespace`) или имени пода (`pod_name`);
* `logFilter` — применяется к полям самого сообщения, если оно в JSON-формате.

##### Сборка логов только для контейнера `nginx`

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: nginx-logs
spec:
  type: KubernetesPods
  labelFilter:
  - field: container
    operator: In
    values: [nginx]
  destinationRefs:
  - loki-storage
```

##### Сборка логов без строки, содержащей `GET /status" 200`

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: all-logs
spec:
  type: KubernetesPods
  destinationRefs:
  - loki-storage
  labelFilter:
  - field: message
    operator: NotRegex
    values:
    - .*GET /status" 200$
```

##### Аудит событий kubelet'а

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: kubelet-audit-logs
spec:
  type: File
  file:
    include:
    - /var/log/kube-audit/audit.log
  logFilter:
  - field: userAgent  
    operator: Regex
    values: ["kubelet.*"]
  destinationRefs:
  - loki-storage
```

##### Системные логи Deckhouse

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: system-logs
spec:
  type: File
  file:
    include:
    - /var/log/syslog
  labelFilter:
  - field: message
    operator: Regex
    values:
    - .*d8-kubelet-forker.*
    - .*containerd.*
    - .*bashible.*
    - .*kernel.*
  destinationRefs:
  - loki-storage
```

{% endraw %}
{% alert -%}
Если вам нужны только логи одного пода или малой группы подов, постарайтесь использовать настройки `kubernetesPods`, чтобы сузить количество читаемых файлов. Фильтры необходимы только для высокогранулярной настройки.
{%- endalert %}
{% raw %}

#### Преобразование логов

##### Преобразование логов в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы преобразовать строку в поле `message` в структурированный объект.
При использовании нескольких трансформаций `ParseMessage`, преобразование строки должно выполняться последним.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: string-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: String
        string:
          targetField: msg
```

Пример изначальной записи в логе:

```text
/docker-entrypoint.sh: Configuration complete; ready for start up
```

Результат преобразования:

```json
{... "message": { 
  "msg": "/docker-entrypoint.sh: Configuration complete; ready for start up"
  }
}
```

##### Преобразование логов в формате Klog в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате Klog и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: klog-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: Klog
```

Пример изначальной записи в логе:

```text
I0505 17:59:40.692994   28133 klog.go:70] hello from klog
```

Результат преобразования:

```json
{... "message": {
  "file":"klog.go",
  "id":28133,
  "level":"info",
  "line":70,
  "message":"hello from klog",
  "timestamp":"2025-05-05T17:59:40.692994Z"
  }
}
```

##### Преобразование логов в формате Syslog в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате Syslog и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: syslog-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: Syslog
```

Пример изначальной записи в логе:

```text
<13>1 2020-03-13T20:45:38.119Z dynamicwireless.name non 2426 ID931 [exampleSDID@32473 iut="3" eventSource= "Application" eventID="1011"] Try to override the THX port, maybe it will reboot the neural interface!
```

Результат преобразования:

```json
{... "message": {
  "appname": "non",
  "exampleSDID@32473": {
    "eventID": "1011",
    "eventSource": "Application",
    "iut": "3"
  },
  "facility": "user",
  "hostname": "dynamicwireless.name",
  "message": "Try to override the THX port, maybe it will reboot the neural interface!",
  "msgid": "ID931",
  "procid": 2426,
  "severity": "notice",
  "timestamp": "2020-03-13T20:45:38.119Z",
  "version": 1
}}
```

##### Преобразование логов в формате CLF в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате CLF и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: clf-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: CLF
```

Пример изначальной записи в логе:

```text
127.0.0.1 bob frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326
```

Результат преобразования:

```json
{... "message": {
  "host": "127.0.0.1",
  "identity": "bob",
  "message": "GET /apache_pb.gif HTTP/1.0",
  "method": "GET",
  "path": "/apache_pb.gif",
  "protocol": "HTTP/1.0",
  "size": 2326,
  "status": 200,
  "timestamp": "2000-10-10T20:55:36Z",
  "user": "frank"
}}
```

##### Преобразование логов в формате LogFmt в структурированный объект

Вы можете использовать трансформацию `ParseMessage`,
чтобы распарсить логи в формате Logfmt и преобразовать их в структурированный объект.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: logfmt-to-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: Logfmt
```

Пример изначальной записи в логе:

```text
@timestamp=\"Sun Jan 10 16:47:39 EST 2021\" level=info msg=\"Stopping all fetchers\" tag#production=stopping_fetchers id=ConsumerFetcherManager-1382721708341 module=kafka.consumer.ConsumerFetcherManager
```

Результат преобразования:

```json
{... "message": {
  "@timestamp": "Sun Jan 10 16:47:39 EST 2021",
  "id": "ConsumerFetcherManager-1382721708341",
  "level": "info",
  "module": "kafka.consumer.ConsumerFetcherManager",
  "msg": "Stopping all fetchers",
  "tag#production": "stopping_fetchers"
}}
```

##### Парсинг JSON и уменьшение вложенности

Вы можете использовать трансформацию `ParseMessage`, чтобы парсить записи в логах в формате JSON.
С помощью параметра `depth` можно контролировать глубину вложенности.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: parse-json
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: JSON
        json:
          depth: 1
```

Пример изначальной записи в логе:

```text
{"level" : { "severity": "info" },"msg" : "fetching.module.release"}
```

Результат преобразования:

```json
{... "message": {
  "level" : "{ \"severity\": \"info\" }",
  "msg" : "fetching.module.release"
  }
}
```

##### Пример парсинга записей смешанных форматов в объект

Преобразование строки должно выполняться последним.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: parse-json
spec:
  ...
  transformations:
  - action: ParseMessage
    parseMessage:
      sourseFormat: JSON
  - action: ParseMessage
    parseMessage:
      sourceFormat: Klog
  - action: ParseMessage
    parseMessage:
      sourceFormat: String
        string:
          targetField: "text"
```

Пример изначальной записи в логе:

```text
/docker-entrypoint.sh: Configuration complete; ready for start up
{"level" : { "severity": "info" },"msg" : "fetching.module.release"}
I0505 17:59:40.692994   28133 klog.go:70] hello from klog
```

Результат преобразования:

```json
{... "message": {
  "text": "/docker-entrypoint.sh: Configuration complete; ready for start up"
  }
}
{... "message": {
  "level" : "{ "severity": "info" }",
  "msg" : "fetching.module.release"
  }
}
{... "message": {
  "file":"klog.go",
  "id":28133,
  "level":"info",
  "line":70,
  "message":"hello from klog",
  "timestamp":"2025-05-05T17:59:40.692994Z"
  }
}
```

##### Замена лейблов

Вы можете использовать трансформацию `ReplaceKeys`, чтобы заменить `source` на `target` в заданных ключах лейблов.

> Перед применением трансформации `ReplaceKeys` к полю `message` или его вложенным полям
> необходимо преобразовать запись лога в структурированный объект с помощью трансформации `ParseMessage`.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: replace-dot
spec:
  ...
  transformations:
    - action: ReplaceKeys
      replaceKeys:
        source: "."
        target: "_"
        labels:
          - .pod_labels
```

Пример изначальной записи в логе:

```text
{"msg" : "fetching.module.release"} # Лейбл пода pod.app=test
```

Результат преобразования:

```json
{... "message": {
  "msg" : "fetching.module.release"
  },
  "pod_labels": {
    "pod_app": "test"
  }
}
```

##### Удаление лейблов

Вы можете использовать трансформацию `DropLabels`, чтобы удалить заданные лейблы из записей логов.

> Перед применением трансформации `DropLabels` к полю `message` или его вложенным полям
> необходимо преобразовать запись лога в структурированный объект с помощью трансформации `ParseMessage`.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: drop-label
spec:
  ...
  transformations:
    - action: DropLabels
      dropLabels:
        labels:
          - .example
```

###### Пример удаления заданного лейбла из структурированного сообщения

В этом примере показано как вы можете удалить лейбл из структурированного JSON-сообщения.
Сначала применяется трансформация `ParseMessage` для парсинга сообщения,
после чего применяется `DropLabels` для удаления указанного лейбла.

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLogDestination
metadata:
  name: drop-label
spec:
  ...
  transformations:
    - action: ParseMessage
      parseMessage:
        sourceFormat: JSON
    - action: DropLabels
      dropLabels:
        labels:
          - .message.example
```

Пример изначальной записи в логе:

```text
{"msg" : "fetching.module.release", "example": "test"}
```

Результат преобразования:

```json
{... "message": {
  "msg" : "fetching.module.release"
  }
}
```

#### Настройка сборки логов с продуктовых пространств имен, используя опцию namespace label selector

```yaml
apiVersion: deckhouse.io/v1alpha2
kind: ClusterLoggingConfig
metadata:
  name: production-logs
spec:
  type: KubernetesPods
  kubernetesPods:
    namespaceSelector:
      labelSelector:
        matchLabels:
          environment: production
  destinationRefs:
  - loki-storage
```

#### Исключение подов и пространств имён, используя label

Существует преднастроенный label для исключения определенных подов и пространств имён: `log-shipper.deckhouse.io/exclude=true`.
Он помогает остановить сбор логов с подов и пространств имён без изменения глобальной конфигурации.

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: test-namespace
  labels:
    log-shipper.deckhouse.io/exclude: "true"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-deployment
spec:
  ...
  template:
    metadata:
      labels:
        log-shipper.deckhouse.io/exclude: "true"
```

#### Включение буферизации

Настройка буферизации логов необходима для улучшения надежности и производительности системы сбора логов. Буферизация может быть полезна в следующих случаях:

1. Временные перебои с подключением. Если есть временные перебои или нестабильность соединения с системой хранения логов (например, с Elasticsearch), буфер позволяет временно сохранять логи и отправить их, когда соединение восстановится.

1. Сглаживание пиков нагрузки. При внезапных всплесках объёма логов буфер позволяет сгладить пиковую нагрузку на систему хранения логов, предотвращая её перегрузку и потенциальную потерю данных.

1. Оптимизация производительности. Буферизация помогает оптимизировать производительность системы сбора логов за счёт накопления логов и отправки их группами, что снижает количество сетевых запросов и улучшает общую пропускную способность.

##### Пример включения буферизации в оперативной памяти

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  buffer:
    memory:
      maxEvents: 4096
    type: Memory
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

##### Пример включения буферизации на диске

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  buffer:
    disk:
      maxSize: 1Gi
    type: Disk
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

##### Пример определения поведения при переполнении буфера

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ClusterLogDestination
metadata:
  name: loki-storage
spec:
  buffer:
    disk:
      maxSize: 1Gi
    type: Disk
    whenFull: DropNewest
  type: Loki
  loki:
    endpoint: http://loki.loki:3100
```

Более подробное описание параметров доступно [в ресурсе ClusterLogDestination](cr.html#clusterlogdestination).

{% endraw %}

### The log-shipper module: FAQ

#### Как добавить авторизацию в ресурс _ClusterLogDestination_?

Чтобы добавить параметры авторизации в ресурс [ClusterLogDestination](cr.html#clusterlogdestination), необходимо:
- изменить [протокол](cr.html#clusterlogdestination-v1alpha1-spec-loki-endpoint) подключения к Loki на HTTPS;
- добавить секцию [auth](cr.html#clusterlogdestination-v1alpha1-spec-loki-auth), в которой:
  - параметр [strategy](cr.html#clusterlogdestination-v1alpha1-spec-loki-auth-strategy) установить в `Bearer`;
  - в параметре [token](cr.html#clusterlogdestination-v1alpha1-spec-loki-auth-token) указать токен `log-shipper-token` из пространства имен `d8-log-shipper`.

Пример:

- Ресурс _ClusterLogDestination_ без авторизации:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: loki
  spec:
    type: Loki
    loki:
      endpoint: "http://loki.d8-monitoring:3100"
  ```

- Получите токен `log-shipper-token` из пространства имен `d8-log-shipper`:

  ```bash
  d8 k -n d8-log-shipper get secret log-shipper-token -o jsonpath='{.data.token}' | base64 -d
  ```

- Ресурс _ClusterLogDestination_ с авторизацией:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ClusterLogDestination
  metadata:
    name: loki
  spec:
    type: Loki
    loki:
      endpoint: "https://loki.d8-monitoring:3100"
      auth:
        strategy: "Bearer"
        token: <log-shipper-token>
      tls:
        verifyHostname: false
        verifyCertificate: false
  ```

### Автоматическая настройка системы мониторинга для сбора метрик с пользовательских приложений

Для мониторинга пользовательских приложений, запущенных в кластере, требуется дополнительная настройка. Этот модуль упрощает процесс настройки, сводя её к указанию для нужного приложения определённого лейбла.

Чтобы организовать сбор метрик с приложений модулем `monitoring-custom`, необходимо:

- Поставить лейбл `prometheus.deckhouse.io/custom-target` на Service или под. Значение лейбла определит имя в списке target'ов Prometheus.
  - В качестве значения лейбла `prometheus.deckhouse.io/custom-target` рекомендуется использовать название приложения (маленькими буквами, разделитель `-`), которое позволяет его уникально идентифицировать в кластере.

     Если приложение ставится в кластер больше одного раза (staging, testing и т. д.) или даже ставится несколько раз в одно пространство имён, достаточно одного общего названия, так как у всех метрик в любом случае будут лейблы `namespace`, `pod` и, если доступ осуществляется через Service, лейбл `service`. Это название, уникально идентифицирующее приложение в кластере, а не его единичную инсталляцию.
- Порту, с которого нужно собирать метрики, указать имя `http-metrics` и `https-metrics` для подключения по HTTP или HTTPS соответственно.

  Если это невозможно (например, порт уже определен и назван другим именем), необходимо воспользоваться аннотациями: `prometheus.deckhouse.io/port: номер_порта` — для указания порта и `prometheus.deckhouse.io/tls: "true"` — если сбор метрик будет проходить по HTTPS.

  > При указании аннотации на Service в качестве значения порта необходимо использовать `targetPort`. То есть тот порт, что открыт и слушается приложением, а не порт Service'а.

  - Пример 1:

    ```yaml
    ports:
    - name: https-metrics
      containerPort: 443
    ```

  - Пример 2:

    ```yaml
    annotations:
      prometheus.deckhouse.io/port: "443"
      prometheus.deckhouse.io/tls: "true"  # Если метрики отдаются по HTTP, эту аннотацию указывать не нужно.
    ```

- При использовании service mesh [Istio](./istio/) в режиме STRICT mTLS указать для сбора метрик следующую аннотацию у Service или Pod: `prometheus.deckhouse.io/istio-mtls: "true"`. Важно, что метрики приложения должны экспортироваться по протоколу HTTP без TLS.

- *(Необязательно)* Укажите дополнительные аннотации для более тонкой настройки:

  * `prometheus.deckhouse.io/path` — путь для сбора метрик (по умолчанию: `/metrics`).
  * `prometheus.deckhouse.io/query-param-$name` — GET-параметры, будут преобразованы в map вида `$name=$value` (по умолчанию: ''):
    - возможно указать несколько таких аннотаций.

      Например, `prometheus.deckhouse.io/query-param-foo=bar` и `prometheus.deckhouse.io/query-param-bar=zxc` будут преобразованы в query: `http://...?foo=bar&bar=zxc`.
  * `prometheus.deckhouse.io/allow-unready-pod` — разрешает сбор метрик с подов в любом состоянии (по умолчанию метрики собираются только с подов в состоянии Ready). Эта опция полезна в редких случаях. Например, если ваше приложение запускается очень долго (при старте загружаются данные в базу или прогреваются кэши), но в процессе запуска уже отдаются полезные метрики, которые помогают следить за запуском приложения.
  * `prometheus.deckhouse.io/sample-limit` — сколько семплов разрешено собирать с пода (по умолчанию 5000). Значение по умолчанию защищает от ситуации, когда приложение внезапно начинает отдавать слишком большое количество метрик, что может нарушить работу всего мониторинга. Аннотация должна быть размещена на том же ресурсе, на котором висит лейбл  `prometheus.deckhouse.io/custom-target`.

##### Пример: Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    prometheus.deckhouse.io/custom-target: my-app
  annotations:
    prometheus.deckhouse.io/port: "8061"                      # По умолчанию будет использоваться порт сервиса с именем http-metrics или https-metrics.
    prometheus.deckhouse.io/path: "/my_app/metrics"           # По умолчанию /metrics.
    prometheus.deckhouse.io/query-param-format: "prometheus"  # По умолчанию ''.
    prometheus.deckhouse.io/allow-unready-pod: "true"         # По умолчанию поды НЕ в Ready игнорируются.
    prometheus.deckhouse.io/sample-limit: "5000"              # По умолчанию принимается не больше 5000 метрик от одного пода.
spec:
  ports:
  - name: my-app
    port: 8060
  - name: http-metrics
    port: 8061
    targetPort: 8061
  selector:
    app: my-app
```

##### Пример: Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app
spec:
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
        prometheus.deckhouse.io/custom-target: my-app
      annotations:
        prometheus.deckhouse.io/sample-limit: "5000"  # По умолчанию принимается не больше 5000 метрик от одного пода.
    spec:
      containers:
      - name: my-app
        image: my-app:1.7.9
        ports:
        - name: https-metrics
          containerPort: 443
```

### Автоматическая настройка системы мониторинга для сбора метрик с пользовательских приложений: настройки

Модуль работает, если включен модуль `prometheus`, и не имеет параметров для настройки.

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['monitoring-custom'].config-values | format_module_configuration: moduleKebabName }}

### Модуль monitoring-kubernetes

Модуль `monitoring-kubernetes` обеспечивает прозрачный и своевременный контроль состояния всех узлов кластера и ключевых инфраструктурных компонентов.

Возможности модуля:

- предоставляет возможность планировать ресурсы инфраструктуры (Capacity planning);
- отслеживает версию container runtime (docker, containerd) на каждом узле и проверяет её на соответствие разрешенным версиям;
- контролирует работоспособность самой подсистемы мониторинга кластера (Dead man’s switch);
- снимает метрики о доступности файловых дескрипторов, сокетов, свободного места и inode на каждом узле;
- следит за корректной работой ключевых компонентов мониторинга: kube-state-metrics, node-exporter, kube-dns;
- проверяет состояние всех узлов (`NotReady`, `drain`, `cordon`) и своевременно сигнализирует о неполадках;
- следит за синхронизацией времени и уведомляет об отклонениях;
- выявляет случаи продолжительного превышения CPU steal (когда узел не получает нужного времени процессора);
- контролирует состояние таблицы Conntrack на узлах;
- показывает поды с некорректными статусами — например, если kubelet не справился со своей работой;
- позволяет экспортировать метрики во внешние системы мониторинга для единой точки контроля.

### Модуль monitoring-kubernetes: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['monitoring-kubernetes'].config-values | format_module_configuration: moduleKebabName }}

### Мониторинг control plane

Мониторинг control plane осуществляется с помощью модуля `monitoring-kubernetes-control-plane`, который организует безопасный сбор метрик и предоставляет базовый набор правил мониторинга следующих компонентов кластера:
* kube-apiserver;
* kube-controller-manager;
* kube-scheduler;
* kube-etcd.

### Мониторинг control plane: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['monitoring-kubernetes-control-plane'].config-values | format_module_configuration: moduleKebabName }}

### Модуль monitoring-ping

#### Описание

Модуль мониторинга сетевого взаимодействия обеспечивает непрерывную проверку связности между всеми основными и, при необходимости, внешними узлами кластера.

Возможности и особенности модуля:

- автоматически проверяет доступность всех узлов кластера (и, опционально, внешних систем) с помощью ICMP (ping) — тестирование запускается каждые две секунды;
- все результаты экспортируются в формате метрик в систему мониторинга Prometheus;
- в комплекте — готовый дашборд для Grafana, где в реальном времени визуализируются текущая доступность, графики задержек и потенциальные проблемы с сетевой связностью;
- позволяет быстро выявлять узлы с деградировавшей связностью и ускоряет реакцию на инциденты.

#### Как работает

Модуль отслеживает любые изменения поля `.status.addresses` узла. Если они обнаружены, срабатывает хук, который собирает полный список имен узлов и их адресов и передает в daemonSet, который заново создает поды.
Таким образом, `ping` проверяет всегда актуальный список узлов.

### Модуль monitoring-ping: настройки

У модуля нет обязательных настроек.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['monitoring-ping'].config-values | format_module_configuration: moduleKebabName }}

### Модуль operator-prometheus

Модуль устанавливает prometheus operator.



Функции устанавливаемого оператора:

- определяет следующие кастомные ресурсы:
  - `Prometheus` — определяет инсталляцию (кластер) *Prometheus*
  - `ServiceMonitor` — определяет, как собирать метрики с сервисов
  - `Alertmanager` — определяет кластер *Alertmanager*'ов
  - `PrometheusRule` — определяет список *Prometheus rules*
- следит за этими ресурсами и:
  - генерирует `StatefulSet` с самим *Prometheus* и необходимые для его работы конфигурационные файлы, сохраняя их в `Secret`;
  - следит за ресурсами `ServiceMonitor` и `PrometheusRule` и на их основании обновляет конфигурационные файлы *Prometheus* через внесение изменений в `Secret`.

#### Prometheus

##### Что делает Prometheus?

В целом, сервер Prometheus делает две ключевых вещи — **собирает метрики** и **выполняет правила**:

* Для каждого *target'а* (цель для мониторинга), каждый `scrape_interval`, делает HTTP запрос на этот *target*, получает в ответ метрики в своем формате, которые сохраняет к себе в базу
* Каждый `evaluation_interval` обрабатывает *rules*, на основании чего:
  * или шлет алерты
  * или записывает (себе же в базу) новые метрики (результат выполнения *rule'а*)

##### Как настраивается Prometheus?

* У сервера Prometheus есть *config* и есть *rule files* (файлы с правилами)
* В `config` имеются следующие секции:
  * `scrape_configs` — настройки поиска *target'ов* (целей для мониторинга, см. подробней следующий раздел).
  * `rule_files` — список директорий, в которых лежат *rule'ы*, которые необходимо загружать:

    ```yaml
    rule_files:
    - /etc/prometheus/rules/rules-0/*
    - /etc/prometheus/rules/rules-1/*
    ```

  * `alerting` — настройки поиска *Alert Manager'ов*, в которые слать алерты. Секция очень похожа на `scrape_configs`, только результатом ее работы является список *endpoint'ов*, в которые Prometheus будет слать алерты.

##### Где Prometheus берет список *target'ов*?

* В целом Prometheus работает следующим образом:

  ![Работа Prometheus](images/targets.png)

  * **(1)** Prometheus читает секцию конфигурации `scrape_configs`, согласно которой настраивает свой внутренний механизм Service Discovery
  * **(2)** Механизм Service Discovery взаимодействует с API Kubernetes (в основном — получает endpoint`ы)
  * **(3)** На основании происходящего в Kubernetes механизм Service Discovery обновляет Targets (список *target'ов*)
* В `scrape_configs` указан список *scrape job'ов* (внутреннее понятие Prometheus), каждый из которых определяется следующим образом:

  ```yaml
  scrape_configs:
    # Общие настройки
  - job_name: d8-monitoring/custom/0    # просто название scrape job'а, показывается в разделе Service Discovery
    scrape_interval: 30s                  # как часто собирать данные
    scrape_timeout: 10s                   # таймаут на запрос
    metrics_path: /metrics                # path, который запрашивать
    scheme: http                          # http или https
    # Настройки service discovery
    kubernetes_sd_configs:                # означает, что target'ы мы получаем из Kubernetes
    - api_server: null                    # означает, что адрес API-сервера использовать из переменных окружения (которые есть в каждом Pod'е)
      role: endpoints                     # target'ы брать из endpoint'ов
      namespaces:
        names:                            # искать endpoint'ы только в этих namespace'ах
        - foo
        - baz
    # Настройки "фильтрации" (какие enpoint'ы брать, а какие нет) и "релейблинга" (какие лейблы добавить или удалить, на все получаемые метрики)
    relabel_configs:
    # Фильтр по значению label'а prometheus_custom_target (полученного из связанного с endpoint'ом service'а)
    - source_labels: [__meta_kubernetes_service_label_prometheus_custom_target]
      regex: .+                           # подходит любой НЕ пустой лейбл
      action: keep
    # Фильтр по имени порта
    - source_labels: [__meta_kubernetes_endpointslice_port_name]
      regex: http-metrics                 # подходит, только если порт называется http-metrics
      action: keep
    # Добавляем label job, используем значение label'а prometheus_custom_target у service'а, к которому добавляем префикс "custom-"
    #
    # Лейбл job это служебный лейбл Prometheus:
    #    * он определяет название группы, в которой будет показываться target на странице targets
    #    * и конечно же он будет у каждой метрики, полученной у этих target'ов, чтобы можно было удобно фильтровать в rule'ах и dashboard'ах
    - source_labels: [__meta_kubernetes_service_label_prometheus_custom_target]
      regex: (.*)
      target_label: job
      replacement: custom-$1
      action: replace
    # Добавляем label namespace
    - source_labels: [__meta_kubernetes_namespace]
      regex: (.*)
      target_label: namespace
      replacement: $1
      action: replace
    # Добавляем label service
    - source_labels: [__meta_kubernetes_service_name]
      regex: (.*)
      target_label: service
      replacement: $1
      action: replace
    # Добавляем label instance (в котором будет имя Pod'а)
    - source_labels: [__meta_kubernetes_pod_name]
      regex: (.*)
      target_label: instance
      replacement: $1
      action: replace
  ```

* Таким образом, Prometheus сам отслеживает:
  * добавление и удаление Pod'ов (при добавлении/удалении Pod'ов Kubernetes изменяет endpoint'ы, а Prometheus это видит и добавляет/удаляет *target'ы*)
  * добавление и удаление сервисов (точнее endpoint'ов) в указанных namespace'ах
* Изменение конфигурации требуется в следующих случаях:
  * нужно добавить новый scrape config (обычно — новый вид сервисов, которые надо мониторить)
  * нужно изменить список namespace'ов

#### Prometheus Operator

##### Что делает Prometheus Operator?

* С помощью механизма CRD (Custom Resource Definitions) определяет четыре кастомных ресурса:
  * prometheus Prometheus
  * servicemonitor — определяет, как "мониторить" (собирать метрики) набор сервисов
  * alertmanager — определяет кластер Alertmanager'ов
  * prometheusrule — определяет список Prometheus rules
* Следит за ресурсами `prometheus` и генерирует для каждого:
  * StatefulSet (с самим Prometheus'ом)
  * Secret с `prometheus.yaml` (конфиг Prometheus'а) и `configmaps.json` (конфиг для `prometheus-config-reloader`)
* Следит за ресурсами `servicemonitor` и `prometheusrule` и на их основании обновляет конфиги (`prometheus.yaml` и `configmaps.json`, которые лежат в секрете).

##### Что в Pod'е с Prometheus'ом?

![Что в Pod Prometheus](images/pod.png)

* Три контейнера:
  * `prometheus` — сам Prometheus
  * `prometheus-config-reloader` — обвязка, которая:
    * следит за изменениями `prometheus.yaml` и, при необходимости, вызывает reload конфигурации Prometheus'у (специальным HTTP-запросом, см. [подробнее ниже](#как-обрабатываются-service-monitorы))
    * следит за PrometheusRule'ами (см. [подробнее ниже](#как-обрабатываются-кастомные-ресурсы-с-ruleами)) и по необходимости скачивает их и перезапускает Prometheus
  * `kube-rbac-proxy` — serves as an authentication and authorization proxy server based on RBAC for accessing Prometheus metrics.
* Pod использует несколько volume, из которых три — ключевые для работы Prometheus:
  * config — примонтированный secret (два файла: `prometheus.yaml` и `configmaps.json`). Подключен в оба контейнера.
  * rules — `emptyDir`, который наполняет `prometheus-config-reloader`, а читает `prometheus`. Подключен в оба контейнера, но в `prometheus` в режиме read only.
  * data — данные Prometheus. Подмонтирован только в `prometheus`.

##### Как обрабатываются Service Monitor'ы?

![Как обрабатываются Service Monitor'ы](images/servicemonitors.png)

1. Prometheus Operator читает (а также следит за добавлением/удалением/изменением) Service Monitor'ы (какие именно Service Monitor'ы — указано в самом ресурсе `prometheus`.
1. Для каждого Service Monitor'а, если в нем НЕ указан конкретный список namespace'ов (указано `any: true`), Prometheus Operator вычисляет (обращаясь к API Kubernetes) список namespace'ов, в которых есть Service'ы (подходящие под указанные в Service Monitor'е label'ы).
1. На основании прочитанных ресурсов `servicemonitor` и на основании вычисленных namespace'ов Prometheus Operator генерирует часть конфигурации (секцию `scrape_configs`) и сохраняет конфиг в соответствующий Secret.
1. Штатными средствами самого Kubernetes данные из секрета прилетают в Pod (файл `prometheus.yaml` обновляется).
1. Изменение файла замечает `prometheus-config-reloader`, который по HTTP отправляет запрос Prometheus'у на перезагрузку.
1. Prometheus перечитывает конфиг и видит изменения в scrape_configs, которые обрабатывает уже согласно своей логике работы (см. подробнее выше).

##### Как обрабатываются кастомные ресурсы с *rule'ами*?

![Как обрабатываются кастомные ресурсы с rule'ами](images/rules.png)

1. Prometheus Operator следит за PrometheusRule'ами (подходящими под указанный в ресурсе `prometheus` `ruleSelector`).
1. Если появился новый (или был удален существующий) PrometheusRule — Prometheus Operator обновляет `prometheus.yaml` (а дальше срабатывает логика в точности соответствующая обработке Service Monitor'ов, которая описана выше).
1. Как в случае добавления/удаления PrometheusRule'а, так и при изменении содержимого PrometheusRule'а, Prometheus Operator обновляет ConfigMap `prometheus-main-rulefiles-0`.
1. Штатными средствами самого Kubernetes данные из ConfigMap прилетают в Pod
1. Изменение файла замечает `prometheus-config-reloader`, который:
   - скачивает изменившиеся ConfigMap'ы в директорию rules (это `emptyDir`)
   - по HTTP отправляет запрос Prometheus'у на перезагрузку
1. Prometheus перечитывает конфиг и видит изменившиеся *rule'ы*.

### Модуль operator-prometheus: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['operator-prometheus'].config-values | format_module_configuration: moduleKebabName }}

### Модуль operator-prometheus: custom resources
{{ site.data.schemas.operator-prometheus.crds.podmonitors | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.probes | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.scrapeconfigs | format_crd: "operator-prometheus" }}
{{ site.data.schemas.operator-prometheus.crds.servicemonitors | format_crd: "operator-prometheus" }}

### Prometheus-мониторинг

Модуль разворачивает стек мониторинга с предустановленными параметрами для Deckhouse Platform Certified Security Edition и приложений, что упрощает начальную настройку.

Ключевые возможности:

- В комплекте — готовые триггеры и дашборды, поддерживается push и pull-модель сбора метрик. Нагрузка оптимизирована за счет использования кеширования.
- Нагрузка оптимизирована за счет использования кешей и Deckhouse Prom++.
- Есть возможность хранить исторические данные с помощью даунсемплинга.
- Модуль покрывает все основные задачи базового мониторинга платформы и приложений.

Модуль покрывает все основные задачи базового мониторинга Deckhouse Platform Certified Security Edition и приложений.

#### Мониторинг аппаратных ресурсов

Реализовано отслеживание загрузки аппаратных ресурсов кластера с графиками по утилизации:

- процессора;
- памяти;
- диска;
- сети.

Графики доступны с агрегацией:

- по подам;
- контроллерам;
- пространствам имен;
- узлам.

#### Мониторинг Kubernetes

Deckhouse настраивает мониторинг параметров «здоровья» Kubernetes и таких его компонентов, как:

- общая утилизация кластера;
- связанность узлов Kubernetes между собой (измеряется rtt между всеми узлами);
- доступность и работоспособность компонентов control plane:
  - `etcd`;
  - `coredns` и `kube-dns`;
  - `kube-apiserver` и др.
- синхронизация времени на узлах и др.

#### Мониторинг Ingress

Подробное описание [здесь](/modules/ingress-nginx/#мониторинг-и-статистика)

#### Режим расширенного мониторинга

В Deckhouse возможно использование [режима расширенного мониторинга](./extended-monitoring/), который предоставляет алерты по дополнительным метрикам:

- свободному месту и inode на дисках узлов,
- утилизации узлов,
- доступности подов и образов контейнеров,
- истечении действия сертификатов,
- другим событиям кластера.

##### Алертинг в режиме расширенного мониторинга

Deckhouse предоставляет возможность гибкой настройки алертинга для каждого пространства имён и указания различной степени критичности в зависимости от порога. Можно определить множество порогов для отправки предупреждений в различные пространства имён, например, для следующих параметров:

- значения свободного места и inodes на диске;
- утилизация CPU узлов и контейнера;
- процент ошибок с кодом `5xx` на `ingress-nginx`;
- количество возможных недоступных подов в `Deployment`, `StatefulSet`, `DaemonSet`.

#### Управление мониторингом как кодом (IaC подход)

Возможности системы мониторинга Deckhouse Platform Certified Security Edition могут быть расширены за счёт использования [модуля Observability](/modules/observability/). С его помощью реализуется:

- управление алертами;
- разграничение прав доступа к настройкам и данным мониторинга;
- централизованное управление дашбордами.

#### Алерты

Мониторинг в составе Deckhouse включает уведомления о событиях. Стандартная поставка включает набор базовых предупреждений, охватывающих состояние кластера и его компоненты. Также остается возможность добавлять кастомные алерты.

##### Отправка алертов во внешние системы

Deckhouse поддерживает отправку алертов с помощью `Alertmanager`:

- по протоколу SMTP;
- в Telegram;
- посредством Webhook;
- по любым другим каналам, поддерживаемым в Alertmanager.

#### Архитектура

![Схема взаимодействия](images/prometheus_monitoring.svg)

##### Базовые компоненты мониторинга

| Компонент                         | Описание                                                                                                                                                                                                                                                                                                                                      |
|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **prometheus-main**               | Основной Prometheus, который выполняет scrape каждые 30 секунд (с помощью параметра `scrapeInterval` можно изменить это значение). Он обрабатывает все правила, отправляет алерты и является основным источником данных.                                                                                                                      |
| **prometheus-longterm**           | Просмотр и анализ исторических трендов.                                                                                                                                                                                                                                                                                                       |
| **trickster**                     | Кеширующий прокси, снижающий нагрузку на Prometheus.                                                                                                                                                                                                                                                                                          |
| **aggregating-proxy**             | Агрегирующий и кеширующий прокси, снижающий нагрузку на Prometheus и объединяющий main и longterm в один источник.                                                                                                                                                                                                                            |
| **memcached**                     | Сервис кеширования данных в оперативной памяти.                                                                                                                                                                                                                                                                                               |
| **grafana**                       | Управляемая платформа визуализации данных. Включает подготовленные dashboard'ы для всех модулей Deckhouse Platform Certified Security Edition и некоторых популярных приложений. Grafana умеет работать в режиме высокой доступности, не хранит состояние и настраивается с помощью CRD.                                                                                          |
| **metrics-adapter**               | Компонент, соединяющий Prometheus и Kubernetes metrics API. Включает поддержку HPA в кластере Kubernetes.                                                                                                                                                                                                                                     |
| **vertical-pod-autoscaler**       | Компонент, позволяющий автоматически изменять размер запрошенных ресурсов для подов с целью оптимальной утилизации CPU и памяти.                                                                                                                                                                                                              |
| **Различные exporter'ы**          | Подготовленные и подключенные к Prometheus exporter'ы. Список включает множество exporter'ов для всех необходимых метрик: `kube-state-metrics`, `node-exporter`, `oomkill-exporter`, `image-availability-exporter` и многие другие.                                                                                                           |
| **Push/pull модель сбора метрик** | Мониторинг по умолчанию использует pull-модель — данные  собираются с приложений по инициативе системы мониторинга. Также поддерживается push-модель: метрики можно передавать через протокол Prometheus Remote Write или с помощью [Prometheus Pushgateway](/modules/prometheus-pushgateway/). |

##### Внешние компоненты

Deckhouse может интегрироваться с большим количеством разнообразных решений следующими способами:

| Название                       | Описание|
|--------------------------------|--------------------------------------------------------------------------|
| **Alertmanagers**              | Alertmanager'ы могут быть подключены к Prometheus и Grafana и находиться как в кластере Deckhouse, так и за его пределами.|
| **Long-term metrics storages** | Используя протокол `remote write`, возможно отсылать метрики из Deckhouse в большое количество хранилищ.|

#### Режим отказоустойчивости и высокой доступности мониторинга (HA)

Модуль мониторинга обеспечивает встроенную отказоустойчивость всех ключевых компонентов Deckhouse Platform Certified Security Edition. Все сервисы мониторинга (Prometheus-серверы, системы хранения, прокси и прочие важные компоненты) по умолчанию развертываются в нескольких копиях. Это гарантирует, что в случае сбоя отдельного экземпляра сервис продолжит работу без потери данных и доступности.

Prometheus — основной компонент сбора метрик — запускается минимум в двух копиях (при наличии достаточного количества узлов в кластере). Оба инстанса Prometheus используют одинаковую конфигурацию и получают одни и те же данные. Чтобы обеспечить бесшовную работу при отказе одной из копий для обращения к Prometheus используется специальный компонент — aggregation-proxy. Он позволяет объединять метрики обоих Prometheus-инстансов и всегда возвращать наиболее полные и актуальные данные, даже если одна из копий временно недоступна.

### Prometheus-мониторинг: настройки

Модуль не требует обязательной конфигурации (все работает из коробки).

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, модуль включит basic auth со сгенерированным паролем и пользователем `admin`.

Посмотреть сгенерированный пароль можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values prometheus -o json | jq '.internal.auth.password'
```

Чтобы сгенерировать новый пароль, нужно удалить Secret:

```shell
d8 k -n d8-monitoring delete secret/basic-auth
```

> **Внимание!** Параметр `auth.password` больше не поддерживается.

#### Примечание

* `retentionSize` для `main` и `longterm` **рассчитывается автоматически, возможности задать значение нет!**
  * Алгоритм расчета:
    * `pvc_size * 0.85` — если PVC существует;
    * `10 GiB` — если PVC нет и StorageClass поддерживает ресайз;
    * `25 GiB` — если PVC нет и StorageClass не поддерживает ресайз.
  * Если используется `local-storage` и требуется изменить `retentionSize`, необходимо вручную изменить размер PV и PVC в нужную сторону. **Внимание!** Для расчета берется значение из `.status.capacity.storage` PVC, поскольку оно отражает реальный размер PV в случае ручного ресайза.
* `40 GiB` — размер PersistentVolumeClaim создаваемого по умолчанию.
* Размер дисков Prometheus можно изменить стандартным для Kubernetes способом (если в StorageClass это разрешено), отредактировав в PersistentVolumeClaim поле `.spec.resources.requests.storage`.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['prometheus'].config-values | format_module_configuration: moduleKebabName }}

### Prometheus-мониторинг: custom resources
{{ site.data.schemas.prometheus.crds.clusteralerts | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customalertmanager | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.customprometheusrules | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaadditionaldatasources | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanaalertschannel | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.grafanadashboarddefinition | format_crd: "prometheus" }}
{{ site.data.schemas.prometheus.crds.prometheusremotewrite | format_crd: "prometheus" }}

### Prometheus-мониторинг: FAQ

{% raw %}

#### Как собирать метрики с приложений, расположенных вне кластера?

1. Сконфигурируйте Service по аналогии с сервисом для [сбора метрик с вашего приложения](./monitoring-custom/#пример-service), но без указания параметра `spec.selector`.
1. Создайте Endpoints для этого Service, явно указав в них `IP:PORT`, по которым ваши приложения отдают метрики.

   > Имена портов в Endpoints должны совпадать с именами этих портов в Service.

##### Пример

Метрики приложения доступны без TLS, по адресу `http://10.182.10.5:9114/metrics`.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
  namespace: my-namespace
  labels:
    prometheus.deckhouse.io/custom-target: my-app
spec:
  ports:
  - name: http-metrics
    port: 9114
---
apiVersion: v1
kind: Endpoints
metadata:
  name: my-app
  namespace: my-namespace
subsets:
  - addresses:
    - ip: 10.182.10.5
    ports:
    - name: http-metrics
      port: 9114
```

#### Как добавить дополнительные дашборды в вашем проекте?

Добавление пользовательских дашбордов для Grafana в Deckhouse реализовано с помощью подхода Infrastructure as a Code.
Чтобы ваш дашборд появился в Grafana, необходимо создать в кластере специальный ресурс — [`GrafanaDashboardDefinition`](cr.html#grafanadashboarddefinition).

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: GrafanaDashboardDefinition
metadata:
  name: my-dashboard
spec:
  folder: My folder # Папка, в которой в Grafana будет отображаться ваш дашборд.
  definition: |
    {
      "annotations": {
        "list": [
          {
            "builtIn": 1,
            "datasource": "-- Grafana --",
            "enable": true,
            "hide": true,
            "iconColor": "rgba(0, 211, 255, 1)",
            "limit": 100,
...
```

{% endraw %}

{% alert level="warning" %}
Системные и добавленные через [GrafanaDashboardDefinition](cr.html#grafanadashboarddefinition) дашборды нельзя изменить через интерфейс Grafana.

Алерты, настроенные в панели dashboard, не работают с шаблонами datasource — такой dashboard является невалидным и не импортируется. В версии Grafana 9.0 функционал legacy alerting был признан устаревшим и заменён на Grafana Alerting. В связи с этим, мы не рекомендуем использовать legacy alerting (оповещения панели мониторинга) в dashboards.
{% endalert %}

{% alert level="info" %}
Если после применения дашборд не появляется в Grafana, возможно, в JSON файле дашборда присутствует ошибка. Чтобы определить источник проблемы, воспользуйтесь командой `d8 k logs -n d8-monitoring deployments/grafana-v10 dashboard-provisioner` для просмотра логов компонента, который осуществляет применение дашбордов.
{% endalert %}

{% raw %}

#### Как добавить алерты и/или recording-правила для вашего проекта?

Для добавления алертов существует специальный ресурс — `CustomPrometheusRules`.

Параметры:

- `groups` — единственный параметр, в котором необходимо описать группы алертов. Структура групп полностью совпадает с аналогичной в prometheus-operator.

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: CustomPrometheusRules
metadata:
  name: my-rules
spec:
  groups:
  - name: cluster-state-alert.rules
    rules:
    - alert: CephClusterErrorState
      annotations:
        description: Storage cluster is in error state for more than 10m.
        summary: Storage cluster is in error state
        plk_markup_format: markdown
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} > 1
```

##### Как подключить дополнительные data source для Grafana?

Для подключения дополнительных data source к Grafana существует специальный ресурс — `GrafanaAdditionalDatasource`.

Пример:

```yaml
apiVersion: deckhouse.io/v1
kind: GrafanaAdditionalDatasource
metadata:
  name: another-prometheus
spec:
  type: prometheus
  access: Proxy
  url: https://another-prometheus.example.com/prometheus
  basicAuth: true
  basicAuthUser: foo
  jsonData:
    timeInterval: 30s
    httpMethod: POST
  secureJsonData:
    basicAuthPassword: bar
```

#### Как обеспечить безопасный доступ к метрикам?

Для обеспечения безопасности настоятельно рекомендуем использовать `kube-rbac-proxy`.

##### Пример безопасного сбора метрик с приложения, расположенного в кластере

Чтобы настроить защиту метрик приложения с использованием `kube-rbac-proxy` и последующего сбора метрик с помощью Prometheus, выполните следующие шаги:

1. Создайте `ServiceAccount` с указанными ниже правами:

   ```yaml
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: rbac-proxy-test
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: rbac-proxy-test
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: d8:rbac-proxy
   subjects:
   - kind: ServiceAccount
     name: rbac-proxy-test
     namespace: default
   ```

   > Обратите внимание, что используется встроенная в Deckhouse ClusterRole `d8:rbac-proxy`.

2. Создайте конфигурацию для `kube-rbac-proxy`:

   ```yaml
   ---
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: rbac-proxy-config-test
     namespace: rbac-proxy-test
   data:
     config-file.yaml: |+
       authorization:
         resourceAttributes:
           namespace: default
           apiVersion: v1
           resource: services
           subresource: proxy
           name: rbac-proxy-test
   ```

3. Создайте `Service` и `Deployment` для вашего приложения, где `kube-rbac-proxy` займет позицию sidecar-контейнера:

   ```yaml
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: rbac-proxy-test
     labels:
       prometheus.deckhouse.io/custom-target: rbac-proxy-test
   spec:
     ports:
     - name: https-metrics
       port: 8443
       targetPort: https-metrics
     selector:
       app: rbac-proxy-test
   ---
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: rbac-proxy-test
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: rbac-proxy-test
     template:
       metadata:
         labels:
           app: rbac-proxy-test
       spec:
         securityContext:
           runAsUser: 65532
         serviceAccountName: rbac-proxy-test
         containers:
         - name: kube-rbac-proxy
           image: quay.io/brancz/kube-rbac-proxy:v0.14.0
           args:
           - "--secure-listen-address=0.0.0.0:8443"
           - "--upstream=http://127.0.0.1:8081/"
           - "--config-file=/kube-rbac-proxy/config-file.yaml"
           - "--logtostderr=true"
           - "--v=10"
           ports:
           - containerPort: 8443
             name: https-metrics
           volumeMounts:
           - name: config
             mountPath: /kube-rbac-proxy
         - name: prometheus-example-app
           image: quay.io/brancz/prometheus-example-app:v0.1.0
           args:
           - "--bind=127.0.0.1:8081"
         volumes:
         - name: config
           configMap:
             name: rbac-proxy-config-test
   ```

4. Назначьте необходимые права на ресурс для Prometheus:

   ```yaml
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: rbac-proxy-test-client
   rules:
   - apiGroups: [""]
     resources: ["services/proxy"]
     verbs: ["get"]
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: rbac-proxy-test-client
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: rbac-proxy-test-client
   subjects:
   - kind: ServiceAccount
     name: prometheus
     namespace: d8-monitoring
   ```

После шага 4 метрики вашего приложения должны появиться в Prometheus.

##### Пример безопасного сбора метрик с приложения, расположенного вне кластера

Предположим, что есть доступный через интернет сервер, на котором работает `node-exporter`. По умолчанию `node-exporter` слушает на порту `9100` и доступен на всех интерфейсах. Необходимо обеспечить контроль доступа к `node-exporter` для безопасного сбора метрик. Ниже приведен пример такой настройки.

Требования:

- Из кластера должен быть доступ к сервису `kube-rbac-proxy`, который запущен на *удаленном сервере*.
- От *удаленного сервера* должен быть доступ к API-серверу кластера.

Выполните следующие шаги:

1. Создайте `ServiceAccount` с указанными ниже правами:

   ```yaml
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: prometheus-external-endpoint-server-01
     namespace: d8-service-accounts
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: prometheus-external-endpoint
   rules:
   - apiGroups: ["authentication.k8s.io"]
     resources:
     - tokenreviews
     verbs: ["create"]
   - apiGroups: ["authorization.k8s.io"]
     resources:
     - subjectaccessreviews
     verbs: ["create"]
   ---
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: prometheus-external-endpoint-server-01
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: prometheus-external-endpoint
   subjects:
   - kind: ServiceAccount
     name: prometheus-external-endpoint-server-01
     namespace: d8-service-accounts
   ```

2. Сгенерируйте `kubeconfig` для созданного `ServiceAccount` ([пример генерации kubeconfig для `ServiceAccount`](./user-authz/usage.html#создание-serviceaccount-для-сервера-и-предоставление-ему-доступа)).

3. Положите получившийся `kubeconfig` на *удаленный сервер*. Необходимо указать путь к этому `kubeconfig` в настройках `kube-rbac-proxy` (в примере используется путь `${PWD}/.kube/config`).

4. Настройте `node-exporter` на *удаленном сервере* с доступом к нему только на локальном интерфейсе (чтобы он слушал `127.0.0.1:9100`).
5. Запустите `kube-rbac-proxy` на *удаленном сервере*:

   ```shell
   docker run --network host -d -v ${PWD}/.kube/config:/config quay.io/brancz/kube-rbac-proxy:v0.14.0 --secure-listen-address=0.0.0.0:8443 \
     --upstream=http://127.0.0.1:9100 --kubeconfig=/config --logtostderr=true --v=10
   ```

6. Проверьте, что порт `8443` доступен по внешнему адресу *удаленного сервера*.

7. Создайте в кластере `Service` и `Endpoint`, указав в качестве `<server_ip_address>` внешний адрес *удаленного сервера*:

   ```yaml
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: prometheus-external-endpoint-server-01
     labels:
       prometheus.deckhouse.io/custom-target: prometheus-external-endpoint-server-01
   spec:
     ports:
     - name: https-metrics
       port: 8443
   ---
   apiVersion: v1
   kind: Endpoints
   metadata:
     name: prometheus-external-endpoint-server-01
   subsets:
     - addresses:
       - ip: <server_ip_address>
       ports:
       - name: https-metrics
         port: 8443
   ```

#### Как добавить Alertmanager?

Создайте кастомный ресурс `CustomAlertmanager` с типом `Internal`.

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: CustomAlertmanager
metadata:
  name: webhook
spec:
  type: Internal
  internal:
    route:
      groupBy: ['job']
      groupWait: 30s
      groupInterval: 5m
      repeatInterval: 12h
      receiver: 'webhook'
    receivers:
    - name: 'webhook'
      webhookConfigs:
      - url: 'http://webhookserver:8080/'
```

Подробно о всех параметрах можно прочитать в описании кастомного ресурса [CustomAlertmanager](cr.html#customalertmanager).

#### Как добавить внешний дополнительный Alertmanager?

Создайте кастомный ресурс `CustomAlertmanager` с типом `External`, который может указывать на Alertmanager по FQDN или через сервис в Kubernetes-кластере.

Пример FQDN Alertmanager:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: CustomAlertmanager
metadata:
  name: my-fqdn-alertmanager
spec:
  external:
    address: https://alertmanager.mycompany.com/myprefix
  type: External
```

Пример Alertmanager с Kubernetes service:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: CustomAlertmanager
metadata:
  name: my-service-alertmanager
spec:
  external:
    service:
      namespace: myns
      name: my-alertmanager
      path: /myprefix/
  type: External
```

Подробно о всех параметрах можно прочитать в описании кастомного ресурса [CustomAlertmanager](cr.html#customalertmanager).

#### Как в Alertmanager игнорировать лишние алерты?

Решение сводится к настройке маршрутизации алертов в вашем Alertmanager.

Требования:

1. Завести получателя без параметров.
1. Смаршрутизировать лишние алерты в этого получателя.

Ниже приведены примеры настройки `CustomAlertmanager`.

Чтобы получать только алерты с лейблами `service: foo|bar|baz`:

```yaml
receivers:
  # Получатель, определенный без параметров, будет работать как "/dev/null".
  - name: blackhole
  # Действующий получатель  
  - name: some-other-receiver
    # ...
route:
  # receiver по умолчанию.
  receiver: blackhole
  routes:
    # Дочерний маршрут
    - matchers:
        - matchType: =~
          name: service
          value: ^(foo|bar|baz)$
      receiver: some-other-receiver
```

Чтобы получать все алерты, кроме `DeadMansSwitch`:

```yaml
receivers:
  # Получатель, определенный без параметров, будет работать как "/dev/null".
  - name: blackhole
  # Действующий получатель.
  - name: some-other-receiver
  # ...
route:
  # receiver по умолчанию.
  receiver: some-other-receiver
  routes:
    # Дочерний маршрут.
    - matchers:
        - matchType: =
          name: alertname
          value: DeadMansSwitch
      receiver: blackhole
```

#### Почему нельзя установить разный scrapeInterval для отдельных таргетов?

Разные scrapeInterval'ы принесут следующие проблемы:

* увеличение сложности конфигурации;
* проблемы при написании запросов и создании графиков;
* короткие интервалы больше похожи на профилирование приложения, и, скорее всего, Prometheus — не самый подходящий инструмент для этого.

Наиболее подходящее значение для scrapeInterval находится в диапазоне 10–60 секунд.

#### Как ограничить потребление ресурсов Prometheus?

Чтобы предотвратить ситуации, когда Variable Policy Agent (VPA) запрашивает у Prometheus или долгосрочного Prometheus больше ресурсов, чем доступно на выделенном узле для этих целей, можно явно установить ограничения для VPA с использованием [параметров модуля](configuration.html):

- `vpa.longtermMaxCPU`;
- `vpa.longtermMaxMemory`;
- `vpa.maxCPU`;
- `vpa.maxMemory`.

#### Как настроить ServiceMonitor или PodMonitor для работы с Prometheus?

Добавьте лейбл `prometheus: main` к Pod/Service Monitor.
Добавьте в пространство имён, в котором находится Pod/Service Monitor, лейбл `prometheus.deckhouse.io/monitor-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/monitor-watcher-enabled: "true"
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: example-app
  namespace: frontend
  labels:
    prometheus: main
spec:
  selector:
    matchLabels:
      app: example-app
  endpoints:
    - port: web
```

#### Как настроить Probe для работы с Prometheus?

Добавьте лейбл `prometheus: main` к Probe.
Добавьте в пространство имён, в котором находится Probe, лейбл `prometheus.deckhouse.io/probe-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/probe-watcher-enabled: "true"
---
apiVersion: monitoring.coreos.com/v1
kind: Probe
metadata:
  labels:
    app: prometheus
    component: probes
    prometheus: main
  name: cdn-is-up
  namespace: frontend
spec:
  interval: 30s
  jobName: httpGet
  module: http_2xx
  prober:
    path: /probe
    scheme: http
    url: blackbox-exporter.blackbox-exporter.svc.cluster.local:9115
  targets:
    staticConfig:
      static:
      - https://example.com/status
```

#### Как настроить PrometheusRules для работы с Prometheus?

Добавьте в пространство имён, в котором находятся PrometheusRules, лейбл `prometheus.deckhouse.io/rules-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/rules-watcher-enabled: "true"
```

#### Как увеличить размер диска

1. Для увеличения размера отредактируйте PersistentVolumeClaim, указав новый размер в поле `spec.resources.requests.storage`.
   * Увеличение размера возможно, если в StorageClass поле `allowVolumeExpansion` установлено в `true`.
2. Если используемое хранилище не поддерживает изменение диска на лету, в статусе PersistentVolumeClaim появится сообщение `Waiting for user to (re-)start a pod to finish file system resize of volume on node.`.
3. Перезапустите под для завершения изменения размера файловой системы.

#### Как получить информацию об алертах в кластере?

Информацию об активных алертах можно получить не только в веб-интерфейсе Grafana/Prometheus, но и в CLI. Это может быть полезным, если у вас есть только доступ к API-серверу кластера и нет возможности открыть веб-интерфейс Grafana/Prometheus.

Выполните следующую команду для получения списка алертов в кластере:

```shell
d8 k get clusteralerts
```

Пример вывода:

```console
NAME               ALERT                                      SEVERITY   AGE     LAST RECEIVED   STATUS
086551aeee5b5b24   ExtendedMonitoringDeprecatatedAnnotation   4          3h25m   38s             firing
226d35c886464d6e   ExtendedMonitoringDeprecatatedAnnotation   4          3h25m   38s             firing
235d4efba7df6af4   D8SnapshotControllerPodIsNotReady          8          5d4h    44s             firing
27464763f0aa857c   D8PrometheusOperatorPodIsNotReady          7          5d4h    43s             firing
ab17837fffa5e440   DeadMansSwitch                             4          5d4h    41s             firing
```

Выполните следующую команду для просмотра конкретного алерта:

```shell
d8 k get clusteralerts <ALERT_NAME> -o yaml
```

Пример:

```shell
### d8 k get clusteralerts 235d4efba7df6af4 -o yaml
alert:
  description: |
    The recommended course of action:
    1. Retrieve details of the Deployment: `kubectl -n d8-snapshot-controller describe deploy snapshot-controller`
    2. View the status of the Pod and try to figure out why it is not running: `kubectl -n d8-snapshot-controller describe pod -l app=snapshot-controller`
  labels:
    pod: snapshot-controller-75bd776d76-xhb2c
    prometheus: deckhouse
    tier: cluster
  name: D8SnapshotControllerPodIsNotReady
  severityLevel: "8"
  summary: The snapshot-controller Pod is NOT Ready.
apiVersion: deckhouse.io/v1alpha1
kind: ClusterAlert
metadata:
  creationTimestamp: "2023-05-15T14:24:08Z"
  generation: 1
  labels:
    app: prometheus
    heritage: deckhouse
  name: 235d4efba7df6af4
  resourceVersion: "36262598"
  uid: 817f83e4-d01a-4572-8659-0c0a7b6ca9e7
status:
  alertStatus: firing
  lastUpdateTime: "2023-05-15T18:10:09Z"
  startsAt: "2023-05-10T13:43:09Z"
```

{% endraw %}

{% alert level="info" %}
Присутствие специального алерта `MissingDeadMansSwitch` в кластере говорит о проблемах в работоспособности компонентов мониторинга.
{% endalert %}

{% raw %}

#### Как добавить дополнительные эндпоинты в scrape config?

Добавьте в пространство имён, в котором находится ScrapeConfig, лейбл `prometheus.deckhouse.io/scrape-configs-watcher-enabled: "true"`.

Пример:

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: frontend
  labels:
    prometheus.deckhouse.io/scrape-configs-watcher-enabled: "true"
```

Добавьте ScrapeConfig, который имеет обязательный лейбл `prometheus: main`:

```yaml
apiVersion: monitoring.coreos.com/v1alpha1
kind: ScrapeConfig
metadata:
  name: example-scrape-config
  namespace: frontend
  labels:
    prometheus: main
spec:
  honorLabels: true
  staticConfigs:
    - targets: ['example-app.frontend.svc.{{ .Values.global.discovery.clusterDomain }}.:8080']
  relabelings:
    - regex: endpoint|namespace|pod|service
      action: labeldrop
    - targetLabel: scrape_endpoint
      replacement: main
    - targetLabel: job
      replacement: kube-state-metrics
  metricsPath: '/metrics'
```

{% endraw %}

### Мониторинг SLA кластера

Модуль тестирует доступность платформы и состояние компонентов кластера в реальном времени, выводит информацию в виде соответствующих дашбордов.

Возможности модуля:

- для всех основных компонентов кластера разработаны сценарии постоянного тестирования;
- результаты тестирования сохраняются в виде метрик;
- отображает дашборд, показывающий работоспособность компонентов;
- данные могут быть экспортированы в любую внешнюю Prometheus-совместимую систему мониторинга.

Состав модуля:

- **agent** — работает на master-узлах и делает пробы доступности, отправляет результаты на сервер.
- **upmeter** — собирает результаты и поддерживает API-сервер для их извлечения.
- **front**:
  - **status** — показывает уровень доступности за последние 10 минут (требует авторизации, но ее можно отключить);
  - **webui** — показывает дашборд со статистикой по пробам и группам доступности (требует авторизации).
- **smoke-mini** — поддерживает постоянное *smoke-тестирование* с помощью StatefulSet.

Модуль отправляет около 100 показаний метрик каждые 5 минут. Это значение зависит от количества включенных модулей Deckhouse Platform Certified Security Edition.

#### Интерфейс

Пример веб-интерфейса:
![Пример веб-интерфейса](images/image1.png)

Пример графиков по метрикам из upmeter в Grafana:
![Пример графиков по метрикам из upmeter в Grafana](images/image2.png)

### Мониторинг SLA кластера: настройки

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, то модуль включит basic auth со сгенерированным паролем.

Посмотреть сгенерированный пароль можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values upmeter -o json | jq '.upmeter.internal.auth.webui.password'
```

Чтобы сгенерировать новый пароль, нужно удалить Secret:

```shell
d8 k -n d8-upmeter delete secret/basic-auth-webui
```

Посмотреть сгенерированный пароль для страницы статуса можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values upmeter -o json | jq '.upmeter.internal.auth.status.password'
```

Чтобы сгенерировать новый пароль для страницы статуса, нужно удалить секрет:

```shell
d8 k -n d8-upmeter delete secret/basic-auth-status
```

> **Внимание!** Параметры `auth.status.password` и `auth.webui.password` больше не поддерживаются.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['upmeter'].config-values | format_module_configuration: moduleKebabName }}

### Мониторинг SLA кластера: кастомные ресурсы
{{ site.data.schemas.upmeter.crds.downtime | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterhookprobe | format_crd: "upmeter" }}
{{ site.data.schemas.upmeter.crds.upmeterremotewrite | format_crd: "upmeter" }}

### Мониторинг SLA кластера: примеры

#### Пример конфигурации remote_write

```yaml
apiVersion: deckhouse.io/v1
kind: UpmeterRemoteWrite
metadata:
  labels:
    heritage: upmeter
    module: upmeter
  name: victoriametrics
spec:
  additionalLabels:
    cluster: cluster-name
    some: fun
  config:
    url: https://upmeter-victoriametrics.whatever/api/v1/write
    basicAuth:
      password: "Cdp#Cd.OxfZsx4*89SZ"
      username: upmeter
  intervalSeconds: 300
```

### Мониторинг SLA кластера: FAQ

#### Почему периодически не могут разместиться поды `upmeter-probe-controller-manager`, почему некоторые поды постоянно удаляются?

В модуле реализованы тесты доступности функционала различных контроллеров Kubernetes.  
Тесты выполняется путем создания и удаления временных подов.

Объекты `upmeter-probe-scheduler`, отвечают за реализацию теста функционала размещения подов на узлы.
В рамках теста создается под, который размещается на узел. Затем этот под удаляется.

Объекты `upmeter-probe-controller-manager` отвечают за тестирование работоспособности `kube-controller-manager`.  
В рамках теста создается StatefulSet, и проверяется что данный объект породил под (т.к. фактическое размещение пода не требуется и проверяется в рамках другого теста, то создается под который гарантированно не может разместиться, т.е. находится в состоянии `Pending`). Затем StatefulSet удаляется и выполняется проверка, что порожденный им под удалился.

Объекты `smoke-mini` реализуют тестирование сетевой связности между узлами.
Для проверки размещаются пять StatefulSet с одной репликой. В рамках теста проверяется связность как между подами `smoke-mini`, так и сетевая связность с подами `upmeter-agent`, работающими на master-узлах.  
Раз в минуту один из подов `smoke-mini` переносится на другой узел.
## Подсистема Масштабирование и управление ресурсами

### Модуль descheduler

Каждые 15 минут модуль анализирует состояние кластера и выполняет вытеснение подов, соответствующих условиям, описанным в активных [стратегиях](#стратегии). Вытесненные поды вновь проходят процесс планирования с учетом текущего состояния кластера. Это позволяет перераспределить рабочие нагрузки в соответствие с выбранной стратегией.

Модуль основан на проекте descheduler.

#### Особенности работы модуля

* Модуль может учитывать класс приоритета пода (параметр [spec.priorityClassThreshold](cr.html#descheduler-v1alpha2-spec-priorityclassthreshold)), ограничивая работу только подами, у которых класс приоритета ниже заданного порога;
* Модуль не вытесняет под в следующих случаях:
  * под находится в пространстве имен `d8-*` или `kube-system`;
  * под имеет `priorityClassName` `system-cluster-critical` или `system-node-critical`;
  * под связан с локальным хранилищем;
  * под связан с DaemonSet;
  * вытеснение пода нарушит Pod Disruption Budget (PDB);
  * нет доступных узлов для запуска вытесненного пода.
* Поды с классом приоритета `Best effort` вытесняются раньше, чем поды с классами `Burstable` и `Guaranteed`.

Для фильтрации подов и узлов модуль использует механизм `labelSelector` Kubernetes:

* `podLabelSelector` — ограничивает поды по меткам;
* `namespaceLabelSelector` — фильтрует поды по пространствам имен.
* `nodeLabelSelector` — выбирает узлы по меткам.

#### Стратегии

##### HighNodeUtilization

{% alert level="info" %}
Концентрирует нагрузку на меньшем числе узлов. Требует настройку планировщика и включение автоматического масштабирования.

Чтобы использовать `HighNodeUtilization`, необходимо явно указать профиль планировщика [high-node-utilization](./control-plane-manager/faq.html#профили-планировщика) для каждого пода (этот профиль не может быть установлен как профиль по умолчанию).
{% endalert %}

Стратегия определяет *недостаточно нагруженные узлы* и вытесняет с них поды, чтобы распределить их компактнее, на меньшем числе узлов.

**Недостаточно нагруженный узел** — узел, использование ресурсов которого меньше **всех** пороговых значений, заданных в секции параметров [strategies.highNodeUtilization.thresholds](cr.html#descheduler-v1alpha2-spec-strategies-highnodeutilization-thresholds).

Стратегия включается параметром [spec.strategies.highNodeUtilization.enabled](cr.html#descheduler-v1alpha2-spec-strategies-highnodeutilization-enabled).

{% alert level="warning" %}
В GKE нельзя настроить конфигурацию планировщика по умолчанию, но можно использовать стратегию `optimize-utilization` или развернуть второй пользовательский планировщик.
{% endalert %}

{% alert level="warning" %}
Использование ресурсов узла учитывает extended-ресурсы и рассчитывается на основе запросов и лимитов подов (requests and limits), а не их фактического потребления. Такой подход обеспечивает согласованность с работой kube-scheduler, который использует аналогичный принцип при размещении подов на узлах. Это означает, что метрики использования ресурсов, отображаемые Kubelet (или командами вроде `kubectl top`), могут отличаться от расчетных показателей, так как Kubelet и связанные инструменты отображают данные о реальном потреблении ресурсов.
{% endalert %}

##### LowNodeUtilization

{% alert level="info" %}
Более равномерно нагружает узлы.
{% endalert %}

Стратегия выявляет *недостаточно нагруженные узлы* и вытесняет поды с других, *избыточно нагруженных узлов*. Стратегия предполагает, что пересоздание вытесненных подов произойдет на недостаточно нагруженных узлах (при обычном поведении планировщика).

**Недостаточно нагруженный узел** — узел, использование ресурсов которого меньше **всех** пороговых значений, заданных в секции параметров [strategies.lowNodeUtilization.thresholds](cr.html#descheduler-v1alpha2-spec-strategies-lownodeutilization-thresholds).

**Избыточно нагруженный узел** — узел, использование ресурсов которого больше **хотя бы одного** из пороговых значений, заданных в секции параметров [strategies.lowNodeUtilization.targetThresholds](cr.html#descheduler-v1alpha2-spec-strategies-lownodeutilization-targetthresholds).

Узлы с использованием ресурсов в диапазоне между `thresholds` и `targetThresholds` считаются оптимально используемыми. Поды на таких узлах вытесняться не будут.

Стратегия включается параметром [spec.strategies.lowNodeUtilization.enabled](cr.html#descheduler-v1alpha2-spec-strategies-lownodeutilization-enabled).

{% alert level="warning" %}
Использование ресурсов узла учитывает extended-ресурсы и рассчитывается на основе запросов и лимитов подов (requests and limits), а не их фактического потребления. Такой подход обеспечивает согласованность с работой kube-scheduler, который использует аналогичный принцип при размещении подов на узлах. Это означает, что метрики использования ресурсов, отображаемые Kubelet (или командами вроде `kubectl top`), могут отличаться от расчетных показателей, так как Kubelet и связанные инструменты отображают данные о реальном потреблении ресурсов.
{% endalert %}

##### RemoveDuplicates

{% alert level="info" %}
Предотвращает запуск нескольких подов одного контроллера (ReplicaSet, ReplicationController, StatefulSet) или заданий (Job) на одном узле.
{% endalert %}

Стратегия следит за тем, чтобы на одном узле не находилось больше одного пода ReplicaSet, ReplicationController, StatefulSet или подов одного задания (Job). Если таких подов два или больше, модуль вытесняет лишние поды, чтобы они лучше распределились по кластеру.

Описанная ситуация может возникнуть, если некоторые узлы кластеры вышли из строя по каким-либо причинам, и поды с них были перемещены на другие узлы. Как только вышедшие из строя узлы снова станут доступны для приема нагрузки, эту стратегию можно будет использовать для выселения дублирующих подов с других узлов.

Стратегия включается параметром [strategies.removeDuplicates.enabled](cr.html#descheduler-v1alpha2-spec-strategies-removeduplicates-enabled).

##### RemovePodsViolatingInterPodAntiAffinity

{% alert level="info" %}
Вытесняет поды, нарушающие правила inter-pod affinity и anti-affinity.
{% endalert %}

Стратегия гарантирует, что поды, нарушающие правила inter-pod affinity и anti-affinity, будут удалены с узлов.

Например, если на узле находится **Под1**, а также **Под2** и **Под3**, имеющие правила anti-affinity, которые запрещают им работать на одном узле с подом **Под1**, то **Под1** будет вытеснен с узла, чтобы **Под2** и **Под3** смогли работать. Такая ситуация может возникнуть, когда правила inter-pod affinity для **Под2** и **Под3** создаются когда поды уже запущены на узле.

Стратегия включается параметром [strategies.removePodsViolatingInterPodAntiAffinity.enabled](cr.html#descheduler-v1alpha2-spec-strategies-removepodsviolatinginterpodantiaffinity-enabled).

##### RemovePodsViolatingNodeAffinity

{% alert level="info" %}
Вытесняет поды, нарушающие правила node affinity.
{% endalert %}

Стратегия гарантирует, что все поды, которые нарушают правила node affinity, в конечном счете будут удалены с узлов.

По сути, в зависимости от настроек параметра [strategies.removePodsViolatingNodeAffinity.nodeAffinityType](cr.html#descheduler-v1alpha2-spec-strategies-removepodsviolatingnodeaffinity-nodeaffinitytype),  
стратегия превращает правило `requiredDuringSchedulingIgnoredDuringExecution` node affinity пода в правило `requiredDuringSchedulingRequiredDuringExecution`, а правило `preferredDuringSchedulingIgnoredDuringExecution` в правило `preferredDuringSchedulingPreferredDuringExecution`.

Пример для `nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution`. Есть под, который был назначен на узел, соответствующий правилу `requiredDuringSchedulingIgnoredDuringExecution` node affinity на момент размещения. Если со временем этот узел перестанет удовлетворять правилу node affinity, и если появится другой доступный узел, соответствующий этому правилу, стратегия вытеснит под с узла, на который он был изначально назначен.

Пример для `nodeAffinityType: preferredDuringSchedulingIgnoredDuringExecution`. Есть под, который был назначен на узел, т.к. на момент размещения отсутствовали другие узлы, удовлетворяющие правилу `preferredDuringSchedulingIgnoredDuringExecution` node affinity. Если со временем в кластере появится доступный узел, соответствующий этому правилу, стратегия вытеснит под с узла, на который он был изначально назначен.

Стратегия включается параметром [strategies.removePodsViolatingNodeAffinity.enabled](cr.html#descheduler-v1alpha2-spec-strategies-removepodsviolatingnodeaffinity-enabled).

### Модуль descheduler: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

У модуля нет обязательных настроек.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['descheduler'].config-values | format_module_configuration: moduleKebabName }}

### Модуль descheduler: Custom Resources
{{ site.data.schemas.descheduler.crds.deschedulers | format_crd: "descheduler" }}

### Модуль descheduler: примеры

#### Пример стратегии LowNodeUtilization

```yaml
---
apiVersion: deckhouse.io/v1alpha2
kind: Descheduler
metadata:
  name: low-node-utilization
spec:
  strategies:
    lowNodeUtilization:
      enabled: true
      thresholds:
        cpu: 20
      targetThresholds:
        cpu: 50
```

#### Пример стратегии HighNodeUtilization

```yaml
---
apiVersion: deckhouse.io/v1alpha2
kind: Descheduler
metadata:
  name: high-node-utilization
spec:
  strategies:
    highNodeUtilization:
      enabled: true
      thresholds:
        cpu: 50
        memory: 50
```

### Модуль prometheus-metrics-adapter

Позволяет работать HPA- и [VPA](./vertical-pod-autoscaler/)-автоскейлерам по «любым» метрикам.

Устанавливает в кластер имплементацию Kubernetes resource metrics API, custom metrics API и external metrics API для получения метрик из Prometheus.

Это позволяет:
- `kubectl top` брать метрики из Prometheus, через адаптер;
- использовать custom resource версии autoscaling/v2 для масштабирования приложений (HPA);
- получать информацию из Prometheus средствами API Kubernetes для других модулей (Vertical Pod Autoscaler, ...).

Модуль позволяет производить масштабирование по следующим параметрам:
* CPU (пода);
* память (пода);
* rps (Ingress'а) — за 1, 5, 15 минут (`rps_Nm`);
* CPU (пода) — за 1, 5, 15 минут (`cpu_Nm`) — среднее потребление CPU за N минут;
* память (пода) — за 1, 5, 15 минут (`memory_Nm`) — среднее потребление памяти за N минут;
* любые Prometheus-метрики и любые запросы на их основе.

#### Как работает

Данный модуль регистрирует `k8s-prometheus-adapter` в качестве external API-сервиса, который расширяет возможности Kubernetes API. Когда какому-то из компонентов Kubernetes (VPA, HPA) требуется информация об используемых ресурсах, он делает запрос в Kubernetes API, а тот, в свою очередь, проксирует запрос в адаптер. Адаптер на основе своего конфигурационного файла выясняет, как посчитать метрику, и отправляет запрос в Prometheus.

### Модуль prometheus-metrics-adapter: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Модуль работает, если включен модуль `prometheus`. В общем случае конфигурации не требуется.

#### Параметры

* `highAvailability` — ручное включение/отключение режима отказоустойчивости. По умолчанию режим отказоустойчивости определяется автоматически. Смотри [подробнее](/reference/api/global.html#параметры) про режим отказоустойчивости для модулей.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['prometheus-metrics-adapter'].config-values | format_module_configuration: moduleKebabName }}

### Модуль prometheus-metrics-adapter: Custom resources
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdaemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterdeploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusteringressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterpodmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterservicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.clusterstatefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.daemonsetmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.deploymentmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.ingressmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.namespacemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.podmetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.servicemetrics | format_crd: "prometheus-metrics-adapter" }}
{{ site.data.schemas.prometheus-metrics-adapter.crds.statefulsetmetrics | format_crd: "prometheus-metrics-adapter" }}

### 
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: speaker
  namespace: d8-{{ .Chart.Name }}
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: DaemonSet
    name: speaker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: speaker
      minAllowed:
        {{- include "speaker_resources" . | nindent 8 }}
      maxAllowed:
        cpu: 20m
        memory: 60Mi
    {{- include "helm_lib_vpa_kube_rbac_proxy_resources" . | nindent 4 }}
```

Helm-функции, описанные в файле `vpa.yaml` используются так же для установки ресурсов контейнеров в случае, если модуль `vertical-pod-autoscaler` отключен.

Для проставления ресурсов для `kube-rbac-proxy` используется специальная helm-функция `helm_lib_container_kube_rbac_proxy_resources`.

Пример:

```yaml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: speaker
  namespace: d8-{{ .Chart.Name }}
spec:
  selector:
    matchLabels:
      app: speaker
  template:
    metadata:
      labels:
        app: speaker
    spec: 
    containers:
      - name: speaker
        resources:
          requests:
          {{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler") }}
            {{- include "speaker_resources" . | nindent 14 }}
          {{- end }}
      - name: kube-rbac-proxy
        resources:
          requests:
          {{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler") }}
            {{- include "helm_lib_container_kube_rbac_proxy_resources" . | nindent 12 }}
          {{- end }}
```

#### Специальные лейблы для VPA-ресурсов

Если Pod'ы присутствуют только на мастер-узлах, для VPA-ресурса добавляется label `workload-resource-policy.deckhouse.io: master`.

Если Pod'ы присутствуют на каждом узле, для VPA-ресурса добавляется label `workload-resource-policy.deckhouse.io: every-node`.

#### TODO

* В настоящий момент для проставления ресурсов контейнеров используются значения из `minAllowed`. В этом случае возможен оверпровижининг на узле. Возможно правильнее было бы использовать значения `maxAllowed`.
* Значения `minAllowed` и `maxAllowed` проставляются вручную, возможно, определять нужно что-то одно, а второе вычислять. Например, определять `minAllowed` а `maxAllowed` считать как `minAllowed` X 2.
* Возможно стоит придумать другой механизм задания значений `minAllowed`, например, отдельный файл, в котором в YAML-структуре будут собраны данные по ресурсам всех контейнеров всех модулей.
* Issue #2084.

### Модуль vertical-pod-autoscaler

Vertical Pod Autoscaler (VPA) — это инфраструктурный сервис, который позволяет не выставлять точные resource requests, если неизвестно, сколько ресурсов необходимо контейнеру для работы. При использовании VPA и включении соответствующего режима работы resource requests выставляются автоматически на основе потребления ресурсов (полученных данных из Prometheus).
Как вариант, возможно только получать рекомендации по ресурсам, без их автоматического изменения.

У VPA есть следующие режимы работы:

- `"Auto"` (default) — в данный момент режимы работы `Auto` и `Recreate` делают одно и то же. Однако, когда в Kubernetes появится Pod in-place resource update, этот режим будет делать именно его.
- `"Recreate"` — режим разрешает VPA изменять ресурсы у запущенных подов (перезапускать их при работе). В случае работы одного пода (`replicas: 1`) это приведет к недоступности сервиса на время рестарта. В данном режиме VPA не пересоздает поды, которые были созданы без контроллера.
- `"Initial"` — VPA изменяет ресурсы подов только при создании подов, но не во время работы.
- `"Off"` — VPA не изменяет автоматически никакие ресурсы. В данном случае, если есть VPA c таким режимом работы, мы можем посмотреть, какие ресурсы рекомендует поставить VPA (`d8 k describe vpa <vpa-name>`).

Ограничения VPA:

- Обновление ресурсов запущенных подов — это экспериментальная возможность VPA. Каждый раз, когда VPA обновляет `resource requests` пода, под пересоздается. Под может быть создан на другом узле.
- VPA **не должен использоваться с HPA по CPU и памяти** в данный момент. Однако VPA можно использовать с HPA на custom/external metrics.
- VPA реагирует почти на все `out-of-memory` events, но не гарантирует реакцию.
- Производительность VPA не тестировалась на огромных кластерах.
- Рекомендации VPA могут превышать доступные ресурсы в кластере, что **может приводить к подам в состоянии Pending**.
- Использование нескольких VPA-ресурсов над одним подом может привести к неопределенному поведению.
- В случае удаления VPA или его «выключения» (режим `Off`) изменения, внесенные ранее VPA, не сбрасываются, а остаются в последнем измененном значении. Из-за этого может возникнуть путаница, что в Helm будут описаны одни ресурсы, при этом в контроллере тоже будут описаны одни ресурсы, но реально у подов ресурсы будут совсем другие и может сложиться впечатление, что они взялись «непонятно откуда».

{% alert level="info" %}
При использовании VPA настоятельно рекомендуется использовать Pod Disruption Budget.
{% endalert %}

#### Grafana dashboard

Представлено на дашбордах:

- `Main / Namespace`, `Main / Namespace / Controller`, `Main / Namespace / Controller / Pod` — столбец `VPA type` показывает значение `updatePolicy.updateMode`;
- `Main / Namespaces` — столбец `VPA %` показывает процент подов с включенным VPA.

#### Архитектура Vertical Pod Autoscaler

VPA состоит из 3 компонентов:

- `Recommender` — мониторит настоящее (делая запросы в Metrics API, который реализован в модуле [`prometheus-metrics-adapter`](/modules/prometheus-metrics-adapter/)) и прошлое потребление ресурсов (делая запросы в Trickster перед Prometheus) и предоставляет рекомендации по CPU и памяти для контейнеров.
- `Updater` — проверяет, что у подов с VPA выставлены корректные ресурсы, если нет — убивает эти поды, чтобы контроллер пересоздал поды с новыми resource requests.
- `Admission Plugin` — задает resource requests при создании новых подов (контроллером или из-за активности Updater'а).

При изменении ресурсов компонентом Updater это происходит с помощью Eviction API, поэтому учитываются `Pod Disruption Budget` для обновляемых подов.

### Модуль vertical-pod-autoscaler: настройки

VPA работает не с контроллером пода, а с самим подом, измеряя и изменяя параметры его контейнеров. Вся настройка происходит с помощью custom resource'а [`VerticalPodAutoscaler`](cr.html#verticalpodautoscaler).

В общем случае конфигурация модуля не требуется. У модуля есть только настройки `nodeSelector/tolerations`.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['vertical-pod-autoscaler'].config-values | format_module_configuration: moduleKebabName }}

### Модуль vertical-pod-autoscaler: custom resources
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscaler | format_crd: "vertical-pod-autoscaler" }}
{{ site.data.schemas.vertical-pod-autoscaler.crds.verticalpodautoscalercheckpoint | format_crd: "vertical-pod-autoscaler" }}

### Модуль vertical-pod-autoscaler: примеры

#### Настройка модуля

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: vertical-pod-autoscaler
spec:
  version: 1
  enabled: true
  settings:
    nodeSelector:
      node-role/system: ""
    tolerations:
    - key: dedicated.deckhouse.io
      operator: Equal
      value: system
```

#### Пример минимального custom resource `VerticalPodAutoscaler`

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: StatefulSet
    name: my-app
```

#### Пример полного custom resource `VerticalPodAutoscaler`

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: hamster
      minAllowed:
        memory: 100Mi
        cpu: 120m
      maxAllowed:
        memory: 300Mi
        cpu: 350m
      mode: Auto
```

### Модуль vertical-pod-autoscaler: FAQ

#### Как посмотреть рекомендации Vertical Pod Autoscaler?

После создания кастомного ресурса [VerticalPodAutoscaler](cr.html#verticalpodautoscaler) посмотреть рекомендации VPA можно следующим образом:

```shell
d8 k describe vpa my-app-vpa
```

В секции `status` отобразятся параметры:

- `Target` — количество ресурсов, которое будет оптимальным для пода (в пределах resourcePolicy);
- `Lower Bound` — минимальное рекомендуемое количество ресурсов для более или менее (но не гарантированно) хорошей работы приложения;
- `Upper Bound` — максимальное рекомендуемое количество ресурсов. Скорее всего, ресурсы, выделенные сверх этого значения, идут в мусорку и совсем никогда не нужны приложению;
- `Uncapped Target` — рекомендуемое количество ресурсов в самый последний момент, то есть данное значение считается на основе самых крайних метрик, не смотря на историю ресурсов за весь период.

#### Как Vertical Pod Autoscaler работает с лимитами?

##### Пример 1

В примере представлен VPA-объект:

```yaml
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: test2
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: test2
  updatePolicy:
    updateMode: "Initial"
```

В VPA-объекте представлен под с ресурсами:

```yaml
resources:
  limits:
    cpu: 2
  requests:
    cpu: 1
```

Если контейнер использует весь CPU, и VPA рекомендует этому контейнеру 1.168 CPU, то отношение между запросами и ограничениями будет равно 100%.
В этом случае при пересоздании пода VPA модифицирует его и проставит такие ресурсы:

```yaml
resources:
  limits:
    cpu: 2336m
  requests:
    cpu: 1168m
```

##### Пример 2

В примере представлен VPA-объект:

```yaml
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: test2
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: test2
  updatePolicy:
    updateMode: "Initial"
```

В VPA-объекте представлен под с ресурсами:

```yaml
resources:
  limits:
    cpu: 1
  requests:
    cpu: 750m
```

Если отношение запросов и ограничений равно 25%, и VPA рекомендует 1.168 CPU для контейнера, VPA изменит ресурсы контейнера следующим образом:

```yaml
resources:
  limits:
    cpu: 1557m
  requests:
    cpu: 1168m
```

Если необходимо ограничить максимальное количество ресурсов, которые могут быть выделены для ограничений контейнера, нужно использовать в спецификации объекта VPA `maxAllowed` или использовать Limit Range объекта Kubernetes.
## Подсистема Сеть

### Модуль cni-cilium

Модуль `cni-cilium` обеспечивает работу сети в кластере. Основан на проекте Cilium.

#### Ограничения

1. Сервисам с типом `NodePort` и `LoadBalancer` не подходят эндпоинты в LB-режиме `DSR`, работающие с hostNetwork. Если это необходимо, переключитесь на режим `SNAT`.
1. Поды `HostPort` связываются только с одним IP-адресом. Если в ОС есть несколько интерфейсов/IP, Cilium выберет один, предпочитая «серые» «белым».
1. Для обеспечения стабильной работы `cni-cilium` на узлах кластера отключите Elastic Agent или ограничьте доступ этого агента к серверу управления Elastic. В состав Elastic Agent входит компонент Elastic Endpoint, который использует технологию Extended Berkeley Packet Filter (eBPF) на узлах кластера и может удалять критически важные eBPF-программы, необходимые для корректной работы `cni-cilium`. Детальная информация и обсуждение проблемы доступны в публикациях проектов Cilium.
1. Требования к ядру:
   * ядро Linux версии не ниже `5.8` для работы модуля `cni-cilium` и его совместной работы с модулями [istio](./istio/), [openvpn](/modules/openvpn/), [node-local-dns]({% if site.d8Revision == 'CE' %}{{ site.urls.ru}}/modules/{% else %}..{% endif %}/node-local-dns/).
1. Совместимость с ОС:
   * Ubuntu:
     * несовместим с версией 18.04;
     * для работы с версией 20.04 необходима установка ядра HWE.
   * Astra Linux:
     * несовместим с изданием «Смоленск».

#### Обработка внешнего трафика в разных режимах работы `bpfLB` (замена kube-proxy от Cilium)

В Kubernetes обычно используются схемы, где трафик приходит на балансировщик, который распределяет его между многими серверами. Через балансировщик проходит и входящий, и исходящий трафик. Таким образом, общая пропускная способность ограничена ресурсами и шириной канала балансировщика. Для оптимизации трафика и разгрузки балансировщика и был придуман механизм `DSR`, в котором входящие пакеты проходят через балансировщик, а исходящие идут напрямую с терминирующих серверов. Так как обычно ответы имеют много больший размер чем запросы, то такой подход позволяет значительно увеличить общую пропускную способность схемы.

В модуле возможен [выбор режима работы](configuration.html#parameters-bpflbmode), влияющий на поведение `Service` с типом `NodePort` и `LoadBalancer`:

* `SNAT` (Source Network Address Translation) — один из подвидов NAT, при котором для каждого исходящего пакета происходит трансляция IP-адреса источника в IP-адрес шлюза из целевой подсети, а входящие пакеты, проходящие через шлюз, транслируются обратно на основе таблицы трансляций. В этом режиме `bpfLB` полностью повторяет логику работы `kube-proxy`:
  * если в `Service` указан `externalTrafficPolicy: Local`, то трафик будет передаваться и балансироваться только в те целевые поды, которые запущены на том же узле, на который этот трафик пришел. Если целевой под не запущен на этом узле, то трафик будет отброшен.
  * если в `Service` указан `externalTrafficPolicy: Cluster`, то трафик будет передаваться и балансироваться во все целевые поды в кластере. При этом, если целевые поды находятся на других узлах, то при передаче трафика на них будет произведен SNAT (IP-адрес источника будет заменен на InternalIP узла).

   ![Схема потоков данных SNAT](images/snat.png)

* `DSR` - (Direct Server Return) — метод, при котором весь входящий трафик проходит через балансировщик нагрузки, а весь исходящий трафик обходит его. Такой метод используется вместо `SNAT`. Часто ответы имеют много больший размер чем запросы и `DSR` позволяет значительно увеличить общую пропускную способность схемы:
  * если в `Service` указан `externalTrafficPolicy: Local`, то поведение абсолютно аналогично `kube-proxy` и `bpfLB` в режиме `SNAT`.
  * если в `Service` указан `externalTrafficPolicy: Cluster`, то трафик так же будет передаваться и балансироваться во все целевые поды в кластере.
  При этом важно учитывать следующие особенности:
    * если целевые поды находятся на других узлах, то при передаче на них входящего трафика будет сохранен IP-адрес источника;
    * исходящий трафик пойдет прямо с узла, на котором был запущен целевой под;
    * IP-адрес источника будет заменен на внешний IP-адрес узла, на которую изначально пришел входящий запрос.

   ![Схема потоков данных DSR](images/dsr.png)

{% alert level="warning" %}
В случае использования режима `DSR` и `Service` с `externalTrafficPolicy: Cluster` требуются дополнительные настройки сетевого окружения.
Сетевое оборудование должно быть готово к ассиметричному прохождению трафика: отключены или настроены соответствующим образом средства фильтрации IP адресов на входе в сеть (`uRPF`, `sourceGuard` и т.п.).
{% endalert %}

* `Hybrid` — в данном режиме TCP-трафик обрабатывается в режиме `DSR`, а UDP — в режиме `SNAT`.

#### Использование CiliumClusterwideNetworkPolicies

{% alert level="danger" %}
Использование CiliumClusterwideNetworkPolicies при отсутствии опции `policyAuditMode` в настройках модуля cni-cilium может привести к некорректной работе Control plane или потере доступа ко всем узлам кластера по SSH.
{% endalert %}

Для использования CiliumClusterwideNetworkPolicies выполните следующие шаги:

1. Примените первичный набор объектов `CiliumClusterwideNetworkPolicy`. Для этого в настройки модуля cni-cilium добавьте конфигурационную опцию [`policyAuditMode`](/modules/cni-cilium/configuration.html#parameters-policyauditmode) со значением `true`.
Опция `policyAuditMode` может быть удалена после применения всех `CniliumClusterwideNetworkPolicy`-объектов и проверки корректности их работы в Hubble UI.

1. Примените правило политики сетевой безопасности:

   ```yaml
   apiVersion: "cilium.io/v2"
   kind: CiliumClusterwideNetworkPolicy
   metadata:
     name: "allow-control-plane-connectivity"
   spec:
     ingress:
     - fromEntities:
       - kube-apiserver
     nodeSelector:
       matchLabels:
         node-role.kubernetes.io/control-plane: ""
   ```

В случае, если CiliumClusterwideNetworkPolicies не будут использованы, Control plane может некорректно работать до одной минуты во время перезагрузки `cilium-agent`-подов. Это происходит из-за сброса Conntrack-таблицы. Привязка к entity `kube-apiserver` позволяет избежать проблемы.

#### Смена режима работы Cilium

При смене режима работы Cilium (параметр [tunnelMode](configuration.html#parameters-tunnelmode)) c `Disabled` на `VXLAN` или обратно, необходимо перезагрузить все узлы, иначе возможны проблемы с доступностью подов.

#### Выключение модуля kube-proxy

Cilium полностью заменяет собой функционал модуля `kube-proxy`, поэтому `kube-proxy` автоматически отключается при включении модуля `cni-cilium`.

#### Использование выборочного алгоритма балансировки нагрузки для сервисов

В Deckhouse Platform Certified Security Edition для балансировки нагрузки трафика сервисов можно применять следующие алгоритмы:

* `Random` — случайный выбор бэкенда для каждого соединения. Прост в реализации, но не всегда обеспечивает равномерное распределение.
* `Maglev` — использует консистентное хеширование для равномерного распределения трафика, подходит для масштабных сервисов с множеством бэкендов, которые часто ротируются.
* `Least Connections` — направляет трафик на бэкенд с наименьшим количеством активных соединений, оптимизируя нагрузку для приложений с длительными соединениями.

По умолчанию для всех сервисов задан алгоритм балансировки **Random**. Однако Deckhouse позволяет переопределять алгоритм для отдельных сервисов. Чтобы использовать выборочный алгоритм балансировки для конкретного сервиса, выполните следующие шаги:

* Отредактируйте конфигурацию модуля `cni-cilium` в Deckhouse, включив параметр [`extraLoadBalancerAlgorithmsEnabled`](configuration.html#parameters-extralbalgorithmsenabled). Это активирует поддержку аннотаций сервисов для выборочных алгоритмов.
* В манифесте сервиса укажите аннотацию `service.cilium.io/lb-algorithm` с одним из значений: `random`, `maglev` или `least-conn`.

{% alert level="warning" %}
Для корректной работы данного механизма требуется версия ядра Linux 5.15 и выше.
{% endalert %}

#### Использование Egress Gateway

{% alert level="warning" %}Доступно в следующих редакциях Deckhouse Platform Certified Security Edition: SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

Egress Gateway в Deckhouse Platform Certified Security Edition может быть использован в одном из двух режимов: [Базовый](#базовый-режим) и [Режим с Virtual IP](#режим-с-virtual-ip). Для выбора режима используйте ресурс [EgressGateway](cr.html#egressgateway) (параметр `spec.sourceIP.node`).

##### Базовый режим

Используются предварительно настроенные IP-адреса на egress-узлах.

<div data-presentation="./presentations/cni-cilium/egressgateway_base_ru.pdf"></div>


##### Режим с Virtual IP

Реализована возможность динамически назначать дополнительные IP-адреса узлам.

<div data-presentation="./presentations/cni-cilium/egressgateway_virtualip_ru.pdf"></div>

### Модуль cni-cilium: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['cni-cilium'].config-values | format_module_configuration: moduleKebabName }}

### Модуль cni-cilium: Custom Resources
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpadvertisements | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpclusterconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigoverrides | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgpnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeerconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumbgppeeringpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumcidrgroups | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwideenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumclusterwidenetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumegressgatewaypolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpoints | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumendpointslices | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumenvoyconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumexternalworkloads | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumidentities | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliuml2announcementpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumloadbalancerippools | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumlocalredirectpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnetworkpolicies | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodeconfigs | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumnodes | format_crd: "cni-cilium" }}
{{ site.data.schemas.cni-cilium.crds.cilium.ciliumpodippools | format_crd: "cni-cilium" }}

### Модуль cni-cilium: примеры

#### Egress Gateway

{% alert level="warning" %}Доступно в следующих редакциях: SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

##### Принцип работы

Для настройки egress-шлюза необходимы два кастомных ресурса:

1. EgressGateway — описывает группу узлов, один из которых будет выбран в качестве активного egress-шлюза, а остальные останутся в резерве на случай отказа:
   - Среди группы узлов, попадающих под `spec.nodeSelector`, будут отобраны пригодные к использованию. Один из них будет назначен активным шлюзом. Выбор активного узла осуществляется в алфавитном порядке.

     Признаки пригодного узла:
     - Узел в состоянии `Ready`.
       - Узел не находится в состоянии технического обслуживания (cordon).
       - `cilium-agent` на узле в состоянии `Ready`.
     - При использовании EgressGateway в режиме `VirtualIP` на активном узле запускается агент, который эмулирует «виртуальный» IP-адрес с использованием протокола ARP. При определении пригодности узла также учитывается состояние пода данного агента.
     - Разные EgressGateway могут использовать одни и те же узлы. Выбор активного узла в каждом EgressGateway осуществляется независимо от других, что позволяет сбалансировать нагрузку между ними.
1. EgressGatewayPolicy — описывает политику перенаправления сетевых запросов от подов в кластере на конкретный egress-шлюз, определённый с помощью EgressGateway.

##### Обслуживание узла

Для проведения работ на узле, который в данный момент является активным egress-шлюзом, выполните следующие шаги:
1. Снимите метку (label) с узла, чтобы исключить его из списка кандидатов для роли egress-шлюза. Egress-label — это метка, указанная в `spec.nodeSelector` вашего EgressGateway.

    ```bash
    d8 k label node <node-name> <egress-label>-
    ```

1. Переведите узел в режим обслуживания (cordon), чтобы предотвратить запуск новых подов:

    ```bash
    d8 k cordon <node-name>
    ```

    После этого Cilium автоматически выберет новый активный узел из оставшихся кандидатов.
    Трафик продолжит направляться через новый шлюз без прерывания.

1. Для возврата узла в работу выполните:

    ```bash
    d8 k uncordon <node-name>
    d8 k label node <node-name> <egress-label>=<value>
    ```

> Важно: повторное добавление метки может привести к тому, что узел снова будет выбран активным egress-шлюзом (если он первый в алфавитном порядке среди доступных кандидатов).
Чтобы избежать немедленного возврата узла в активное состояние, временно уменьшите количество реплик в EgressGateway или настройте приоритет выбора через дополнительные метки.

##### Сравнение с CiliumEgressGatewayPolicy

`CiliumEgressGatewayPolicy` подразумевает настройку только одного узла в качестве egress-шлюза. При выходе его из строя не предусмотрено failover-механизмов и сетевая связь будет нарушена.

##### Примеры настроек

###### EgressGateway в режиме PrimaryIPFromEgressGatewayNodeInterface (базовый режим)

```yaml
apiVersion: network.deckhouse.io/v1alpha1
kind: EgressGateway
metadata:
  name: myegressgw
spec:
  nodeSelector:
    dedicated/egress: ""
  sourceIP:
    mode: PrimaryIPFromEgressGatewayNodeInterface
    primaryIPFromEgressGatewayNodeInterface:
      # На всех узлах, попадающих под nodeSelector, «публичный» интерфейс должен называться одинаково.
      # При выходе из строя активного узла, трафик будет перенаправлен через резервный и
      # IP-адрес отправителя у сетевых пакетов поменяется.
      interfaceName: eth1
```

###### EgressGateway в режиме VirtualIPAddress (режим с Virtual IP)

```yaml
apiVersion: network.deckhouse.io/v1alpha1
kind: EgressGateway
metadata:
  name: myeg
spec:
  nodeSelector:
    dedicated/egress: ""
  sourceIP:
    mode: VirtualIPAddress
    virtualIPAddress:
      # На каждом узле должны быть настроены все необходимые маршруты для доступа на все внешние публичные сервисы,
      # «публичный» интерфейс должен быть подготовлен к автоматической настройке «виртуального» IP в качестве дополнительного (secondary) IP-адреса.
      # При выходе из строя активного узла, трафик будет перенаправлен через резервный и
      # IP-адрес отправителя у сетевых пакетов не поменяется.
      ip: 172.18.18.242
      # Список сетевых интерфейсов для «виртуального» IP.
      interfaces:
      - eth1
```

###### EgressGatewayPolicy

```yaml
apiVersion: network.deckhouse.io/v1alpha1
kind: EgressGatewayPolicy
metadata:
  name: my-egressgw-policy
spec:
  destinationCIDRs:
  - 0.0.0.0/0
  egressGatewayName: my-egressgw
  selectors:
  - podSelector:
      matchLabels:
        app: backend
        io.kubernetes.pod.namespace: my-ns
```

### Модуль cilium-hubble

Модуль `cilium-hubble` обеспечивает визуализацию сетевого стека кластера, если включен Cilium CNI.

#### Требования

Для работы модуля `cilium-hubble` необходимы:

- Версия ядра Linux >= 5.8 с поддержкой eBPF.
- Поддержка формата метаданных BPF Type Format (BTF). Проверить можно следующими способами:
  - выполнить команду `ls -lah /sys/kernel/btf/vmlinux` — наличие файла подтверждает поддержку BTF;
  - выполнить команду `grep -E "CONFIG_DEBUG_INFO_BTF=(y|m)" /boot/config-*` — если параметр включён, BTF поддерживается.

### Модуль cilium-hubble: настройки

{% include module-alerts.liquid %}

{% include module-bundle.liquid %}

Если модуль `cni-cilium` выключен, параметр `ciliumHubbleEnabled:` не повлияет на включение модуля `cilium-hubble`.

{% include module-conversion.liquid %}

{% include module-settings.liquid %}

#### Аутентификация

По умолчанию используется модуль [user-authn](/modules/user-authn/). Также можно настроить аутентификацию через `externalAuthentication`.
Если эти варианты отключены, модуль включит базовую аутентификацию со сгенерированным паролем.

Чтобы просмотреть сгенерированный пароль, выполните команду:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values cilium-hubble -o json | jq '.ciliumHubble.internal.auth.password'
```

Чтобы сгенерировать новый пароль, удалите Secret:

```shell
d8 k -n d8-cni-cilium delete secret/hubble-basic-auth
```

{% alert level="info" %}
Параметр `auth.password` больше не поддерживается.
{% endalert %}
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['cilium-hubble'].config-values | format_module_configuration: moduleKebabName }}

### Модуль ingress-nginx

Устанавливает и управляет NGINX Ingress controller с помощью Custom Resources. Если узлов для размещения Ingress-контроллера больше одного, он устанавливается в отказоустойчивом режиме и учитывает все особенности реализации инфраструктуры облаков и bare metal, а также кластеров Kubernetes различных типов.

Поддерживает запуск и раздельное конфигурирование одновременно нескольких NGINX Ingress controller'ов — один **основной** и неограниченное количество **дополнительных**. Например, это позволяет отделять внешние и intranet Ingress-ресурсы приложений.

#### Варианты терминирования трафика

Трафик к `ingress-nginx` может быть отправлен несколькими способами:

- напрямую без внешнего балансировщика;
- через внешний LoadBalancer.

#### Терминация HTTPS

Модуль позволяет управлять политиками безопасности HTTPS каждого NGINX Ingress controller, в том числе:

- параметрами HSTS;
- набором доступных версий SSL/TLS и протоколов шифрования.

Также модуль интегрирован с модулем [cert-manager](/modules/cert-manager/), при взаимодействии с которым, возможны автоматический заказ SSL-сертификатов и дальнейшее использование сертификатов NGINX Ingress controller.

#### Мониторинг и статистика

В нашей реализации `ingress-nginx` добавлена система сбора статистики в Prometheus с множеством метрик:

- по длительности времени всего ответа и апстрима отдельно;
- кодам ответа;
- количеству повторов запросов (retry);
- размерам запроса и ответа;
- методам запросов;
- типам `content-type`;
- географии распределения запросов и т. д.

Данные доступны в нескольких параметрах:

- по `namespace`;
- `vhost`;
- `ingress`-ресурсу;
- `location` (в nginx).

Все графики собраны в виде удобных дашбордов в Grafana, при этом есть возможность drill-down'а по графикам. Например, при просмотре статистики пространств имён, можно нажать ссылку на дашборд в Grafana, углубиться в статистику по `vhosts` в соответствующем `namespace` и т.д.

#### Статистика

##### Основные принципы сбора статистики

1. На каждый запрос на стадии `log_by_lua_block` вызывается модуль, который рассчитывает необходимые данные и складывает их в буфер (у каждого nginx worker собственный буфер).
1. На стадии `init_by_lua_block` для каждого nginx worker запускается процесс, который раз в секунду асинхронно отправляет данные в формате `protobuf` через TCP socket в `protobuf_exporter` (разработка Deckhouse Platform Certified Security Edition).
1. `protobuf_exporter` запущен sidecar-контейнером в поде с ingress-controller, принимает сообщения в формате `protobuf`, разбирает, агрегирует их по установленным правилам и экспортирует в формате для Prometheus.
1. Prometheus каждые 30 секунд собирает метрики как в ingress-controller (там есть небольшое количество нужных нам метрик), так и в protobuf_exporter, на основании этих данных все работает!

##### Какая статистика собирается и как она представлена

У всех собираемых метрик есть служебные лейблы, позволяющие идентифицировать экземпляр контроллера: `controller`, `app`, `instance` и `endpoint` (они видны в `/prometheus/targets`).

- Все метрики (кроме geo), экспортируемые protobuf_exporter, представлены в трех уровнях детализации:
  - `ingress_nginx_overall_*` — «вид с вертолета», у всех метрик есть лейблы `namespace`, `vhost` и `content_kind`;
  - `ingress_nginx_detail_*` — кроме лейблов уровня `overall`, добавляются `ingress`, `service`, `service_port` и `location`;
  - `ingress_nginx_detail_backend_*` — ограниченная часть данных, собирается в разрезе по бэкендам. У этих метрик, кроме лейблов уровня detail, добавляется лейбл `pod_ip`.

- Для уровней overall и detail собираются следующие метрики:
  - `*_requests_total` — counter количества запросов (дополнительные лейблы — `scheme`, `method`);
  - `*_responses_total` — counter количества ответов (дополнительный лейбл — `status`);
  - `*_request_seconds_{sum,count,bucket}` — histogram времени ответа;
  - `*_bytes_received_{sum,count,bucket}` — histogram размера запроса;
  - `*_bytes_sent_{sum,count,bucket}` — histogram размера ответа;
  - `*_upstream_response_seconds_{sum,count,bucket}` — histogram времени ответа upstream'а (используется сумма времен ответов всех upstream'ов, если их было несколько);
  - `*_lowres_upstream_response_seconds_{sum,count,bucket}` — то же самое, что и предыдущая метрика, только с меньшей детализацией (подходит для визуализации, но не подходит для расчета quantile);
  - `*_upstream_retries_{count,sum}` — количество запросов, при обработке которых были retry бэкендов, и сумма retry'ев.

- Для уровня overall собираются следующие метрики:
  - `*_geohash_total` — counter количества запросов с определенным geohash (дополнительные лейблы — `geohash`, `place`).

- Для уровня detail_backend собираются следующие метрики:
  - `*_lowres_upstream_response_seconds` — то же самое, что аналогичная метрика для overall и detail;
  - `*_responses_total` — counter количества ответов (дополнительный лейбл — `status_class`, а не просто `status`);
  - `*_upstream_bytes_received_sum` — counter суммы размеров ответов бэкенда.

### Модуль ingress-nginx: настройки

{% alert level="info" %}
Если модуль был выключен и необходимо его включить, обратите внимание на глобальный параметр [publicDomainTemplate](/reference/api/global.html#параметры). Укажите его, если он не указан, иначе Ingress-ресурсы для служебных компонентов Deckhouse Platform Certified Security Edition (dashboard, user-auth, grafana, upmeter  и т. п.) не будут созданы.
{% endalert %}

Конфигурация Ingress-контроллеров выполняется с помощью Custom Resource [IngressNginxController](cr.html#ingressnginxcontroller).

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['ingress-nginx'].config-values | format_module_configuration: moduleKebabName }}

### Модуль ingress-nginx: Custom Resources
{{ site.data.schemas.ingress-nginx.crds.ingress-nginx | format_crd: "ingress-nginx" }}

### Модуль ingress-nginx: пример

{% raw %}

#### Пример для bare metal

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: nginx
  inlet: HostWithFailover
  nodeSelector:
    node-role.deckhouse.io/frontend: ""
  tolerations:
  - effect: NoExecute
    key: dedicated.deckhouse.io
    value: frontend
```

#### Пример для bare metal (при использовании внешнего балансировщика

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: nginx
  inlet: HostPort
  hostPort:
    httpPort: 80
    httpsPort: 443
    behindL7Proxy: true
```

{% endraw %}

#### Пример для bare metal (балансировщик MetalLB в режиме BGP LoadBalancer)

{% alert level="warning" %}Доступно в следующих редакциях: EE, CSE Pro (1.67).{% endalert %}

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: nginx
  inlet: LoadBalancer
  nodeSelector:
    node-role.deckhouse.io/frontend: ""
  tolerations:
  - effect: NoExecute
    key: dedicated.deckhouse.io
    value: frontend
```

В случае использования MetalLB его speaker-поды должны быть запущены на тех же узлах, что и поды Ingress–контроллера.

Контроллер должен получать реальные IP-адреса клиентов — поэтому его Service создается с параметром `externalTrafficPolicy: Local` (запрещая межузловой SNAT), и для принятия данного параметра MetalLB speaker анонсирует этот Service только с тех узлов, в которых запущены целевые поды.

Для этого примера [конфигурация модуля `metallb`](./metallb/configuration.html) должна быть следующей:

```yaml
metallb:
 speaker:
   nodeSelector:
     node-role.deckhouse.io/frontend: ""
   tolerations:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      value: frontend
```

#### Пример для bare metal (балансировщик MetalLB в режиме L2 LoadBalancer)

{% alert level="warning" %}Доступно в следующих редакциях: SE, SE+, EE, CSE Lite (1.67), CSE Pro (1.67).{% endalert %}

1. Включите модуль `metallb`:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: metallb
   spec:
     enabled: true
     version: 2
   ```

1. Создайте ресурс _MetalLoadBalancerClass_:

   ```yaml
   apiVersion: network.deckhouse.io/v1alpha1
   kind: MetalLoadBalancerClass
   metadata:
     name: ingress
   spec:
     addressPool:
       - 192.168.2.100-192.168.2.150
     isDefault: false
     nodeSelector:
       node-role.kubernetes.io/loadbalancer: "" # селектор узлов-балансировщиков
     type: L2
   ```

1. Создайте ресурс _IngressNginxController_:

   ```yaml
   apiVersion: deckhouse.io/v1
   kind: IngressNginxController
   metadata:
     name: main
   spec:
     ingressClass: nginx
     inlet: LoadBalancer
     loadBalancer:
       loadBalancerClass: ingress
       annotations:
         # Количество адресов, которые будут выделены из пула, объявленного в MetalLoadBalancerClass.
         network.deckhouse.io/l2-load-balancer-external-ips-count: "3"
     # Селектор и tolerations. Поды ingress-controller должны быть размещены на тех же узлах, что и поды MetalLB speaker.
     nodeSelector:
        node-role.kubernetes.io/loadbalancer: ""
     tolerations:
     - effect: NoSchedule
       key: node-role/loadbalancer
       operator: Exists
   ```

1. Платформа создаст сервис с типом `LoadBalancer`, которому будет присвоено заданное количество адресов:

   ```shell
   $ d8 k -n d8-ingress-nginx get svc
   NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP                                 PORT(S)                      AGE
   main-load-balancer     LoadBalancer   10.222.130.11   192.168.2.100,192.168.2.101,192.168.2.102   80:30689/TCP,443:30668/TCP   11s
   ```

### Модуль ingress-nginx: FAQ

#### Как разрешить доступ к приложению внутри кластера только от Ingress-контроллера?

Если необходимо ограничить доступ к вашему приложению внутри кластера исключительно от подов Ingress-контроллера, необходимо в под с приложением добавить контейнер с kube-rbac-proxy, как показано в примере ниже:

{% raw %}

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: my-namespace
spec:
  selector:
    matchLabels:
      app: my-app
  replicas: 1
  template:
    metadata:
      labels:
        app: my-app
    spec:
      serviceAccountName: my-sa
      containers:
      - name: my-cool-app
        image: mycompany/my-app:v0.5.3
        args:
        - "--listen=127.0.0.1:8080"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 443
            scheme: HTTPS
      - name: kube-rbac-proxy
        image: flant/kube-rbac-proxy:v0.1.0
        # Рекомендуется использовать прокси из репозитория Deckhouse.
        args:
        - "--secure-listen-address=0.0.0.0:443"
        - "--config-file=/etc/kube-rbac-proxy/config-file.yaml"
        - "--v=2"
        - "--logtostderr=true"
        # Если kube-apiserver недоступен, аутентификация и авторизация пользователей невозможна.
        # Stale Cache хранит результаты успешной авторизации и используется лишь в случае, если apiserver недоступен.
        - "--stale-cache-interval=1h30m"
        ports:
        - containerPort: 443
          name: https
        volumeMounts:
        - name: kube-rbac-proxy
          mountPath: /etc/kube-rbac-proxy
      volumes:
      - name: kube-rbac-proxy
        configMap:
          name: kube-rbac-proxy
```

{% endraw %}

Приложение принимает запросы на адресе `127.0.0.1`, это означает, что по незащищенному соединению к нему можно подключиться только внутри пода.
Прокси прослушивает порт на адресе `0.0.0.0` и перехватывает весь внешний трафик к поду.

##### Как выдать минимальные права для ServiceAccount?

Чтобы аутентифицировать и авторизовывать пользователей с помощью kube-apiserver, у прокси должны быть права на создание `TokenReview` и `SubjectAccessReview`.

В кластерах Deckhouse Platform Certified Security Edition уже есть готовая ClusterRole — **d8-rbac-proxy**, создавать её самостоятельно не требуется! Свяжите её с ServiceAccount вашего Deployment'а, как показано в примере ниже.
{% raw %}

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
  namespace: my-namespace
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-namespace:my-sa:d8-rbac-proxy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: d8:rbac-proxy
subjects:
- kind: ServiceAccount
  name: my-sa
  namespace: my-namespace
```

##### Конфигурация Kube-RBAC-Proxy

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-rbac-proxy
data:
  config-file.yaml: |+
    excludePaths:
    - /healthz 
  # Не требуем авторизацию для liveness пробы.
    upstreams:
    - upstream: http://127.0.0.1:8081/
  # Адрес upstream-сервиса, на который будет перенаправлен входящий трафик.
      path: / 
  # Путь, обрабатываемый прокси, по которому принимаются запросы и перенаправляются на upstream.
      authorization:
        resourceAttributes:
          namespace: my-namespace
          apiGroup: apps
          apiVersion: v1
          resource: deployments
          subresource: http
          name: my-app
```

{% endraw %}

Согласно конфигурации, у пользователя должны быть права доступа к Deployment с именем `my-app`
и его дополнительному ресурсу `http` в пространстве имён `my-namespace`.

Выглядят такие права в виде RBAC следующим образом:

{% raw %}

```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kube-rbac-proxy:my-app
  namespace: my-namespace
rules:
- apiGroups: ["apps"]
  resources: ["deployments/http"]
  resourceNames: ["my-app"]
  verbs: ["get", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-rbac-proxy:my-app
  namespace: my-namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-rbac-proxy:my-app
subjects:
### Все пользовательские сертификаты ingress-контроллеров выписаны для одной конкретной группы.
- kind: Group
  name: ingress-nginx:auth
```

Для Ingress-ресурса добавьте параметры:

```yaml
nginx.ingress.kubernetes.io/backend-protocol: HTTPS
nginx.ingress.kubernetes.io/configuration-snippet: |
  proxy_ssl_certificate /etc/nginx/ssl/client.crt;
  proxy_ssl_certificate_key /etc/nginx/ssl/client.key;
  proxy_ssl_protocols TLSv1.2;
  proxy_ssl_session_reuse on;
```

{% endraw %}

#### Как сконфигурировать балансировщик нагрузки для проверки доступности IngressNginxController?

В ситуации, когда `IngressNginxController` размещен за балансировщиком нагрузки, рекомендуется сконфигурировать балансировщик для проверки доступности
узлов `IngressNginxController` с помощью HTTP-запросов или TCP-подключений. В то время как тестирование с помощью TCP-подключений представляет собой простой и универсальный механизм проверки доступности, мы рекомендуем использовать проверку на основе HTTP-запросов со следующими параметрами:

- протокол: `HTTP`;
- путь: `/healthz`;
- порт: `80` (в случае использования инлета `HostPort` нужно указать номер порта, соответствующий параметру [httpPort](cr.html#ingressnginxcontroller-v1-spec-hostport-httpport).

#### Как настроить работу через MetalLB с доступом только из внутренней сети?

Пример MetalLB с настройками доступа только из внутренней сети:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: "nginx"
  inlet: "LoadBalancer"
  loadBalancer:
    sourceRanges:
    - 192.168.0.0/24
```

{% alert level="warning" %}
Для работы необходимо включить параметр [`svcSourceRangeCheck`](/modules/cni-cilium/configuration.html#parameters-svcsourcerangecheck) в модуле cni-cilium.
{% endalert %}

#### Как добавить дополнительные поля для логирования в nginx-controller?

Пример добавления дополнительных полей:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: main
spec:
  ingressClass: "nginx"
  inlet: "LoadBalancer"
  additionalLogFields:
    my-cookie: "$cookie_MY_COOKIE"
```

#### Как включить HorizontalPodAutoscaling для IngressNginxController?

{% alert level="warning" %}
Режим HPA возможен только для контроллеров с инлетом `LoadBalancer` или `LoadBalancerWithProxyProtocol`.

Режим HPA возможен только при `minReplicas` != `maxReplicas`, в противном случае deployment `hpa-scaler` не создается.
{% endalert %}

Для включения HPA используйте атрибуты `minReplicas` и `maxReplicas` в [IngressNginxController CR](cr.html#ingressnginxcontroller).

IngressNginxController разворачивается с помощью DaemonSet. DaemonSet не предоставляет возможности горизонтального масштабирования, поэтому создается дополнительный deployment `hpa-scaler` и HPA resource, который следит за предварительно созданной метрикой `prometheus-metrics-adapter-d8-ingress-nginx-cpu-utilization-for-hpa`. Если CPU utilization превысит 50%, HPA закажет новую реплику для `hpa-scaler` (с учетом minReplicas и maxReplicas).

Deployment `hpa-scaler` обладает HardPodAntiAffinity (запрет на размещение подов с одинаковыми метками на одном узле), поэтому он попытается выделить для себя новый узел (если это возможно
в рамках своей группы узлов), куда автоматически будет размещен еще один instance Ingress-контроллера.

{% alert level="info" %}

- Минимальное реальное количество реплик IngressNginxController не может быть меньше минимального количества узлов в группе узлов, куда он разворачивается.
- Максимальное реальное количество реплик IngressNginxController не может быть больше максимального количества узлов в группе узлов, куда он разворачивается.

{% endalert %}

#### Как использовать IngressClass с установленными IngressClassParameters?

Начиная с версии 1.1 IngressNginxController, Deckhouse создает объект IngressClass самостоятельно. Если вы хотите использовать свой IngressClass с установленными IngressClassParameters, достаточно добавить к нему label `ingress-class.deckhouse.io/external: "true"`:

```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    ingress-class.deckhouse.io/external: "true"
  name: my-super-ingress
spec:
  controller: ingress-nginx.deckhouse.io/my-super-ingress
  parameters:
    apiGroup: elbv2.k8s.aws
    kind: IngressClassParams
    name: awesome-class-cfg
```

В этом случае, при указании данного IngressClass в CRD IngressNginxController, Deckhouse не будет создавать объект, а использует существующий.

#### Как отключить сборку детализированной статистики Ingress-ресурсов?

По умолчанию Deckhouse собирает подробную статистику со всех Ingress-ресурсов в кластере. Этот процесс может приводить к высокой нагрузке системы мониторинга.

Для отключения сбора статистики добавьте лейбл `ingress.deckhouse.io/discard-metrics: "true"` к соответствующему пространству имён или Ingress-ресурсу.

Пример отключения сбора статистики (метрик) для всех Ingress-ресурсов в пространстве имен `review-1`:

```shell
d8 k label ns review-1 ingress.deckhouse.io/discard-metrics=true
```

Пример отключения сбора статистики (метрик) для всех Ingress-ресурсов `test-site` в пространстве имен `development`:

```shell
d8 k label ingress test-site -n development ingress.deckhouse.io/discard-metrics=true
```

#### Как корректно вывести из эксплуатации (drain) узел с запущенным IngressNginxController?

Доступно два способа корректного вывода из эксплуатации узла, на котором запущен IngressNginxController.

1. С помощью аннотации.

    Аннотация будет автоматически удалена после завершения операции.

    ```shell
    d8 k annotate node <node_name> update.node.deckhouse.io/draining=user
    ```

1. С помощью d8 k drain.

    При использовании стандартной команды d8 k drain необходимо указать флаг `--force` даже при наличии `--ignore-daemonsets`,
    поскольку IngressNginxController развёрнут с использованием Advanced DaemonSet:

    ```shell
    d8 k drain <node_name> --delete-emptydir-data --ignore-daemonsets --force
    ```

#### Как включить Web Application Firewall (WAF)?

Для защиты веб-приложений от L7-атак используется программное обеспечение известное как Web Application Firewall (WAF).
В ingress-nginx контроллер встроен WAF под названием `ModSecurity` (проект Open Worldwide Application Security).

По умолчанию ModSecurity выключен.

##### Включение ModSecurity

Для включения ModSecurity необходимо задать параметры в кастомном ресурсе IngressNginxController, в секции `config`:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: <имя_контроллера>
spec:
  config:
    enable-modsecurity: "true"
    modsecurity-snippet: |
      Include /etc/nginx/modsecurity/modsecurity.conf
```

После применения настроек ModSecurity начнет работать для всего трафика, проходящего через данный ingress-nginx контроллер.
При этом используется режим аудита (`DetectionOnly`) и базовая рекомендуемая конфигурация.

##### Настройка ModSecurity

ModSecurity можно настраивать двумя способами:
1. Для всего ingress-nginx контроллера
   - необходимые директивы описываются в секции `config.modsecurity-snippet` в кастомном ресурсе IngressNginxController, как в примере выше.
1. Для каждого кастомного ресурса Ingress по отдельности
   - необходимые директивы описываются в аннотации `nginx.ingress.kubernetes.io/modsecurity-snippet: |` непосредственно в манифестах Ingress.

Чтобы включить выполнение правил (а не только логирование), добавьте директиву `SecRuleEngine On` по примеру ниже:

```yaml
apiVersion: deckhouse.io/v1
kind: IngressNginxController
metadata:
  name: <имя_контролера>
spec:
  config:
    enable-modsecurity: "true"
    modsecurity-snippet: |
      Include /etc/nginx/modsecurity/modsecurity.conf
      SecRuleEngine On
```

На данный момент использование набора правил OWASP Core Rule Set (CRS) недоступно.

### Модуль istio

#### Таблица совместимости поддерживаемых версий

| Версия Istio | Версии K8S, поддерживаемые Istio | Статус в текущем релизе D8 |
|:------------:|:------------------------------------------------------------------------------------------------------------------------------:|:--------------------------:|
|     1.25      |                                                1.29, 1.30, 1.31, 1.32                                                | Поддерживается |
|     1.21     |                                           1.26, 1.27, 1.28, 1.29, 1.30, 1.31                                          |  Поддерживается  |
|     1.19     |                                                     1.25<sup>*</sup>, 1.26, 1.27, 1.28, 1.28, 1.29, 1.30                                                     |       Устарела и будет удалена       |

<sup>*</sup> — версия Kubernetes **НЕ поддерживается** в текущем релизе Deckhouse Platform Certified Security Edition.

{::options parse_block_html="false" /}

#### Задачи, которые решает Istio

Istio — фреймворк централизованного управления сетевым трафиком, реализующий подход Service Mesh.

Istio решает для приложений следующие задачи:

- [Использование Mutual TLS:](#mutual-tls)
  - Взаимная достоверная аутентификация сервисов.
  - Шифрование трафика.
- [Авторизация доступа между сервисами.](#авторизация)
- [Маршрутизация запросов:](#маршрутизация-запросов)
  - Canary-deployment и A/B-тестирование — позволяют отправлять часть запросов на новую версию приложения.
- [Управление балансировкой запросов между эндпойнтами сервиса:](#управление-балансировкой-запросов-между-эндпойнтами-сервиса)
  - Circuit Breaker:
    - временное исключение эндпойнта из балансировки, если превышен лимит ошибок;
    - настройка лимитов на количество TCP-соединений и количество запросов в сторону одного эндпойнта;
    - выявление зависших запросов и обрывание их с кодом ошибки (HTTP request timeout).
  - Sticky Sessions:
    - привязка запросов от конечных пользователей к эндпойнту сервиса.
  - Locality Failover — позволяет отдавать предпочтение эндпойнтам в локальной зоне доступности.
  - Балансировка gRPC-сервисов.
- [Повышение Observability:](#observability)
  - Сбор и визуализация данных для трассировки прикладных запросов с помощью Jaeger.
  - Сбор метрик о трафике между сервисами в Prometheus и визуализация их в Grafana.
  - Визуализация состояния связей между сервисами и состояния служебных компонентов Istio с помощью Kiali.
- [Организация мульти-ЦОД кластера за счет объединения кластеров в единый Service Mesh (мультикластер).](#мультикластер)
- [Объединение разрозненных кластеров в федерацию с возможностью предоставлять стандартный (в понимании Service Mesh) доступ к избранным сервисам.](#федерация)

#### Mutual TLS

Это основной метод взаимной аутентификации сервисов. Принцип основан на том, что для всех исходящих запросов проверяется сертификат сервера, а для входящих - клиентский сертификат. После проверки sidecar-proxy получает возможность идентифицировать удаленный узел и использовать эти данные для аутентификации или для прикладных целей.

Каждый сервис получает собственный идентификатор в формате `<TrustDomain>/ns/<Namespace>/sa/<ServiceAccount>`, где `TrustDomain` в нашем случае — это домен кластера. Каждому сервису можно выделять собственный ServiceAccount или использовать стандартный «default». Полученный идентификатор сервиса можно использовать как в правилах авторизации, так и в прикладных целях. Именно этот идентификатор используется в качестве удостоверяемого имени в TLS-сертификатах.

Данные настройки можно переопределить на уровне namespace.

#### Авторизация

Управление авторизацией осуществляется с помощью ресурса [AuthorizationPolicy](istio-cr.html#authorizationpolicy). В момент, когда для сервиса создается этот ресурс, начинает работать следующий алгоритм принятия решения о судьбе запроса:

- Если запрос попадает под политику DENY — запретить запрос.
- Если для данного сервиса нет политик ALLOW — разрешить запрос.
- Если запрос попадает под политику ALLOW — разрешить запрос.
- Все остальные запросы — запретить.

Иными словами, если явно что-то запретить, работает только запрет. Если же что-то явно разрешить, будут разрешены только явно одобренные запросы (запреты при этом имеют приоритет).

Для написания правил авторизации доступны следующие аргументы:

- идентификаторы сервисов и wildcard на их основе (`mycluster.local/ns/myns/sa/myapp` или `mycluster.local/*`);
- пространства имён;
- диапазоны IP;
- HTTP-заголовки;
- JWT-токены из прикладных запросов.

#### Маршрутизация запросов

Основной ресурс для управления маршрутизацией — [VirtualService](istio-cr.html#virtualservice), он позволяет переопределять судьбу HTTP- или TCP-запроса. Доступные аргументы для принятия решения о маршрутизации:

- Host и любые другие заголовки;
- URI;
- метод (GET, POST и пр.);
- лейблы пода или namespace источника запросов;
- dst-IP или dst-порт для не-HTTP-запросов.

#### Управление балансировкой запросов между эндпойнтами сервиса

Основной ресурс для управления балансировкой запросов — [DestinationRule](istio-cr.html#destinationrule), он позволяет настроить нюансы исходящих из подов запросов:

- лимиты/таймауты для TCP;
- алгоритмы балансировки между эндпойнтами;
- правила определения проблем на стороне эндпойнта для выведения его из балансировки;
- нюансы шифрования.

{% alert level="warning" %}
Все настраиваемые лимиты работают для каждого пода клиента по отдельности! Если настроить для сервиса ограничение на одно TCP-соединение, а клиентских подов — три, то сервис получит три входящих соединения.
{% endalert %}

#### Observability

##### Трассировка

Istio позволяет осуществлять сбор трейсов с приложений и инъекцию трассировочных заголовков, если таковых нет. При этом важно понимать следующее:

- Если запрос инициирует на сервисе вторичные запросы, для них необходимо наследовать трассировочные заголовки средствами приложения.
- Jaeger для сбора и отображения трейсов потребуется устанавливать самостоятельно.

##### Grafana

В стандартной комплектации с модулем предоставлены дополнительные дашборды:

- дашборд для оценки производительности и успешности запросов/ответов между приложениями;
- дашборд для оценки работоспособности и нагрузки на control plane.

##### Kiali

Инструмент для визуализации дерева сервисов вашего приложения. Позволяет быстро оценить обстановку в сетевой связности благодаря визуализации запросов и их количественных характеристик непосредственно на схеме.

#### Архитектура кластера с включенным Istio

Компоненты кластера делятся на две категории:

- control plane — управляющие и обслуживающие сервисы. Под control plane обычно подразумевают поды istiod.
- data plane — прикладная часть Istio. Представляет собой контейнеры sidecar-proxy.

![Архитектура кластера с включенным Istio](images/istio-architecture.svg)


Все сервисы из data plane группируются в mesh. Его характеристики:

- Общее пространство имен для генерации идентификатора сервиса в формате `<TrustDomain>/ns/<Namespace>/sa/<ServiceAccount>`. Каждый mesh имеет идентификатор TrustDomain, который в нашем случае совпадает с доменом кластера. Например: `mycluster.local/ns/myns/sa/myapp`.
- Сервисы в рамках одного mesh имеют возможность аутентифицировать друг друга с помощью доверенных корневых сертификатов.

Элементы control plane:

- `istiod` — ключевой сервис, обеспечивающий решение следующих задач:
  - Непрерывная связь с API Kubernetes и сбор информации о прикладных сервисах.
  - Обработка и валидация с помощью механизма Kubernetes Validating Webhook всех Custom Resources, которые связаны с Istio.
  - Компоновка конфигурации для каждого sidecar-proxy индивидуально:
    - генерация правил авторизации, маршрутизации, балансировки и пр.;
    - распространение информации о других прикладных сервисах в кластере;
    - выпуск индивидуальных клиентских сертификатов для организации схемы Mutual TLS. Эти сертификаты не связаны с сертификатами, которые использует и контролирует сам Kubernetes для своих служебных нужд.
  - Автоматическая подстройка манифестов, определяющих прикладные поды через механизм Kubernetes Mutating Webhook:
    - внедрение дополнительного служебного контейнера sidecar-proxy;
    - внедрение дополнительного init-контейнера для адаптации сетевой подсистемы (настройка DNAT для перехвата прикладного трафика);
    - перенаправление readiness- и liveness-проб через sidecar-proxy.
- `operator` — компонент, отвечающий за установку всех ресурсов, необходимых для работы control plane определенной версии.
- `kiali` — панель управления и наблюдения за ресурсами Istio и пользовательскими сервисами под управлением Istio, позволяющая следующее:
  - Визуализировать связи между сервисами.
  - Диагностировать проблемные связи между сервисами.
  - Диагностировать состояние control plane.

Для приема пользовательского трафика необходима доработка Ingress-контроллера:

- К подам контроллера добавляется sidecar-proxy, который обслуживает только трафик от контроллера в сторону прикладных сервисов (параметр IngressNginxController [`enableIstioSidecar`](./ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-enableistiosidecar) у ресурса IngressNginxController).
- Сервисы не под управлением Istio продолжают работать как раньше, запросы в их сторону не перехватываются сайдкаром контроллера.
- Запросы в сторону сервисов под управлением Istio перехватываются сайдкаром и обрабатываются в соответствии с правилами Istio (подробнее о том, [как активировать Istio для приложения](#как-активировать-istio-для-приложения)).

Контроллер istiod и каждый контейнер sidecar-proxy экспортируют собственные метрики, которые собирает кластерный Prometheus.

#### Архитектура прикладного сервиса с включенным Istio

##### Особенности

- Каждый под сервиса получает дополнительный контейнер — sidecar-proxy. Технически этот контейнер содержит два приложения:
  - **Envoy** — проксирует прикладной трафик и реализует все функции, которые предоставляет Istio, включая маршрутизацию, аутентификацию, авторизацию и пр.
  - **pilot-agent** — часть Istio, отвечает за поддержание конфигурации Envoy в актуальном состоянии, а также содержит в себе кэширующий DNS-сервер.
- В каждом поде настраивается DNAT входящих и исходящих прикладных запросов в sidecar-proxy. Делается это с помощью дополнительного init-контейнера. Таким образом, трафик будет перехватываться прозрачно для приложений.
- Так как входящий прикладной трафик перенаправляется в sidecar-proxy, readiness/liveness-трафика это тоже касается. Подсистема Kubernetes, которая за это отвечает, не рассчитана на формирование проб в формате Mutual TLS. Для адаптации все существующие пробы автоматически перенастраиваются на специальный порт в sidecar-proxy, который перенаправляет трафик на приложение в неизменном виде.
- Для приема запросов извне кластера необходимо использовать подготовленный Ingress-контроллер:
  - Поды контроллера аналогично имеют дополнительный контейнер sidecar-proxy.
  - В отличие от подов приложения, sidecar-proxy Ingress-контроллера перехватывает только трафик от контроллера к сервисам. Входящий трафик от пользователей обрабатывает непосредственно сам контроллер.
- Ресурсы типа Ingress требуют минимальной доработки в виде добавления аннотаций:
  - `nginx.ingress.kubernetes.io/service-upstream: "true"` — Ingress-контроллер в качестве upstream будет использовать ClusterIP сервиса вместо адресов подов. Балансировкой трафика между подами теперь занимается sidecar-proxy. Используйте эту опцию, только если у вашего сервиса есть ClusterIP.
  - `nginx.ingress.kubernetes.io/upstream-vhost: "myservice.myns.svc"` — sidecar-proxy Ingress-контроллера принимает решения о маршрутизации на основе заголовка Host. Без данной аннотации контроллер оставит заголовок с адресом сайта, например `Host: example.com`.
- Ресурсы типа Service не требуют адаптации и продолжают выполнять свою функцию. Приложениям все так же доступны адреса сервисов вида servicename, servicename.myns.svc и пр.
- DNS-запросы изнутри подов прозрачно перенаправляются на обработку в sidecar-proxy:
  - Требуется для разыменования DNS-имен сервисов из соседних кластеров.

##### Жизненный цикл пользовательского запроса

###### Приложение с выключенным Istio

<div data-presentation="presentations/request_lifecycle_istio_disabled_ru.pdf"></div>


###### Приложение с включенным Istio

<div data-presentation="presentations/request_lifecycle_istio_enabled_ru.pdf"></div>


#### Как активировать Istio для приложения

Основная цель активации — добавить sidecar-контейнер к подам приложения, после чего Istio сможет управлять трафиком.

Рекомендованный способ добавления sidecar-ов — использовать sidecar-injector. Istio умеет «подселять» к вашим подам sidecar-контейнер с помощью механизма Admission Webhook. Настраивается с помощью лейблов и аннотаций:

- Лейбл к `namespace` — обозначает ваше пространство имён для компонента sidecar-injector. После применения лейбла к новым подам будут добавлены sidecar-контейнеры:
  - `istio-injection=enabled` — использует глобальную версию Istio (`spec.settings.globalVersion` в `ModuleConfig`);
  - `istio.io/rev=v1x16` — использует конкретную версию Istio для этого пространства имён.
- Аннотация к **поду** `sidecar.istio.io/inject` (`"true"` или `"false"`) позволяет локально переопределить политику `sidecarInjectorPolicy`. Эти аннотации работают только в пространствах имён, обозначенных лейблами из списка выше.

Также существует возможность добавить sidecar к индивидуальному поду в пространстве имён без установленных лейблов `istio-injection=enabled` или `istio.io/rev=vXxYZ` путем установки лейбла `sidecar.istio.io/inject=true`.

**Важно!** Istio-proxy, который работает в качестве sidecar-контейнера, тоже потребляет ресурсы и добавляет накладные расходы:

- Каждый запрос DNAT'ится в Envoy, который обрабатывает это запрос и создает еще один. На принимающей стороне — аналогично.
- Каждый Envoy хранит информацию обо всех сервисах в кластере, что требует памяти. Больше кластер — больше памяти потребляет Envoy. Решение — CustomResource [Sidecar](istio-cr.html#sidecar).

Также важно подготовить Ingress-контроллер и Ingress-ресурсы приложения:

- Включите [`enableIstioSidecar`](./ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-enableistiosidecar) у ресурса IngressNginxController.
- Добавьте аннотации на Ingress-ресурсы приложения:
  - `nginx.ingress.kubernetes.io/service-upstream: "true"` — Ingress-контроллер в качестве upstream использует ClusterIP сервиса вместо адресов подов. Балансировкой трафика между подами теперь занимается sidecar-proxy. Используйте эту опцию, только если у вашего сервиса есть ClusterIP;
  - `nginx.ingress.kubernetes.io/upstream-vhost: "myservice.myns.svc"` — sidecar-proxy Ingress-контроллера принимает решения о маршрутизации на основе заголовка Host. Без этой аннотации контроллер оставит заголовок с адресом сайта, например `Host: example.com`.

#### Федерация и мультикластер

Поддерживаются две схемы межкластерного взаимодействия:

- [федерация](#федерация);
- [мультикластер](#мультикластер).

Принципиальные отличия:

- Федерация объединяет суверенные кластеры:
  - у каждого кластера собственное пространство имен (для namespace, Service и пр.);
  - доступ к отдельным сервисам между кластерами явно обозначен.
- Мультикластер объединяет созависимые кластеры:
  - пространство имен у кластеров общее — каждый сервис доступен для соседних кластеров так, словно он работает на локальном кластере (если это не запрещают правила авторизации).

##### Федерация

###### Требования к кластерам

- У каждого кластера должен быть уникальный домен в параметре [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration). Обратите внимание, что ни один из кластеров не должен иметь домен `cluster.local`, который является значением по умолчанию.

  > `cluster.local` — неизменяемый псевдоним для домена локального кластера.
  > Указание `cluster.local` как principals в AuthorizationPolicy всегда будет указывать на локальный кластер, даже если в mesh существует кластер, у которого [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain) явно определен как `cluster.local`.
- Подсети сервисов и подов в параметрах [`serviceSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-servicesubnetcidr) и [`podSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-podsubnetcidr) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration) должны быть уникальными для каждого кластера.

  > - При анализе HTTP и HTTPS запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно по заголовкам.
  > - А при анализе TCP-запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно только по IP-адресу назначения и номеру порта.
  >
  > Если IP-адреса сервисов или подов пересекутся между кластерами, то под маршрутизирующие, запрещающие или разрешающие правила istio могут попасть запросы других подов иных кластеров.
  > Пересечение подсетей сервисов и подов жестко запрещено в single-network режиме, и допустимо, но не рекомендуется в режиме multi-networks.
  >
  > - В режиме single-network поды разных кластеров могут взаимодействовать друг с другом напрямую.
  > - В режиме multi-networks поды разных кластеров могут взаимодействовать друг с другом только при использовании istio-gateway.

###### Общие принципы федерации

- Федерация требует установления взаимного доверия между кластерами. Соответственно, для установления федерации нужно в кластере A сделать кластер Б доверенным и аналогично в кластере Б сделать кластер А доверенным. Это достигается взаимным обменом корневыми сертификатами.
- Для прикладной эксплуатации федерации необходимо также обменяться информацией о публичных сервисах. Чтобы опубликовать сервис bar из кластера Б в кластере А, необходимо в кластере А создать ресурс ServiceEntry, который описывает публичный адрес ingress-gateway кластера Б.

<div data-presentation="presentations/federation_common_principles_ru.pdf"></div>


###### Включение федерации

При включении федерации (параметр модуля `istio.federation.enabled = true`) происходит следующее:

- В кластер добавляется сервис `ingressgateway`, чья задача — проксировать mTLS-трафик извне кластера на прикладные сервисы.
- В кластер добавляется сервис, который экспортирует метаданные кластера наружу:
  - корневой сертификат Istio (доступен без аутентификации);
  - список публичных сервисов в кластере (доступен только для аутентифицированных запросов из соседних кластеров);
  - список публичных адресов сервиса `ingressgateway` (доступен только для аутентифицированных запросов из соседних кластеров).

###### Управление федерацией

<div data-presentation="presentations/federation_istio_federation_ru.pdf"></div>


Для построения федерации необходимо сделать следующее:

- В каждом кластере создать набор ресурсов `IstioFederation`, которые описывают все остальные кластеры.
  - После успешного автосогласования между кластерами, в ресурсе `IstioFederation` заполнятся разделы `status.metadataCache.public` и `status.metadataCache.private` служебными данными, необходимыми для работы федерации.
- Каждый ресурс(`service`), который считается публичным в рамках федерации, пометить лейблом `federation.istio.deckhouse.io/public-service: ""`.
  - В кластерах из состава федерации, для каждого `service` создадутся соответствующие `ServiceEntry`, ведущие на `ingressgateway` оригинального кластера.

{% alert level="warning" %}
В разделе `.spec.ports` этих `service` у каждого порта должно быть заполнено поле `name`.
{% endalert %}

##### Мультикластер

###### Требования к кластерам

- Домены кластеров в параметре [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration) должны быть одинаковыми для всех членов мультикластера. По умолчанию значение параметра — `cluster.local`.
* Подсети сервисов и подов в параметрах [`serviceSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-servicesubnetcidr) и [`podSubnetCIDR`](/reference/api/cr.html#clusterconfiguration-podsubnetcidr) ресурса [ClusterConfiguration](/reference/api/cr.html#clusterconfiguration) должны быть уникальными для каждого кластера.

  > - При анализе HTTP и HTTPS запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно по заголовкам.
  > - А при анализе TCP запросов *(в терминологии istio)* идентифицировать их и принять решение о дальнейшей маршрутизации, запрещении или разрешении возможно только по IP-адресу назначения и номеру порта.
  >
  > Если IP адреса сервисов или подов пересекутся между кластерами, то под маршрутизирующие, запрещающие или разрешающие правила istio могут попасть запросы других подов иных кластеров.
  > Пересечение подсетей сервисов и подов жестко запрещено в single-network режиме, и не рекомендуется в режиме multiple-networks.
  >
  > - В режиме single-network поды разных кластеров могут взаимодействовать друг с другом напрямую.
  > - В режиме multiple-networks поды разных кластеров могут взаимодействовать друг с другом только при использовании istio-gateway.

###### Общие принципы

<div data-presentation="presentations/multicluster_common_principles_ru.pdf"></div>


- Мультикластер требует установления взаимного доверия между кластерами. Соответственно, для построения мультикластера нужно в кластере A сделать кластер Б доверенным и в кластере Б сделать кластер А доверенным. Технически это достигается взаимным обменом корневыми сертификатами.
- Для сбора информации о соседних сервисах Istio подключается напрямую к API-серверу соседнего кластера. Данный модуль Deckhouse берет на себя организацию соответствующего канала связи.

###### Включение мультикластера

При включении мультикластера (параметр модуля `istio.multicluster.enabled = true`) происходит следующее:

- В кластер добавляется прокси для публикации доступа к API-серверу посредством стандартного Ingress-ресурса:
  - Доступ через данный публичный адрес ограничен авторизацией на основе Bearer-токенов, подписанных доверенными ключами. Обмен доверенными публичными ключами происходит автоматически средствами Deckhouse при взаимной настройке мультикластера.
  - Сам прокси имеет read-only-доступ к ограниченному набору ресурсов.
- В кластер добавляется сервис, который экспортирует метаданные кластера наружу:
  - Корневой сертификат Istio (доступен без аутентификации).
  - Публичный адрес, через который доступен API-сервер (доступен только для аутентифицированных запросов из соседних кластеров).
  - Список публичных адресов сервиса `ingressgateway` (доступен только для аутентифицированных запросов из соседних кластеров).
  - Публичные ключи сервера для аутентификации запросов к API-серверу и закрытым метаданным (см. выше).

###### Управление мультикластером

<div data-presentation="presentations/multicluster_istio_multicluster_ru.pdf"></div>


Для сборки мультикластера необходимо в каждом кластере создать набор ресурсов `IstioMulticluster`, которые описывают все остальные кластеры.

#### Накладные расходы

Внедрение Istio повлечёт за собой дополнительные расходы ресурсов, как для **control-plane** (контроллер istiod), так и для **data-plane** (istio-сайдкары приложений).

##### control-plane

Контроллер istiod непрерывно наблюдает за конфигурацией кластера, компонует настройки для istio-сайдкаров data-plane и рассылает их по сети. Соответственно, чем больше приложений и их экземпляров, чем больше сервисов и чем чаще эта конфигурация меняется, тем больше требуется вычислительных ресурсов и больше нагрузка на сеть. При этом, поддерживается два подхода для снижения нагрузки на экземпляры контроллеров:

- горизонтальное масштабирование (настройка модуля [`controlPlane.replicasManagement`](configuration.html#parameters-controlplane-replicasmanagement)) — чем больше экземпляров контроллеров, тем меньше экземпляров istio-сайдкаров обслуживать каждому из них и тем меньше нагрузка на CPU и на сеть.
- сегментация data-plane с помощью ресурса [*Sidecar*](istio-cr.html#sidecar) (рекомендуемый подход) — чем меньше область видимости у отдельного istio-сайдкара, тем меньше требуется обновлять данных в data-plane и тем меньше нагрузка на CPU и на сеть.

Примерная оценка накладных расходов для экземпляра control-plane, который обслуживает 1000 сервисов и 2000 istio-сайдкаров — 1 vCPU и 1.5 GB RAM.

##### data-plane

На потребление ресурсов data-plane (istio-сайдкары) влияет множество факторов:

- количество соединений,
- интенсивность запросов,
- размер запросов и ответов,
- протокол (HTTP/TCP),
- количество ядер CPU,
- сложность конфигурации Service Mesh.

Примерная оценка накладных расходов для экземпляра istio-сайдкара — 0.5 vCPU на 1000 запросов/сек и 50 MB RAM.

istio-сайдкары также вносят задержку в сетевые запросы — примерно 2.5мс на запрос.

### Модуль istio: настройки

 
<!-- SCHEMA -->

#### Аутентификация

По умолчанию используется модуль [user-authn](./user-authn/). Также можно настроить аутентификацию через `externalAuthentication` (см. ниже).
Если эти варианты отключены, модуль включит basic auth со сгенерированным паролем.

Посмотреть сгенерированный пароль можно командой:

```shell
d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module values istio -o json | jq '.istio.internal.auth.password'
```

Чтобы сгенерировать новый пароль, нужно удалить Secret:

```shell
d8 k -n d8-istio delete secret/kiali-basic-auth
```

> **Внимание!** Параметр `auth.password` больше не поддерживается.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['istio'].config-values | format_module_configuration: moduleKebabName }}

### Модуль istio: Custom Resources
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}

### Модуль istio: Custom Resources (от istio.io)
{{ site.data.schemas.istio.crds.ingress-istio | format_crd: "istio" }}

### Модуль istio: примеры

#### Circuit Breaker

Для выявления проблемных эндпоинтов используются настройки `outlierDetection` в кастомном ресурсе [DestinationRule](istio-cr.html#destinationrule).

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: reviews-cb-policy
spec:
  host: reviews.prod.svc.cluster.local
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100 # Максимальное число коннектов в сторону host, суммарно для всех эндпоинтов.
      http:
        maxRequestsPerConnection: 10 # Каждые 10 запросов коннект будет пересоздаваться.
    outlierDetection:
      consecutive5xxErrors: 7 # Допустимо 7 ошибок (включая пятисотые, TCP-таймауты и HTTP-таймауты)
      interval: 5m            # в течение пяти минут,
      baseEjectionTime: 15m   # после которых эндпоинт будет исключен из балансировки на 15 минут.
```

А также для настройки HTTP-таймаутов используется ресурс [VirtualService](istio-cr.html#virtualservice). Эти таймауты также учитываются при подсчете статистики ошибок на эндпоинтах.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-productpage-rule
  namespace: myns
spec:
  hosts:
  - productpage
  http:
  - timeout: 5s
    route:
    - destination:
        host: productpage
```

#### Балансировка gRPC

**Важно!** Чтобы балансировка gRPC-сервисов заработала автоматически, присвойте name с префиксом или значением `grpc` для порта в соответствующем Service.

#### Locality Failover

> При необходимости ознакомьтесь с основной документацией.

Istio позволяет настроить приоритетный географический фейловер между эндпоинтами. Для определения зоны Istio использует лейблы узлов с соответствующей иерархией:

* `topology.istio.io/subzone`;
* `topology.kubernetes.io/zone`;
* `topology.kubernetes.io/region`.

Это полезно для межкластерного failover при использовании совместно с [мультикластером](#устройство-мультикластера-из-двух-кластеров-с-помощью-ресурса-istiomulticluster).

> **Важно!** Для включения Locality Failover используется ресурс DestinationRule, в котором также необходимо настроить `outlierDetection`.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: helloworld
spec:
  host: helloworld
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true # Включили LF.
    outlierDetection: # outlierDetection включить обязательно.
      consecutive5xxErrors: 1
      interval: 1s
      baseEjectionTime: 1m
```

#### Retry

С помощью ресурса [VirtualService](istio-cr.html#virtualservice) можно настроить Retry для запросов.

**Внимание!** По умолчанию при возникновении ошибок все запросы (включая POST-запросы) выполняются повторно до трех раз.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: ratings-route
spec:
  hosts:
  - ratings.prod.svc.cluster.local
  http:
  - route:
    - destination:
        host: ratings.prod.svc.cluster.local
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: gateway-error,connect-failure,refused-stream
```

#### Canary

**Важно!** Istio отвечает лишь за гибкую маршрутизацию запросов, которая опирается на спецзаголовки запросов (например, cookie) или просто на случайность. За настройку этой маршрутизации и «переключение» между канареечными версиями отвечает CI/CD-система.

Подразумевается, что в одном пространстве имён развёрнуты два Deployment с разными версиями приложения. У подов разных версий разные лейблы (`version: v1` и `version: v2`).

Требуется настроить два кастомных ресурса:
* [DestinationRule](istio-cr.html#destinationrule) с описанием, как идентифицировать разные версии вашего приложения (subset'ы);
* [VirtualService](istio-cr.html#virtualservice) с описанием, как распределять трафик между разными версиями приложения.

Пример:

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: productpage-canary
spec:
  host: productpage
  # subset'ы доступны только при обращении к хосту через VirtualService из пода под управлением Istio.
  # Эти subset'ы должны быть указаны в маршрутах.
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

##### Распределение по наличию cookie

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: productpage-canary
spec:
  hosts:
  - productpage
  http:
  - match:
    - headers:
       cookie:
         regex: "^(.*;?)?(canary=yes)(;.*)?"
    route:
    - destination:
        host: productpage
        subset: v2 # Ссылка на subset из DestinationRule.
  - route:
    - destination:
        host: productpage
        subset: v1
```

##### Распределение по вероятности

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: productpage-canary
spec:
  hosts:
  - productpage
  http:
  - route:
    - destination:
        host: productpage
        subset: v1 # Ссылка на subset из DestinationRule.
      weight: 90 # Процент трафика, который получат поды с лейблом version: v1.
  - route:
    - destination:
        host: productpage
        subset: v2
      weight: 10
```

#### Ingress для публикации приложений

##### Istio Ingress Gateway

Пример:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IngressIstioController
metadata:
 name: main
spec:
  # ingressGatewayClass содержит значение селектора меток, используемое при создании ресурса Gateway.
  ingressGatewayClass: istio-hp
  inlet: HostPort
  hostPort:
    httpPort: 80
    httpsPort: 443
  nodeSelector:
    node-role.deckhouse.io/frontend: ""
  tolerations:
    - effect: NoExecute
      key: dedicated.deckhouse.io
      operator: Equal
      value: frontend
  resourcesRequests:
    mode: VPA
```

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-tls-secert
  namespace: d8-ingress-istio # Обратите внимание, пространство имён не является app-ns.
type: kubernetes.io/tls
data:
  tls.crt: |
    <tls.crt data>
  tls.key: |
    <tls.key data>
```

```yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: gateway-app
  namespace: app-ns
spec:
  selector:
    # Селектор меток для использования Istio Ingress Gateway main-hp.
    istio.deckhouse.io/ingress-gateway-class: istio-hp
  servers:
    - port:
        # Стандартный шаблон для использования протокола HTTP.
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - app.example.com
    - port:
        # Стандартный шаблон для использования протокола HTTPS.
        number: 443
        name: https
        protocol: HTTPS
      tls:
        mode: SIMPLE
        # Ресурс Secret с сертификатом и ключом, который должен быть создан в пространстве имён d8-ingress-istio.
        credentialName: app-tls-secrets
      hosts:
        - app.example.com
```

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: vs-app
  namespace: app-ns
spec:
  gateways:
    - gateway-app
  hosts:
    - app.example.com
  http:
    - route:
        - destination:
            host: app-svc
```

##### NGINX Ingress

Для работы с NGINX Ingress требуется подготовить:
* Ingress-контроллер, добавив к нему sidecar от Istio. В нашем случае включить параметр `enableIstioSidecar` в кастомном ресурсе [IngressNginxController](/modules/ingress-nginx/cr.html#ingressnginxcontroller) модуля [ingress-nginx](/modules/ingress-nginx/).
* Ingress-ресурс, который ссылается на Service. Обязательные аннотации для Ingress-ресурса:
  * `nginx.ingress.kubernetes.io/service-upstream: "true"` — с этой аннотацией Ingress-контроллер будет отправлять запросы на ClusterIP сервиса (из диапазона Service CIDR) вместо того, чтобы слать их напрямую в поды приложения. Sidecar-контейнер `istio-proxy` перехватывает трафик только в сторону диапазона Service CIDR, остальные запросы отправляются напрямую;
  * `nginx.ingress.kubernetes.io/upstream-vhost: myservice.myns.svc` — с данной аннотацией sidecar сможет идентифицировать прикладной сервис, для которого предназначен запрос.

Примеры:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: productpage
  namespace: bookinfo
  annotations:
    # Просим nginx проксировать трафик на ClusterIP вместо собственных IP подов.
    nginx.ingress.kubernetes.io/service-upstream: "true"
    # В Istio вся маршрутизация осуществляется на основе `Host:` заголовка запросов.
    # Чтобы не сообщать Istio о существовании внешнего домена `productpage.example.com`,
    # мы просто используем внутренний домен, о котором Istio осведомлен.
    nginx.ingress.kubernetes.io/upstream-vhost: productpage.bookinfo.svc
spec:
  rules:
    - host: productpage.example.com
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: productpage
              port:
                number: 9080
```

```yaml
apiVersion: v1
kind: Service
metadata:
  name: productpage
  namespace: bookinfo
spec:
  ports:
  - name: http
    port: 9080
  selector:
    app: productpage
  type: ClusterIP
```

#### Примеры настройки авторизации

##### Алгоритм принятия решения

**Важно!** Как только для приложения создается `AuthorizationPolicy`, начинает работать следующий алгоритм принятия решения о судьбе запроса:
* Если запрос попадает под политику DENY — запретить запрос.
* Если для данного приложения нет политик ALLOW — разрешить запрос.
* Если запрос попадает под политику ALLOW — разрешить запрос.
* Все остальные запросы — запретить.

Иными словами, если вы явно что-то запретили, работает только ваш запрет. Если же вы что-то явно разрешили, теперь разрешены только явно одобренные запросы (запреты никуда не исчезают и имеют приоритет).

**Важно!** Для работы политик, основанных на высокоуровневых параметрах, таких как пространства имён или principal, необходимо, чтобы все вовлеченные сервисы работали под управлением Istio. Также между приложениями должен быть организован Mutual TLS.

Примеры:
* Запретим POST-запросы для приложения myapp. Отныне, так как для приложения появилась политика, согласно алгоритму выше будут запрещены только POST-запросы к приложению.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-post-requests
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    action: DENY
    rules:
    - to:
      - operation:
          methods: ["POST"]
  ```

* Здесь для приложения создана политика ALLOW. При ней будут разрешены только запросы из NS `bar`, остальные запрещены.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    action: ALLOW # default, можно не указывать.
    rules:
    - from:
      - source:
          namespaces: ["bar"]
  ```

* Здесь для приложения создана политика ALLOW. При этом она не имеет ни одного правила, и поэтому ни один запрос под нее не попадет, но она таки есть. Поэтому, согласно алгоритму, раз что-то разрешено, то все остальное запрещено. В данном случае все остальное — это вообще все запросы.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    action: ALLOW # default, можно не указывать.
    rules: []
  ```

* Здесь для приложения созданы политика ALLOW (это default) и одно пустое правило. Под это правило попадает любой запрос и автоматически получает добро.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: allow-all
    namespace: foo
  spec:
    selector:
      matchLabels:
        app: myapp
    rules:
    - {}
  ```

##### Запретить все действия в рамках пространства имён foo

Два способа:

* Запретить явно. Здесь мы создаем политику DENY с единственным универсальным фильтром `{}`, под который попадают все запросы:

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec:
    action: DENY
    rules:
    - {}
  ```

* Неявно. Здесь мы создаем политику ALLOW (по умолчанию), но не создаем ни одного фильтра, так что ни один запрос под нее не попадет и будет автоматически запрещен.

  ```yaml
  apiVersion: security.istio.io/v1beta1
  kind: AuthorizationPolicy
  metadata:
    name: deny-all
    namespace: foo
  spec: {}
  ```

##### Запретить доступ только из пространства имён foo

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: deny-from-ns-foo
 namespace: myns
spec:
 action: DENY
 rules:
 - from:
   - source:
       namespaces: ["foo"]
```

##### Разрешить запросы только в рамках нашего пространства имён foo

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-intra-namespace-only
 namespace: foo
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       namespaces: ["foo"]
```

##### Разрешить из любого места в нашем кластере

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-my-cluster
 namespace: myns
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       principals: ["mycluster.local/*"]
```

##### Разрешить любые запросы только кластеров foo или bar

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-foo-or-bar-clusters-to-ns-baz
 namespace: baz
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       principals: ["foo.local/*", "bar.local/*"]
```

##### Разрешить любые запросы только кластеров foo или bar, при этом из пространства имён baz

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-foo-or-bar-clusters-to-ns-baz
 namespace: baz
spec:
 action: ALLOW
 rules:
 - from:
   - source: # Правила ниже логически перемножаются.
       namespaces: ["baz"]
       principals: ["foo.local/*", "bar.local/*"]
```

##### Разрешить из любого кластера (по mTLS)

**Важно!** Если есть запрещающие правила, у них будет приоритет. Смотри [алгоритм](#алгоритм-принятия-решения).

Пример:

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-any-cluster-with-mtls
 namespace: myns
spec:
 action: ALLOW
 rules:
 - from:
   - source:
       principals: ["*"] # To set mTLS mandatory.
```

##### Разрешить вообще откуда угодно (в том числе без mTLS)

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: allow-all-from-any
 namespace: myns
spec:
 action: ALLOW
 rules: [{}]
```

#### Устройство федерации из двух кластеров с помощью кастомного ресурса IstioFederation

Cluster A:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioFederation
metadata:
  name: cluster-b
spec:
  metadataEndpoint: https://istio.k8s-b.example.com/metadata/
  trustDomain: cluster-b.local
```

Cluster B:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioFederation
metadata:
  name: cluster-a
spec:
  metadataEndpoint: https://istio.k8s-a.example.com/metadata/
  trustDomain: cluster-a.local
```

#### Устройство мультикластера из двух кластеров с помощью ресурса IstioMulticluster

Cluster A:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioMulticluster
metadata:
  name: cluster-b
spec:
  metadataEndpoint: https://istio.k8s-b.example.com/metadata/
```

Cluster B:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: IstioMulticluster
metadata:
  name: cluster-a
spec:
  metadataEndpoint: https://istio.k8s-a.example.com/metadata/
```

#### Управление поведением data plane

##### Предотвратить завершение работы istio-proxy до завершения соединений основного приложения

По умолчанию в процессе остановки пода все контейнеры, включая istio-proxy, получают сигнал SIGTERM одновременно. Но некоторым приложениям для правильного завершения работы необходимо время и иногда дополнительная сетевая активность. Это невозможно, если istio-proxy завершился раньше.

Решение — добавить в istio-proxy preStop-хук для оценки активности прикладных контейнеров, а единственный доступный метод — это выявление сетевых сокетов приложения, и если таковых нет, тогда можно останавливать контейнер.

Аннотация ниже добавляет описанный выше preStop-хук в контейнер istio-proxy прикладного пода:

```yaml
annotations:
  inject.istio.io/templates: "sidecar,d8-hold-istio-proxy-termination-until-application-stops"
```

#### Ограничения режима перенаправления прикладного трафика `CNIPlugin`

В отличие от режима `InitContainer`, настройка перенаправления осуществляется в момент создании пода, а не в момент срабатывания init-контейнера `istio-init`. Это значит, что прикладные init-контейнеры не смогут взаимодействовать с остальными сервисами так как весь трафик будет перенаправлен на обработку в sidecar-контейнер `istio-proxy`, который ещё не запущен. Обходные пути:

* Запустить прикладной init-контейнер от пользователя с uid `1337`. Запросы данного пользователя не перехватываются под управление Istio.
* Исключить IP-адрес или порт сервиса из-под контроля Istio с помощью аннотаций `traffic.sidecar.istio.io/excludeOutboundIPRanges` или `traffic.sidecar.istio.io/excludeOutboundPorts`.

{% alert level="warning" %}Каждый из обходных вариантов выводит трафик из-под контроля Istio, что в свою очередь убирает шифрование трафика между прикладными сервисами.{% endalert %}

#### Обновление Istio

#### Обновление control plane Istio

* Deckhouse позволяет инсталлировать несколько версий control plane одновременно:
  * Одна глобальная, обслуживает пространства имён или поды без явного указания версии (лейбл у пространства имён `istio-injection: enabled`). Настраивается параметром [globalVersion](configuration.html#parameters-globalversion).
  * Остальные — дополнительные, обслуживают пространства имён или поды с явным указанием версии (лейбл у пространства имён или пода `istio.io/rev: v1x21`). Настраиваются параметром [additionalVersions](configuration.html#parameters-additionalversions).
* Istio заявляет обратную совместимость между data plane и control plane в диапазоне двух минорных версий:
* Алгоритм обновления (для примера, с версии `1.19` на версию `1.21`):
  * Добавить желаемую версию в параметр модуля [additionalVersions](configuration.html#parameters-additionalversions) (`additionalVersions: ["1.21"]`).
  * Дождаться появления соответствующего пода `istiod-v1x21-xxx-yyy` в пространства имён `d8-istio`.
  * Для каждого прикладного пространства имён, где включен istio:
    * поменять лейбл `istio-injection: enabled` на `istio.io/rev: v1x21`;
    * по очереди пересоздать поды в пространстве имён, параллельно контролируя работоспособность приложения.
  * Поменять настройку `globalVersion` на `1.21` и удалить `additionalVersions`.
  * Убедиться, что старый под `istiod` удалился.
  * Поменять лейблы прикладных пространств имён на `istio-injection: enabled`.

Чтобы найти все поды под управлением старой ревизии Istio (в примере — версия 19), выполните команду:

```shell
d8 k get pods -A -o json | jq --arg revision "v1x19" \
  '.items[] | select(.metadata.annotations."sidecar.istio.io/status" // "{}" | fromjson |
   .revision == $revision) | .metadata.namespace + "/" + .metadata.name'
```

{% alert level="warning" %}Обновление до версии Istio 1.25 возможно только с версии 1.21.{% endalert %}

##### Автоматическое обновление data plane Istio

Для автоматизации обновления istio-sidecar'ов установите лейбл `istio.deckhouse.io/auto-upgrade="true"` на `Namespace` либо на отдельный ресурс — `Deployment`, `DaemonSet` или `StatefulSet`.

#### Настройка ресурсов istio-proxy sidecar

Для переопределения глобальных ограничений ресурсов для istio-proxy sidecar в отдельных рабочих нагрузках через аннотации, поддерживаются следующие аннотации:

##### Поддерживаемые аннотации

| Аннотация                          | Описание                     | Пример значения |
|-------------------------------------|-----------------------------|---------------|
| `sidecar.istio.io/proxyCPU`         | Запрос CPU для sidecar      | `200m`        |
| `sidecar.istio.io/proxyCPULimit`    | Лимит CPU для sidecar       | `"1"`         |
| `sidecar.istio.io/proxyMemory`      | Запрос памяти для sidecar   | `128Mi`       |
| `sidecar.istio.io/proxyMemoryLimit` | Лимит памяти для sidecar    | `512Mi`       |

##### Примеры конфигурации

Для Deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
### ...
spec:
  template:
    metadata:
      annotations:
          sidecar.istio.io/proxyCPU: 200m
          sidecar.istio.io/proxyCPULimit: "1"
          sidecar.istio.io/proxyMemory: 128Mi
          sidecar.istio.io/proxyMemoryLimit: 512Mi
### ... остальная часть манифеста
```

Для ReplicaSet:

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
### ...
spec:
  template:
    metadata:
      annotations:
          sidecar.istio.io/proxyCPU: 200m
          sidecar.istio.io/proxyCPULimit: "1"
          sidecar.istio.io/proxyMemory: 128Mi
          sidecar.istio.io/proxyMemoryLimit: 512Mi
### ... остальная часть манифеста
```

Для Pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    sidecar.istio.io/proxyCPU: 200m
    sidecar.istio.io/proxyCPULimit: "1"
    sidecar.istio.io/proxyMemory: 128Mi
    sidecar.istio.io/proxyMemoryLimit: 512Mi
### ... остальная часть манифеста
```

{% alert level="warning" %}Все четыре параметра должны быть указаны вместе — `sidecar.istio.io/proxyCPU`, `sidecar.istio.io/proxyCPULimit`, `sidecar.istio.io/proxyMemory` и `sidecar.istio.io/proxyMemoryLimit`. Частичная конфигурация не поддерживается.{% endalert %}

### Модуль kube-dns

Модуль устанавливает компоненты CoreDNS для управления DNS в кластере Kubernetes.

> **Внимание!** Модуль удаляет ранее установленные kubeadm'ом Deployment, ConfigMap и RBAC для CoreDNS.

### Модуль kube-dns: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['kube-dns'].config-values | format_module_configuration: moduleKebabName }}

### Модуль kube-dns: примеры

#### Пример конфигурации

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: kube-dns
spec:
  version: 1
  enabled: true
  settings:
    upstreamNameservers:
    - 8.8.8.8
    - 8.8.4.4
    hosts:
    - domain: one.example.com
      ip: 192.168.0.1
    - domain: two.another.example.com
      ip: 10.10.0.128
    stubZones:
    - zone: consul.local
      upstreamNameservers:
      - 10.150.0.1
    enableLogs: true
    clusterDomainAliases:
    - foo.bar
    - baz.qux
```

### Модуль kube-dns: FAQ

#### Как поменять домен кластера с минимальным простоем?

Добавьте новый домен и сохраните предыдущий. Для этого измените конфигурацию параметров:

1. В [controlPlaneManager.apiserver](./control-plane-manager/configuration.html):

   - [controlPlaneManager.apiserver.certSANs](./control-plane-manager/configuration.html#parameters-apiserver-certsans),
   - [apiserver.serviceAccount.additionalAPIAudiences](./control-plane-manager/configuration.html#parameters-apiserver-serviceaccount-additionalapiaudiences),
   - [apiserver.serviceAccount.additionalAPIIssuers](./control-plane-manager/configuration.html#parameters-apiserver-serviceaccount-additionalapiissuers).

   Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: control-plane-manager
   spec:
     version: 1
     enabled: true
     settings:
       apiserver:
         certSANs:
          - kubernetes.default.svc.<старый clusterDomain>
          - kubernetes.default.svc.<новый clusterDomain>
         serviceAccount:
           additionalAPIAudiences:
           - https://kubernetes.default.svc.<старый clusterDomain>
           - https://kubernetes.default.svc.<новый clusterDomain>
           additionalAPIIssuers:
           - https://kubernetes.default.svc.<старый clusterDomain>
           - https://kubernetes.default.svc.<новый clusterDomain>
   ```

1. В [kubeDns.clusterDomainAliases](configuration.html#параметры):

   Пример:

   ```yaml
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: kube-dns
   spec:
     version: 1
     enabled: true
     settings:
       clusterDomainAliases:
         - <старый clusterDomain>
         - <новый clusterDomain>
   ```

1. Дождитесь перезапуска `kube-apiserver`:

   ```bash
   d8 k -n kube-system get pods -l component=kube-apiserver
   ```

1. Поменяйте `clusterDomain` на новый. Для этого выполните команду:

   ```bash
   d8 system edit cluster-configuration
   ```

1. Перезапустите поды deckhouse:

   ```bash
   d8 k -n d8-system rollout restart deployment deckhouse
   ```

{% alert level="warning" %}

**Важно.** В Kubernetes, контроллеры используют расширенные токены для ServiceAccount для работы с API-server. Это означает, что каждый такой токен содержит дополнительные поля `iss:` и `aud:`, которые включают в себя старый `clusterDomain` (например, `"iss": "https://kubernetes.default.svc.cluster.local"`).
При смене `clusterDomain` API-server начнет выдавать токены с новым `service-account-issuer`, но благодаря произведенной конфигурации `additionalAPIAudiences` и `additionalAPIIssuers` по-прежнему будет принимать старые токены. По истечении 48 минут (80% от 3607 секунд) Kubernetes начнет обновлять выпущенные токены, при обновлении будет использован новый `service-account-issuer`. Через 90 минут (3607 секунд плюс дополнительный буфер) после перезагрузки kube-apiserver можете удалить конфигурацию `serviceAccount` из конфигурации `control-plane-manager`.

{% endalert %}

{% alert level="warning" %}

**Важно.** Если необходимо убрать старый домен из `clusterDomainAliases` в конфигурации kube-dns, необходимо пересоздать все поды в кластере, чтобы они запустились с новым search domain в `/etc/resolv.conf`. Это приведет к недоступности сервисов кластера, пока поды не перезапустятся.

```bash
d8 k delete pods --all-namespaces --all
```

{% endalert %}

{% alert level="warning" %}

**Важно.** Если вы используете модуль [istio](/modules/istio/), после смены `clusterDomain` обязательно потребуется рестарт всех прикладных подов под управлением Istio.

{% endalert %}

#### Как увеличить количество подов kube-dns?

Deckhouse распределяет поды kube-dns по следующему принципу: выполняется поиск узлов с метками `node-role.deckhouse.io/` и `node-role.kubernetes.io/`, затем применяются следующие правила:

* Если в кластере есть узлы с ролью `kube-dns`, количество реплик вычисляется как сумма таких узлов и master-узлов, но не больше чем количество master-узлов + 2.
* Если узлы kube-dns отсутствуют, производится поиск узлов с ролью `system`, и тогда количество реплик определяется как сумма system-узлов и master-узлов, но не больше чем количество master-узлов + 2.
* Если в кластере присутствуют только мастер-узлы, количество реплик kube-dns будет равно числу мастеров.
## Подсистема Хранение данных

### Модуль snapshot-controller

Этот модуль включает поддержку снапшотов для совместимых CSI-драйверов в кластере Kubernetes.

CSI-драйверы в Deckhouse, которые поддерживают снапшоты:
- [sds-local-volume](/modules/sds-local-volume/)
- [csi-ceph](/modules/csi-ceph/)
- [csi-nfs](/modules/csi-nfs/)

### Модуль snapshot-controller: настройки

> Модуль работает только в кластерах Kubernetes, начиная с версии 1.20.

В общем случае конфигурация модуля не требуется.

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['snapshot-controller'].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-ceph

{% alert level="warning" %}
При переключении на данный модуль с модуля ceph-csi производится автоматическая миграция, но ее запуск требует подготовки:
1. Необходимо сделать scale всех операторов (redis, clickhouse, kafka и т.д) в ноль реплик, в момент миграции операторы в кластере работать не должны. Единственное исключение - оператор prometheus в составе Deckhouse, в процессе миграции его отключит автоматически
2. Выключить модуль ceph-csi и включить модуль csi-ceph
3. В логах Deckhouse дождаться окончания процесса миграции (Finished migration from Ceph CSI module)
4. Создать тестовые pod/pvc для проверки работоспособности CSI
5. Вернуть операторы в работоспособное состояние
При наличии Ceph StorageClass, созданного не с помощью ресурса CephCSIDriver потребуется ручная миграция.
В этом случае необходимо связаться с техподдержкой.
{% endalert %}

{% alert level="info" %}
Для работы с снапшотами требуется подключенный модуль [snapshot-controller](./snapshot-controller/).
{% endalert %}

Ceph — это масштабируемая распределённая система хранения, обеспечивающая высокую доступность и отказоустойчивость данных. В Deckhouse поддерживается интеграция с Ceph-кластерами, что позволяет динамически управлять хранилищем и использовать StorageClass на основе RBD (RADOS Block Device) или CephFS.

На этой странице представлены инструкции по подключению Ceph в Deckhouse, настройке аутентификации, созданию объектов StorageClass, а также проверке работоспособности хранилища.

#### Включение модуля

Для подключения Ceph-кластера в Deckhouse необходимо включить модуль `csi-ceph`. Для этого примените ресурс ModuleConfig:

```yaml
d8 k apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-ceph
spec:
  enabled: true
EOF
```

#### Подключение к Ceph-кластеру

Чтобы настроить подключение к Ceph-кластеру, необходимо применить ресурс [CephClusterConnection](cr.html#cephclusterconnection). Пример команды:

```yaml
d8 k apply -f - <<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephClusterConnection
metadata:
  name: ceph-cluster-1
spec:
  # FSID/UUID Ceph-кластера.
  # Получить FSID/UUID Ceph-кластера можно с помощью команды `ceph fsid`.
  clusterID: 2bf085fc-5119-404f-bb19-820ca6a1b07e
  # Список IP-адресов ceph-mon’ов в формате 10.0.0.10:6789.
  monitors:
    - 10.0.0.10:6789
  # Имя пользователя без `client.`.
  # Получить имя пользователя можно с помощью команды `ceph auth list`.
  userID: admin
  # Ключ авторизации, соответствующий userID.
  # Получить ключ авторизации можно с помощью команды `ceph auth get-key client.admin`.
  userKey: AQDiVXVmBJVRLxAAg65PhODrtwbwSWrjJwssUg==
EOF
```

Проверить создание подключения можно командой (фаза должна быть в статусе `Created`):

```shell
d8 k get cephclusterconnection ceph-cluster-1
```

#### Создание StorageClass

Создание объектов StorageClass осуществляется через ресурс [CephStorageClass](cr.html#cephstorageclass), который определяет конфигурацию для желаемого класса хранения. Ручное создание ресурса StorageClass без [CephStorageClass](cr.html#cephstorageclass) может привести к ошибкам. Пример создания StorageClass на основе RBD (RADOS Block Device):

```yaml
d8 k apply -f - <<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-rbd-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: RBD
  rbd:
    defaultFSType: ext4
    pool: ceph-rbd-pool
EOF
```

Пример создания StorageClass на основе файловой системы Ceph:

```yaml
d8 k apply -f - <<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-fs-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: CephFS
  cephFS:
    fsName: cephfs
EOF
```

Проверьте, что созданные ресурсы [CephStorageClass](cr.html#cephstorageclass) перешли в состояние `Created`, выполнив следующую команду:

```shell
d8 k get cephstorageclass
```

В результате будет выведена информация о созданных ресурсах [CephStorageClass](cr.html#cephstorageclass):

```console
NAME          PHASE     AGE
ceph-rbd-sc   Created   1h
ceph-fs-sc    Created   1h
```

Проверьте созданный StorageClass с помощью следующей команды:

```shell
d8 k get sc
```

В результате будет выведена информация о созданном StorageClass:

```console
NAME          PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ceph-rbd-sc   rbd.csi.ceph.com   Delete          WaitForFirstConsumer   true                   15s
ceph-fs-sc    rbd.csi.ceph.com   Delete          WaitForFirstConsumer   true                   15s
```

Если объекты StorageClass появились, значит настройка модуля `csi-ceph` завершена. Теперь пользователи могут создавать PersistentVolume, указывая созданные объекты StorageClass.

### Модуль csi-ceph: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['csi-ceph'].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-ceph: custom resources
{{ site.data.schemas.csi-ceph.crds.cephclusterauthentication | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephclusterconnection | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephmetadatabackup | format_crd: "csi-ceph" }}
{{ site.data.schemas.csi-ceph.crds.cephstorageclass | format_crd: "csi-ceph" }}

### Модуль csi-ceph: примеры

#### Пример описания `CephClusterConnection`

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephClusterConnection
metadata:
  name: ceph-cluster-1
spec:
  clusterID: 0324bfe8-c36a-4829-bacd-9e28b6480de9
  monitors:
  - 172.20.1.28:6789
  - 172.20.1.34:6789
  - 172.20.1.37:6789
  userID: admin
  userKey: AQDiVXVmBJVRLxAAg65PhODrtwbwSWrjJwssUg==
```

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get cephclusterconnection <имя cephclusterconnection>
```

#### Пример описания `CephStorageClass`

##### RBD

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-rbd-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: RBD
  rbd:
    defaultFSType: ext4
    pool: ceph-rbd-pool  
```

##### CephFS

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: CephStorageClass
metadata:
  name: ceph-fs-sc
spec:
  clusterConnectionName: ceph-cluster-1
  reclaimPolicy: Delete
  type: CephFS
  cephFS:
    fsName: cephfs
```

##### Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get cephstorageclass <имя storage class>
```

### Модуль csi-ceph: FAQ

#### Как получить список томов RBD, разделенный по узлам?

```shell
kubectl -n d8-csi-ceph get po -l app=csi-node-rbd -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName --no-headers \
  | awk '{print "echo "$2"; kubectl -n d8-csi-ceph exec  "$1" -c node -- rbd showmapped"}' | bash
```

#### Какие версии Ceph кластеров поддерживаются

Официально сейчас поддерживаются версии >= 16.2.0. Из нашей практики текущая версия способна работать с кластерами версий >=14.2.0, но мы рекомендуем обновить версию Ceph.

#### Какие режимы работы томов поддерживаются

RBD поддерживает только ReadWriteOnce (RWO, доступ к тому в рамках одной ноды). CephFS поддерживает как ReadWriteOnce, так и ReadWriteMany (RWX, одновременный доступ к тому с нескольких нод)

### Модуль csi-nfs

Модуль предоставляет CSI для управления NFS-томами и позволяет создавать StorageClass в Kubernetes через [пользовательские ресурсы Kubernetes](./cr.html#nfsstorageclass) `NFSStorageClass`.

{% alert level="warning" %}
**Предостережение про использовании снапшотов (Volume Snapshots)**

При создании снапшотов NFS-томов важно понимать схему их создания и связанные ограничения. Мы рекомендуем по возможности избегать использования snapshots в csi-nfs:

1. CSI-драйвер создает снапшот на уровне NFS-сервера.
2. Для этого используется tar, которой упаковывается содержимое тома, со всеми ограничениями, могущими возникнуть из-за этого
3. **Перед созданием снапшота обязательно остановите рабочую нагрузку** (pods), использующую NFS-том
4. NFS не обеспечивает атомарность операций на уровне файловой системы при создании снапшота

{% endalert %}

{% alert level="info" %}
Для работы с снапшотами требуется подключенный модуль [snapshot-controller](/modules/snapshot-controller/).
{% endalert %}

{% alert level="info" %}
Создание StorageClass для CSI-драйвера `nfs.csi.k8s.io` пользователем запрещено.
{% endalert %}

#### Системные требования и рекомендации

##### Требования

- Используйте стоковые ядра, поставляемые вместе с поддерживаемыми дистрибутивами;
- Убедитесь в наличии развернутого и настроенного NFS-сервера;
- Для поддержки RPC-with-TLS включите в ядре Linux опции `CONFIG_TLS` и `CONFIG_NET_HANDSHAKE`.

##### Рекомендации

Чтобы поды модуля перезапускались при изменении параметра `tlsParameters` в настройках модуля, должен быть включен модуль [pod-reloader](/modules/pod-reloader) (включен по умолчанию).

#### Ограничения режима RPC-with-TLS

- Для политики безопасности `mtls` поддерживается только один сертификат клиента.
- Один NFS-сервер не может одновременно работать в разных режимах безопасности: `tls`, `mtls` и стандартный режим (без TLS).
- На узлах кластера не должен быть запущен демон `tlshd`, иначе он будет конфликтовать с демоном нашего модуля. Для предотвращения конфликтов при включении TLS на узлах автоматически останавливается сторонний `tlshd` и отключается его автозапуск.

#### Быстрый старт

Все команды следует выполнять на машине, имеющей доступ к API Kubernetes с правами администратора.

##### Включение модуля

1. Включите модуль `csi-nfs`.  Это приведет к тому, что на всех узлах кластера будет:
   - Зарегистрирован CSI драйвер;
   - Запущены служебные поды компонентов `csi-nfs`.

   ```yaml
   kubectl apply -f - <<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: csi-nfs
   spec:
     enabled: true
     version: 1
   EOF
   ```

2. Дождитесь, когда модуль перейдет в состояние `Ready`:

   ```shell
   kubectl get module csi-nfs -w
   ```

##### Создание StorageClass

Для создания StorageClass необходимо использовать ресурс [NFSStorageClass](./cr.html#nfsstorageclass). Пример создания ресурса:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: NFSStorageClass
metadata:
  name: nfs-storage-class
spec:
  connection:
    host: 10.223.187.3
    share: /
    nfsVersion: "4.1"
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
EOF
```

Для каждого PV будет создаваться каталог `<директория из share>/<имя PV>`.

##### Проверка работоспособности модуля

Процесс проверки работоспособности модуля описан в разделе FAQ [Как проверить работоспособность модуля](./faq.html#как-проверить-работоспособность-модуля)

##### Выбор метода очистки тома перед удалением PV

На удаляемом томе могут остаться файлы с пользовательскими данными. Эти файлы будут удалены и не будут доступны другим пользователям через NFS.

Однако данные удаленных файлов могут оказаться доступными другим клиентам, если сервер предоставит доступ к своему хранилищу на уровне блочных устройств.

Выбрать метод очистки тома перед удалением поможет параметр `volumeCleanup`.

> **Внимание.** Эта опция не влияет на файлы, уже удаленные клиентским приложением.

> **Внимание.** Эта опция влияет только на команды отправляемые по протоколу NFS. Проведение этих команд на стороне сервера определено:
>
> - сервисом NFS сервера;
> - файловой системой;
> - уровнем блочных устройств и их виртуализации (например LVM);
> - самими физическими устройствами.
>
> Убедитесь в доверенности сервера. Не отправляйте деликатные данные на сервера, в которых нет уверенности.

###### Метод `SinglePass`

Используется, если для параметра `volumeCleanup` задано значение `RandomFillSinglePass`.

Содержимое файлов переписывается случайной последовательностью перед удалением. Случайная последовательность передается по сети.

###### Метод `ThreePass`

Используется, если для параметра `volumeCleanup` задано значение `RandomFillThreePass`.

Содержимое файлов трижды переписывается случайной последовательностью перед удалением. Три случайных последовательности передаются по сети. 
<!-- Имеет смысл только если сервер хранит данные на жестком диске, и есть риск, что у злоумышленника появится физический доступ к устройству. -->

###### Метод `Discard`

Используется, если для параметра `volumeCleanup` задано значение `Discard`.

Многие файловые системы реализуют поддержку твердотельных накопителей, позволяя освободить место, занятое файлом, на блочном уровне без записи новых данных для увеличения срока службы твердотельного накопителя. Однако не все накопители гарантируют недоступность данных освобожденных блоков.

Если для `volumeCleanup` установлено значение `Discard`, содержимое файлов помечается как свободное через системный вызов `falloc` с флагом `FALLOC_FL_PUNCH_HOLE`. Файловая система освободит полностью используемые файлом блоки, через вызов `blkdiscard`, а остальное место будет перезаписано нулями.

Преимущества этого метода:

- объем трафика не зависит от размера файлов, а только от их количества;
- метод может обеспечить недоступность старых данных при некоторых конфигурациях сервера;
- работает как для жестких дисков, так и для твердотельных накопителей;
- позволяет увеличить время жизни твердотельного накопителя.

<!-- TODO: Может разделим на две или три (PunchHole, ZeroOut, PunchHoleOrZeroOut)? -->

### Модуль csi-nfs: примеры

#### Конфигурация модуля с поддержкой RPC-with-TLS

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-nfs
spec:
  enabled: true
  version: 1
  settings:
    tlsParameters:
      ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZFVENDQXZtZ...
      mtls:
        clientCert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1J...
        clientKey: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUpRd0lCQ...
```

#### Создание StorageClass с поддержкой RPC-with-TLS

```yaml
apiVersion: storage.deckhouse.io/v1alpha1
kind: NFSStorageClass
metadata:
  name: nfs-storage-class
spec:
  connection:
    host: nfs-server-name.io
    share: /
    nfsVersion: "4.1"
    tls: true
    mtls: true
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
```

### Модуль csi-nfs: FAQ

#### Как проверить работоспособность модуля?

Для этого необходимо проверить состояние подов в пространстве имён `d8-csi-nfs`. Все поды должны быть в состоянии `Running` или `Completed`, и запущены на всех узлах. Проверить можно командой:

```shell
kubectl -n d8-csi-nfs get pod -owide -w
```

#### Возможно ли изменение параметров NFS-сервера уже созданных PV?

Нет, данные для подключения к NFS-серверу сохраняются непосредственно в манифесте PV, и не подлежат изменению. Изменение StorageClass также не повлечет изменений настроек подключения в уже существующих PV.

#### Как делать снимки томов (снапшоты)?

{% alert level="warning" %}
**Предостережение про использовании снапшотов (Volume Snapshots)**

При создании снапшотов NFS-томов важно понимать схему их создания и связанные ограничения. Мы рекомендуем по возможности избегать использования snapshots в csi-nfs:

1. CSI-драйвер создает снапшот на уровне NFS-сервера.
2. Для этого используется tar, которой упаковывается содержимое тома, со всеми ограничениями, могущими возникнуть из-за этого
3. **Перед созданием снапшота обязательно остановите рабочую нагрузку** (pods), использующую NFS-том
4. NFS не обеспечивает атомарность операций на уровне файловой системы при создании снапшота

{% endalert %}

В `csi-nfs` снимки создаются путем архивирования папки тома. Архив сохраняется в корне папки NFS-сервера, указанной в параметре `spec.connection.share`.

1. Включите `snapshot-controller`:

   ```yaml
   kubectl apply -f -<<EOF
   apiVersion: deckhouse.io/v1alpha1
   kind: ModuleConfig
   metadata:
     name: snapshot-controller
   spec:
     enabled: true
     version: 1
   EOF
   ```

1. Создайте снимки томов. Для этого выполните следующую команду, указав нужные параметры:

   ```yaml
   kubectl apply -f -<<EOF
   apiVersion: snapshot.storage.k8s.io/v1
   kind: VolumeSnapshot
   metadata:
     name: my-snapshot
     namespace: <имя namespace, в котором находится PVC>
   spec:
     volumeSnapshotClassName: csi-nfs-snapshot-class
     source:
       persistentVolumeClaimName: <имя PVC, для которого необходимо создать снимок>
   EOF
   ```

1. Проверьте состояние созданного снимка командой:

   ```shell
   kubectl get volumesnapshot
   ```

Эта команда покажет список всех снимков и их текущее состояние.

#### Почему не удаляются PV созданные в StorageClass с поддержкой RPC-with-TLS, а вместе с ними и каталоги `<имя PV>` на NFS сервере?

Если ресурс [NFSStorageClass](./cr.html#nfsstorageclass) был настроен с поддержкой RPC-with-TLS, может возникнуть ситуация, когда PV не удастся удалить.
Это происходит из-за удаления секрета (например, после удаления `NFSStorageClass`), который хранит параметры монтирования. В результате контроллер не может смонтировать NFS-папку для удаления папки `<имя PV>`.

#### Как в настройках ModuleConfig в параметре `tlsParameters.ca` разместить несколько CA?

- для двух CA
```shell
cat CA1.crt CA2.crt | base64 -w0
```

- для трех CA
```shell
cat CA1.crt CA2.crt CA3.crt | base64 -w0
```

- и т.д.

#### Какие требования к Linux дистрибутиву для разворачивания NFS-сервера с поддержкой RPC-with-TLS?

- Ядро должно быть собрано с включенными параметрами `CONFIG_TLS` и `CONFIG_NET_HANDSHAKE`;
- Пакет nfs-utils (в дистрибутивах основанных на Debian - nfs-common) должен быть >= 2.6.3.

### Модуль csi-scsi-generic

Модуль предоставляет CSI для управления томами c использованием СХД с подключением через SCSI.

На данный момент поддерживается:
  - обнаружение LUN через iSCSI
  - создание PV из заранее подготовленных LUN
  - удаление PV и обнуление данных на LUN
  - подключение LUN к узлам через iSCSI
  - создание multipath устройств и монтирование их в поды
  - отключение LUN от узлов

Не поддерживается:
  - создание LUN на СХД
  - изменение размера LUN
  - создание снимков

#### Системные требования и рекомендации

##### Требования

- Наличие развернутой и настроенной СХД с подключением через SCSI.
- Уникальные iqn в /etc/iscsi/initiatorname.iscsi на каждой из Kubernetes Nodes

#### Быстрый старт

Все команды следует выполнять на машине, имеющей доступ к API Kubernetes с правами администратора.

##### Включение модуля

- Включить модуль `csi-scsi-generic`.  Это приведет к тому, что на всех узлах кластера будет:
    - зарегистрирован CSI драйвер;
    - запущены служебные поды компонентов `csi-scsi-generic`.

```shell
kubectl apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-scsi-generic
spec:
  enabled: true
  version: 1
EOF
```

- Дождаться, когда модуль перейдет в состояние `Ready`.

```shell
kubectl get module csi-scsi-generic -w
```

##### Создание SCSITarget

Для создания SCSITarget необходимо использовать ресурс [SCSITarget](./cr.html#scsitarget). Пример команд для создания такого ресурса:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: SCSITarget
metadata:
  name: hpe-3par-1
spec:
  deviceTemplate:
    metadata:
      labels:
        my-key: some-label-value
  iSCSI:
    auth:
      login: ""
      password: ""
    iqn: iqn.2000-05.com.3pardata:xxxx1
    portals:
    - 192.168.1.1

---
apiVersion: storage.deckhouse.io/v1alpha1
kind: SCSITarget
metadata:
  name: hpe-3par-2
spec:
  deviceTemplate:
    metadata:
      labels:
        my-key: some-label-value
  iSCSI:
    auth:
      login: ""
      password: ""
    iqn: iqn.2000-05.com.3pardata:xxxx2
    portals:
    - 192.168.1.2
EOF

```

Обратите внимание, что в примере выше используются два SCSITarget. Таким образом можно создать несколько SCSITarget как для одного, так и для разных СХД. Это позволяет использовать multipath для повышения отказоустойчивости и производительности.

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get scsitargets.storage.deckhouse.io <имя scsitarget>
```

##### Создание StorageClass

Для создания StorageClass необходимо использовать ресурс [SCSIStorageClass](./cr.html#scsistorageclass). Пример команд для создания такого ресурса:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: SCSIStorageClass
metadata:
  name: scsi-all
spec:
  scsiDeviceSelector:
    matchLabels:
      my-key: some-label-value
  reclaimPolicy: Delete
EOF
```

Обратите внимание на `scsiDeviceSelector`. Этот параметр позволяет выбрать SCSITarget для создания PV по меткам. В примере выше выбираются все SCSITarget с меткой `my-key: some-label-value`. Эта метка будет выставлена на все девайсы, которые будут обнаружены в указанных SCSITarget.

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get scsistorageclasses.storage.deckhouse.io <имя scsistorageclass>
```

##### Отчистка PV 

Поскольку мы не управляем сервером **iSCSI Target** и повторно используем доступные тома из **iSCSI Target**, мы должны очистить их после удаления **PV**.
Изменение режима очистки **PV** происходит автоматически и зависит от поддержки **trim**.
Пошаговый процесс очистки:
1. Проверка поддержки **trim**. Поддержка **trim** проверяется чтением значения «discard_max_bytes» и `/sys/block/${device name}/queue/discard_max_bytes`. Если размер > 0, **trim** поддерживается.
2. Очистка:
    1. Если **trim** поддерживается, команда выглядит так: `blkdiscard ${device name}`
    2. Если **trim** не поддерживается, команда выглядит так: `blkdiscard -z ${device name}`, `-z` означает, что устройство будет обнулено (полностью заполнено нулями).

### Модуль csi-scsi-generic: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['csi-scsi-generic'].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-scsi-generic: custom resources
{{ site.data.schemas.csi-scsi-generic.crds.pendingresizerequest | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsidevice | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsistorageclass | format_crd: "csi-scsi-generic" }}
{{ site.data.schemas.csi-scsi-generic.crds.scsitarget | format_crd: "csi-scsi-generic" }}

### Модуль csi-scsi-generic: FAQ

#### Как проверить работоспособность модуля?

Для этого необходимо проверить состояние подов в namespace `d8-csi-scsi-generic`. Все поды должны быть в состоянии `Running` или `Completed` и запущены на всех узлах.

```shell
kubectl -n d8-csi-scsi-generic get pod -owide -w
```

### Модуль csi-yadro-tatlin-unified

Модуль предоставляет CSI для управления томами c использованием СХД TATLIN.UNIFIED. Модуль позволяет создавать `StorageClass` в `Kubernetes` через создание [пользовательских ресурсов Kubernetes](./cr.html#yadrotatlinunifiedstorageclass) `YadroTatlinUnifiedStorageClass`.

> **Внимание!** Создание `StorageClass` для CSI-драйвера `csi-tatlinunified.yadro.com` пользователем запрещено.

{% alert level="info" %}
Для работы с снапшотами требуется подключенный модуль [snapshot-controller](/modules/snapshot-controller/).
{% endalert %}

#### Системные требования и рекомендации

##### Требования

- Наличие развернутой и настроенной СХД TATLIN.
- Уникальные iqn в /etc/iscsi/initiatorname.iscsi на каждой из Kubernetes Nodes

#### Быстрый старт

Все команды следует выполнять на машине, имеющей доступ к API Kubernetes с правами администратора.

##### Включение модуля

- Включить модуль `csi-yadro-tatlin-unified`.  Это приведет к тому, что на всех узлах кластера будет:
    - зарегистрирован CSI драйвер;
    - запущены служебные поды компонентов `csi-yadro-tatlin-unified`.

```shell
kubectl apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-yadro-tatlin-unified
spec:
  enabled: true
  version: 1
EOF
```

- Дождаться, когда модуль перейдет в состояние `Ready`.

```shell
kubectl get module csi-yadro-tatlin-unified -w
```

##### Создание StorageClass

Для создания StorageClass необходимо использовать ресурсы [YadroTatlinUnifiedStorageClass](./cr.html#yadrotatlinunifiedstorageclass) и [YadroTatlinUnifiedStorageConnection](./cr.html#yadrotatlinunifiedstorageconnection). Пример команд для создания таких ресурсов:

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: YadroTatlinUnifiedStorageConnection
metadata:
  name: yad1
spec:
  controlPlane:
    address: "172.19.28.184"
    username: "admin"
    password: "cGFzc3dvcmQ=" # ДОЛЖЕН БЫТЬ ЗАКОДИРОВАН В BASE64
    ca: "base64encoded"
    skipCertificateValidation: true
  dataPlane:
    protocol: "iscsi"
    iscsi:
      volumeExportPort: "p50,p51,p60,p61"
EOF
```

```yaml
kubectl apply -f -<<EOF
apiVersion: storage.deckhouse.io/v1alpha1
kind: YadroTatlinUnifiedStorageClass
metadata:
  name: yad1
spec:
  fsType: "xfs"
  pool: "pool-hdd"
  storageConnectionName: "yad1"
  reclaimPolicy: Delete
EOF
```

- Проверить создание объекта можно командой (Phase должен быть `Created`):

```shell
kubectl get yadrotatlinunifiedstorageconnections.storage.deckhouse.io <имя yadrotatlinunifiedstorageconnection>
```

```shell
kubectl get yadrotatlinunifiedstorageclasses.storage.deckhouse.io <имя yadrotatlinunifiedstorageclass>
```

##### Проверка работоспособности модуля.

Проверить работоспособность модуля можно [так](./faq.html#как-проверить-работоспособность-модуля)

### Модуль csi-yadro-tatlin-unified: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['csi-yadro-tatlin-unified'].config-values | format_module_configuration: moduleKebabName }}

### Модуль csi-yadro-tatlin-unified: custom resources
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageclass | format_crd: "csi-yadro-tatlin-unified" }}
{{ site.data.schemas.csi-yadro-tatlin-unified.crds.yadrotatlinunifiedstorageconnection | format_crd: "csi-yadro-tatlin-unified" }}

### Модуль csi-yadro-tatlin-unified: FAQ

#### Как проверить работоспособность модуля?

Для этого необходимо проверить состояние подов в namespace `d8-csi-yadro-tatlin-unified`. Все поды должны быть в состоянии `Running` или `Completed` и запущены на всех узлах.

```shell
kubectl -n d8-csi-yadro-tatlin-unified get pod -owide -w
```
## Подсистема Инфраструктура

### Cloud provider — DVP

Взаимодействие с облачными ресурсами провайдера DVP осуществляется с помощью модуля `cloud-provider-dvp`. Он позволяет [модулю управления узлами `node-manager`](/modules/node-manager/) задействовать ресурсы DVP при создании узлов для [заданной группы узлов](/modules/node-manager/cr.html#nodegroup).

Основные возможности модуля `cloud-provider-dvp`:

- управление ресурсами DVP через модуль `cloud-controller-manager`;
- заказ дисков с использованием компонента `CSI storage`;
- интеграция с [модулем `node-manager`](/modules/node-manager/) для поддержки [DVPInstanceClass](cr.html#dvpinstanceclass) при описании [NodeGroup](/modules/node-manager/cr.html#nodegroup).

### Cloud provider — DVP: настройки

Модуль автоматически включается для всех облачных кластеров, развернутых в DVP.

{% include module-alerts.liquid %}

Модуль не имеет настроек.
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['cloud-provider-dvp'].config-values | format_module_configuration: moduleKebabName }}

### Cloud provider — DVP: Custom Resources
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhouseclusters | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachines | format_crd: "cloud-provider-dvp" }}
{{ site.data.schemas.cloud-provider-dvp.crds.external.deckhousemachinetemplates | format_crd: "cloud-provider-dvp" }}

### Модуль terraform-manager

Модуль предоставляет инструменты для работы с состоянием Terraform'а в кластере Kubernetes.

* Модуль состоит из двух частей:
  * `terraform-auto-converger` — проверяет состояние Terraform'а и применяет недеструктивные изменения;
  * `terraform-state-exporter` — проверяет состояние Terraform'а и экспортирует метрики кластера.

* Модуль включен по умолчанию, если в кластере есть Secret'ы:
  * `kube-system/d8-provider-cluster-configuration`;
  * `d8-system/d8-cluster-terraform-state`.

### Модуль terraform-manager: настройки

 
<!-- SCHEMA -->
#### {{ site.data.i18n.common['parameters'][page.lang] }}
{{ site.data.schemas['terraform-manager'].config-values | format_module_configuration: moduleKebabName }}
## Справка
### API
<div markdown=1>

#### Глобальные настройки
</div>

Глобальные настройки Deckhouse Platform Certified Security Edition позволяют вам настраивать параметры, которые используются по умолчанию всеми модулями и компонентами. Некоторые модули могут переопределять часть этих параметров (это можно узнать в разделе настройки соответствующего модуля в документации модуля).

Глобальные настройки Deckhouse хранятся в ModuleConfig `global`.

{% alert %}
В параметре [publicDomainTemplate](#parameters-modules-publicdomaintemplate) указывается шаблон DNS-имен, с учётом которого некоторые модули Deckhouse создают Ingress-ресурсы. Если параметр не указан, Ingress-ресурсы создаваться не будут.

Домен, указанный в шаблоне, не может совпадать или быть поддоменом домена, заданного в параметре [`clusterDomain`](/reference/api/cr.html#clusterconfiguration-clusterdomain). Мы не рекомендуем менять значение `clusterDomain` без особой необходимости.

Для корректной работы шаблона необходимо предварительно настроить службы DNS как в сетях, где будут располагаться узлы кластера, так и в сетях, из которых к служебным веб-интерфейсам платформы будут обращаться клиенты.

В случае, если шаблон совпадает с доменом сети узлов, используйте только А записи для назначения служебным веб-интерфейсам платформы адресов Frontend узлов. Например, для узлов заведена зона `company.my`, а шаблон имеет вид `%s.company.my`.
{% endalert %}

<div>
Пример ресурса `ModuleConfig/global`:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: global
spec:
  version: 2
  settings: # <-- Параметры модуля из раздела "Параметры" ниже.
    defaultClusterStorageClass: 'default-fast'
    modules:
      publicDomainTemplate: '%s.kube.company.my'
      resourcesRequests:
        controlPlane:
          cpu: 1000m
          memory: 500M
      placement:
        customTolerationKeys:
        - dedicated.example.com
      storageClass: 'default-fast'
```

{% include module-conversion.liquid %}

#### Параметры

{{ site.data.schemas.modules.global.config-values | format_module_configuration: "global" }}
<div markdown=1>

#### Custom Resources
</div>

{{ site.data.schemas.crds.cluster_configuration | format_cluster_configuration }}

{{ site.data.schemas.crds.deckhouse-release | format_crd: "global" }}

{{ site.data.schemas.crds.init_configuration | format_cluster_configuration }}

{{ site.data.schemas.crds.module | format_crd: "global" }}
{{ site.data.schemas.crds.module-config | format_crd: "global" }}
{{ site.data.schemas.crds.module-documentation | format_crd: "global" }}
{{ site.data.schemas.crds.module-pull-override | format_crd: "global" }}
{{ site.data.schemas.crds.module-release | format_crd: "global" }}
{{ site.data.schemas.crds.module-settings-definition | format_crd: "global" }}
{{ site.data.schemas.crds.module-source | format_crd: "global" }}
{{ site.data.schemas.crds.module-update-policy | format_crd: "global" }}

{{ site.data.schemas.crds.static_cluster_configuration | format_cluster_configuration }}

{{ site.data.schemas.crds.ssh_configuration | format_cluster_configuration }}
{{ site.data.schemas.crds.ssh_host_configuration | format_cluster_configuration }}
<div markdown=1>

### Сетевое взаимодействие компонентов платформы

Если на площадке, где работает Deckhouse Platform Certified Security Edition, есть требования для ограничения сетевого взаимодействия между серверами на уровне инфраструктуры, то необходимо соблюсти следующие условия:

* Включен режим туннелирования трафика между подами ([настройки](/modules/cni-cilium/configuration.html#parameters-tunnelmode) для CNI Cilium, [настройки](/modules/cni-flannel/configuration.html#parameters-podnetworkmode) для CNI Flannel).
* Разрешена передача трафика между [podSubnetCIDR](/reference/api/cr.html#clusterconfiguration), инкапсулированного внутри VXLAN (если выполняется инспектирование и фильтрация трафика внутри VXLAN-туннеля).
* В случае необходимости интеграции с внешними системами (например, LDAP, SMTP или прочие внешние API), с ними разрешено сетевое взаимодействие.
* Локальное сетевое взаимодействие полностью разрешено в рамках каждого отдельно взятого узла кластера.
* Разрешено взаимодействие между узлами по портам, приведенным в таблицах на текущей странице. Обратите внимание, что большинство портов входит в диапазон 4200-4299. При добавлении новых компонентов платформы им будут назначаться порты из этого диапазона (при наличии возможности).

{% offtopic title="Как проверить текущий порт VXLAN..." %}

```bash
d8 k -n d8-cni-cilium get cm cilium-config -o yaml | grep tunnel
```

Пример вывода команды:

```console
routing-mode: tunnel
tunnel-port: "4298"
tunnel-protocol: vxlan
```

{%- endofftopic %}

{% include network_security_setup.liquid %}
</div>
<div markdown=1>

### Параметры sysctl, настраиваемые платформой

Deckhouse автоматически настраивает и управляет рядом параметров работы ядра сервера, используя утилиту `sysctl`.
Заданные параметры повышают сетевую пропускную способность, предотвращают нехватку ресурсов
и оптимизируют управление памятью.

{% alert level="info" %}
При изменении этих параметров Deckhouse автоматически вернет их к значениям, перечисленным ниже.
{% endalert %}

<table>
  <thead>
    <tr>
      <th>Параметр</th>
      <th>Значение, которое устанавливает Deckhouse</th>
      <th>Описание</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>/sys/block/*/queue/nr_requests</code></td>
      <td><code>256</code></td>
      <td>Количество запросов в очереди для блочных устройств.</td>
    </tr>
    <tr>
      <td><code>/sys/block/*/queue/read_ahead_kb</code></td>
      <td><code>256</code></td>
      <td>Объем дополнительных данных, которые ядро считывает с диска для ускорения чтения в дальнейшем.</td>
    </tr>
    <tr>
      <td><code>/sys/kernel/mm/transparent_hugepage/enabled</code></td>
      <td><code>never</code></td>
      <td>Отключает Transparent HugePage.</td>
    </tr>
    <tr>
      <td><code>/sys/kernel/mm/transparent_hugepage/defrag</code></td>
      <td><code>never</code></td>
      <td>Отключает дефрагментацию Transparent HugePage.</td>
    </tr>
    <tr>
      <td><code>/sys/kernel/mm/transparent_hugepage/use_zero_page</code></td>
      <td><code>0</code></td>
      <td>Отключает использование нулевых huge-страниц.</td>
    </tr>
    <tr>
      <td><code>/sys/kernel/mm/transparent_hugepage/khugepaged/defrag</code></td>
      <td><code>0</code></td>
      <td>Отключает дефрагментацию через <code>khugepaged</code>.</td>
    </tr>
    <tr>
      <td><code>/proc/sys/net/ipv4/conf/*/rp_filter</code></td>
      <td><code>0</code></td>
      <td>Отключает «фильтрацию обратного пути» (reverse path filtering) для всех интерфейсов.</td>
    </tr>
    <tr>
      <td><code>fs.file-max</code></td>
      <td><code>1000000</code></td>
      <td>Максимальное количество открытых файлов.</td>
    </tr>
    <tr>
      <td><code>fs.inotify.max_user_instances</code></td>
      <td><code>5120</code></td>
      <td>Максимальное количество экземпляров inotify.</td>
    </tr>
    <tr>
      <td><code>fs.inotify.max_user_watches</code></td>
      <td><code>524288</code></td>
      <td>Максимальное количество файлов, отслеживаемых одним экземпляром inotify.</td>
    </tr>
    <tr>
      <td><code>fs.may_detach_mounts</code></td>
      <td><code>1</code></td>
      <td>Разрешает отмонтировать файловую систему в режиме lazy unmounting.</td>
    </tr>
    <tr>
      <td><code>kernel.numa_balancing</code></td>
      <td><code>0</code></td>
      <td>Запрещает автоматическую балансировку памяти с архитектурой NUMA.</td>
    </tr>
    <tr>
      <td><code>kernel.panic</code></td>
      <td><code>10 (0, если включен fencing)</code></td>
      <td>Время в секундах, после которого произойдет перезагрузка узла при возникновении фатальной ошибки kernel panic. По умолчанию устанавливается значение <code>10</code>. Если на узле включен режим <a href="/modules/node-manager/cr.html#nodegroup-v1-spec-fencing"><code>fencing</code></a>, устанавливается значение <code>0</code>, тем самым предотвращая перезагрузку узла.</td>
    </tr>
    <tr>
      <td><code>kernel.panic_on_oops</code></td>
      <td><code>1</code></td>
      <td>Разрешает системе активировать kernel panic при возникновении непредвиденной ошибки oops. Параметр необходим для корректной работы kubelet.</td>
    </tr>
    <tr>
      <td><code>kernel.pid_max</code></td>
      <td><code>2000000</code></td>
      <td>Максимальное количество ID процессов (PID), которое можно назначить в системе.</td>
    </tr>
    <tr>
      <td><code>net.bridge.bridge-nf-call-arptables</code></td>
      <td><code>1</code></td>
      <td>Разрешает фильтрацию трафика с помощью arptables. Параметр необходим для корректной работы kube-proxy.</td>
    </tr>
    <tr>
      <td><code>net.bridge.bridge-nf-call-ip6tables</code></td>
      <td><code>1</code></td>
      <td>Разрешает фильтрацию трафика с помощью ip6tables. Параметр необходим для корректной работы kube-proxy.</td>
    </tr>
    <tr>
      <td><code>net.bridge.bridge-nf-call-iptables</code></td>
      <td><code>1</code></td>
      <td>Разрешает фильтрацию трафика с помощью iptables. Параметр необходим для корректной работы kube-proxy.</td>
    </tr>
    <tr>
      <td><code>net.core.netdev_max_backlog</code></td>
      <td><code>5000</code></td>
      <td>Максимальное количество пакетов в очереди на обработку.</td>
    </tr>
    <tr>
      <td><code>net.core.rmem_max</code></td>
      <td><code>16777216</code></td>
      <td>Максимальный размер приемного буфера в байтах.</td>
    </tr>
    <tr>
      <td><code>net.core.somaxconn</code></td>
      <td><code>1000</code></td>
      <td>Максимальное количество соединений в очереди.</td>
    </tr>
    <tr>
      <td><code>net.core.wmem_max</code></td>
      <td><code>16777216</code></td>
      <td>Максимальный размер пересылочного буфера в байтах.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.conf.all.forwarding</code></td>
      <td><code>1</code></td>
      <td>Разрешает перенаправление IPv4-пакетов между сетевыми интерфейсами. Равнозначно параметру <code>net.ipv4.ip_forward</code>.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.ip_local_port_range</code></td>
      <td><code>"32768 61000"</code></td>
      <td>Диапазон портов для исходящих TCP- и UDP-соединений.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.neigh.default.gc_thresh1</code></td>
      <td><code>16384</code></td>
      <td>Нижний порог количества ARP-записей, после которого система начнет удалять старые записи.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.neigh.default.gc_thresh2</code></td>
      <td><code>28672</code></td>
      <td>Средний порог количества ARP-записей, после которого система запустит очистку памяти.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.neigh.default.gc_thresh3</code></td>
      <td><code>32768</code></td>
      <td>Предельно допустимое количество ARP-записей.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.tcp_max_syn_backlog</code></td>
      <td><code>8096</code></td>
      <td>Максимальное количество SYN-соединений в очереди.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.tcp_no_metrics_save</code></td>
      <td><code>1</code></td>
      <td>Запрещает сохранение TCP-метрик закрытых соединений и их повторное использование для новых соединений.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.tcp_rmem</code></td>
      <td><code>"4096 12582912 16777216"</code></td>
      <td>Размеры приемного буфера для входящих TCP-пакетов в байтах: <code>"<минимальный> <по умолчанию> <максимальный>"</code>.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.tcp_slow_start_after_idle</code></td>
      <td><code>0</code></td>
      <td>Запрещает использование окна перезагрузки (congestion window, CWND) и алгоритма медленного старта для TCP-соединений.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.tcp_tw_reuse</code></td>
      <td><code>1</code></td>
      <td>Разрешает повторное использование исходящих TCP-соединений в состоянии <code>TIME-WAIT</code>.</td>
    </tr>
    <tr>
      <td><code>net.ipv4.tcp_wmem</code></td>
      <td><code>"4096 12582912 16777216"</code></td>
      <td>Размеры пересылочного буфера для исходящих TCP-пакетов в байтах: <code>"<минимальный> <по умолчанию> <максимальный>"</code>.</td>
    </tr>
    <tr>
      <td><code>net.netfilter.nf_conntrack_max</code></td>
      <td><code><кол-во ядер * 131072> или 524288</code></td>
      <td>Максимальное количество отслеживаемых соединений в таблице conntrack. Рассчитывается по формуле: «количество выделенных CPU-ядер» * 131072, но не менее <code>524288</code>.</td>
    </tr>
    <tr>
      <td><code>net.nf_conntrack_max</code></td>
      <td><code><кол-во ядер * 131072> или 524288</code></td>
      <td>Максимальное количество отслеживаемых соединений в таблице conntrack для старых версий ядра. Рассчитывается по формуле: «количество выделенных CPU-ядер» * 131072, но не менее <code>524288</code>.</td>
    </tr>
    <tr>
      <td><code>vm.dirty_background_ratio</code></td>
      <td><code>5</code></td>
      <td>Доля системной памяти в процентах, которую допустимо занимать "грязными" страницами (dirty pages), прежде чем начнется запись на диск в асинхронном режиме.</td>
    </tr>
    <tr>
      <td><code>vm.dirty_expire_centisecs</code></td>
      <td><code>12000</code></td>
      <td>Продолжительность периода в сотых долях секунды, пока "грязная" страница (dirty page) может оставаться в системной памяти, после чего она должна быть записана на диск.</td>
    </tr>
    <tr>
      <td><code>vm.dirty_ratio</code></td>
      <td><code>80</code></td>
      <td>Доля системной памяти в процентах, которую допустимо занимать "грязными" страницами (dirty pages), прежде чем все процессы остановятся и будет выполнен сброс данных на диск.</td>
    </tr>
    <tr>
      <td><code>vm.min_free_kbytes</code></td>
      <td><code>131072</code></td>
      <td>Минимальный объем свободной памяти в килобайтах, который резервируется ядром для выполнения критических операций.</td>
    </tr>
    <tr>
      <td><code>vm.overcommit_memory</code></td>
      <td><code>1</code></td>
      <td>Разрешает избыточное выделение памяти (memory overcommitment).</td>
    </tr>
    <tr>
      <td><code>vm.swappiness</code></td>
      <td><code>0</code></td>
      <td>Запрещает использование файла подкачки.</td>
    </tr>
  </tbody>
</table>
</div>
<div markdown=1>

### Используемые директории

Если узлы кластера Kubernetes анализируются сканерами безопасности (антивирусными средствами), то может потребоваться их настройка для исключения ложноположительных срабатываний.

Deckhouse Platform Certified Security Edition использует следующие директории при работе ([скачать в csv](./deckhouse-directories.csv)):

{% include used_directories.liquid %}
</div>
## Консольные утилиты
<div markdown=1>

## Описание и установка Deckhouse CLI

Deckhouse CLI — это интерфейс командной строки для работы с кластерами от Deckhouse Platform Certified Security Edition. Начиная с релиза 1.59, интерфейс автоматически устанавливается на все узлы кластера. Утилиту можно также [установить](#как-установить-deckhouse-cli) на любую машину и использовать для работы с кластерами без Deckhouse Platform Certified Security Edition.

В командной строке к утилите можно обратиться как `d8`. Все команды сгруппированы по функциям:

* `d8 k` — команды, которые в кластерах Kubernetes выполняет `kubectl`.  
    Например, в кластере можно выполнить `kubectl get pods` как `d8 k get pods`.
* `d8 dk` — команды, отвечающие за доставку по аналогии с утилитой `werf`.  
    Например, вместо `werf plan --repo registry.deckhouse.io` можно выполнить `d8 d plan --repo registry.deckhouse.io`.

* `d8 mirror` — команды, которые позволяют скопировать образы дистрибутива Deckhouse Platform Certified Security Edition в частный container registry (ранее для этого использовалась утилита `dhctl mirror`).
  Например, можно выполнить `d8 mirror pull -l <LICENSE> <TAR-BUNDLE-PATH>` вместо `dhctl mirror --license <LICENSE> --images-bundle-path <TAR-BUNDLE-PATH>`.

  Флаг `--only-extra-images` позволяет загружать только дополнительные образы для модулей (например, базы данных уязвимостей) без загрузки основных образов модулей.

  Сценарии использования:

  - ручная загрузка образов в изолированный приватный registry.
  - Обновление дополнительных образов модулей (например, баз данных уязвимостей): `d8 mirror pull --include-module <module-name> --only-extra-images bundle.tar`

* `d8 v` — команды, отвечающие за работу с виртуальными машинами.  
    Например, команда `d8 virtualization console` подключает к консоли виртуальной машины.

    <div markdown="0">
    <details><summary>Больше команд для виртуализации...</summary>
    <ul>
    <li><code>d8 v console</code> подключает к консоли виртуальной машины.</li>
    <li><code>d8 v port-forward</code> перенаправляет локальные порты на виртуальную машину.</li>
    <li><code>d8 v scp</code> использует клиент SCP для работы с файлами на виртуальной машине.</li>
    <li><code>d8 v ssh</code> подключает к виртуальной машине по SSH.</li>
    <li><code>d8 v vnc</code> подключает к виртуальной машине по VNC.</li>
    </ul>
    </details>
    </div>

* `d8 backup` — команды для создания резервных копий ключевых компонентов кластера:

  * `etcd` — полная резервная копия ключевого хранилища etcd;
  * `cluster-config` — архив конфигурационных объектов;
  * `loki` — диагностическая выгрузка логов из встроенного Loki API (не предназначена для восстановления).

    Например:

    ```console
    d8 backup etcd ./etcd.snapshot
    d8 backup cluster-config ./cluster-config.tar
    d8 backup loki --days 1 > ./loki.log
    ```

    Список доступных флагов `d8 backup` можно получить через команду `d8 backup --help`.

#### Как установить Deckhouse CLI

{% include d8-cli-install/main.liquid %}
</div>
