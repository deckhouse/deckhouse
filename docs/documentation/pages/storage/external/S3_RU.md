---
title: "S3-хранилище"
permalink: ru/storage/admin/external/s3.html
lang: ru
---

Модуль `csi-s3` позволяет создавать `StorageClass` и `Secret` в `Kubernetes` с помощью пользовательских ресурсов [S3StorageClass](../../../reference/cr/s3storageclass). В качестве файловой системы используется [GeeseFS](https://github.com/yandex-cloud/geesefs), работающая через FUSE поверх S3.

## Системные требования

- Kubernetes версии 1.17+ с поддержкой привилегированных контейнеров.
- Настроенное S3-хранилище с доступными ключами.
- Достаточный объем памяти на узлах. `GeeseFS` использует кеш для работы с файлами, загружаемыми из S3. Размер кеша задается параметром `maxCacheSize` в `S3StorageClass`.  
  [Результаты стресс-теста](../images/s3/load-test-mem.jpg): 7 узлов, 600 подов и PVC, `maxCacheSize` = 500 МБ, каждый под записывает 300 МБ, читает их и завершает работу.

## Руководство по быстрому старту

Обратите внимание, что все команды должны выполняться на машине с административными правами в Kubernetes API.

Необходимые шаги:
- Включение модуля
- Создание `S3StorageClass`.

### Включение модуля

- Включите `csi-s3` модуль. В результате на узлах кластера произойдёт:
  - Регистрация CSI драйвера;
  - Запуск сервисных подов `csi-s3` и создание необходимых компонентов.

```yaml
d8 k apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-s3
spec:
  enabled: true
  version: 1
EOF
```

Дождитесь, пока модуль не перейдет в состояние `Ready`.

```shell
d8 k get module csi-s3 -w
```

### Создание StorageClass

Модуль настраивается с помощью манифеста [S3StorageClass](../../../reference/cr/s3storageclass). Пример конфигурации:

```yaml
apiVersion: csi.s3.storage.k8s.io/v1
kind: S3StorageClass
metadata:
  name: example-s3
spec:
  bucketName: example-bucket
  endpoint: https://s3.example.com
  region: us-east-1
  accessKey: <your-access-key>
  secretKey: <your-secret-key>
  maxCacheSize: 500
```

Если `bucketName` не указан, для каждого `PersistentVolume` будет создан отдельный bucket. Если `bucketName` задан, в нем будут создаваться папки под каждый `PersistentVolume`. Если такого bucket нет, он создастся автоматически.

### Проверка работоспособности модуля

Для проверки состояния модуля, необходимо проверить статус подов в пространстве имён `d8-csi-s3`. Для проверки воспользуйтесь командой:

```shell
d8 k -n d8-csi-s3 get pod -owide -w
```

Статус всех подов должен быть `Running` или `Completed`, поды должны быть запущены на каждой из нод.

## Известные ограничение GeeseFS

S3 не является полноценной файловой системой, поэтому существуют определенные ограничения. Совместимость с POSIX зависит от используемого монтирующего модуля и провайдера S3. Некоторые хранилища могут не гарантировать консистентность данных [детали](https://github.com/gaul/are-we-consistent-yet#observed-consistency).

Вы можете просмотреть таблицу совместимости с POSIX [здесь](https://github.com/yandex-cloud/geesefs#posix-compatibility-matrix).

Основные ограничения:

- Разрешения файловой системы, символические ссылки, пользовательские `mtimes` и специальные файлы (блочные/символьные устройства, именованные каналы, сокеты UNIX) не поддерживаются, поскольку стандарт S3 не возвращает метаданные пользователя в списках и не читает все эти метаданные в стандарте S3. Потребуется дополнительный запрос `HEAD` для каждого файла в листинге, что сделает листинги слишком медленными.
- Поддержка специальных файлов включена по умолчанию для `Яндекс S3` и отключена для других.
- Разрешения файловой системы отключены по умолчанию.
- Пользовательское время модификации также отключено по умолчанию: `ctime`, `atime` и `mtime` всегда одинаковы.
- Время изменения файла не может быть установлено пользователем (например, с помощью `cp --preserve`, `rsync -a` или `utimes(2)`)
- Не поддерживаются жесткие ссылки.
- Не поддерживается блокировка.
- Не поддерживаются «невидимые» удаленные файлы. Если приложение сохраняет открытый файловый дескриптор после удаления файла, оно получит ошибки `ENOENT` от операций FS.
- Ограничение размера файла по умолчанию составляет 1,03 ТБ и достигается путем разделения файла на 1000 частей по 5 МБ, 1000 частей по 25 МБ и 8000 частей по 125 МБ. Вы можете изменить размеры частей, но собственный лимит AWS в любом случае составляет 5 ТБ.

## Известные ошибки и проблемы

- Запрос размера тома в PVC никак не отражается на создаваемых bucket.
- `df -h` показывает размер смонтированного хранилища в 1 петабайт, `used` никак не меняется во время использования.
  - CSI не проверяет корректность ключей для доступа к хранилищу, статус пода будет `Running`, а `PersistentVolume` и `PersistentVolumeClaim` будут `Bound` даже если ключи неправильные. Попытка доступа к смонтированной директории в поде приведёт к перезапуску пода.
