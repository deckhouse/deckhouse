---
title: "S3-хранилище"
permalink: ru/storage/admin/external/s3.html
lang: ru
---

Данный модуль хранилища S3 позволяет создавать `StorageClass` и `Secret` в `Kubernetes` с помощью создания `пользовательских ресурсов Kubernetes` [S3StorageClass](./cr.html#s3storageclass).

## Системные требования

- Kubernetes версии 1.17+
- Kubernetes должен поддерживать привилегированные контейнеры
- Развернутое и настроенное S3-хранилище с доступными ключами.
- Достаточно памяти на узлах. GeeseFS использует кеш для загрузки/выгрузки файлов из S3-хранилища. Размер кеша определяется параметром `maxCacheSize` в `S3StorageClass`. Результаты стресс-теста в следующих условиях: 7 узлов, 600 подов и PVC, maxCacheSize 500 мегабайт, каждый под создает файл размером 300 МБ, считывает его и завершает работу: [ссылка](./images/load-test/load-test-mem.jpg).

## Руководство по быстрому старту

Обратите внимание, что все команды, приведенные в этом README должны выполняться на машине с административными правами в Kubernetes API.

Необходимые шаги:
- Включение модуля
- Создание S3StorageClass

### Включение модуля

- Включите `csi-s3` mмодуль. В результате этого произойдет следующее на нодах кластера:
  - Регистрация CSI драйвера;
  - Запуск сервисных подов `csi-s3` и создание необходимых компонентов.

```shell
kubectl apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-s3
spec:
  enabled: true
  version: 1
EOF
```

- Дождитесь пока модуль не перейдет в состояние `Ready`.

```shell
kubectl get module csi-s3 -w
```

### Создание StorageClass

Настройка модуля осуществляется через манифесты `S3StorageClass`. Пример такого манифеста:

```yaml
apiVersion: csi.s3.storage.k8s.io/v1
kind: S3StorageClass
metadata:
  name: example-s3
spec:
  bucketName: example-bucket
  endpoint: https://s3.example.com
  region: us-east-1
  accessKey: <your-access-key>
  secretKey: <your-secret-key>
  maxCacheSize: 500
```

Если значение `bucketName` пустое, то S3 bucket'ы будут создаваться автоматически для каждого PV. Если значение `bucketName` не пустое, то в указанном bucket будут создаваться директории для каждого PV. Если указаный bucket не существует, он будет создан.

### Проверка состояния модуля

Вы можете проверить состояние модуля, следуя инструкции [здесь](./faq.html#how-to-check-module-health)

## Известные ограничение GeeseFS

Поскольку S3 не является настоящей файловой системой, здесь следует учитывать некоторые ограничения. В зависимости от того, какой модуль монтирования вы используете, у вас будут разные уровни совместимости с POSIX. Кроме того, в зависимости от того, какое серверное хранилище S3 вы используете, не всегда есть гарантии согласованности, см. [здесь](https://github.com/gaul/are-we-consistent-yet#observed-consistency).

Вы можете просмотреть таблицу совместимости с POSIX [здесь](https://github.com/yandex-cloud/geesefs#posix-compatibility-matrix).

Фактические ограничения:

- Разрешения файловой системы, символические ссылки, пользовательские mtimes и специальные файлы (блочные/символьные устройства, именованные каналы, сокеты UNIX) не поддерживаются, поскольку стандарт S3 не возвращает метаданные пользователя в списках и не читает все эти метаданные в стандарте S3. потребуется дополнительный запрос HEAD для каждого файла в листинге, что сделает листинги слишком медленными.
- Поддержка специальных файлов включена по умолчанию для Яндекс S3 и отключена для других.
- Разрешения файловой системы отключены по умолчанию.
- Пользовательское время модификации также отключено по умолчанию: ctime, atime и mtime всегда одинаковы.
  время изменения файла не может быть установлено пользователем (например, с помощью cp --preserve, rsync -a или utimes(2))
- Не поддерживает жесткие ссылки
- Не поддерживает блокировку
- Не поддерживает «невидимые» удаленные файлы. Если приложение сохраняет открытый файловый дескриптор после удаления файла, оно получит ошибки ENOENT от операций FS.
- Ограничение размера файла по умолчанию составляет 1,03 ТБ и достигается путем разделения файла на 1000 частей по 5 МБ, 1000 частей по 25 МБ и 8000 частей по 125 МБ. Вы можете изменить размеры частей, но собственный лимит AWS в любом случае составляет 5 ТБ.

## Известные ошибки и проблемы

- Запрос размера тома в PVC никак не отражается на создаваемых bucket.
- `df -h` показывает размер смонтированного хранилища в 1 петабайт, `used` никак не меняется во время использования.
- CSI не проверяет корректность ключей для доступа к хранилищу, под в любом случае будет `Running`, PV,PVC будут `Bound` даже если ключи неправильные. Попытка доступа к смонтированной директории в поде приводит к перезапуску пода.

