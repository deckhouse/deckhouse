---
title: "Объектное хранилище на основе S3"
permalink: ru/storage/admin/external/s3.html
lang: ru
---

{% alert level="info" %}
Доступно в некоторых коммерческих редакциях:  **EE**

Подробнее см. в разделе [Условия и цены](../../../../../pricing/).
{% endalert %}

Deckhouse поддерживает работу с объектным хранилищем на основе S3, обеспечивая возможность использования его в Kubernetes для хранения данных в виде томов. В качестве файловой системы применяется [GeeseFS](https://github.com/yandex-cloud/geesefs), работающая через FUSE поверх S3, что позволяет монтировать S3-хранилища как стандартные файловые системы.

На этой странице представлены инструкции по настройке S3-хранилища в Deckhouse, включая подключение, создание StorageClass, а также проверку работоспособности системы.

## Системные требования

- Kubernetes версии 1.17+ с поддержкой привилегированных контейнеров.
- Настроенное S3-хранилище с доступными ключами.
- Достаточный объем памяти на узлах. `GeeseFS` использует кэш для работы с файлами, загружаемыми из S3. Размер кэша задается параметром `maxCacheSize` в [S3StorageClass](../../../reference/cr/s3storageclass/). Результаты стресс-теста: 7 узлов, 600 подов и PVC, `maxCacheSize` = 500 МБ, каждый под записывает 300 МБ, читает их и завершает работу.

![testresults](../../../images/storage/s3/load-test-mem.jpg)

## Настройка и конфигурация

Обратите внимание, что все команды должны выполняться на машине с административными правами в Kubernetes API.

Необходимые шаги:
- Включение модуля;
- Создание [S3StorageClass](../../../reference/cr/s3storageclass/).

### Включение модуля

Для поддержки работы с S3-хранилищем включите модуль `csi-s3`, который позволяет создавать StorageClass и Secret в Kubernetes с помощью пользовательских ресурсов [S3StorageClass](../../../reference/cr/s3storageclass/). После включения модуля на узлах кластера произойдёт:
- Регистрация CSI-драйвера;
- Запуск сервисных подов `csi-s3` и создание необходимых компонентов.

```yaml
d8 k apply -f - <<EOF
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: csi-s3
spec:
  enabled: true
  version: 1
EOF
```

Дождитесь, пока модуль не перейдет в состояние `Ready`.

```shell
d8 k get module csi-s3 -w
```

### Создание StorageClass

Модуль настраивается с помощью манифеста [S3StorageClass](../../../reference/cr/s3storageclass/). Пример конфигурации:

```yaml
apiVersion: csi.s3.storage.k8s.io/v1
kind: S3StorageClass
metadata:
  name: example-s3
spec:
  bucketName: example-bucket
  endpoint: https://s3.example.com
  region: us-east-1
  accessKey: <your-access-key>
  secretKey: <your-secret-key>
  maxCacheSize: 500
```

Если `bucketName` не указан, для каждого PersistentVolume будет создан отдельный bucket. Если `bucketName` задан, в нем будут создаваться папки под каждый PersistentVolume. Если такого bucket нет, он создастся автоматически.

### Проверка работоспособности модуля

Для проверки состояния модуля, необходимо проверить статус подов в пространстве имён `d8-csi-s3`. Для проверки воспользуйтесь командой:

```shell
d8 k -n d8-csi-s3 get pod -owide -w
```

Статус всех подов должен быть `Running` или `Completed`, поды должны быть запущены на каждом из узлов.

## Известные ограничение GeeseFS

S3 не является полноценной файловой системой, поэтому существуют определенные ограничения. Совместимость с POSIX зависит от используемого монтирующего модуля и провайдера S3. Некоторые хранилища могут не гарантировать консистентность данных [детали](https://github.com/gaul/are-we-consistent-yet#observed-consistency).

Вы можете просмотреть таблицу совместимости с POSIX [здесь](https://github.com/yandex-cloud/geesefs#posix-compatibility-matrix).

Основные ограничения:

- Разрешения файловой системы, символические ссылки, пользовательские `mtimes` и специальные файлы (блочные/символьные устройства, именованные каналы, сокеты UNIX) не поддерживаются, поскольку стандарт S3 не возвращает метаданные пользователя в списках и не читает все эти метаданные в стандарте S3. Потребуется дополнительный запрос `HEAD` для каждого файла в листинге, что сделает листинги слишком медленными.
- Поддержка специальных файлов включена по умолчанию для `Яндекс S3` и отключена для других.
- Разрешения файловой системы отключены по умолчанию.
- Пользовательское время модификации также отключено по умолчанию: `ctime`, `atime` и `mtime` всегда одинаковы.
- Время изменения файла не может быть установлено пользователем (например, с помощью `cp --preserve`, `rsync -a` или `utimes(2)`)
- Не поддерживаются жесткие ссылки.
- Не поддерживается блокировка.
- Не поддерживаются «невидимые» удаленные файлы. Если приложение сохраняет открытый файловый дескриптор после удаления файла, оно получит ошибки `ENOENT` от операций FS.
- Ограничение размера файла по умолчанию составляет 1,03 ТБ и достигается путем разделения файла на 1000 частей по 5 МБ, 1000 частей по 25 МБ и 8000 частей по 125 МБ. Вы можете изменить размеры частей, но собственный лимит AWS в любом случае составляет 5 ТБ.

## Известные ошибки и проблемы

- Запрос размера тома в PVC никак не отражается на создаваемых bucket.
- `df -h` показывает размер смонтированного хранилища в 1 петабайт, `used` никак не меняется во время использования.
  - CSI не проверяет корректность ключей для доступа к хранилищу, статус пода будет `Running`, а PersistentVolume и PersistentVolumeClaim будут `Bound` даже если ключи неправильные. Попытка доступа к смонтированной директории в поде приведёт к перезапуску пода.
