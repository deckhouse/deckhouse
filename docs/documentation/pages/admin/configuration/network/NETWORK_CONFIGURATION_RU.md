---
title: "Настройка внутренней сети"
permalink: ru/admin/network/configuration.html
lang: ru
---

## Настройка сети подов

В Deckhouse Kubernetes Platform для настройки сети подов используются CNI-плагины.
Сеть может быть настроена по одному из вариантов:

* C использованием VXLAN-туннеля.
* С использованием маршрутизации.

Выбор подходящего варианта зависит от особенностей сети кластера.

Если узлы кластера находятся в разных сетях, VXLAN позволяет создать виртуальную сеть поверх существующей инфраструктуры. При этом обеспечивается изоляция трафика и возможность работы с различными сетевыми сегментами.

Если узлы кластера находятся в одной сети, могут быть использованы оба варианта.
При этом нужно учитывать, что инкапсуляция VXLAN-пакетов приводит к уменьшению minimal transmission unit (MTU), а использование маршрутизации добавляет промежуточный узел на пути пакета.
На практике разница в скорости при использовании обоих вариантов настолько мала, что ей можно пренебречь.

<!-- Для специфических задач можно использовать CNI-плагины Flannel и Simple Bridge, которые также поддерживаются Deckhouse Kubernetes Platform. -->

### Выбор режима работы туннеля в Cilium

Режим работы устанавливается в настройках модуля cni-cilium (параметр `tunnelMode`).

<!-- перенесено с изменениями из https://deckhouse.ru/products/kubernetes-platform/documentation/latest/modules/cni-cilium/#%D1%81%D0%BC%D0%B5%D0%BD%D0%B0-%D1%80%D0%B5%D0%B6%D0%B8%D0%BC%D0%B0-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B-cilium -->

> После смены режима работы Cilium перезагрузите все узлы, иначе возможны проблемы с доступностью подов.

Чтобы изменить режим работы туннеля, задайте требуемое значение параметра `tunnelMode` в настройках модуля cni-cilium.

Для настройки модуля используйте ресурс `ModuleConfig` с именем `cni-cilium`.

Пример ресурса `ModuleConfig/cni-cilium` для настройки модуля (изменение режима работы туннеля):

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: ModuleConfig
metadata:
  name: cni-cilium
spec:
  version: 1
  enabled: true
  settings:
    tunnelMode: VXLAN
```

[Подробнее о настройках](/#) модуля `cni-cilium`.

## Настройка сети сервисов

Сервисы позволяют предоставлять доступ к группам подов, выполняющих одинаковые функции, и реализовать балансировку трафика между ними.
Каждый объект Service определяет логический набор конечных точек (подов), а также политику того, как сделать эти поды доступными.

Deckhouse Kubernetes Platform поддерживает следующие типы сервисов:

<!-- информация из обучающих материалов -->

* ClusterIP. Создаёт внутренний IP-адрес, который выбирается из сети сервисов. Весь трафик с этого IP перенаправляется на поды, указанные в селекторе. Тем самым обеспечивается балансировка.
В этом типе сервиса можно отказаться от назначения IP-адреса. В таком случае адрес из сети сервисов не будет назначаться, а сам сервис при обращении будет резолвиться в IP-адреса подов. Балансировка будет работать на round-robin DNS.
* NodePort. Открывает порт на каждом узле кластера и перенаправляет трафик, который приходит на этот порт, в поды, указанные в селекторе.
Из соображений безопасности в Deckhouse порт слушается только на внутреннем IP узлов. Это поведение можно изменить, если добавить аннотацию `node.deckhouse.io/nodeport-bind-internal-ip: "false"` на группу узлов.
* LoadBalancer. Это специальный тип сервисов. Он создаётся в облачном провайдере, на котором установлена Deckhouse Kubernetes Platform, и позволяет принимать трафик извне кластера. Для bare-metal-кластеров поддержку таких сервисов можно добавить с помощью модуля MetalLB.
* ExternalName. По сути, это просто CNAME DNS-запись, созданная в кластерном DNS.

Пример настроек сервиса ClusterIP:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app-service  # Имя сервиса
  namespace: default    # Namespace, в котором создается сервис
  labels:
    app: my-app         # Метка для связи с подами
spec:
  type: ClusterIP       # Тип сервиса — ClusterIP
  selector:
    app: my-app         # Селектор для выбора подов, которые будут частью сервиса
  ports:
    - protocol: TCP     # Протокол (TCP или UDP)
      port: 80          # Порт, на котором сервис будет доступен внутри кластера
      targetPort: 8080  # Порт, на котором работает приложение в подах
  selector:
    app: my-app         # Селектор для выбора подов с меткой app=my-app
```

<!-- перенесено из https://deckhouse.ru/products/kubernetes-platform/documentation/latest/modules/service-with-healthchecks/ -->

### ServiceWithHealthCheck для  обеспечения отказоустойчивости и контроля состояния сервисов

Deckhouse Kubernetes Platform ServiceWithHealthCheck используется для обеспечения отказоустойчивости сервисов и контроля их состояния.

#### Ограничения стандартного балансировщика Service

В Kubernetes за внутреннюю и внешнюю балансировку запросов отвечает ресурс типа `Service`. Он распределяет запросы между рабочими подами приложения и исключает из балансировки повреждённые экземпляры. Для проверки способности пода обрабатывать входящие запросы применяются readiness-пробы, которые указываются в спецификации контейнеров, входящих в этот под.

Стандартный инструмент балансировки Service подходит для большинства задач облачных приложений, но имеет два ограничения:

* Если хотя бы один контейнер в поде не проходит проверку готовности (readiness-пробу), весь под отмечается как `NotReady` и исключается из балансировки всех сервисов, с которыми он связан.
* Для каждого контейнера можно настроить только одну пробу, поэтому невозможно создать отдельные пробы для проверки, например, доступности чтения и записи.

Примеры сценариев, где стандартного балансировщика недостаточно:

* База данных:
  * Работает в трёх подах — `db-0`, `db-1` и `db-2`, каждый из которых содержит один контейнер с запущенным процессом базы данных.
  * Необходимо создать два сервиса (Service) — `db-write` для записи и `db-read` для чтения.
  * Запросы на чтение должны балансироваться между всеми подами.
  * Запросы на запись балансируются только на тот под, который назначен мастером средствами самой базы данных.
* Виртуальная машина:
  * Под содержит единственный контейнер, в котором запущен процесс `qemu`, выполняющий роль гипервизора для гостевой виртуальной машины.
  * В гостевой виртуальной машине запущены независимые процессы, например, веб-сервер и SMTP-сервер.
  * Требуется создать два Service — `web` и `smtp`, каждый из которых которых будет иметь свои readiness-пробы.

#### Возможности балансировщика ServiceWithHealthcheck

В отличие от стандартного балансировщика, где readiness-пробы привязаны к состоянию контейнеров, `ServiceWithHealthcheck` позволяет настраивать активные пробы на отдельные TCP-порты. Таким образом, каждый балансировщик, обслуживающий один и тот же под, может работать независимо от других.

Настроить данный способ балансировки можно при помощи ресурса [ServiceWithHealthchecks](cr.html#servicewithhealthchecks):

* Его спецификация идентична стандартному `Service` с добавлением раздела `healthcheck`, который содержит набор проверок.
* На данный момент поддерживается три вида проб:
  * `TCP` — обычная проверка с помощью установки TCP-соединения.
  * `HTTP` — возможность отправить HTTP-запрос и ожидать определённый код ответа.
  * `PostgreSQL` — возможность отправить SQL-запрос и ожидать его успешного завершения.

Ознакомиться с примерами можно в [документации](examples.html).

#### Внутреннее устройство балансировщика ServiceWithHealthcheck

Балансировщик состоит из двух компонентов:

* контроллер — работает на мастер-узлах кластера и управляет ресурсами `ServiceWithHealthcheck`,
* агенты — работают на каждом узле кластера и выполняют пробы для подов, запущенных на этом узле.

Балансировщик ServiceWithHealthcheck спроектирован так, чтобы не зависеть от реализации CNI, используя при этом стандартные ресурсы `Service` и `EndpointSlice`:

* Контроллер при создании ресурса `ServiceWithHealthcheck` автоматически создает одноименный ресурс Service в том же пространстве имен с пустым полем `selector`. Это позволяет избежать создания стандартным контроллером `EndpointSlice`, которые используются для настройки балансировки.
* Каждый агент при появлении на своём узле подов, которые попадают под управление `ServiceWithHealthcheck`, осуществляет настроенные пробы и создаёт для них `EndpointSlice` со списком проверенных IP-адресов и портов. Данный `EndpointSlice` привязан к дочернему ресурсу `Service`, созданному выше.
* CNI сопоставит все `EndpointSlice` со стандартными сервисами, созданными выше и осуществит балансировку по проверенным IP-адресам и портам на всех узлах кластера.

Миграция с Service на ресурс ServiceWithHealthchecks, например в рамках CI/CD, не должна вызвать затруднений. Спецификация ServiceWithHealthchecks в основе своей повторяет спецификацию Service, но содержит дополнительный раздел healthchecks. Во время жизненного цикла ресурса ServiceWithHealthchecks создается одноименный сервис в том же namespace, чтобы привычным способом (kube-proxy или cni) направить трафик на рабочие нагрузки в кластере.
