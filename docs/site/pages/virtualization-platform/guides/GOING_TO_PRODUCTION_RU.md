---
title: Подготовка DVP к production
permalink: ru/virtualization-platform/guides/production.html
description: Рекомендации по подготовке Deckhouse Virtualization Platform для работы в продуктивной среде.
documentation_state: develop
lang: ru
---

Приведённые ниже рекомендации важны для production-кластера и могут быть неактуальны для тестового кластера или кластера разработки.

## Канал и режим обновлений

{% alert level="info" %}
Используйте канал обновлений `Early Access` или `Stable`. Установите [окно автоматических обновлений](/modules/deckhouse/usage.html#конфигурация-окон-обновлений) или [ручной режим](/modules/deckhouse/usage.html#ручное-подтверждение-обновлений).
{% endalert %}

Выберите [канал обновлений](../documentation/about/release-channels.html) и [режим обновлений](../documentation/admin/update/update.html##каналы-обновлений) в соответствии с вашими требованиями. Чем стабильнее канал обновлений, тем позже в нём появляется новая функциональность.

По возможности используйте разные каналы обновлений для разных кластеров. Для кластера разработки используйте менее стабильный канал обновлений, чем для тестового или stage-кластера (pre-production-кластер).

Мы рекомендуем использовать канал обновлений `Early Access` или `Stable` для production-кластеров. Если в production-окружении больше одного кластера, предпочтительно использовать для них разные каналы обновлений. Например, `Early Access` для одного, а `Stable` — для другого. Если использовать разные каналы обновлений по каким-либо причинам невозможно, рекомендуется устанавливать разные окна обновлений.

{% alert level="warning" %}
Даже в очень нагруженных и критичных кластерах не стоит отключать использование канала обновлений. Лучшая стратегия — плановое обновление. В кластерах, которые не обновлялись полгода и более, могут присутствовать ошибки, которые, как правило, уже устранены в новых версиях. В этом случае оперативно решить возникшую проблему будет непросто.
{% endalert %}

Управление [окнами обновлений](/modules/deckhouse/configuration.html#parameters-update-windows) позволяет планово обновлять релизы платформы в автоматическом режиме в периоды низкой нагрузки на кластер.

## Версия Kubernetes

{% alert level="info" %}
Используйте автоматический [выбор версии Kubernetes](/products/kubernetes-platform/documentation/v1/reference/api/cr.html#clusterconfiguration-kubernetesversion) либо установите версию явно.
{% endalert %}

В большинстве случаев предпочтительно использовать автоматический выбор версии Kubernetes. В платформе такое поведение установлено по умолчанию, но его можно изменить с помощью параметра [kubernetesVersion](/products/kubernetes-platform/documentation/v1/reference/api/cr.html#clusterconfiguration-kubernetesversion). Обновление версии Kubernetes в кластере не оказывает влияния на приложения и проходит [последовательно и безопасно](/modules/control-plane-manager/#управление-версиями).

Если включён автоматический выбор версии Kubernetes, платформа может обновить версию Kubernetes в кластере при обновлении релиза платформы (при обновлении минорной версии). Если версия Kubernetes явно указана в параметре [kubernetesVersion](/products/kubernetes-platform/documentation/v1/reference/api/cr.html#clusterconfiguration-kubernetesversion), обновление платформы может завершиться неудачей, если используемая в кластере версия Kubernetes более не поддерживается.

Если приложение использует устаревшие версии ресурсов или требует конкретной версии Kubernetes по какой-либо другой причине, проверьте, что эта версия [поддерживается](/products/virtualization-platform/documentation/about/requirements.html), и [установите ее явно](/products/kubernetes-platform/documentation/v1/deckhouse-faq.html#как-обновить-версию-kubernetes-в-кластере).

## Выбор архитектуры кластера

Deckhouse Virtualization Platform поддерживает несколько вариантов архитектуры кластера — от одноузловых установок до распределённых конфигураций. Выбор конкретного варианта зависит от требований: необходимости быстрого тестового развёртывания, обеспечения высокой доступности или изоляции системных сервисов от пользовательских нагрузок.

**Одноузловой кластер (Single Node / Edge)** — все компоненты управления, служебные сервисы и виртуальные машины размещаются на одном сервере. Подходит для тестовых окружений и edge-сценариев. Требует минимального объёма ресурсов, но не обеспечивает отказоустойчивость.

**Кластер с одним master-узлом и worker-узлами** — один узел выполняет функции управления, виртуальные машины размещаются на выделенных worker-узлах. Подходит для небольших кластеров, где требуется разделение системных сервисов и пользовательских нагрузок. Отказоустойчивость отсутствует.

**Трёхузловой кластер (High Availability)** — компоненты управления распределяются по трём master-узлам, что обеспечивает отказоустойчивость control plane и продолжение работы при сбое одного из узлов. Пользовательская нагрузка может выполняться как на этих же серверах, так и на выделенных worker-узлах. Рекомендуется для production-окружений.

**Высокодоступный распределённый кластер** — компоненты управления развёртываются на трёх выделенных master-узлах, при необходимости системные сервисы, мониторинг и ingress выносятся на отдельные system- или frontend-узлы. Пользовательские виртуальные машины выполняются исключительно на worker-гипервизорах. Обеспечивает высокую доступность, масштабируемость и изоляцию пользовательских нагрузок от системных сервисов. Применяется в крупных кластерах.

Дополнительная информация об архитектурных решениях приведена в разделе [Архитектурные решения](/products/virtualization-platform/documentation/about/architecture-options.html).

## Требования к ресурсам

Прежде чем приступить к развёртыванию кластера, необходимо провести планирование ресурсов, которые могут потребоваться для его работы. Для этого следует ответить на несколько вопросов:

- Какая нагрузка планируется на кластер?
- Сколько виртуальных машин планируется запускать?
- Требуется ли кластеру режим высокой доступности?
- Какой тип хранилища будет использоваться (SDS или внешнее)?

Ответы на эти вопросы помогут определить необходимую архитектуру кластера и ресурсы узлов.

{% alert level="info" %}
При использовании программно-определяемых хранилищ (SDS) на узлах, которые участвуют в организации хранилища, необходимо предусмотреть дополнительные диски сверх указанных минимальных требований. Эти диски будут использоваться SDS для хранения данных виртуальных машин.
{% endalert %}

В зависимости от архитектуры, для корректной работы платформы требуются следующие минимальные ресурсы:

| Архитектура                                                             | Запуск нагрузки        | Master-узел          | Worker-узел         | Системный узел       | Frontend-узел       |
|-------------------------------------------------------------------------|------------------------|----------------------|---------------------|----------------------|---------------------|
| Одноузловая платформа<br/>(Single Node / Edge)                          | На одном узле          | 3 vCPU<br/>10 ГБ ОЗУ | —                   | —                    | —                   |
| Многоузловая платформа<br/>(1 master-узел + worker-узлы)                | На всех узлах          | 6 vCPU<br/>6 ГБ ОЗУ  | 2 vCPU<br/>4 ГБ ОЗУ | —                    | —                   |
| Трёхузловая платформа<br/>(3 master-узла, High Availability)            | На всех узлах          | 6 vCPU<br/>14 ГБ ОЗУ | —                   | —                    | —                   |
| Платформа с выделенными worker-узлами<br/>(3 master-узла + worker-узлы) | Только на worker-узлах | 5 vCPU<br/>11 ГБ ОЗУ | 2 vCPU<br/>5 ГБ ОЗУ | —                    | —                   |
| Распределённая архитектура                                              | Только на worker-узлах | 4 vCPU<br/>9 ГБ ОЗУ  | 1 vCPU<br/>2 ГБ ОЗУ | 4 vCPU<br/>10 ГБ ОЗУ | 1 vCPU<br/>2 ГБ ОЗУ |

Количество виртуальных машин, которые можно запускать на узлах, ограничивается параметром `maxPods` в свойствах ресурса [NodeGroup](/modules/node-manager/cr.html#nodegroup). При планировании количества ВМ учитывайте, что лимит `maxPods` включает все поды на узле: системные поды, контейнеризированную нагрузку и виртуальные машины. Каждая виртуальная машина занимает один под в кластере.

{% alert level="info" %}
Минимальные требования указаны для базовой конфигурации платформы. При увеличении нагрузки, количества виртуальных машин или включении дополнительных модулей может потребоваться увеличение ресурсов узлов.
{% endalert %}

## Особенности конфигурации

### Планирование подсетей кластера

{% alert level="warning" %}
Все подсети кластера не должны пересекаться друг с другом.
{% endalert %}

Определите подсети IP-адресов для кластера:

- **Подсеть узлов** — используется узлами для связи между собой. Это единственная подсеть, которая физически существует в сети организации и маршрутизируется в вашей инфраструктуре. Должна быть реальной сетью вашего датацентра.
- **Подсеть для подов** (`podSubnetCIDR`) — виртуальная сеть внутри кластера, используется для назначения IP-адресов подам Kubernetes (включая системные поды и контейнеризированную нагрузку).
- **Подсеть для сервисов** (`serviceSubnetCIDR`) — виртуальная сеть внутри кластера, используется для назначения IP-адресов сервисам Kubernetes типа ClusterIP для внутрикластерного взаимодействия.
- **Подсети адресов для виртуальных машин** (`virtualMachineCIDRs`) — виртуальные сети внутри кластера, используются для назначения IP-адресов виртуальным машинам. Поддерживается указание нескольких подсетей.

Дополнительная информация о настройке сетей виртуальных машин приведена в разделе [Сети виртуальных машин](/products/virtualization-platform/documentation/admin/platform-management/network/vm-network.html).

### Master-узлы

{% alert level="info" %}
В кластере должно быть три master-узла с быстрыми дисками 400+ IOPS.
{% endalert %}

Всегда используйте три master-узла — такое количество обеспечит отказоустойчивость и позволит безопасно выполнять обновление master-узлов. В большем числе master-узлов нет необходимости, а два узла не обеспечат кворума.

{% alert level="info" %}
Если на узлах control plane нужно запускать рабочую нагрузку (виртуальные машины), что характерно для конфигураций **Одноузловой кластер (Single Node / Edge)** и **Трёхузловой кластер (High Availability)**, необходимо настроить допуски (tolerations) в конфигурации виртуальных машин или в классе виртуальных машин, чтобы разрешить размещение ВМ на master-узлах.
{% endalert %}

Дополнительная информация о настройке статических узлов приведена в разделе [Работа со статическими узлами](/products/kubernetes-platform/documentation/v1/architecture/node.html#работа-со-статическими-узлами).

### Frontend-узлы

{% alert level="info" %}
Выделите два или более frontend-узла.

Используйте инлет `HostPort` с внешним балансировщиком.
{% endalert %}

Frontend-узлы балансируют входящий трафик. На них работают Ingress-контроллеры. У NodeGroup frontend-узлов установлен лейбл `node-role.deckhouse.io/frontend`. Подробнее о [выделении узлов под определённый вид нагрузки](/products/kubernetes-platform/documentation/v1/#выделение-узлов-под-определенный-вид-нагрузки).

Используйте более одного frontend-узла. Frontend-узлы должны выдерживать трафик при отказе как минимум одного frontend-узла.

Например, если в кластере два frontend-узла, каждый frontend-узел должен обрабатывать всю нагрузку на кластер в случае выхода из строя второго узла. Если в кластере три frontend-узла, каждый frontend-узел должен выдерживать увеличение нагрузки как минимум в полтора раза.

Платформа поддерживает три способа поступления трафика из внешнего мира:

- `HostPort` — устанавливается Ingress-контроллер, который доступен на портах узлов через `hostPort`;
- `HostPortWithProxyProtocol` — устанавливается Ingress-контроллер, который доступен на портах узлов через `hostPort` и использует proxy-protocol для получения настоящего IP-адреса клиента;
- `HostWithFailover` — устанавливаются два Ingress-контроллера (основной и резервный).

{% alert level="warning" %}
Инлет `HostWithFailover` подходит для кластеров с одним frontend-узлом. Он позволяет сократить время недоступности Ingress-контроллера при его обновлении. Такой тип инлета подойдет, например, для важных сред разработки, но **не рекомендуется для production**.
{% endalert %}

### Узлы мониторинга

{% alert level="info" %}
Для нагруженных кластеров выделите два узла мониторинга с быстрыми дисками.
{% endalert %}

Узлы мониторинга служат для запуска Grafana, Prometheus и других компонентов мониторинга. У [NodeGroup](/modules/node-manager/cr.html#nodegroup) узлов мониторинга установлен лейбл `node-role.deckhouse.io/monitoring`.

В нагруженных кластерах со множеством алертов и большими объёмами метрик рекомендуется выделить отдельные узлы для мониторинга. Если этого не сделать, компоненты мониторинга будут размещены на [системных узлах](#системные-узлы).

При выделении узлов под мониторинг важно, чтобы на них были быстрые диски. Для этого можно привязать `storageClass` на быстрых дисках ко всем компонентам платформы (глобальный параметр [storageClass](/products/kubernetes-platform/documentation/v1/reference/api/global.html#parameters-storageclass)) или выделить отдельный `storageClass` только для компонентов мониторинга (параметры [storageClass](/modules/prometheus/configuration.html#parameters-storageclass) и [longtermStorageClass](/modules/prometheus/configuration.html#parameters-longtermstorageclass) модуля `prometheus`).

Если кластер изначально создаётся с узлами, выделенными под определённый вид нагрузки (системные узлы, узлы под мониторинг и т. п.), то для модулей, использующих тома постоянного хранилища (например, для модуля `prometheus`), рекомендуется явно указывать соответствующий nodeSelector в конфигурации модуля. Например, для модуля `prometheus` это параметр [nodeSelector](/modules/prometheus/configuration.html#parameters-nodeselector).

### Системные узлы

{% alert level="info" %}
Выделите два системных узла.
{% endalert %}

Системные узлы предназначены для запуска модулей платформы. У [NodeGroup](/modules/node-manager/cr.html#nodegroup) системных узлов установлен лейбл `node-role.deckhouse.io/system`.

Выделение двух системных узлов обеспечивает работу модулей платформы без пересечения с пользовательскими приложениями кластера. Подробнее о [выделении узлов под определённый вид нагрузки](/products/kubernetes-platform/documentation/v1/#выделение-узлов-под-определенный-вид-нагрузки).

Компонентам платформы желательно выделить быстрые диски (глобальный параметр [storageClass](/products/kubernetes-platform/documentation/v1/reference/api/global.html#parameters-storageclass)).

### Конфигурирование хранилища

Настройте одно или несколько хранилищ для дисков виртуальных машин:

- **Программно-определяемые хранилища (SDS)**:
  - `sds-local-volume` — локальное хранилище на основе LVM. Высокая производительность, но без репликации. Подходит для временных данных или при наличии внешнего бэкапа.
  - `sds-replicated-volume` — реплицируемое хранилище на основе DRBD. Обеспечивает отказоустойчивость за счет репликации между узлами. Рекомендуется для production.

- **Внешние хранилища**: Ceph, NFS, TATLIN.UNIFIED (Yadro), Huawei Dorado, HPE 3par, iSCSI. Подключаются через соответствующие CSI-драйверы.

{% alert level="info" %}
При использовании SDS на узлах, участвующих в организации хранилища, необходимо предусмотреть дополнительные диски сверх минимальных требований к ресурсам узлов. Размер дополнительных дисков зависит от планируемого объёма данных виртуальных машин.
{% endalert %}

Дополнительная информация о настройке хранилищ приведена в разделе [Настройка хранилищ](/products/virtualization-platform/documentation/admin/platform-management/storage/).

### Конфигурирование классов виртуальных машин

Создайте собственный класс виртуальной машины (один или несколько).

{% alert level="info" %}
Класс `generic` по умолчанию не рекомендуется использовать в production.
{% endalert %}

Настройте политики сайзинга для контроля ресурсов ВМ:
- используйте `type: Host` для одинаковых узлов (одинаковая архитектура процессора);
- используйте `type: Discovery` для разных типов процессоров.

Политики сайзинга ограничивают допустимые конфигурации ресурсов ВМ (количество ядер, память, доля использования ядра) и предотвращают создание ВМ с неоптимальными конфигурациями. Параметр `coreFractions` управляет переподпиской (overcommit) по CPU: задавая минимальную долю использования ядра, вы гарантируете каждой ВМ соответствующую часть CPU и тем самым ограничиваете максимально допустимый уровень переподписки ресурсов.

Дополнительная информация о настройках VirtualMachineClass приведена в разделе [Настройки VirtualMachineClass](/products/virtualization-platform/documentation/admin/platform-management/virtualization/virtual-machine-classes.html#настройки-virtualmachineclass).

## Разграничение доступа

{% alert level="info" %}
Настройте аутентификацию пользователей и разграничение доступа. Для production рекомендуется использовать проекты с настройкой ролевой модели.
{% endalert %}

Платформа поддерживает управление внутренними пользователями и группами, а также интеграцию с внешними провайдерами аутентификации:

- **Внутренние пользователи и группы** — создаются через ресурсы [User](/modules/user-authn/cr.html#user) и [Group](/modules/user-authn/cr.html#group) модуля `user-authn`.
- **Внешние провайдеры аутентификации** — LDAP, OIDC, GitHub, GitLab, Crowd, Bitbucket Cloud. Можно подключить несколько провайдеров одновременно.

Дополнительная информация об интеграции с внешними системами аутентификации приведена в разделе [Интеграция с внешними системами аутентификации](/products/virtualization-platform/documentation/admin/platform-management/access-control/user-management.html).

Настройте проекты и права доступа в соответствии с планируемым использованием кластера. Проекты (ресурс [Project](/modules/multitenancy-manager/cr.html#project)) обеспечивают изолированные окружения для создания ресурсов пользователя. Настройки проекта позволяют задать квоты для ресурсов, ограничить сетевое взаимодействие и настроить профиль безопасности.

Разграничение доступа настраивается через ролевую модель с использованием стандартного подхода RBAC Kubernetes. Можно использовать существующие роли или создать свои:

- **Manage-роли** — для администраторов платформы. Позволяют конфигурировать кластер, управлять виртуальными машинами на уровне всей платформы и создавать проекты (тенанты) для пользователей.
- **Use-роли** — для пользователей проектов. Позволяют управлять ресурсами (включая виртуальные машины) внутри назначенных проектов.

Дополнительная информация о настройке проектов и ролевой модели приведена в разделах [Проекты](/products/virtualization-platform/documentation/admin/platform-management/access-control/projects.html) и [Ролевая модель](/products/virtualization-platform/documentation/admin/platform-management/access-control/role-model.html).


## Уведомление о событиях мониторинга

{% alert level="info" %}
Настройте отправку алертов через [внутренний](/modules/prometheus/faq.html#как-добавить-alertmanager) Alertmanager или подключите [внешний](/modules/prometheus/faq.html#как-добавить-внешний-дополнительный-alertmanager).
{% endalert %}

Мониторинг будет работать сразу после установки платформы, однако для production этого недостаточно. Чтобы получать уведомления об инцидентах, настройте [встроенный](/modules/prometheus/faq.html#как-добавить-alertmanager) в платформе Alertmanager или [подключите свой](/modules/prometheus/faq.html#как-добавить-внешний-дополнительный-alertmanager) Alertmanager.

С помощью custom resource [CustomAlertmanager](/modules/prometheus/cr.html#customalertmanager) можно настроить отправку уведомлений на [электронную почту](/modules/prometheus/cr.html#customalertmanager-v1alpha1-spec-internal-receivers-emailconfigs), в [Slack](/modules/prometheus/cr.html#customalertmanager-v1alpha1-spec-internal-receivers-slackconfigs), в [Telegram](/modules/prometheus/usage.html#отправка-алертов-в-telegram), через [webhook](/modules/prometheus/cr.html#customalertmanager-v1alpha1-spec-internal-receivers-webhookconfigs), а также другими способами.

Список всех доступных алертов системы мониторинга приведён на [отдельной странице документации](/products/kubernetes-platform/documentation/v1/reference/alerts.html).

<!-- ## Сбор логов

{% alert %}
[Настройте](/modules/log-shipper/) централизованный сбор логов.
{% endalert %}

Настройте централизованный сбор логов с системных и пользовательских приложений с помощью модуля [log-shipper](/modules/log-shipper/).

Достаточно создать custom resource с описанием того, *что нужно собирать*: [ClusterLoggingConfig](/modules/log-shipper/cr.html#clusterloggingconfig) или [PodLoggingConfig](/modules/log-shipper/cr.html#podloggingconfig); кроме того, необходимо создать custom resource с данными о том, *куда отправлять* собранные логи: [ClusterLogDestination](/modules/log-shipper/cr.html#clusterlogdestination).

Дополнительная информация:
- [Пример для Grafana Loki](/modules/log-shipper/examples.html#чтение-логов-из-всех-подов-кластера-и-направление-их-в-loki)
- [Пример для Logstash](/modules/log-shipper/examples.html#простой-пример-logstash)
- [Пример для Splunk](/modules/log-shipper/examples.html#пример-интеграции-со-splunk)
-->

## Резервное копирование

{% alert level="warning" %}
Обязательно настройте [резервное копирование данных etcd](/products/virtualization-platform/documentation/admin/backup-and-restore.html#резервное-копирование-etcd) — это ваша последняя возможность восстановить кластер в случае непредвиденных событий. Храните резервные копии как можно *дальше* от кластера.
{% endalert %}

Резервные копии не помогут, если они не работают или вы не знаете, как их использовать для восстановления. Рекомендуем составить [план восстановления на случай аварии](https://habr.com/ru/search/?q=%5BDRP%5D&target_type=posts&order=date) (Disaster Recovery Plan), содержащий конкретные шаги и команды по развертыванию кластера из резервной копии.

Этот план должен периодически актуализироваться и проверяться учебными тревогами.

## Сообщество

{% alert level="info" %}
Следите за новостями проекта в [Telegram](https://t.me/deckhouse_ru).
{% endalert %}

Вступите в [сообщество](/community/), чтобы быть в курсе важных изменений и новостей. Вы сможете общаться с людьми, занятыми общим делом. Это позволит избежать многих типичных проблем.

Команда платформы знает, каких усилий требует организация работы production-кластера в Kubernetes. Мы будем рады, если платформа позволит вам реализовать задуманное. Поделитесь своим опытом и вдохновите других на переход в Kubernetes.
