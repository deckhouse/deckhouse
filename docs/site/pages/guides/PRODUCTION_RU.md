---
title: Администрирование
permalink: ru/guides/production.html
lang: ru
---

## Доступ

<!-- ### Подключение -->

Платформа Deckhouse позволяет взаимодействовать с её API и веб-интерфейсами.
Основной метод - публикация через HTTPS-балансировщик. Используется по умолчанию. 
Также можно публиковать API напрямую в корпоративной сети при условии интеграции платформы для публикации сервисов и возможности подключения по OpenVPN.
Эти возможности обеспечивают простое и надежное подключение пользователей, учитывая особенности сетевой инфраструктуры и требования по безопасности конкретной компании.

### Публикация API напрямую в корпоративной сети

1. Интеграция платформы для публикации сервисов.

2. Настройка сети с API Deckhouse. (Получается тут надо описать настройку брандмауэра и других сетевых устройств).

3. Подключение по OpenVPN.

### Публикация API через HTTPS-балансировщик

1. Установка и настройка Deckhouse.

Необходимо установить и настроить Deckhouse....

Идет сценарий установки

2. Создание и настройка балансировщика

 Создайте и настройте балансировщик HTTPS на стороне Deckhouse.
 
3. Подключение к балансировщику.

На стороне пользователя, подключитесь к балансировщику через Интернет или корпоративную сеть.

### Настроить доступ для пользователей

### Сгеенерировать kubeconfig

1. Создание учетных записей пользователей. 

Создайте пользователей с соответствующими ролями и правами на доступ к кластеру.

2. Создание объекта пользователя в Kubernetes API.

`kubectl`, например, или другой инструмент управления, создайте файл с расширением .yaml, описывающий пользователя и его роли.

3. Публикация объекта пользователя.

Например, используя `kubctl apply`, опубликуйте созданный объект пользователя в кластере.

4. Загрузка файла ролей и учетных записей пользователей в сервер аутентификации (например, Keycloak, Dex, или OpenID Connect).

Пользователи должны пройти аутентификацию и авторизацию с помощью сервера аутентификации.
После успешной аутентификации и авторизации, пользователь получает токен, который можно использовать для подключения к Kubernetes API.
Пользователь использует `kubectl` или другое приложение для подключения к кластеру с использованием токена.

### Принципы

#### Файл kubeconfig

По умолчанию инструмент командной строки для управления кластерами `kubectl` задействуется через командную строку (например, через bash) и использует контексты для взаимодействия с кластером. Контекст - это набор параметров доступа, содержащий кластер, пользователя и пространство имен. Контексты используются для доступа к определенному кластеру и пространству имен с помощью учетной записи пользователя. 
Установка того, с каким кластером взаимодействует ДКП и изменение конфигурационной информации осуществляются посредством `kubectl`.

#### Control plane

Компоненты управляющего слоя, которые отвечают за основные операции кластера ДКП (например, планирование), а также обрабатывают события кластера ДКП (например, запускают новый pod, когда поле развертывания `replicas` не соответствует требуемому количеству реплик).
Минимальный набор таких компонентов: `etcd`, `API-server`, `controller manager`.
Помимо управляющих компонентов, в ДКП входят `kubelet` и `kubeproxy`, которые запущены на всех узлах кластера ДКП. Компоненты Control plane могут быть запущены на любой машине в кластере. Однако сценарии настройки обычно запускают все компоненты управляющего слоя на одном компьютере и в то же время не позволяют запускать пользовательские контейнеры на этом компьютере.

#### API-server

Центральный компонент кластера ДКП. Он является связывающим компонентом для всех остальных сервисов. Все взаимодействие как самих компонентов, так и обращение извне к кластеру проходит через kube-apiserver и проверяется им. 

<!--Уточнить-->

Это единственный компонент кластера ДКП, который общается с базой данных `etcd`, где хранится состояние кластера ДКП.
В самом API Server нет бизнес-логики. API Server не принимает решения, например, за то, на каком узле запустить тот или иной сервис.
В нем существует логика проверки формата запросов, аутентификации, проверки прав и т.д.
Еще одной функцией API Server является рассылка изменений конфигураций и состояния кластера ДКП.
Другие компоненты подписываются на события, отслеживают их и обрабатывают, либо с определенной периодичностью считывают конфигурацию через API Server.

#### Etcd

Высоконадежное распределенное хранилище данных. ДКП хранит в нем информацию о состоянии существующих кластеров ДКП, сервисах, сети и т. д.
Доступ к данным осуществляется через `REST API`. При изменениях записей вместо поиска и изменения предыдущей копии, все предыдущие записи помечаются как устаревшие, а новые значения записываются в конец. Позже устаревшие значения удаляются специальным процессом. В небольших временных кластерах `etcd` можно запускать в единственном экземпляре и на одном узле с ведущими компонентами.
В важных системах, требующих избыточности и высокой доступности, `etcd` следует создавать в виде кластера ДКП, состоящего из нескольких узлов.
Сервер API является основной точкой управления всего кластера ДКП. Он обрабатывает операции REST, проверяет их и обновляет соответствующие объекты в `etcd`. Сервер API обслуживает ДКП API и задуман как простой сервер, с большей частью бизнес-логики, реализованной в отдельных компонентах или в плагинах. Ниже приведены некоторые примеры API, доступных на сервере API.

#### Kube-scheduler

Компонент управляющего слоя, с его помощью необходимо отслеживать созданные поды без привязанного узла и осуществлять выбор узла, на котором поды должны работать. При развертывании подов на узлах, учитывается множество факторов, включая требования к ресурсам, ограничения, связанные с аппаратными/программными политиками, принадлежности (affinity) и непринадлежности (anti-affinity) узлов/подов, местонахождения данных, предельных сроков.
Используя механизм оповещения об изменениях API Server, `kube-scheduler` получает сообщения от управляющего слоя, когда необходимо запустить экземпляр сервиса и принимает решение о том, на каком узле он должен быть запущен, и через API server обновляет состояние кластера ДКП. Сам `kube-scheduler` ничего не запускает.

#### Kubelet

Сервис, который управляет подами, основываясь на их спецификации, является главным сервисом для рабочих узлов. Сервис взаимодействует с `API-server`.
Для запуска полезной нагрузки на узлах используется `kubelet`. В отличие от компонентов управляющего слоя, он запущен на каждом узле: управляющем и рабочем. `kubelet` читает событие с помощью `API Server`, что экземпляр сервиса распределен посредством `kube-scheduler` на узел, на котором он работает, и запускает экземпляр сервиса.
Для изоляции сервисов друг от друга они запускаются `kubelet` в контейнерном окружении `CRI-O`. <!--Уточнить--> Также c использованием `kubelet` возможно отслеживание работоспособности контейнеров, которые были запущены с его помощью.

#### Controller-manager

Компонент, отвечающий за запуск контроллеров, которые определяют текущее состояние системы.
Каждый контроллер представляет собой отдельный процесс, и для упрощения все такие процессы скомпилированы в один двоичный файл и выполняются в одном процессе.
Эти контроллеры включают:

* контроллер узла (Node Controller): уведомляет и реагирует на сбои узла;
* контроллер репликации (Replication Controller): поддерживает правильное количество pods для каждого объекта контроллера репликации в системе;
* контроллер конечных точек (Endpoints Controller): заполняет объект конечных точек (Endpoints), то есть связывает сервисы (Services) и pods;
* контроллеры учетных записей и токенов (Account & Token Controllers): создают стандартные учетные записи и токены доступа API для новых пространств имен.

<!-- ### Аутентификация

Единая система аутентификации обеспечивает вам безопасный доступ ко всем веб-интерфейсом.

Чтобы взаимодействовать с API:

сгенерируйте персональный файл конфигурации через специальный веб-интерфейс самообслуживания (kubeconfig-generator) и используйте с CLI и другими инструментами (werf, helm, Lens и др.).
Платформа предлагает гибкость в управлении учетными записями:

интеграция со внешними системами (OID, LDAP, Github, Github);
использование внутреннего каталога пользователей.
Для автоматизированных задач:

1.Настройте аутентификацию через служебные учетные записи с JWT ключом (service account).

Также доступны временные пользователи с [ограниченным временем доступа](ссылка на раздел Авторизации Ограничение доступа), обеспечивающие гибкость и безопасность при необходимости временного доступа.

https://deckhouse.ru.test.flant.com/documentation/v1/modules/150-user-authn/

Создание пользователя и пользовательская аутентификация (Проверка подлинности пользователей и предоставление им доступа к ресурсам. Обычно это включает создание и управление пользователями, ролями и авторизациями в кластере с использованием стандартных механизмов аутентификации, таких как LDAP, OAuth, SAML или x509 сертификаты).
Авторизация ресурсов Предоставление пользователям прав на доступ к ресурсам в кластере - подам, сервисам через RBAC и вебхук.
Шифрование Шифрование секретов: пароли, токены и ключи API. Всякие Public Key Infrastructure.
Контроль доступа RBAC Управление доступом к ресурсам на основе предопределенных ролей, назначаемых пользователям.
Конфигурации модуля аутентификации
Аутентификация на основе токенов
(Способ аутентификации запросов API заключается в использовании bearer заголовка, содержащего действительный токен JWT для учетной записи службы. Как и в случае с пользователями, разные учетные записи служб будут иметь разные уровни доступа. Учетная запись службы имеет токен.Этот токен хранится как Secret и может быть прочитан как secret. Этот токен необходимо использовать для аутентификации стороннего приложения на сервере API.

Примерный сценарий Аутентификация на основе токенов:

Cоздайте учетную запись службы ДКП с именем serviceaccount.

Определите разрешение на аутентификацию для всего кластера (например, для get, list, watch pods, services, namespaces, deployments, jobs) с помощью CluserRole. ClusterRole будет присвоено имя учетной записи службы через ClusterRoleBinding. Токен внутри учетной записи службы может использоваться для аутентификации/авторизации сервера API. Можно вызывать API-интерфейсы сервера DropApp API с помощью этого токена (с bearer заголовком) и получить доступ к ресурсам (например, pods, services, namespaces и т.д.).)

### Авторизация

https://deckhouse.ru.test.flant.com/documentation/v1/modules/140-user-authz/

Роли и права доступа
Ограничения доступа
ДКП - это API-first система, где управление на 100% происходит через API (сценарий управления API).

Управление уровнем доступа к проектам происходит также на уровне API, а все последующие интерфейсы, такие как CLI и web, функционируют в соответствии с этими правами доступа (сценарии соответствующие описать).

Чиобы настроить права доступа к проектам, основанные либо на стандартном RBAC Kubernetes, либо на предустановленных ролях (User, Privileged User, Editor, Admin):

Все это обеспечивает гибкость в определении, как администраторы будут видеть и взаимодействовать с проектами на платформе - начиная от простого просмотра и до выполнения сложных операций. -->

## Установка и сопровождение

Платформа Deckhouse облегчает процесс установки и обновления. Она поддерживает установку в любых условиях, автоматически настраивает все функции и обеспечивает масштабирование для выполнения практически любых задач. Функции мониторинга и управления ресурсами помогают администраторам эффективно контролировать систему и рано обнаруживать возможные проблемы. Deckhouse также уделяет особое внимание безопасности, гарантируя защиту всех компонентов платформы и обеспечивая безопасное взаимодействие между ними. Все эти функции делают Deckhouse незаменимым инструментом для администраторов, позволяя им обеспечивать непрерывное и безопасное функционирование существующей IT-инфраструктуры.

### Настройка окружения

Настраиваем облака и железо.

### Установка платформы

Deckhouse отличается мощным инфраструктурно-агностическим фундаментом, позволяя установку во всех востребованных условиях, преобразуя “любую” инфраструктуру в “стандартную”. Все параметры установки определены в одном конфигурационном файле, что гарантирует воспроизводимость и упрощает автоматизацию. Он совместим со всеми ведущими ОС, включая open-source и ФСТЭК-сертифицированные. Установка осуществляется на ""голую"" ОС без дополнительных зависимостей, с автоматической настройкой всех функций платформы. Deckhouse поддерживает все широко применяемые системы виртуализации, публичные и частные облака, предоставляя референсные архитектуры. В платформе просто и удобно достигается глубокая интеграция с сетью компании, обеспечивая связь приложений с внешним миром и другими корпоративными системами. Кроме того, Deckhouse поддерживает интеграцию с некоторыми системами хранения данных, предоставляя возможности хранения для приложений на платформе (без необходимости в развертывании дополнительной SDS внутри платформы). Возможна установка в изолированных средах (air-gapped), причём LTS-версия доступна на физических носителях.

### Обновление платформы

Обновление платформы
Deckhouse предоставляет целый спектр возможностей управления запуском обновлений: от автоматического запуска при поступлении обновления в канал (early-access, stable, lts), включая такие инструменты как ""окна обновлений"", до полностью контролируемого режима, где обновления утверждаются вручную. Сам процесс обновления осуществляется полностью автоматически, включая как инфраструктурные компоненты и ПО на узлах, так и всю высокоуровневую функциональность (платформа Kubernetes, аутентификация, мониторинг, журналирование, управление сертификатами, балансировка трафика, безопасность, виртуализация и др.). Deckhouse обеспечивает стабильность работы платформы и приложений при обновлениях, при условии соблюдения последними рекомендаций отказаустойчевости: наличие более чем одной реплики и указания максимального количества недоступных реплик (PDB). Для обеспечения обратной совместимости Deckhouse применяет трёхкомпонентный подход: сохранение обратной совместимости между версиями, правила устаревания (deprecation policy) и автоматическую блокировку запуска обновлений при возникновении проблем. Всегда поддерживается не менее 4 версий Kubernetes. Платформа поддерживает обновления в изолированных средах и при помощи твердых носителей. Все эти возможности гарантируют предсказуемость обновления, обеспечивая баланс между свежестью и стабильностью.	

#### Окна и каналы обновлений

Обновление платформы
Deckhouse предоставляет целый спектр возможностей управления запуском обновлений: от автоматического запуска при поступлении обновления в канал (early-access, stable, lts), включая такие инструменты как ""окна обновлений"", до полностью контролируемого режима, где обновления утверждаются вручную. Сам процесс обновления осуществляется полностью автоматически, включая как инфраструктурные компоненты и ПО на узлах, так и всю высокоуровневую функциональность (платформа Kubernetes, аутентификация, мониторинг, журналирование, управление сертификатами, балансировка трафика, безопасность, виртуализация и др.). Deckhouse обеспечивает стабильность работы платформы и приложений при обновлениях, при условии соблюдения последними рекомендаций отказаустойчевости: наличие более чем одной реплики и указания максимального количества недоступных реплик (PDB). Для обеспечения обратной совместимости Deckhouse применяет трёхкомпонентный подход: сохранение обратной совместимости между версиями, правила устаревания (deprecation policy) и автоматическую блокировку запуска обновлений при возникновении проблем. Всегда поддерживается не менее 4 версий Kubernetes. Платформа поддерживает обновления в изолированных средах и при помощи твердых носителей. Все эти возможности гарантируют предсказуемость обновления, обеспечивая баланс между свежестью и стабильностью.		

### Высокая надежность и доступность

Высокая надежность и доступность платформы
Все компоненты платформы Deckhouse полностью зарезервированы, позволяя выборочно или полностью отключать резервирование для экономии ресурсов. В случае выхода из строя узлов, платформа автоматически восстанавливается перераспределяя нагрузку по доступным и дожидается появления новых узлов, чтобы обеспечить непрерывную работу системы. QoS-приоритеты гарантируют стабильное функционирование критически важных элементов платформы даже при нехватке ресурсов. При этом, платформа позволяет размещать разные типы нагрузок (как разные свои компоненты, так и определенные приложения пользователей) на выделенные группы узлов, реализуя принцип ""разделяй и властвуй"". Функционал chaos-monkey позволяет проводить тестирование системы на устойчивость, автоматически ""отключая"" случайные узлы. Возможности Multi AZ, Multi Region и мультикластер (объединение кластеров) позволяют обеспечить устойчивость к катастрофам. Все это дает администраторам возможность гарантировать своим пользователям высокий уровень доступности как самой платформы и ее сервисов, так и приложений пользователей.

#### Катастрофоустойчивость

##### Использование нескольких зон доступности (Multi AZ)
Платформа Deckhouse предоставляет возможность распределения узлов кластера по зонам доступности. В большинстве облачных решений этот процесс происходит автоматически, однако при работе вне облаков, потребуется явная маркировка узлов с помощью лейблов с указанимем идентификатора зоны. Администраторы платформы должны гарантировать наличие маркировки. При наличии у приложения возможности работать в нескольких репликах, оно будет устойчивым к катастрофам, продолжая работать даже при выходе из строя целой зоны в ЦОД или облаке.

##### Использование нескольких регионов (Multi Region)
Платформа Deckhouse предоставляет возможность распределения узлов кластера по разным географическим регионам. Для этого необходима маркировка узлов специальными лейблами и наличие стабильных каналов связи между регионами. Администраторы платформы должны гарантировать наличие на каждом узле лейбла, указывающего в каком регионе он расположен. Отличие этой возможности от Multi AZ заключается в том, что она обычно подходит только для ситуаций, когда приложения могут функционировать в каждом регионе автономно (не обращаясь к другим регионам), например, как кеш-сервера для CDN или узлы распределенного мониторинга. Рекомендуем уточнить у компании Флант применимость для вашей задачи. Эта функция повышает устойчивость приложений к катастрофам, обеспечивая их работоспособность даже при выходе из строя целого ЦОД или целого облака.

##### Мультикластер (объединение нескольких кластеров)
Платформа Deckhouse выходит за рамки поддержки Multi AZ и Multi Region, предлагая инструментарий для объединения нескольких кластеров. Благодаря этой возможности, при отсутствии доступных экземпляров приложения в одном кластере, внешний или внутренний трафик будет автоматически перенаправлен в другой кластер с доступной копией. Это достигается как с помощью встроенного service mesh на основе Istio, так и без его участия благодаря сетевому функционалу платформы на базе Cilium. Для включения функции мультикластер необходимо создать по кластеру Deckhouse в каждом регионе, а после объединить их используя декларативный API. Для успешного внедрения необходимо, чтобы ваше приложение могло работать параллельно в разных регионах, а так же наличие стабильного сетевого канала между регионами. Правильно настроенная система обеспечивает глобальную катастрофоустойчивость приложения, гарантируя его бесперебойную работу даже при выходе из строя целого региона или облака.					

#### Подготовка к production

Приведенные ниже рекомендации могут быть неактуальны для тестового кластера или кластера разработки, но важны для production-кластера.

##### Канал и режим обновлений

{% alert %}
Используйте канал обновлений `Early Access` или `Stable`. Установите [окно автоматических обновлений](/documentation/v1/modules/002-deckhouse/usage.html#конфигурация-окон-обновлений) или [ручной режим](/documentation/v1/modules/002-deckhouse/usage.html#ручное-подтверждение-обновлений).
{% endalert %}

Выберите [канал обновлений]( /documentation/v1/deckhouse-release-channels.html) и [режим обновлений](/documentation/v1/modules/002-deckhouse/configuration.html#parameters-releasechannel), который соответствует вашим ожиданиям. Чем стабильнее канал обновлений, тем позже до него доходит новая функциональность.

По возможности используйте разные каналы обновлений для кластеров. Для кластера разработки используйте менее стабильный канал обновлений, нежели для тестового или stage-кластера (pre-production-кластер).

Мы рекомендуем использовать канал обновлений `Early Access` или `Stable` для production-кластеров. Если в production-окружении более одного кластера, предпочтительно использовать для них разные каналы обновлений. Например, `Early Access` для одного, а `Stable` — для другого. Если использовать разные каналы обновлений по каким-либо причинам невозможно, рекомендуется устанавливать разные окна обновлений.

{% alert level="warning" %}
Даже в очень нагруженных и критичных кластерах не стоит отключать использование канала обновлений. Лучшая стратегия — плановое обновление. В инсталляциях Deckhouse, которые не обновлялись полгода или более, могут присутствовать ошибки. Как правило, эти ошибки давно устранены в новых версиях. В этом случае оперативно решить возникшую проблему будет непросто.
{% endalert %}

Управление [окнами обновлений](/documentation/v1/modules/002-deckhouse/configuration.html#parameters-update-windows) позволяет планово обновлять релизы Deckhouse в автоматическом режиме в периоды «затишья», когда нагрузка на кластер далека от пиковой.

##### Версия Kubernetes

{% alert %}
Используйте автоматический [выбор версии Kubernetes](/documentation/v1/installing/configuration.html#clusterconfiguration-kubernetesversion) либо установите версию явно.
{% endalert %}

В большинстве случаев предпочтительно использовать автоматический выбор версии Kubernetes. В Deckhouse такое поведение установлено по умолчанию, но его можно изменить с помощью параметра [kubernetesVersion](/documentation/v1/installing/configuration.html#clusterconfiguration-kubernetesversion). Обновление версии Kubernetes в кластере не оказывает влияния на приложения и проходит [последовательно и безопасно](/documentation/v1/modules/040-control-plane-manager/#управление-версиями).

Если указан автоматический выбор версии Kubernetes, Deckhouse может обновить версию Kubernetes в кластере при обновлении релиза Deckhouse (при обновлении минорной версии). Когда версия Kubernetes явно прописана в параметре [kubernetesVersion](/documentation/v1/installing/configuration.html#clusterconfiguration-kubernetesversion), очередное обновление Deckhouse может завершиться неудачей, если окажется, что используемая в кластере версия Kubernetes более не поддерживается.

Если приложение использует устаревшие версии ресурсов или требует конкретной версии Kubernetes по какой-либо другой причине, проверьте, что эта версия [поддерживается](/documentation/v1/supported_versions.html), и [установите ее явно](/documentation/v1/deckhouse-faq.html#как-обновить-версию-kubernetes-в-кластере).  

##### Требования к ресурсам

{% alert %}
Выделяйте от 4 CPU / 8 ГБ RAM на инфраструктурные узлы. Для мастер-узлов и узлов мониторинга используйте быстрые диски.
{% endalert %}

Рекомендуются следующие минимальные ресурсы для инфраструктурных узлов в зависимости от их роли в кластере:
- **Мастер-узел** — 4 CPU, 8 ГБ RAM; быстрый диск с не менее чем 400 IOPS.  
- **Frontend-узел** — 2 CPU, 4 ГБ RAM;
- **Узел мониторинга** (для нагруженных кластеров) — 4 CPU, 8 ГБ RAM; быстрый диск.
- **Системный узел**:
  - 2 CPU, 4 ГБ RAM — если в кластере есть выделенные узлы мониторинга;
  - 4 CPU, 8 ГБ RAM, быстрый диск — если в кластере нет выделенных узлов мониторинга.

Примерный расчет ресурсов, необходимых для кластера:
- **Типовой кластер**: 3 мастер-узла, 2 frontend-узла, 2 системных узла. Такая конфигурация потребует **от 24 CPU и 48 ГБ RAM**, плюс быстрые диски с 400+ IOPS для мастер-узлов.
- **Кластер с повышенной нагрузкой** (с выделенными узлами мониторинга): 3 мастер-узла, 2 frontend-узла, 2 системных узла, 2 узла мониторинга. Такая конфигурация потребует **от 28 CPU и 64 ГБ RAM**, плюс быстрые диски с 400+ IOPS для мастер-узлов и узлов мониторинга.
- Для компонентов Deckhouse желательно выделить отдельный [storageClass](/documentation/v1/deckhouse-configure-global.html#parameters-storageclass) на быстрых дисках.
- Добавьте к этому ресурсы, необходимые для запуска полезной нагрузки.

##### Особенности конфигурации

А нужен ли раздел выше?

###### Мастер-узлы

{% alert %}
В кластере должно быть три мастер-узла с быстрыми дисками 400+ IOPS.
{% endalert %}

Всегда используйте три мастер-узла — такое количество обеспечит отказоустойчивость и позволит безопасно выполнять обновление мастер-узлов. В большем числе мастер-узлов нет необходимости, а два узла не обеспечат кворума.

Конфигурация мастер-узлов для облачных кластеров настраивается в параметре [masterNodeGroup](/documentation/v1/modules/030-cloud-provider-aws/cluster_configuration.html#awsclusterconfiguration-masternodegroup).

Может быть полезно:
- [Как добавить мастер-узлы в облачном кластере...](/documentation/v1/modules/040-control-plane-manager/faq.html#как-добавить-master-узлы-в-облачном-кластере-single-master-в-multi-master)
- [Работа со статическими узлами...](/documentation/latest/modules/040-node-manager/#работа-со-статическими-узлами)

###### Frontend-узлы

{% alert %}
Выделите два или более frontend-узла.

Используйте inlet `LoadBalancer` для OpenStack и облачных сервисов, где возможен автоматический заказ балансировщика (Yandex Cloud, VK Cloud, Selectel Cloud, AWS, GCP, Azure и т. п.). Используйте inlet `HostPort` с внешним балансировщиком для bare metal или vSphere.
{% endalert %}

Frontend-узлы балансируют входящий трафик. На них работают Ingress-контроллеры. У [NodeGroup](/documentation/v1/modules/040-node-manager/cr.html#nodegroup) frontend-узлов установлен label `node-role.deckhouse.io/frontend`. Читайте подробнее про [выделение узлов под определенный вид нагрузки...](/documentation/v1/#выделение-узлов-под-определенный-вид-нагрузки)

Используйте более одного frontend-узла. Frontend-узлы должны выдерживать трафик при отказе как минимум одного frontend-узла.

Например, если в кластере два frontend-узла, то каждый frontend-узел должен справляться со всей нагрузкой на кластер в случае, если второй выйдет из строя. Если в кластере три frontend-узла, то каждый frontend-узел должен выдерживать увеличение нагрузки как минимум в полтора раза.

Выберите [тип inlet'а](/documentation/v1/modules/402-ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-inlet) (он определяет способ поступления трафика).  

При развертывании кластера с помощью Deckhouse в облачной инфраструктуре, в которой поддерживается заказ балансировщиков (например, решения на базе OpenStack, сервисы Yandex Cloud, VK Cloud, Selectel Cloud, AWS, GCP, Azure и т. п.), используйте inlet `LoadBalancer` или `LoadBalancerWithProxyProtocol`.

В средах, в которых автоматический заказ балансировщиков недоступен (в bare-metal-кластерах, vSphere, некоторых решениях на базе OpenStack), используйте inlet `HostPort` или `HostPortWithProxyProtocol`. В этом случае можно либо добавить несколько A&#8209;записей в DNS для соответствующего домена, либо использовать внешний сервис балансировки нагрузки (например, взять решения от Cloudflare, Qrator или настроить metallb).

{% alert level="warning" %}
Inlet `HostWithFailover` подходит для кластеров с одним frontend-узлом. Он позволяет сократить время недоступности Ingress-контроллера при его обновлении. Такой тип inlet'а подойдет, например, для важных сред разработки, но **не рекомендуется для production**.
{% endalert %}

Алгоритм выбора inlet'а:

![Алгоритм выбора inlet'а]({{ assets["guides/going_to_production/ingress-inlet-ru.svg"].digest_path }})

###### Узлы мониторинга

{% alert %}
Для нагруженных кластеров выделите два узла мониторинга с быстрыми дисками.
{% endalert %}

Узлы мониторинга служат для запуска Grafana, Prometheus и других компонентов мониторинга. У [NodeGroup](/documentation/v1/modules/040-node-manager/cr.html#nodegroup) узлов мониторинга установлен label `node-role.deckhouse.io/monitoring`.

В нагруженных кластерах со множеством алертов и большими объемами метрик под мониторинг рекомендуется выделить отдельные узлы. Если этого не сделать, компоненты мониторинга будут размещены на [системных узлах](#системные-узлы).

При выделении узлов под мониторинг важно, чтобы на них были быстрые диски. Для этого можно привязать `storageClass` на быстрых дисках ко всем компонентам Deckhouse (глобальный параметр [storageClass](/documentation/v1/deckhouse-configure-global.html#parameters-storageclass)) или выделить отдельный `storageClass` только для компонентов мониторинга (параметры [storageClass](/documentation/v1/modules/300-prometheus/configuration.html#parameters-storageclass) и [longtermStorageClass](/documentation/v1/modules/300-prometheus/configuration.html#parameters-longtermstorageclass) модуля `prometheus`).

###### Системные узлы

{% alert %}
Выделите два системных узла.
{% endalert %}

Системные узлы предназначены для запуска модулей Deckhouse. У [NodeGroup](/documentation/v1/modules/040-node-manager/cr.html#nodegroup) системных узлов установлен label `node-role.deckhouse.io/system`.

Выделите два системных узла. В этом случае модули Deckhouse будут работать на них, не пересекаясь с пользовательскими приложениями кластера. Читайте подробнее про [выделение узлов под определенный вид нагрузки...](/documentation/v1/#выделение-узлов-под-определенный-вид-нагрузки).

Компонентам Deckhouse желательно выделить быстрые диски (глобальный параметр [storageClass](/documentation/v1/deckhouse-configure-global.html#parameters-storageclass)).

##### Уведомление о событиях мониторинга

{% alert %}
Настройте отправку алертов через [внутренний](/documentation/v1/modules/300-prometheus/faq.html#как-добавить-alertmanager) Alertmanager или подключите [внешний](/documentation/v1/modules/300-prometheus/faq.html#как-добавить-внешний-дополнительный-alertmanager).
{% endalert %}

Мониторинг будет работать сразу после установки Deckhouse, однако для production этого недостаточно. Чтобы получать уведомления об инцидентах, настройте [встроенный](/documentation/v1/modules/300-prometheus/faq.html#как-добавить-alertmanager) в Deckhouse Alertmanager или [подключите свой](/documentation/v1/modules/300-prometheus/faq.html#как-добавить-внешний-дополнительный-alertmanager) Alertmanager.

С помощью custom resource [CustomAlertmanager](/documentation/v1/modules/300-prometheus/cr.html#customalertmanager) можно настроить отправку уведомлений на [электронную почту](/documentation/v1/modules/300-prometheus/cr.html#customalertmanager-v1alpha1-spec-internal-receivers-emailconfigs), в [Slack](/documentation/v1/modules/300-prometheus/cr.html#customalertmanager-v1alpha1-spec-internal-receivers-slackconfigs), в [Telegram](/documentation/v1/modules/300-prometheus/usage.html#отправка-алертов-в-telegram), через [webhook](/documentation/v1/modules/300-prometheus/cr.html#customalertmanager-v1alpha1-spec-internal-receivers-webhookconfigs), а также другими способами.

##### Сбор логов

{% alert %}
[Настройте](/documentation/v1/modules/460-log-shipper/) централизованный сбор логов.
{% endalert %}

Настройте централизованный сбор логов с системных и пользовательских приложений с помощью модуля [log-shipper](/documentation/v1/modules/460-log-shipper/).

Достаточно создать custom resource с описанием того, *что нужно собирать*: [ClusterLoggingConfig](/documentation/v1/modules/460-log-shipper/cr.html#clusterloggingconfig) или [PodLoggingConfig](/documentation/v1/modules/460-log-shipper/cr.html#podloggingconfig); кроме того, необходимо создать custom resource с данными о том, *куда отправлять* собранные логи: [ClusterLogDestination](/documentation/v1/modules/460-log-shipper/cr.html#clusterlogdestination).

Дополнительная информация:
- [Пример для Grafana Loki](/documentation/v1/modules/460-log-shipper/examples.html#чтение-логов-из-всех-подов-кластера-и-направление-их-в-loki)
- [Пример для Logstash](/documentation/v1/modules/460-log-shipper/examples.html#простой-пример-logstash)
- [Пример для Splunk](/documentation/v1/modules/460-log-shipper/examples.html#пример-интеграции-со-splunk)

##### Резервное копирование

{% alert %}
Настройте [резервное копирование etcd](/documentation/v1/modules/040-control-plane-manager/faq.html#как-сделать-бэкап-etcd). Напишите план восстановления.
{% endalert %}

Обязательно настройте [резервное копирование данных etcd](/documentation/v1/modules/040-control-plane-manager/faq.html#как-сделать-бэкап-etcd). Это будет ваш последний шанс на восстановление кластера в случае самых неожиданных событий. Храните резервные копии как можно *дальше* от кластера.  

Резервные копии не помогут, если они не работают или вы не знаете, как их использовать для восстановления. Рекомендуем составить [план восстановления на случай аварии](https://habr.com/ru/search/?q=%5BDRP%5D&target_type=posts&order=date) (Disaster Recovery Plan), содержащий конкретные шаги и команды по развертыванию кластера из резервной копии.

Этот план должен периодически актуализироваться и проверяться учебными тревогами.

##### Сообщество

{% alert %}
Следите за новостями проекта в [Telegram](https://t.me/deckhouse_ru).
{% endalert %}

Вступите в [сообщество](https://deckhouse.ru/community/about.html), чтобы быть в курсе важных изменений и новостей. Вы сможете общаться с людьми, занятыми общим делом. Это позволит избежать многих типичных проблем.

Команда Deckhouse знает, каких усилий требует организация работы production-кластера в Kubernetes. Мы будем рады, если Deckhouse позволит вам реализовать задуманное. Поделитесь своим опытом и вдохновите других на переход в Kubernetes.

### Управление вычислительными ресурсами платформы

Управление (вычислительными) ресурсами платформы
Платформа Deckhouse полностью автоматизирует распределение ресурсов с помощью продвинутого планировщика из Kubernetes. Через простые интерфейсы, администраторы задают квоты, определяя доступные ресурсы, в то время как пользователи указывают желаемые ресурсы и число реплик (см. подробнее ""Разделение ресурсов платформы""). Панели графиков отображают утилизацию ресурсов прикладными приложениями, помогая администраторам оценить утилизацию ресурсов платформы пользователями. Для оптимизации распределения нагрузки между узлами в Deckhouse внедрен перепланировщик (descheduler), корректирующий размещение pod’ов на узлах на основе установленных правил.		

### Масштабирование платформы

Все значимые компоненты масштабируются горизонтально, что обеспечивает высокий потенциал масштабирования платформы Deckhouse: гарантируется возможность расширения кластера платформы до 1,000 серверов и 50,000 pod'ов. В том числе **горизонтально масштабируются ядро платформы и инфраструктурно-агностический слой (за исключением control-plane), сетевая подсистема (SDN), интеграция с СХД и балансировщики и маршрутизаторы трафика**. **Вертикальное масштабирование применяется только к control-plane, супер-оператору, и платформе мониторинга**, при этом для двух последних планируется перейти на горизонтально-масштабируемый вариант. Масштабирование вычислительных мощностей обеспечивает удобное и независимое управление каждой группой узлов, с автоматическим масштабированием как в облачных так и статических кластерах (последнее в разработке). Также предусмотрены **инструменты для планирования мощностей**. На практике, платформа позволяет добиться почти бесконечного масштабирования через создание дополнительных кластеров.		
