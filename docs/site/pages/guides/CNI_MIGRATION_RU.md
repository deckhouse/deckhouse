---
title: Переключение CNI в кластере
permalink: ru/guides/cni-migration.html
description: Инструкция по переключению (миграции) CNI в кластере Deckhouse.
lang: ru
layout: sidebar-guides
---

Данный документ описывает процедуру смены сетевого плагина (CNI) в кластере Kubernetes под управлением Deckhouse. Используемый в Deckhouse инструмент позволяет выполнить миграцию (например, с Flannel на Cilium) с минимальным простоем приложений и без полной перезагрузки узлов кластера.

{% alert level="danger" %}

* Инструмент НЕ предназначен для переключения на любой (сторонний) CNI.
* В процессе миграции автоматически будет включен модуль целевого CNI, который предварительно должен быть настроен пользователем/администратором.

{% endalert %}

{% alert level="warning" %}

* В процессе миграции произойдет перезапуск всех pods в кластере, использующих сеть (в PodNetwork), созданную текущим CNI. Это вызовет перерыв в доступности сервисов. С целью минимизации рисков потери критичных данных, крайне рекомендуется перед проведением работ остановить работу наиболее критичных сервисов самостоятельно.
* Рекомендуется проводить работы в согласованное технологическое окно.
* Перед проведением работ необходимо отключить внешние системы управления кластером (CI/CD, GitOps, ArgoCD и т.д.), которые могут конфликтовать с процессом (например, пытаться восстановить удаленные pods раньше времени или откатывать настройки).

{% endalert %}

## Способ 1: Использование утилиты `d8`

Утилита `d8` предоставляет удобный интерфейс для управления процессом миграции.

### 1. Запуск миграции

Для начала процесса выполните команду `switch`, указав целевой CNI (например, `cilium`, `flannel` или `simple-bridge`).

```bash
d8 cni-migration switch --to-cni cilium
```

Эта команда создаст необходимый ресурс в кластере и запустит контроллер миграции. Deckhouse автоматически развернет необходимые компоненты: Менеджер (Manager) и Агенты (Agents) в namespace `d8-system`.

### 2. Наблюдение за прогрессом

Чтобы следить за ходом выполнения в реальном времени, используйте команду ( которая запустится автоматически при старте миграции):

```bash
d8 cni-migration watch
```

Вы увидите динамический интерфейс со следующей информацией:

* **Текущая фаза:** Что именно происходит в данный момент (например, `CleaningNodes` или `RestartingPods`).
* **Прогресс:** Список успешно завершенных этапов и текущий статус ожидания действий в кластере.
* **Ошибки:** Если на каком-то узле возникнет проблема, она будет отображена в списке `Failed Nodes`.

Основные фазы процесса:

1. **Preparing:** Проверка запроса и ожидание готовности среды (например, отключение webhooks).
2. **WaitingForAgents:** Ожидание запуска агентов миграции на всех узлах.
3. **EnablingTargetCNI:** Включение модуля целевого CNI в конфигурации Deckhouse.
4. **DisablingCurrentCNI:** Выключение модуля текущего CNI.
5. **CleaningNodes:** Агенты очищают сетевые настройки текущего CNI на узлах.
6. **WaitingTargetCNI:** Ожидание готовности pods нового CNI (DaemonSet).
7. **RestartingPods:** Перезапуск прикладных pods для переключения их на новую сеть.
8. **Completed:** Миграция успешно завершена.

### 3. Завершение и очистка

После того как статус миграции перейдет в `Succeeded`, необходимо удалить ресурсы миграции (контроллеры и агенты), чтобы они не потребляли ресурсы кластера.

```bash
d8 cni-migration cleanup
```

## Способ 2: Ручное управление (через kubectl)

У пользователя есть возможность управлять миграцией напрямую через Kubernetes API.

### 1. Запуск миграции

Создайте манифест `cni-migration.yaml` с указанием целевого CNI:

```yaml
---
apiVersion: network.deckhouse.io/v1alpha1
kind: CNIMigration
metadata:
  name: migration-to-cilium
spec:
  targetCNI: cilium
```

Примените его в кластере:

```bash
kubectl create -f cni-migration.yaml
```

### 2. Наблюдение за прогрессом

Отслеживайте статус ресурса `CNIMigration`:

```bash
kubectl get cnimigration migration-to-cilium -o yaml -w
# OR
watch -n 1 "kubectl get cnimigration migration-to-cilium -o yaml"
```

Обращайте внимание на поля:

* `status.phase`: Текущий этап.
* `status.conditions`: Детальная история переходов.
* `status.failedSummary`: Список узлов с ошибками.

Для детальной диагностики конкретного узла можно проверить его локальный ресурс:

```bash
kubectl get cninodemigrations
kubectl get cninodemigration <node-name> -o yaml
```

Для просмотра логов контроллеров миграции в кластере выполните следующие команды:

```bash
kubectl -n d8-system get pods -o wide | grep cni-migration
kubectl -n d8-system logs <node-name>
```

### 3. Завершение и очистка

После успешного завершения (в статусе `CNIMigration` появится условие `Type: Succeeded, Status: True`), удалите ресурс:

```bash
kubectl delete cnimigration migration-to-cilium
```

Это действие даст сигнал Deckhouse удалить все ранее созданные ресурсы в кластере.

## Устранение неполадок

{% alert %}

Инструмент переключения CNI не выполняется оценку сетевой связанности pods и компонентов кластера после миграции CNI в кластере.

{% endalert %}

### Агент не запускается на узле

Проверьте статус DaemonSet `cni-migration-agent` в namespace `d8-system`. Возможно, на узле есть taints, которые не покрыты tolerations агента.

### Узел застрял в фазе CleaningNodes

Проверьте логи pod агента на соответствующем узле:

```bash
kubectl -n d8-system logs <node-name>
```

Возможная причина: невозможность удалить файлы конфигурации CNI из-за прав доступа, зависших процессов, невозможности пройти процедуру проверки Webhooks.

### Pods целевого CNI не стартуют

Если целевой CNI (например, Cilium) находится в статусе `Init:0/1`, проверьте логи его init-контейнера `cni-migration-init-checker`. Он ожидает завершения очистки узла. Если очистка не завершена (см. пункт выше), новый CNI не запустится. В критической ситуации можно отредактировать Daemonset с целью удаления init-контейнера `cni-migration-init-checker`.

### Миграция зависла

Если процесс остановился и не двигается долгое время:

1. Проверьте `failedSummary` в статусе `CNIMigration`.
2. Если есть проблемные узлы, которые невозможно починить (например, узел в статусе NotReady), Вы можете временно удалить этот узел из кластера или попробовать его перезагрузить.
