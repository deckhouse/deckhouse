diff --git a/pkg/kubelet/kubelet.go b/pkg/kubelet/kubelet.go
index 71068f0e7ed..4886979609a 100644
--- a/pkg/kubelet/kubelet.go
+++ b/pkg/kubelet/kubelet.go
@@ -892,8 +892,8 @@ func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
 
 	if kubeDeps.TLSOptions != nil {
 		if kubeCfg.ServerTLSBootstrap && utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) {
-			klet.serverCertificateManager, err = kubeletcertificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, klet.getLastObservedNodeAddresses, certDirectory)
-			if err != nil {
+		klet.serverCertificateManager, err = kubeletcertificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, klet.getLastObservedNodeAddresses, certDirectory)
+		if err != nil {
 				return nil, fmt.Errorf("failed to initialize certificate manager: %w", err)
 			}
 
@@ -905,12 +905,12 @@ func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
 		}
 
 		if klet.serverCertificateManager != nil {
-			kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) {
-				cert := klet.serverCertificateManager.Current()
-				if cert == nil {
-					return nil, fmt.Errorf("no serving certificate available for the kubelet")
-				}
-				return cert, nil
+		kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) {
+			cert := klet.serverCertificateManager.Current()
+			if cert == nil {
+				return nil, fmt.Errorf("no serving certificate available for the kubelet")
+			}
+			return cert, nil
 			}
 		}
 	}
@@ -1039,6 +1039,7 @@ func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
 		NodeRef:                          nodeRef,
 		GetPodsFunc:                      klet.GetActivePods,
 		KillPodFunc:                      killPodNow(klet.podWorkers, kubeDeps.Recorder),
+		GetNodeFunc:                      klet.GetNode,
 		SyncNodeStatusFunc:               klet.syncNodeStatus,
 		ShutdownGracePeriodRequested:     kubeCfg.ShutdownGracePeriod.Duration,
 		ShutdownGracePeriodCriticalPods:  kubeCfg.ShutdownGracePeriodCriticalPods.Duration,
@@ -2068,7 +2069,7 @@ func (kl *Kubelet) SyncPod(ctx context.Context, updateType kubetypes.SyncPodType
 	result := kl.containerRuntime.SyncPod(sctx, pod, podStatus, pullSecrets, kl.crashLoopBackOff)
 	kl.reasonCache.Update(pod.UID, result)
 
-	for _, r := range result.SyncResults {
+		for _, r := range result.SyncResults {
 		if r.Action == kubecontainer.ResizePodInPlace {
 			if r.Error == nil {
 				// The pod was resized successfully, clear any pod resize errors in the PodResizeInProgress condition.
@@ -2272,18 +2273,18 @@ func (kl *Kubelet) SyncTerminatedPod(ctx context.Context, pod *v1.Pod, podStatus
 	}
 	klog.V(4).InfoS("Pod termination unmounted volumes", "pod", klog.KObj(pod), "podUID", pod.UID)
 
-	// This waiting loop relies on the background cleanup which starts after pod workers respond
-	// true for ShouldPodRuntimeBeRemoved, which happens after `SyncTerminatingPod` is completed.
-	if err := wait.PollUntilContextCancel(ctx, 100*time.Millisecond, true, func(ctx context.Context) (bool, error) {
-		volumesExist := kl.podVolumesExist(pod.UID)
-		if volumesExist {
-			klog.V(3).InfoS("Pod is terminated, but some volumes have not been cleaned up", "pod", klog.KObj(pod), "podUID", pod.UID)
+		// This waiting loop relies on the background cleanup which starts after pod workers respond
+		// true for ShouldPodRuntimeBeRemoved, which happens after `SyncTerminatingPod` is completed.
+		if err := wait.PollUntilContextCancel(ctx, 100*time.Millisecond, true, func(ctx context.Context) (bool, error) {
+			volumesExist := kl.podVolumesExist(pod.UID)
+			if volumesExist {
+				klog.V(3).InfoS("Pod is terminated, but some volumes have not been cleaned up", "pod", klog.KObj(pod), "podUID", pod.UID)
+			}
+			return !volumesExist, nil
+		}); err != nil {
+			return err
 		}
-		return !volumesExist, nil
-	}); err != nil {
-		return err
-	}
-	klog.V(3).InfoS("Pod termination cleaned up volume paths", "pod", klog.KObj(pod), "podUID", pod.UID)
+		klog.V(3).InfoS("Pod termination cleaned up volume paths", "pod", klog.KObj(pod), "podUID", pod.UID)
 
 	// After volume unmount is complete, let the secret and configmap managers know we're done with this pod
 	if kl.secretManager != nil {
diff --git a/pkg/kubelet/nodeshutdown/nodeshutdown_manager.go b/pkg/kubelet/nodeshutdown/nodeshutdown_manager.go
index 3113c1db779..14f43367489 100644
--- a/pkg/kubelet/nodeshutdown/nodeshutdown_manager.go
+++ b/pkg/kubelet/nodeshutdown/nodeshutdown_manager.go
@@ -56,6 +56,7 @@ type Config struct {
 	NodeRef                          *v1.ObjectReference
 	GetPodsFunc                      eviction.ActivePodsFunc
 	KillPodFunc                      eviction.KillPodFunc
+	GetNodeFunc                      func() (*v1.Node, error)
 	SyncNodeStatusFunc               func()
 	ShutdownGracePeriodRequested     time.Duration
 	ShutdownGracePeriodCriticalPods  time.Duration
diff --git a/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go b/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go
index bb03c780c23..5c6199a7148 100644
--- a/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go
+++ b/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go
@@ -21,6 +21,7 @@ limitations under the License.
 package nodeshutdown
 
 import (
+	"context"
 	"fmt"
 	"os"
 	"path/filepath"
@@ -38,6 +39,7 @@ import (
 	"k8s.io/kubernetes/pkg/kubelet/metrics"
 	"k8s.io/kubernetes/pkg/kubelet/nodeshutdown/systemd"
 	"k8s.io/kubernetes/pkg/kubelet/prober"
+	"k8s.io/utils/clock"
 )
 
 const (
@@ -70,6 +72,8 @@ type managerImpl struct {
 	dbusCon     dbusInhibiter
 	inhibitLock systemd.InhibitLock
 
+	conditionChecker *conditionChecker
+
 	nodeShuttingDownMutex sync.Mutex
 	nodeShuttingDownNow   bool
 	podManager            *podManager
@@ -106,6 +110,10 @@ func NewManager(conf *Config) Manager {
 			Path: filepath.Join(conf.StateDirectory, localStorageStateFile),
 		},
 	}
+	manager.conditionChecker = &conditionChecker{
+		logger:  conf.Logger,
+		getNode: conf.GetNodeFunc,
+	}
 	manager.logger.Info("Creating node shutdown manager",
 		"shutdownGracePeriodRequested", conf.ShutdownGracePeriodRequested,
 		"shutdownGracePeriodCriticalPods", conf.ShutdownGracePeriodCriticalPods,
@@ -211,7 +219,7 @@ func (m *managerImpl) start() (chan struct{}, error) {
 		return nil, err
 	}
 
-	events, err := m.dbusCon.MonitorShutdown()
+	dbusEvents, err := m.dbusCon.MonitorShutdown()
 	if err != nil {
 		releaseErr := m.dbusCon.ReleaseInhibitLock(m.inhibitLock)
 		if releaseErr != nil {
@@ -220,6 +228,30 @@ func (m *managerImpl) start() (chan struct{}, error) {
 		return nil, fmt.Errorf("failed to monitor shutdown: %v", err)
 	}
 
+	shutdownAfterPostpone := m.conditionChecker.MonitorGracefulShutdownPostpone()
+
+	// Set capacity to 2 to not block our sources.
+	shutdownEvents := make(chan bool, 2)
+	go func() {
+		for {
+			select {
+			case isShuttingDown, ok := <-dbusEvents:
+				if !ok {
+					m.logger.Info("Ended to watching the DBus for shutdown events")
+					close(shutdownEvents)
+					return
+				}
+				m.logger.Info("Got event from DBus", "isShuttingDown", isShuttingDown)
+				shutdownEvents <- isShuttingDown
+			case isShuttingDown := <-shutdownAfterPostpone:
+				// Only one event from shutdownAfterPostpone is expected.
+				m.logger.Info("Got event from condition checker", "isShuttingDown", isShuttingDown)
+				shutdownEvents <- isShuttingDown
+				return
+			}
+		}
+	}()
+
 	stop := make(chan struct{})
 	go func() {
 		// Monitor for shutdown events. This follows the logind Inhibit Delay pattern described on https://www.freedesktop.org/wiki/Software/systemd/inhibit/
@@ -227,10 +259,12 @@ func (m *managerImpl) start() (chan struct{}, error) {
 		// 2. When shutdown(true) event is received, process the shutdown and release the inhibit lock.
 		// 3. When shutdown(false) event is received, this indicates a previous shutdown was cancelled. In this case, acquire the inhibit lock again.
 		for {
+			m.logger.V(1).Info("Blocking read from shutdownEvents")
 			select {
-			case isShuttingDown, ok := <-events:
+			case isShuttingDown, ok := <-shutdownEvents:
 				if !ok {
 					m.logger.Error(err, "Ended to watching the node for shutdown events")
+					m.conditionChecker.Cleanup()
 					close(stop)
 					return
 				}
@@ -245,8 +279,35 @@ func (m *managerImpl) start() (chan struct{}, error) {
 				m.logger.V(1).Info("Shutdown manager detected new shutdown event", "event", shutdownType)
 				if isShuttingDown {
 					m.recorder.Event(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Shutdown manager detected shutdown event")
-			} else {
-				m.recorder.Event(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Shutdown manager detected shutdown cancellation")
+				} else {
+					m.recorder.Event(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Shutdown manager detected shutdown cancellation")
+				}
+
+			// Check if shutdown is postponed by the condition and prevent setting m.nodeShuttingDownNow=true to keep Ready status for the Node.
+			if isShuttingDown {
+				postponeStatus, err := m.conditionChecker.PostponeStatus()
+				if err != nil {
+					m.logger.Error(err, "Read postponed condition", "status", postponeStatus)
+					m.recorder.Eventf(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Fail to check GracefulShutdownPostpone condition: %v", err)
+					break
+				}
+
+				if isPostponed(postponeStatus) {
+					switch postponeStatus {
+					case v1.ConditionTrue:
+						m.logger.V(1).Info("managerImpl.start: Graceful shutdown postponed, d8-shutdown-inhibitor requested shutdown block", "GracefulShutdownPostpone", postponeStatus)
+						m.recorder.Eventf(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Shutdown postponed by d8-shutdown-inhibitor (GracefulShutdownPostpone=%s)", postponeStatus)
+					case v1.ConditionUnknown:
+						m.logger.V(1).Info("managerImpl.start: Graceful shutdown postponed, waiting for d8-shutdown-inhibitor response", "GracefulShutdownPostpone", postponeStatus)
+						m.recorder.Eventf(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Shutdown postponed, waiting for d8-shutdown-inhibitor response (GracefulShutdownPostpone=%s)", postponeStatus)
+					}
+
+					m.conditionChecker.StartMonitor()
+					break
+				}
+
+				m.logger.V(1).Info("managerImpl.start: Graceful shutdown postpone disabled, stopping GracefulShutdownPostpone monitor and proceeding with the shutdown sequence", "GracefulShutdownPostpone", postponeStatus)
+				m.conditionChecker.PauseMonitor()
 			}
 
 			// Clean up memory manager state before shutdown
@@ -254,9 +315,9 @@ func (m *managerImpl) start() (chan struct{}, error) {
 				m.cleanupMemoryManagerState()
 			}
 
-			m.nodeShuttingDownMutex.Lock()
-			m.nodeShuttingDownNow = isShuttingDown
-			m.nodeShuttingDownMutex.Unlock()
+				m.nodeShuttingDownMutex.Lock()
+				m.nodeShuttingDownNow = isShuttingDown
+				m.nodeShuttingDownMutex.Unlock()
 
 				if isShuttingDown {
 					// Update node status and ready condition
@@ -347,3 +408,124 @@ func (m *managerImpl) cleanupMemoryManagerState() {
 
 	m.logger.Info("Successfully removed memory manager state file before shutdown", "path", memoryManagerStatePath)
 }
+
+type conditionChecker struct {
+	logger klog.Logger
+
+	getNode func() (*v1.Node, error)
+
+	events chan bool
+	cancel context.CancelFunc
+
+	actions chan string
+}
+
+func (c *conditionChecker) MonitorGracefulShutdownPostpone() <-chan bool {
+	c.events = make(chan bool, 1)
+	c.actions = make(chan string, 10)
+	ctx, cancel := context.WithCancel(context.Background())
+	c.cancel = cancel
+	go c.checkLoop(ctx)
+	return c.events
+}
+
+func (c *conditionChecker) StartMonitor() {
+	c.actions <- "start"
+}
+
+func (c *conditionChecker) PauseMonitor() {
+	c.actions <- "pause"
+}
+
+func (c *conditionChecker) Cleanup() {
+	close(c.events)
+	c.cancel()
+}
+
+// checkLoop is a main loop that checks GracefulShutdownPostpone condition.
+// The loop is designed with "shutdown cancel" in mind, so it can be paused.
+// If loop is not paused and condition is removed, it sends shutdown event to unlock
+// shutdown sequence in managerImpl.start.
+//
+// TODO We may have a rather long InhibitorsDelayMaxSec, but we should (should we?) let kubelet to do its job,
+// TODO so global timer can be added to unlock and run shutdown sequence anyway some minutes before InhibitorsDelayMaxSec is passed.
+// TODO It is still uncertain if we should implement this timer.
+func (c *conditionChecker) checkLoop(ctx context.Context) {
+	const (
+		checkInterval       = 5 * time.Second
+		progressLogInterval = 20 * time.Second
+	)
+
+	realClock := clock.RealClock{}
+	ticker := realClock.NewTicker(checkInterval)
+	defer ticker.Stop()
+
+	var lastLogTime time.Time
+
+	isStarted := false
+
+	for {
+		select {
+		case action := <-c.actions:
+			switch action {
+			case "start":
+				isStarted = true
+			case "pause":
+				isStarted = false
+			}
+		case <-ticker.C():
+			// Check postpone condition if in the "started" state. If not postponed, send event to unpause shutdown sequence.
+			if isStarted {
+				postponeStatus, err := c.PostponeStatus()
+				if err != nil {
+					c.logger.V(1).Error(err, "Check postpone condition failed, will try later")
+					break
+				}
+
+				switch {
+				case isPostponed(postponeStatus):
+					now := time.Now()
+					if lastLogTime.IsZero() || now.Sub(lastLogTime) > progressLogInterval {
+						lastLogTime = now
+						c.logger.V(1).Info("conditionChecker.checkLoop: Graceful shutdown postponed", "GracefulShutdownPostpone", postponeStatus)
+					}
+				default:
+					c.logger.V(1).Info("conditionChecker.checkLoop: Graceful shutdown postpone condition cleared, continue with shutdown sequence", "GracefulShutdownPostpone", postponeStatus)
+					// Send event to unpause shutdown sequence. Self pause to prevent events spamming.
+					isStarted = false
+					lastLogTime = time.Time{}
+					c.events <- true
+				}
+			}
+		case <-ctx.Done():
+			return
+		}
+	}
+}
+
+// PostponeStatus returns the raw status of the GracefulShutdownPostpone
+func (c *conditionChecker) PostponeStatus() (v1.ConditionStatus, error) {
+	node, err := c.getNode()
+	if err != nil {
+		c.logger.V(1).Error(err, "Get node on shutdown event failed, will try later")
+		return v1.ConditionFalse, fmt.Errorf("get node for postpone condition check: %v", err)
+	}
+
+	if node == nil {
+		return v1.ConditionFalse, fmt.Errorf("get node for postpone condition check: node is nil")
+	}
+
+	var status v1.ConditionStatus
+	for _, cond := range node.Status.Conditions {
+		if cond.Type == "GracefulShutdownPostpone" {
+			status = cond.Status
+			break
+		}
+	}
+
+	return status, nil
+}
+
+func isPostponed(status v1.ConditionStatus) bool {
+	return status == v1.ConditionTrue || status == v1.ConditionUnknown
+}
