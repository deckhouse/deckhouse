---
title: "Модуль metallb"
---

Модуль реализует механизм `LoadBalancer` у сервисов в кластерах bare metal.

Основан на решении [MetalLB](https://metallb.universe.tf/).

## Режим Layer2

В режиме layer2 один узел берет на себя ответственность за предоставление сервиса в локальной сети. С точки зрения сети это выглядит так, как будто узел имеет несколько IP-адресов, назначенных на его сетевой интерфейс.
Под капотом MetalLB отвечает на ARP-запросы для сервисов IPv4 и NDP-запросы для IPv6. Основным преимуществом режима layer2 является его универсальность: он будет работать в любой сети Ethernet, не требуя специального оборудования.

### Поведение при балансировке нагрузки

В режиме layer2 весь трафик поступает на один узел. Оттуда kube-proxy распределяет трафик на все поды сервиса. В этом смысле уровень 2 не реализует балансировщик нагрузки. Скорее, он реализует механизм failover,
чтобы другой узел мог взять на себя управление, если текущий ведущий узел по какой-то причине выйдет из строя. Если ведущий узел по какой-то причине выходит из строя, failover происходит автоматически:
отказавший узел определяется с помощью списка узлов, и в этот момент новые узлы принимают IP-адреса от отказавшего узла.

### Ограничения

Режим layer2 имеет два основных ограничения, о которых вы должны знать: узкое место на одном узле и потенциально медленный failover. Как объяснялось выше, в режиме layer2 один узел, выбранный лидером,
получает весь трафик. Это означает, что пропускная способность входящего трафика вашего сервиса ограничена пропускной способностью одного узла. Это фундаментальное ограничение использования ARP и NDP для перенаправления трафика.
В текущей реализации failover между узлами зависит от настроек вышестоящих роутеров. Когда происходит failover, MetalLB посылает ряд "незапрашиваемых" пакетов уровня 2, чтобы уведомить клиентов о том, что MAC-адрес, связанный со служебным IP, изменился.
Большинство операционных систем правильно обрабатывают "незапрашиваемые" пакеты и быстро обновляют кэш соседей. В этом случае failover происходит в течение нескольких секунд. Однако некоторые системы либо вообще не реализуют такую обработку,
либо имеют ошибочную реализацию, которая задерживает обновление кэша. Все современные версии основных ОС (Windows, Mac, Linux) правильно реализуют failover layer2, поэтому единственная ситуация, когда могут возникнуть проблемы- это старые
или менее распространенные ОС. Чтобы минимизировать влияние failover на глючные клиенты, следует поддерживать старый ведущий узел в рабочем состоянии в течение нескольких минут после смены лидера, чтобы он мог продолжать пересылать трафик
для старых клиентов, пока их кэш не обновится. Во время failover служебные IP-адреса будут недоступны, пока глючные клиенты не обновят свои кэш-записи.

### Сравнение с Keepalived

Режим layer2 в MetalLB имеет много общего с Keepalived, поэтому если вы знакомы с Keepalived, все это должно звучать довольно знакомо. Однако есть и несколько отличий, о которых стоит упомянуть.
Если вы не знакомы с Keepalived, можете пропустить этот раздел. Keepalived использует протокол Virtual Router Redundancy Protocol (VRRP). Экземпляры Keepalived постоянно обмениваются VRRP-сообщениями друг с другом,
как для выбора лидера, так и для того, чтобы заметить, когда этот лидер уходит. MetalLB, с другой стороны, полагается на список участников, чтобы знать, когда узел в кластере больше не доступен, и служебные IP-адреса
с этого узла должны быть перемещены в другое место. Keepalived и MetalLB выглядят одинаково с точки зрения клиента: служебный IP-адрес как бы мигрирует с одной машины на другую, когда происходит failover, а в остальное время все выглядит так,
как будто у машин несколько IP-адресов. Поскольку MetalLB не использует VRRP, на него не распространяются некоторые ограничения этого протокола. Например, ограничение VRRP в 255 балансировщиков нагрузки на сеть не существует в MetalLB.
Вы можете иметь столько IP-адресов балансировки нагрузки, сколько захотите, если в вашей сети есть свободные IP-адреса. MetalLB также требует меньше конфигурации, чем VRRP - например, здесь нет идентификаторов виртуальных маршрутизаторов.
С другой стороны, поскольку MetalLB полагается на список узлов для получения информации о членстве в кластере, он не может взаимодействовать с маршрутизаторами и инфраструктурой VRRP сторонних производителей.
Все работает так, как задумано: MetalLB специально разработан для обеспечения балансировки нагрузки failover в кластере Kubernetes, и в этом сценарии взаимодействие со сторонним программным обеспечением LB не является обязательным.

## Режим BGP

В режиме BGP каждый узел вашего кластера устанавливает пиринговый сеанс BGP с вашими сетевыми маршрутизаторами и использует этот пиринговый сеанс для публикации IP-адресов внешних служб кластера.
Если ваши маршрутизаторы настроены на поддержку multipath, это позволяет добиться истинной балансировки нагрузки: маршруты, опубликованные MetalLB, эквивалентны друг другу, за исключением их nexthop.
Это означает, что маршрутизаторы будут использовать все nexthop вместе и балансировать нагрузку между ними. После того, как пакеты поступают на узел, kube-proxy отвечает за конечный хоп маршрутизации трафика,
чтобы доставить пакеты на конкретный pod в сервисе.

### Поведение балансировки нагрузки

Точное поведение балансировки нагрузки зависит от конкретной модели и конфигурации маршрутизатора, но общим является балансировка по каждому соединению, основанная на хэше пакетов. Что это значит?
Балансировка по соединению означает, что все пакеты для одного сеанса TCP или UDP будут направлены на одну машину в вашем кластере. Распределение трафика происходит только между различными соединениями,
а не для пакетов внутри одного соединения. Это хорошо, потому что распространение пакетов между несколькими узлами кластера привело бы к плохому поведению на нескольких уровнях:
распространение одного соединения по нескольким путям приводит к переупорядочиванию пакетов на конечном узле, что значительно влияет на производительность конечного узла.
Маршрутизация трафика на узле в Kubernetes не гарантирует согласованности между узлами. Это означает, что два разных узла могут решить направить пакеты для одного и того же соединения в разные поды,
что приведет к обрыву соединения. Хеширование пакетов - это способ, с помощью которого высокопроизводительные маршрутизаторы могут без статических данных распределять соединения между несколькими бэкендами.
Для каждого пакета они извлекают некоторые поля и используют их в качестве исходных данных для детерминированного выбора одного из возможных бэкендов. Если все поля одинаковы, будет выбран один и тот же бэкенд.
Точные методы хэширования зависят от аппаратного и программного обеспечения маршрутизатора. Двумя типичными вариантами являются хэширование 3-tuple и 5-tuple. 3-tuple использует (протокол, source-ip, dest-ip) в качестве ключа,
что означает, что все пакеты между двумя уникальными IP будут направляться на один и тот же бэкэнд. Хеширование 5-tuple добавляет порты источника и назначения, что позволяет различным соединениям от одних и тех же клиентов быть
распределенными по кластеру. В целом, предпочтительно закладывать как можно больше энтропии в хэш пакета, то есть использование большего количества полей, как правило, хорошо. Это связано с тем, что увеличение энтропии приближает
нас к "идеальному" состоянию балансировки нагрузки, когда каждый узел получает одинаковое количество пакетов. Мы никогда не сможем достичь такого идеального состояния из-за проблем, перечисленных выше, но что мы можем сделать,
так это попытаться распределить соединения как можно более равномерно, чтобы предотвратить образование узких мест.

### Ограничения

Использование BGP в качестве механизма балансировки нагрузки имеет то преимущество, что вы можете использовать стандартное оборудование маршрутизатора, а не изготовленные на заказ балансировщики нагрузки. Однако это имеет и свои недостатки.
Самым большим недостатком является то, что балансировка нагрузки на основе BGP плохо реагирует на изменения в бэкендах, установленных для адреса. Это означает, что при выходе из строя узла кластера следует ожидать разрыва всех активных соединений
с вашей службой (пользователи увидят сообщение "Connection reset by peer"). Маршрутизаторы на основе BGP реализуют stateless распределение нагрузки. Они назначают данный пакет определенному следующему хопу путем хэширования некоторых полей в заголовке
пакета и используют этот хэш в качестве индекса в массиве доступных бэкендов. Проблема в том, что хэши, используемые в маршрутизаторах, обычно не стабильны, поэтому всякий раз, когда размер набора бэкендов меняется
(например, когда BGP-сессия узла обрывается), существующие соединения будут перехэшированы фактически случайным образом, что означает, что большинство существующих соединений будут внезапно перенаправлены на другой бэкенд,
который не имеет никакого представления о данном соединении. Следствием этого является то, что при любом изменении связки IP→Node для вашей службы, вы должны ожидать одноразового отказа, при котором большинство активных соединений
со службой прерываются. При этом не происходит потери пакетов или блэкхолинга, только одноразовый полный разрыв. В зависимости от того, чем занимаются ваши службы, можно использовать несколько стратегий снижения риска:

Ваши BGP-маршрутизаторы могут иметь возможность использовать более стабильный алгоритм хэширования ECMP. Это иногда называют "устойчивым ECMP" или "устойчивым LAG". Использование такого алгоритма значительно снижает количество затронутых соединений
при изменении набора бэкендов. Привязывайте развертывание служб к определенным узлам, чтобы минимизировать количество узлов, за которыми нужно следить. Планируйте изменения в развертывании сервисов во время минимального трафика, когда большинство
пользователей спят и трафик низкий. Разделите каждый логический сервис на два сервиса Kubernetes с разными IP-адресами и используйте DNS для миграции пользовательского трафика с одного на другой, прежде чем прервать работу drained сервиса.
Добавьте прозрачную логику повторных попыток на стороне клиента, чтобы восстанавливаться после внезапных отключений. Это особенно хорошо работает, если ваши клиенты - это мобильные приложения или одностраничные веб-приложения.
Разместите свои сервисы за ингресс-контроллером. Контроллер входа сам по себе может использовать MetalLB для приема трафика, но наличие stateful layer между BGP и вашими службами означает, что вы можете менять свои службы без опасений.
Вы должны быть осторожны только при изменении развертывания самого ингресс-контроллера (например, при добавлении дополнительных подов NGINX для увеличения масштаба). Примите тот факт, что время от времени будут возникать всплески сброшенных соединений.
