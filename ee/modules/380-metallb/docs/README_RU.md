---
title: "Модуль metallb"
---

Модуль реализует механизм `LoadBalancer` у сервисов в кластерах bare metal.

Основан на решении [MetalLB](https://metallb.universe.tf/).

## Режим layer 2

В режиме layer 2 один узел берет на себя ответственность за предоставление сервиса в локальной сети. С точки зрения сети это выглядит так, как будто узел имеет несколько IP-адресов, назначенных на его сетевой интерфейс.
Под капотом, MetalLB отвечает на ARP-запросы для сервисов IPv4 и NDP-запросы для IPv6. Основным преимуществом режима layer 2 является его универсальность: он будет работать в любой сети Ethernet, не требуя специального оборудования.

### Поведение при балансировке нагрузки

В режиме layer 2 весь трафик поступает на один узел, с которого kube-proxy распределяет трафик на все Pod'ы сервиса. В этом смысле режим layer 2 не реализует балансировку нагрузки. Скорее, он реализует механизм failover, когда какой-то узел берет на себя управление, если ведущий узел по какой-то причине выходит из строя. Если ведущий узел по какой-то причине выходит из строя, то failover происходит автоматически. Отказавший узел определяется с помощью списка узлов, после чего другие узлы принимают IP-адреса отказавшего узла.

### Ограничения

Необходимо учитывать два основных ограничения режима layer 2:
- **Работа через один узел — это узкое место.**

  В режиме layer 2 один узел, выбранный лидером, получает весь трафик. Это означает, что пропускная способность входящего трафика вашего сервиса ограничена пропускной способностью одного узла. Это фундаментальное ограничение использования ARP и NDP для перенаправления трафика.
- **Потенциально медленный failover.**

  В текущей реализации failover между узлами зависит от настроек вышестоящих роутеров. Когда происходит failover, MetalLB посылает ряд "незапрошенных" пакетов уровня 2, чтобы уведомить клиентов о том, что связанный со служебным IP MAC-адрес изменился. Большинство операционных систем правильно обрабатывают "незапрошенные" пакеты и быстро обновляют кэш соседей. В этом случае failover происходит в течение нескольких секунд. Однако некоторые системы либо вообще не реализуют такую обработку, либо имеют ошибочную реализацию, которая задерживает обновление кэша. Все современные версии популярных ОС (Windows, Mac, Linux) правильно реализуют layer 2 failover, поэтому единственная ситуация, когда могут возникнуть проблемы — это старые или нераспространенные ОС.
  Чтобы минимизировать влияние failover на такие клиенты (с ошибочной реализацией), следует поддерживать старый ведущий узел в рабочем состоянии в течение нескольких минут после смены лидера, чтобы он мог продолжать пересылать трафик для старых клиентов (пока их кэш не обновится). Во время failover служебные IP-адреса будут недоступны, пока такие клиенты не обновят свои кэш-записи.

### Сравнение с Keepalived

Режим layer 2 в MetalLB имеет много общего с Keepalived, поэтому если вы знакомы с Keepalived, все это должно звучать довольно знакомо. Однако есть и несколько отличий, о которых стоит упомянуть.

Keepalived использует протокол Virtual Router Redundancy Protocol (VRRP). Экземпляры Keepalived постоянно обмениваются VRRP-сообщениями друг с другом, как для выбора лидера, так и для того, чтобы заметить, когда этот лидер уходит. С другой стороны, MetalLB полагается на список участников, чтобы знать, когда узел в кластере больше не доступен, и служебные IP-адреса с этого узла должны быть перемещены в другое место.

Keepalived и MetalLB выглядят одинаково с точки зрения клиента: когда происходит failover служебный IP-адрес как бы мигрирует с одной машины на другую, а в остальное время все выглядит так,
как будто у машин несколько IP-адресов. Поскольку MetalLB не использует VRRP, на него не распространяются некоторые ограничения этого протокола. Например, в MetalLB нет ограничения VRRP в 255 балансировщиков (load balancer) на сеть. Можно создать любое количество балансировщиков, если в сети есть необходимое количество свободных IP-адресов. MetalLB также требует меньше конфигурации, чем VRRP — например, здесь нет идентификаторов виртуальных маршрутизаторов.

С другой стороны, поскольку MetalLB полагается на список узлов для получения информации о членстве в кластере, он не может взаимодействовать с маршрутизаторами и инфраструктурой VRRP сторонних производителей.
Все работает так, как задумано: MetalLB специально разработан для обеспечения балансировки нагрузки failover в кластере Kubernetes, и в этом сценарии взаимодействие со сторонним программным обеспечением LB не является обязательным.

## Режим BGP

В режиме BGP каждый узел кластера устанавливает пиринговый сеанс BGP с сетевыми маршрутизаторами и использует этот пиринговый сеанс для публикации IP-адресов внешних служб кластера.
Если маршрутизаторы настроены на поддержку multipath, это позволяет добиться настоящей балансировки нагрузки: маршруты, опубликованные MetalLB, эквивалентны друг другу, за исключением их nexthop.
Это означает, что маршрутизаторы будут использовать все nexthop вместе и балансировать нагрузку между ними. После того как пакеты поступают на узел, kube-proxy отвечает за конечный адрес маршрутизации трафика,
чтобы доставить пакеты на конкретный Pod в сервисе.

### Поведение балансировки нагрузки

Точное поведение балансировки нагрузки зависит от конкретной модели и конфигурации маршрутизатора, но общим является балансировка по каждому соединению, основанная на хэше пакетов.

Балансировка по соединению означает, что все пакеты для одного сеанса TCP или UDP будут направлены на одну машину в кластере. Распределение трафика происходит только между различными соединениями,
а не для пакетов внутри одного соединения. Это хорошо, потому что распределение пакетов между несколькими узлами кластера привело бы к ряду проблем:
- Распространение одного соединения по нескольким путям приводит к переупорядочиванию пакетов на конечном узле, что значительно влияет на его производительность.
- Маршрутизация трафика на узле в Kubernetes не гарантирует согласованности между узлами. Это означает, что два разных узла могут решить направить пакеты для одного и того же соединения в разные Pod'ы,
что приведет к обрыву соединения.

Хеширование пакетов — это способ, с помощью которого высокопроизводительные маршрутизаторы могут без статических данных распределять соединения между несколькими бэкендами.
Для каждого пакета они извлекают некоторые поля и используют их в качестве исходных данных для детерминированного выбора одного из возможных бэкендов. Если все поля одинаковы, будет выбран один и тот же бэкенд.
Точные методы хэширования зависят от аппаратного и программного обеспечения маршрутизатора. Двумя типичными вариантами являются хэширование по 3-м элементам и по 5-ти элементам. Хеширование по 3-м элементам использует протокол, IP-адрес источника и IP-адрес получателя для построения ключа хэша. Это означает, что все пакеты между двумя уникальными IP будут направляться на один и тот же бэкэнд. Хеширование по 5-ти элементам использует дополнительно порты источника и назначения, что позволяет различным соединениям от одних и тех же клиентов быть распределенными по кластеру.

В целом, предпочтительно закладывать как можно больше энтропии в хэш пакета, то есть использование большего количества полей, как правило предпочтительней. Это связано с тем, что увеличение энтропии приближает к "идеальному" состоянию балансировки нагрузки, когда каждый узел получает одинаковое количество пакетов. Но из-за перечисленных выше проблем достичь такого идеального состояния невозможно. Все что можно сделать — это попытаться распределить соединения как можно более равномерно, чтобы предотвратить образование узких мест.

### Ограничения

Преимущество использования BGP для балансировки в том, что можно использовать обычные маршрутизаторы, вместо специфичных реализаций балансировщиков. Однако в этом есть и свои недостатки. Самым большим недостатком является то, что балансировка нагрузки на основе BGP плохо реагирует на изменения в бэкендах. При выходе из строя узла кластера разумно ожидать разрыва всех активных соединений с сервисом (пользователи должны получить сообщение "Connection reset by peer"). Маршрутизаторы на основе BGP реализуют stateless-распределение нагрузки. Они вычисляют хэш пакета на основе полей его заголовка, и используют вычисленный хэш в качестве индекса в массиве для определения адреса бэкенда. Проблема в том, что используемые в маршрутизаторах хеши обычно не стабильны. Когда набор бэкендов меняется
(например, когда BGP-сессия узла обрывается), существующие соединения будут перехэшированы фактически случайным образом, что означает, что большинство существующих соединений могут быть внезапно перенаправлены на другой бэкенд, который не имеет никакого представления о данном соединении. Следствием этого является то, что при любом изменении связки IP с узлом для вашей службы, можно ожидать разового отказа, при котором большинство активных соединений со службой прервется. При этом происходит только полный разовый разрыв, и не происходит потери пакетов или blackhol'инга.

Для снижения риска можно использовать несколько стратегий, в зависимости от назначения службы:
- Настроить BGP-маршрутизатор на использование более стабильного алгоритма хэширования — ECMP (часто называется resilient ECMP или resilient LAG). Использование такого алгоритма значительно снижает количество затронутых соединений при изменении набора бэкендов.
- *Привязывать* службы к определенным узлам, чтобы сократить количество узлов, за которыми нужно следить.
- Планировать изменения на время минимального трафика, когда большинство пользователей неактивны и трафик низкий.
- Разделить каждый логический сервис на два Service'а Kubernetes с разными IP-адресами и использовать DNS для миграции пользовательского трафика с одного на другой до того, как произойдет прерывание работы сервиса.
- Добавить прозрачную логику повторных запросов на стороне клиента, для восстановления после внезапных обрывов. Это особенно актуально для мобильных приложений или одностраничных веб-приложений.
- Разместить сервисы за Ingress-контроллером. Ingress-контроллер сам по себе может использовать MetalLB для приема трафика, но наличие промежуточного слоя между BGP и сервисами даст возможность изменять сервисы без опасений обрывов трафика. Нужно быть внимательным только при изменении самого Ingress-контроллера (например, при его масштабировании).
- Принять факт, что время от времени будут возникать обрывы соединений.
