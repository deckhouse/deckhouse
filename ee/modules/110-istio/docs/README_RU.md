---
title: "Модуль istio"
---

Модуль устанавливает [Istio](https://istio.io/).

## Задачи, которые решает Istio

Istio — это реализация подхода Service Mesh. Рекомендуется ознакомиться с [видео](https://www.youtube.com/watch?v=9CUfaeT3T-A), где мы раскрываем этот термин, изучаем архитектуру Istio и оцениваем накладные расходы.

В частности, Istio решает следующие задачи прозрачно для приложений:

* [Mutual TLS](#Mutual-TLS):
  * Взаимная достоверная аутентификация сервисов.
  * Шифрование трафика.
* [Авторизация доступа между сервисами](#Авторизация):
* [Маршрутизация запросов](#Маршрутизация-запросов):
  * Canary-deployment и A/B тестирование — отправить часть запросов на новую версию приложения.
* [Управление балансировкой запросов между Endpoint-ами сервиса](#Управление-балансировкой-запросов-между-Endpoint-ами-сервиса):
  * Circuit Breaker:
    * Временное исключение endpoint-а из балансировки если превышен лимит ошибок.
    * Настройка лимитов на количество TCP-соединений и количество запросов в сторону одного endpoint.
    * Выявление зависших запросов и обрывание их с кодом ошибки.
  * Sticky Sessions:
    * Привязка запросов от конечных пользователей к endpoint сервиса.
  * Locality Failover — отдать предпочтение endpoint-ам в локальной зоне доступности.
* [Observability](Observability):
  * Сбор и визуализация данных для трассировки прикладных запросов с помощью Jaeger.
  * Сбор метрик о трафике между сервисами в Prometheus и визуализация их в Grafana.
  * Визуализация состояния связей между сервисами и состояния служебных компонентов Istio с помощью Kiali.
* [Организация мульти-ЦОД кластера за счёт объединения кластеров в единый Service Mesh (мультикластер)](#мультикластер).
* [Объединение разрозненных кластеров в федерацию с возможностью предоставлять нативный в понимании Service Mesh доступ к избранным сервисам](#Федерация).

## Mutual TLS

Данный механизм — это главный метод взаимной аутентификации сервисов. Принцип основывается на том, что при всех исходящих запросах проверяется серверный сертификат, а при входящих — клиентский. После проверок, sidecar-proxy получают возможность идентифицировать удалённый узел и использовать эти данные для авторизации, либо в прикладных целях.

Каждый сервис получает собственный идентификатор в формате `<TrustDomain>/ns/<Namespace>/sa/<ServiceAccount>`, где `TrustDomain` в нашем случае — это домен кластера. Каждому сервису можно выделять собственный ServiceAccount или использовать стандартный “default”. Полученный идентификатор сервиса можно использовать как в правилах авторизации, так и в прикладных целях. Именно этот идентификатор используется в качестве удостоверяемого имени в TLS-сертификатах.

Каждый кластер имеет [глобальную настройку Mutual TLS](configuration.html#parameters-tlsmode), предусмотрено несколько режимов работы:
* `Off` — Mutual TLS выключен совсем.
* `MutualPermissive` — входящие соединения принимаются как в шифрованном виде, так и в классическом. Исходящие соединения сервисов под управлением Istio устанавливаются в шифрованном виде.
* `Mutual` — как входящие, так и исходящие соединения принимаются и устанавливаются только в шифрованном виде.

Данные настройки можно переопределить на уровне Namespace.

## Авторизация

Управление авторизацией осуществляется с помощью ресурса [AuthorizationPolicy](istio-cr.html#authorizationpolicy). В момент, когда для сервиса создаётся этот ресурс, начинает работать следующий алгоритм принятия решения о судьбе запроса:

* Если под запрос есть правило DENY — запретить.
* Если для данного сервиса нет политик ALLOW — разрешить запрос.
* Если запрос попадает под политику ALLOW — разрешить запрос.
* Все остальные запросы — запретить.

Иными словами, если явно что-то запретить, то работает только запрет. Если же что-то явно разрешить, то теперь разрешены только явно одобренные запросы (запреты, при этом, имеют приоритет).

Для написания правил авторизации можно использовать аргументы:
* идентификаторы сервисов и wildcard на их основе (`mycluster.local/ns/myns/sa/myapp` или `mycluster.local/*`),
* namespace,
* диапазоны IP,
* HTTP-заголовки,
* JWT-токены из прикладных запросов.

## Маршрутизация запросов

Основной ресурс для управления маршрутизацией — [VirtualService](istio-cr.html#virtualservice), он позволяет переопределять судьбу HTTP или TCP-запроса. Доступные аргументы для принятия решения о маршрутизации:
* Host и любые другие заголовки,
* uri,
* Method (GET, POST и пр.),
* лейблы пода или namespace источника запросов,
* dst-IP или dst-порт для не-HTTP запросов.

## Управление балансировкой запросов между Endpoint-ами сервиса

Основной ресурс для управления балансировкой запросов — [DestinationRule](istio-cr.html#destinationrule), он позволяет настроить нюансы исходящих из подов запросов:
* лимиты/таймауты для TCP,
* алгоритмы балансировки между Endpoint-ами,
* правила определения проблем на стороне Endpoint-а для выведения его из балансировки,
* нюансы шифрования.

**Важно!** Все настраиваемые лимиты работают для каждого пода-клиента по отдельности! Если настроить для сервиса ограничение на одно TCP-соединение, а клиентских пода три, то сервис получит три входящих соединения.

## Observability
### Трассировка

Istio позволяет осуществлять сбор трейсов с приложений и инъекцию трассировочных заголовков если таковых нет. При этом важно понимать:
* Если запрос инициирует на сервисе вторичные запросы, то для них необходимо наследовать трассировочные заголовки средствами приложения.
* Jaeger для сбора и отображения трейсов потребуется устанавливать самостоятельно.

### Grafana

В стандартной комплектации с модулем предоставлены дополнительные дашборды:
* для оценки производительности и успешности запросов/ответов между приложениями,
* для оценки работоспособности и нагрузки на control-plane.

### Kiali

Инструмент для визуализации дерева сервисов вашего приложения. Позволяет быстро оценить обстановку в сетевой связности благодаря визуализации запросов и их количественных характеристик непосредственно на схеме.

## Архитектура кластера с включенным Istio

Компоненты кластера делятся на две категории:
* control plane — управляющие и обслуживающие сервисы, под control-plane обычно подразумевают поды istiod,
* data plane — прикладная часть Istio, представляет собой контейнеры sidecar-proxy.

![resources](https://docs.google.com/drawings/d/e/2PACX-1vRt0avuNi0cC_PiZmzuvbuYnFbx8rEyi4lUqB2l4pDIq2j1b3MY3HUeNHKhT3S9EeFC0tQdcY3Q8ydw/pub?w=1314&h=702)
<!--- Исходник: https://docs.google.com/drawings/d/1wXwtPwC4BM9_INjVVoo1WXj5Cc7Wbov2BjxKp84qjkY/edit --->

Все сервисы из data plane группируются в mesh. Его характеристики:
* Общее пространство имён для генерации идентификатора сервиса в формате <TrustDomain>/ns/<Namespace>/sa/<ServiceAccount>. Каждый mesh имеет идентификатор TrustDomain, который в нашем случае совпадает с доменом кластера. Например: `mycluster.local/ns/myns/sa/myapp`.
* Сервисы в рамках одного mesh имеют возможность аутентифицировать друг друга с помощью доверенных корневых сертификатов.

Элементы control plane:
* istiod — ключевой сервис, его задачи:
    * непрерывная связь с API Kubernetes и сбор информации о прикладных сервисах,
    * обработка и валидация с помощью механизма Kubernetes Validating Webhook всех Custom Resources, которые связаны с Istio,
    * компоновка конфигурации для каждого sidecar-proxy индивидуально:
      * генерация правил авторизации, маршрутизации, балансировки и пр.,
      * распространение информации о других прикладных сервисах в кластере,
      * выпуск индивидуальных клиентских сертификатов для организации схемы Mutual TLS. Эти сертификаты не связаны с сертификатами, которые использует и контролирует сам Kubernetes для своих служебных нужд.
    * автоматическая подстройка манифестов, определяющих прикладные поды через механизм Kubernetes Mutating Webhook:
      * внедрение дополнительного служебного контейнера sidecar-proxy,
      * внедрение дополнительного init-контейнера для адаптации сетевой подсистемы (настройка DNAT для перехвата прикладного трафика),
      * перенаправление readiness и leaveness-проб через sidecar-proxy.
* operator — компонент, отвечающий за установку всех ресурсов, необходимых для работы control plane определённой версии.
* kiali — панель управления и наблюдения за ресурсами Istio и пользовательскими сервисами под управлением Istio. Позволяет:
    * Визуализировать связи между сервисами.
    * Диагностировать проблемные связи между сервисами.
    * Диагностировать состояние control plane.

Для приёма пользовательского трафика требуется доработка ingress-контроллера:
* К подам контроллера добавляется sidecar-proxy, который обслуживает только трафик от контроллера в сторону прикладных сервисов (параметр IngressNginxController [`enableIstioSidecar`](https://deckhouse.io/en/documentation/v1/modules/402-ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-enableistiosidecar) у ресурса IngressNginxController).
* Сервисы не под управлением Istio продолжают работать как раньше, запросы в их сторону не перехватываются сайдкаром контроллера.
* Запросы в сторону сервисов под управлением Istio перехватываются сайдкаром и обрабатваются в соответствии с правилами Istio (см. [Как активировать Istio для приложения](#Как активировать Istio для приложения)).

Контроллер istiod и каждый контейнер sidecar-proxy экспортируют собственные метрики, которые собирает кластерный Prometheus.

## Архитектура прикладного сервиса с включенным Istio

### Особенности

* Каждый под сервиса получает дополнительный контейнер — sidecar-proxy. Технически этот контейнер содержит два приложения:
  * Envoy — проксирует прикладной трафик и реализует весь функционал, который предоставляет Istio, включая маршрутизацию, аутентификацию, авторизацию и пр.
  * pilot-agent — часть Istio, отвечает за поддержание конфигурации Envoy в актуальном состоянии, а также содержит в себе кеширующий DNS-сервер.
* В каждом поде настраивается DNAT входящих и исходящих прикладных запросов в sidecar-proxy. Делается это с помощью дополнительного init-контейнера. Таким образом, трафик будет перехватываться прозрачно для приложений.
* Так как входящий прикладной трафик перенаправляется в sidecar-proxy, то readiness/liveness-трафика это тоже касается. Подсистема Kubernetes, которая за это отвечает, не рассчитана на формирование проб в формате Mutual TLS. Для адаптации, все существующие пробы автоматически перенастраиваются на специальный порт в sidecar-proxy, который перенаправляет трафик на приложение в неизменном виде.
* Для приёма запросов извне кластера, необходимо использовать подготовленный ingress-контроллер:
  * Поды контроллера аналогично имеют дополнительный контейнер sidecar-proxy.
  * В отличие от подов приложения, sidecar-proxy ingress-контроллера перехватывает только трафик от контроллера к сервисам. Входящий трафик от пользователей обрабатывает непосредственно сам контроллер.
* Ресурсы типа Ingress требуют минимальной доработки в виде добавления аннотаций:
    * `nginx.ingress.kubernetes.io/service-upstream: "true"` — контроллер ingress-nginx в качестве upstream будет использовать ClusterIP сервиса вместо адресов подов. Балансировкой трафика между подами теперь занимается sidecar-proxy. Используйте эту опцию только если у вашего сервиса есть ClusterIP.
    * `nginx.ingress.kubernetes.io/upstream-vhost: "myservice.myns.svc.cluster-dns-suffix"` — sidecar-proxy ingress-контроллера принимает решения о маршрутизации на основе заголовка Host. Без данной аннотации, контроллер оставит заголовок с адресом сайта, например `Host: example.com`.
* Ресурсы типа Service не требуют адаптации и продолжают выполнять свою функцию. Приложениям всё так же доступны адреса сервисов вида servicename, servicename.myns.svc и пр.
* DNS-запросы изнутри подов прозрачно перенаправляются на обработку в sidecar-proxy:
  * Требуется для разыменования DNS-имён сервисов из соседних кластеров.

### Жизненный цикл пользовательского запроса

#### Приложение с выключенным Istio

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSN2yCNumnHC-Q9sgQ7LstaLuG8lWjYkvKrN27zNM4P8JxejasMeCazGIX5zYNSLuv6DieoXgI1Mx7u/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<!--- Исходник: https://docs.google.com/presentation/d/1_lw3EyDNTFTYNirqEfrRANnEAVjGhrOCdFJc-zCOuvs/edit --->

#### Приложение с включенным Istio

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSBqX8-US32uDhUYKpra4Co9rYsh9wqbhUV2pMh69WC-daXwW7CYeaofH_yhDOl4pdN-tO5pIPDMqtw/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<!--- Исходник: https://docs.google.com/presentation/d/1gQfX9ge2vhp74yF5LOfpdK2nY47l_4DIvk6px_tAMPU/edit --->

## Как активировать Istio для приложения

Основная цель активации — добавить sidecar-контейнер к подам приложения, после чего Istio сможет управлять трафиком.

Рекомендованный способ добавления sidecar-ов — использовать sidecar-injector. Istio умеет "подселять" к вашим подам sidecar-контейнер с помощью механизма [Admission Webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/). Настраивается с помощью лейблов и аннотаций:
* Лейбл к **namespace** — обращает внимание sidecar-injector-а на ваш namespace, после установки лейбла, к новым подам будут подселяться sidecar-ы:
  * `istio-injection=enabled` — использовать самую свежую установленную версию Istio.
  * `istio.io/rev=v1x13` — использовать конкретную версию Istio для данного namespace.
* Аннотация к **поду** — `sidecar.istio.io/inject` (`"true"` или `"false"`), позволяет локально переопределить политику `sidecarInjectorPolicy`. Эти аннотации работают только в namespace, обозначенных лейблами из списка выше.

**Важно знать!** Istio-proxy, который работает в качестве sidecar-контейнера тоже потребляет ресурсы и добавляет оверхед:
* Каждый запрос DNAT-ится в envoy, который обрабатывает реквест и создаёт ещё один. На принимающей стороне аналогично.
* Каждый envoy хранит информацию обо всех сервисах в кластере, что требует памяти. Больше кластер — больше памяти потребляет envoy. Решение — CustomResource [Sidecar](istio-cr.html#sidecar).

Также важно подготовить ingress-контроллер и Ingress-ресурсы приложения:
* включить [`enableIstioSidecar`](https://deckhouse.io/en/documentation/v1/modules/402-ingress-nginx/cr.html#ingressnginxcontroller-v1-spec-enableistiosidecar) у ресурса IngressNginxController,
* добавить аннотации на Ingress-ресурсы приложения:
  * `nginx.ingress.kubernetes.io/service-upstream: "true"` — контроллер ingress-nginx в качестве upstream будет использовать ClusterIP сервиса вместо адресов подов. Балансировкой трафика между подами теперь занимается sidecar-proxy. Используйте эту опцию только если у вашего сервиса есть ClusterIP.
  * `nginx.ingress.kubernetes.io/upstream-vhost: "myservice.myns.svc.cluster-dns-suffix"` — sidecar-proxy ingress-контроллера принимает решения о маршрутизации на основе заголовка Host. Без данной аннотации, контроллер оставит заголовок с адресом сайта, например `Host: example.com`.


## Федерация и мультикластер

Поддерживается две схемы межкластерного взаимодействия:

* федерация
* мультикластер

Принципиальные отличия:
* Федерация объединяет суверенные кластеры:
  * у каждого кластера собственное пространство имён (для Namespace, Service и пр.),
  * у каждого кластера собственная сетевая инфраструктура и произвольные адресные диапазоны (podSubnetCIDR и serviceSubnetCIDR),
  * доступ к отдельным сервисам между кластерами явно обозначен.
* Мультикластер объединяет созависимые кластеры:
  * сетевая связность между кластерами плоская — поды разных кластеров имеют взаимный прямой доступ,
  * пространство имён у кластеров общее — каждый сервис доступен для соседних кластеров так, словно он работает на локальном кластере (если это не запрещают правила авторизации).

### Федерация

#### Общие принципы

![resources](https://docs.google.com/drawings/d/e/2PACX-1vQj76KcY7cqhX_cHscCXdPqzrZk_nip-5vvEeRpB_1A9AXjc64uMq6uEhILn5iw8aUbLQERx1jV1yfp/pub?w=1087&h=626)
<!--- Исходник: https://docs.google.com/drawings/d/1VQ4yZl_39j2WSi7Iif5jn-ItWkjD3_W8uqNPULqEz4A/edit --->

* Федерация требует установления взаимного доверия между кластерами. Соответственно, для установления федерации, нужно в кластере A сделать кластер Б доверенным, и в кластере Б сделать кластер А доверенным. Технически это достигается взаимным обменом корневыми сертификатами.
* Для прикладной эксплуатации федерации необходимо также обменяться информацией о публичных сервисах. Чтобы опубликовать сервис bar из кластера Б в кластере А, необходимо в кластере А создать ресурс ServiceEntry, который определяет публичный адрес ingress-gateway кластера Б.

#### Включение федерации

При включении федерации (параметр модуля `istio.federation.enabled = true`) происходит следующее:
* В кластер добавляется сервис ingressgateway, чья задача проксировать mTLS-трафик извне кластера на прикладные сервисы.
* В кластер добавляется сервис, который экспортит метаданные кластера наружу:
  * корневой сертификат Istio (доступен без аутентификации),
  * список публичных сервисов в кластере (доступен только для аутентифицированных запросов из соседних кластеров),
  * список публичных адресов сервиса ingressgateway (доступен только для аутентифицированных запросов из соседних кластеров).

#### Управление федерацией

![resources](https://docs.google.com/drawings/d/e/2PACX-1vT9c5TGwE4MQHxO548h8nrZ8SicSXWNX9KlFl5RmD2BoDce1pnxWj9ZSxZUydOa-9Z7kJMt8WLsdjgZ/pub?w=1393&h=937)
<!--- Исходник: https://docs.google.com/drawings/d/1qNyGLyPUFR2E6qLkDLnqN42sWZzPZ5u782NJJxe-7r8/edit --->

Для автоматизации процесса федерации, в рамках deckhouse реализован специальный контроллер. Алгоритм установления доверия с следующий:
* Доверяемый кластер (cluster-b):
  * Местный контроллер собирает мета-информацию о кластере и (1) публикует её через стандартный Ingress:
    * (1a) публичная часть корневого сертификата,
    * (1b) список публичных сервисов в кластере (публичный сервис обозначается специальным лейблом `federation.istio.deckhouse.io/public-service=`),
    * (1c) публичные адреса ingress-gateway.
* Доверяющий кластер (cluster-a):
  * Контроллер доверяющего кластера необходимо проинструктировать о доверяемом кластере с помощью специального ресурса IstioFederation (2), который описывает:
    * (2a) доменный префикс удалённого кластера,
    * (2b) URL, где доступна вся метаинформация об удалённом кластере (описание метаданных выше).
  * Контроллер забирает (3) метаданные по URL и настраивает локальный Istio:
    * (3a) добавляет удалённый публичный корневой сертификат в доверенные,
    * (3b) для каждого публичного сервиса из удалённого кластера он создаёт соответствующий ресурс ServiceEntry, который содержит исчерпывающую информацию о координатах сервиса:
      * hostname сервиса, который состоит из комбинации имени и namespace сервиса в удалённом кластере (3с), а также из доменного суффикса кластера (3d),
      * (3e) публичный IP удалённого ingress-gateway.

Для установления взаимного доверия, данный алгоритм необходимо реализовать в обе стороны. Соответственно, для построения полной федерации, необходимо:
* В каждом кластере создать набор ресурсов IstioFederation, которые описывают все остальные кластеры.
* Каждый ресурс, который считается публичным, необходимо пометить лейблом `federation.istio.deckhouse.io/public-service=`.

### Мультикластер

#### Общие принципы

![resources](https://docs.google.com/drawings/d/e/2PACX-1vQj76KcY7cqhX_cHscCXdPqzrZk_nip-5vvEeRpB_1A9AXjc64uMq6uEhILn5iw8aUbLQERx1jV1yfp/pub?w=1087&h=626)
<!--- Исходник: https://docs.google.com/drawings/d/1VQ4yZl_39j2WSi7Iif5jn-ItWkjD3_W8uqNPULqEz4A/edit --->

* Мультикластер требует установления взаимного доверия между кластерами. Соответственно, для построения мультикластера, нужно в кластере A сделать кластер Б доверенным, и в кластере Б сделать кластер А доверенным. Технически это достигается взаимным обменом корневыми сертификатами.
* Для сбора информации о соседних сервисах, Istio подключается напрямую к apiserver соседнего кластера. Данный модуль берёт на себя организацию соответствующего канала связи.

#### Включение мультикластера

При включении мультикластера (параметр модуля `istio.multicluster.enabled = true`) происходит следующее:
* В кластер добавляется прокси для публикации доступа к apiserver посредством стандартного Ingress:
  * Доступ через данный публичный адрес ограничен корневыми сертификатами Istio доверенных кластеров. Клиентский сертфикат должен содержать Subject: CN=deckhouse.
  * Непосредственно прокси имеет доступ на чтение к ограниченному набору ресурсов.
* В кластер добавляется сервис, который экспортит метаданные кластера наружу:
  * Корневой сертификат Istio (доступен без аутентификации),
  * Публичный адрес, через который доступен apiserver (доступен только для аутентифицированных запросов из соседних кластеров).

#### Управление мультикластером

![resources](https://docs.google.com/drawings/d/e/2PACX-1vTLsBzlI4m9g0BZL13XWHlhUtgSJp7TEEvUuvzYNd_7H-HGz1hSw3CbfC5OR5EyAKppD-g1wMWoeglT/pub?w=1393&h=937)
<!--- Исходник: https://docs.google.com/drawings/d/1aF9BXxQFQpuCj_j3wmMdsVz8vuDOkvQQQ_8UsOmRaGo/edit --->

Для автоматизации процесса сбора мультикластера, в рамках deckhouse реализован специальный контроллер. Алгоритм установления доверия с следующий:
* Доверяемый кластер (cluster-b):
  * Местный контроллер собирает мета-информацию о кластере и (1) публикует её через стандартный Ingress:
    * (1a) публичная часть корневого сертификата,
    * (1b) публичный адрес, через который доступен apiserver (доступ ограничен правами на чтение ограниченного набора ресурсов и открыт только для клиентов с сертификатом, который подписан корневым сертификатом Istio и с CN=deckhouse),
* Доверяющий кластер (cluster-a):
  * Контроллер доверяющего кластера необходимо проинструктировать о доверяемом кластере с помощью специального ресурса IstioMulticluster (2), который описывает:
    * (2a) URL, где доступна вся метаинформация об удалённом кластере (описание метаданных выше).
  * Контроллер забирает (3) метаданные по URL и настраивает локальный Istio:
    * (3a) добавляет удалённый публичный корневой сертификат в доверенные,
    * (3b) создаёт kubeconfig для подключения к удалённому кластеру через публичный адрес.
* После чего, доверяющий istiod знает, как достучаться до api соседнего кластера (4). Но доступ он получит только после того, как симметричный ресурс IstioMulticluster будет создан на стороне доверяемого кластера (5).
* При взаимном доверии, сервисы общаются друг с другом напрямую (6).
Для установления взаимного доверия, данный алгоритм необходимо реализовать в обе стороны. Соответственно, для сборки мультикластера, необходимо:
* В каждом кластере создать набор ресурсов IstioMulticluster, которые описывают все остальные кластеры.

## Накладные расходы

**Важно знать!** Istio-proxy, который работает в качестве sidecar-контейнера тоже потребляет ресурсы и добавляет оверхед:
* Каждый запрос DNAT-ится в envoy, который обрабатывает реквест и создаёт ещё один. На принимающей стороне аналогично.
* Каждый envoy хранит информацию обо всех сервисах в кластере, что требует памяти. Больше кластер — больше памяти потребляет envoy. Решение — CustomResource [Sidecar](istio-cr.html#sidecar).

TODO лейтенси

TODO про сервис меш в целом
