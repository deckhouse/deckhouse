- name: d8.control-plane.etcd.availability
  rules:

  - alert: KubeEtcdTargetDown
    expr: max by (job) (up{job="kube-etcd3"} == 0)
    labels:
      severity_level: "5"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "1m"
      plk_grouped_by__main: "KubeEtcdUnavailable,tier=cluster,prometheus=deckhouse"
      plk_ignore_labels: "job"
      description: >
        Check the status of the etcd Pods: `kubectl -n kube-system get pod -l component=etcd`
        or Prometheus logs: `kubectl -n d8-monitoring logs -l app=prometheus -c prometheus`.
      summary: Prometheus is unable to scrape etcd metrics.

  - alert: KubeEtcdTargetAbsent
    expr: absent(up{job="kube-etcd3"}) == 1
    labels:
      severity_level: "5"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "1m"
      plk_ignore_labels: "job"
      plk_grouped_by__main: "KubeEtcdUnavailable,tier=cluster,prometheus=deckhouse"
      description: >
        Check the status of the etcd Pods: `kubectl -n kube-system get pod -l component=etcd`
        or Prometheus logs: `kubectl -n d8-monitoring logs -l app=prometheus -c prometheus`
      summary: There is no etcd target in Prometheus.

  - alert: KubeEtcdNoLeader
    expr: max by (node) (etcd_server_has_leader{job="kube-etcd3"}) == 0
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "1m"
      plk_grouped_by__main: "KubeEtcdUnavailable,tier=cluster,prometheus=deckhouse"
      description: >
        Check the status of the etcd Pods: `kubectl -n kube-system get pod -l component=etcd | grep {{ $labels.node }}`.
      summary: The etcd cluster member running on the {{ $labels.node }} Node has lost the leader.

  - alert: KubeEtcdUnavailable
    expr: count(ALERTS{alertname=~"KubeEtcdTargetDown|KubeEtcdTargetAbsent|KubeEtcdNoLeader", alertstate="firing"}) > 0
    labels:
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_alert_type: "group"
      summary: etcd is down.
      description: |
        etcd is down.

        The detailed information is available in one of the relevant alerts.

- name: d8.control-plane.etcd.malfunctioning
  rules:

  - alert: KubeEtcdHighNumberOfLeaderChanges
    expr: max by (node) (increase(etcd_server_leader_changes_seen_total{job="kube-etcd3"}[10m]) > 3)
    labels:
      severity_level: "5"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_caused_by__ping: "NodePingPacketLoss,tier=cluster,prometheus=deckhouse,destination_node={{ $labels.node }}"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        There were {{ $value }} leader re-elections for the etcd cluster member running on the {{ $labels.node }} Node in the last 10 minutes.

        Possible causes:
        1. High latency of the disk where the etcd data is located;
        2. High CPU usage on the Node;
        3. Degradation of network connectivity between cluster members in the multi-master mode.
      summary: The etcd cluster re-elects the leader too often.

  - alert: KubeEtcdInsufficientMembers
    expr: count(up{job="kube-etcd3"} == 0) > (count(up{job="kube-etcd3"}) / 2 - 1)
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "3m"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: >
        Check the status of the etcd pods: `kubectl -n kube-system get pod -l component=etcd`.
      summary: There are insufficient members in the etcd cluster; the cluster will fail if one of the remaining members will become unavailable.

  - alert: KubeEtcdHighFsyncDurations
    expr: max by (node) (histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="kube-etcd3"}[5m])) > 0.5)
    labels:
      severity_level: "7"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "10m"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        In the last 15 minutes, the 99th percentile of the fsync duration for WAL files is longer than 0.5 seconds: {{ $value }}.

        Possible causes:
        1. High latency of the disk where the etcd data is located;
        2. High CPU usage on the Node.
      summary: Synching (fsync) WAL files to disk is slow.

  - alert: KubeEtcdMalfunctioning
    expr: count(ALERTS{alertname=~"KubeEtcdHighNumberOfLeaderChanges|KubeEtcdInsufficientMembers|KubeEtcdHighFsyncDurations|KubeEtcdDatabaseSizeCloseToTheLimit", alertstate="firing"}) > 0
    labels:
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_alert_type: "group"
      summary: The etcd cluster is malfunctioning.
      description: |
        The etcd cluster is malfunctioning.

        The detailed information is available in one of the relevant alerts.
