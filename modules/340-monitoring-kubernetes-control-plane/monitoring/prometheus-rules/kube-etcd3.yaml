- name: d8.control-plane.etcd.availability
  rules:

  - alert: KubeEtcdTargetDown
    expr: max by (job) (up{job="kube-etcd3"} == 0)
    labels:
      severity_level: "5"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "1m"
      plk_grouped_by__main: "KubeEtcdUnavailable,tier=cluster,prometheus=deckhouse"
      plk_ignore_labels: "job"
      description: >
        Следует проверить состояние Pod'ов etcd: `kubectl -n kube-system get pod -l component=etcd`
        или логи Prometheus: `kubectl -n d8-monitoring logs -l app=prometheus -c prometheus`
      summary: Prometheus не может получить метрики etcd

  - alert: KubeEtcdTargetAbsent
    expr: absent(up{job="kube-etcd3"}) == 1
    labels:
      severity_level: "5"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "1m"
      plk_ignore_labels: "job"
      plk_grouped_by__main: "KubeEtcdUnavailable,tier=cluster,prometheus=deckhouse"
      description: >
        Следует проверить состояние Pod'ов etcd: `kubectl -n kube-system get pod -l component=etcd`
        или логи Prometheus: `kubectl -n d8-monitoring logs -l app=prometheus -c prometheus`
      summary: В Prometheus отсутствует etcd target

  - alert: KubeEtcdNoLeader
    expr: max by (node) (etcd_server_has_leader{job="kube-etcd3"}) == 0
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "1m"
      description: >
        Следует проверить состояние Pod'ов etcd: `kubectl -n kube-system get pod -l component=etcd | grep {{ $labels.node }}`.
      summary: Участник etcd кластера на ноде {{ $labels.node }} потерял лидера.

  - alert: KubeEtcdUnavailable
    expr: count(ALERTS{alertname=~"KubeEtcdTargetDown|KubeEtcdTargetAbsent|KubeEtcdNoLeader", alertstate="firing"}) > 0
    labels:
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_alert_type: "group"
      summary: etcd не работает
      description: |
        etcd не работает. Что именно с ним не так можно узнать в одном из связанных алертов.

- name: d8.control-plane.etcd.malfunctioning
  rules:

  - alert: KubeEtcdHighNumberOfLeaderChanges
    expr: max by (node) (increase(etcd_server_leader_changes_seen_total{job="kube-etcd3"}[10m]) > 3)
    labels:
      severity_level: "5"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_caused_by__ping: "NodePingPacketLoss,tier=cluster,prometheus=deckhouse,destination_node={{ $labels.node }}"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        Участник etcd кластера на ноде {{ $labels.node }} видел {{ $value }} перевыборов лидера за последние 10 минут.

        Возможные причины:

        1. Высокий latency диска, где находятся данные etcd;
        2. Высокий CPU usage на ноде;
        3. Ухудшение сетевой связности до членов кластера в multi-master конфигурации.
      summary: etcd кластер перевыбирает лидера слишком часто

  - alert: KubeEtcdInsufficientMembers
    expr: count(up{job="kube-etcd3"} == 0) > (count(up{job="kube-etcd3"}) / 2 - 1)
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "3m"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: >
        Следует проверить состояние Pod'ов etcd: `kubectl -n kube-system get pod -l component=etcd`.
      summary: В etcd кластере не хватает участников, если ещё хотя бы один участник станет недоступен, кластер упадёт.

  - alert: KubeEtcdMemberCommunicationSlow
    expr: max by (node) (histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job="kube-etcd3"}[5m])) > 0.15)
    labels:
      severity_level: "6"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "10m"
      plk_caused_by__ping: "NodePingPacketLoss,tier=cluster,prometheus=deckhouse,destination_node={{ $labels.node }}"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        Участник etcd кластера на ноде {{ $labels.node }} взаимодействует с {{ $labels.To }} слишком медленно.

        Возможна проблема в сетевой связности между нодами.
      summary: Лаг между участниками кластера

  - alert: KubeEtcdHighNumberOfFailedProposals
    expr: max by (node) (increase(etcd_server_proposals_failed_total{job="kube-etcd3"}[10m]) > 5)
    labels:
      severity_level: "6"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "10m"
      plk_caused_by__ping: "NodePingPacketLoss,tier=cluster,prometheus=deckhouse,destination_node={{ $labels.node }}"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        Участник etcd кластера не смог закоммитить proposal {{ $value }} раз за последние 10 минут.

        Возможные причины:

        1. Высокий latency диска, где находятся данные etcd;
        2. Высокий CPU usage на ноде;
        3. Ухудшение сетевой связности до членов кластера в multi-master конфигурации.
      summary: Кластер не может записать данные.

  - alert: KubeEtcdHighFsyncDurations
    expr: max by (node) (histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="kube-etcd3"}[5m])) > 0.5)
    labels:
      severity_level: "7"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "10m"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        За последние 15 минут 99й перцентиль скорости fsync'а WAL файлов больше 0.5 секунды: {{ $value }}.

        Возможные причины:

        1. Высокий latency диска, где находятся данные etcd;
        2. Высокий CPU usage на ноде;
      summary: fsync WAL файлов тормозит

  - alert: KubeEtcdHighCommitDurations
    expr: max by (node) (histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job="kube-etcd3"}[5m])) > 0.25)
    labels:
      severity_level: "7"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_pending_until_firing_for: "10m"
      plk_grouped_by__main: "KubeEtcdMalfunctioning,tier=cluster,prometheus=deckhouse"
      description: |
        За последние 15 минут 99й перцентиль скорости коммита больше 0.25 секунды: {{ $value }}.

        Возможные причины:

        1. Высокий latency диска, где находятся данные etcd;
        2. Высокий CPU usage на ноде;
      summary: commit тормозит

  - alert: KubeEtcdMalfunctioning
    expr: count(ALERTS{alertname=~"KubeEtcdHighNumberOfLeaderChanges|KubeEtcdInsufficientMembers|KubeEtcdMemberCommunicationSlow|KubeEtcdHighNumberOfFailedProposals|KubeEtcdHighFsyncDurations|KubeEtcdHighCommitDurations", alertstate="firing"}) > 0
    labels:
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_alert_type: "group"
      summary: etcd кластер работает некорректно
      description: |
        etcd кластер работает некорректно. Что именно с ним не так можно узнать в одном из связанных алертов.
