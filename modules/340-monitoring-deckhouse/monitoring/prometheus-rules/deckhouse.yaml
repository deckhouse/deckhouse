- name: d8.deckhouse.availability
  rules:
  - alert: D8DeckhouseSelfTargetDown
    expr: max by (job) (up{job="deckhouse", scrape_source="self"} == 0)
    for: 2m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      plk_ignore_labels: "job"
      summary: Prometheus is unable to scrape Deckhouse metrics.

  - alert: D8DeckhouseCustomTargetDown
    expr: max by (job) (up{job="deckhouse", scrape_source="custom"} == 0)
    for: 10m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      plk_ignore_labels: "job"
      summary: Prometheus is unable to scrape custom metrics generated by Deckhouse hooks.

  - alert: D8DeckhouseSelfTargetAbsent
    expr: absent(up{job="deckhouse", scrape_source="self"}) == 1
    for: 2m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: There is no Deckhouse target in Prometheus.

  - alert: D8DeckhousePodIsNotReady
    expr: |
      min by (pod) (
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() kube_pod_status_ready{condition="true", namespace="d8-system"}
      ) != 1
    for: 10m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "pod"
      summary: The Deckhouse Pod is NOT Ready.

  - alert: D8DeckhousePodIsNotRunning
    expr: |
      absent(
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() kube_pod_status_phase{namespace="d8-system",phase="Running"}
      )
    for: 2m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: The Deckhouse Pod is NOT Running.

  - alert: D8DeckhouseIsHung
    expr: max without (container, job) (increase(deckhouse_live_ticks[__SCRAPE_INTERVAL_X_4__])) < 1
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is down.
      description: |
        Deckhouse is probably down, since the `deckhouse_live_ticks` metric in Prometheus has stopped increasing.
        This metric is expected to increment every 10 seconds.

- name: d8.deckhouse.malfunctioning
  rules:
  - alert: D8DeckhousePodIsRestartingTooOften
    expr: |
      max by (pod) (
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() increase(kube_pod_container_status_restarts_total{namespace="d8-system"}[1h])
        and
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() kube_pod_container_status_restarts_total{namespace="d8-system"}
      ) > 3
    labels:
      severity_level: "9"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "pod"
      summary: Excessive Deckhouse restarts detected.
      description: |
        Number of restarts in the last hour: {{ $value }}.

        Excessive Deckhouse restarts indicate a potential issue. Normally, Deckhouse should be up and running continuously.

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseHasNoAccessToRegistry
    expr: max by (pod, instance) (increase(deckhouse_registry_errors[__SCRAPE_INTERVAL_X_4__])) > 0
    for: 1h
    labels:
      severity_level: "7"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to connect to the registry.
      description: |
        Deckhouse can't connect to the registry (typically `registry.deckhouse.io`) to check for a new Docker image. These checks are performed every 15 seconds. Without access to the registry, automatic updates are unavailable.

        This alert often indicates that the Deckhouse Pod is experiencing connectivity issues with the Internet.

  - alert: D8DeckhouseQueueIsHung
    expr: max by (pod, instance, queue) (min_over_time(deckhouse_tasks_queue_length{queue!~"main-subqueue-kubernetes-.*|/modules/upmeter/update_selector.*|/modules/secret-copier|/modules/deckhouse/update_deckhouse_image"}[__SCRAPE_INTERVAL_X_3__])) != 0
    for: 10m
    labels:
      severity_level: "7"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: The `{{ $labels.queue }}` Deckhouse queue is stuck with {{ $value }} pending task(s).
      description: |
        Deckhouse cannot finish processing of the `{{ $labels.queue }}` queue, which currently has {{ $value }} pending task(s).

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseGlobalHookFailsTooOften
    for: 10m
    expr: |
      max by (pod, instance, hook) (
        increase(deckhouse_global_hook_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
        or
        increase(deckhouse_global_hook_allowed_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
      ) > 1
    labels:
      severity_level: "9"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: The `{{ $labels.hook }}` Deckhouse global hook is crashing too frequently.
      description: |
        The `{{ $labels.hook }}` hook has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:
        
        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseModuleHookFailsTooOften
    for: 10m
    expr: |
      max by (pod, instance, module, hook) (
        increase(deckhouse_module_hook_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
        or
        increase(deckhouse_module_hook_allowed_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
      ) > 1
    labels:
      severity_level: "9"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: The `{{ $labels.module }}/{{ $labels.hook }}` Deckhouse hook is crashing too frequently.
      description: |
        The `{{ $labels.hook }}` hook of the `{{ $labels.module }}` module has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:

        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotDiscoverModules
    expr: max by (pod, instance) (increase(deckhouse_modules_discover_errors_total[__SCRAPE_INTERVAL_X_4__])) > 1
    for: 3m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to discover modules.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotRunModule
    expr: max(increase(deckhouse_module_run_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, module) > 1
    for: 3m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to start the `{{ $labels.module }}` module.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotDeleteModule
    expr: max(increase(deckhouse_module_delete_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, module) > 1
    for: 3m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to delete the `{{ $labels.module }}` module.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotRunGlobalHook
    expr: max(increase(deckhouse_global_hook_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, hook) > 1
    for: 3m
    labels:
      severity_level: "5"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to run the `{{ $labels.hook }}` global hook.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotRunModuleHook
    expr: max(increase(deckhouse_module_hook_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, module, hook) > 1
    for: 3m
    labels:
      severity_level: "7"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to run the `{{ $labels.module }}`/`{{ $labels.hook }}` module hook.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        kubectl -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseConfigInvalid
    expr: increase(deckhouse_config_values_errors_total[__SCRAPE_INTERVAL_X_2__]) > 0
    for: 1m
    labels:
      severity_level: "5"
      d8_module: deckhouse
      d8_component: deckhouse
      tier: cluster
    annotations:
      plk_markup_format: "markdown"
      plk_protocol_version: "1"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: |
        Deckhouse configuration is invalid.
      description: |
        Deckhouse configuration contains errors.

        Steps to troubleshoot:

        1. Check Deckhouse logs by running the following command:
        
           ```bash
           kubectl -n d8-system logs -f -l app=deckhouse
           ```

        1. Edit the Deckhouse global configuration:

           ```bash
           kubectl edit mc global
           ```
           
           Or edit configuration of a specific module:

           ```bash
           kubectl edit mc <MODULE_NAME>
           ```

  - alert: DeckhouseUpdating
    expr: max by (deployingRelease) (d8_is_updating) == 1
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      summary: Deckhouse is being updated to {{ $labels.deployingRelease }}.

  - alert: DeckhouseUpdatingFailed
    expr: max (d8_updating_is_failed) == 1
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      summary: Deckhouse update has failed.
      description: |
        The Deckhouse update has failed.

        Possible reasons:

        - The next minor or patch version of the Deckhouse image is not available in the registry.
        - The Deckhouse image is corrupted.

        Current version: {{ $labels.version }}.

        To resolve this issue, ensure that the next version of the Deckhouse image is available in the registry.

  - alert: D8DeckhouseWatchErrorOccurred
    expr: increase(deckhouse_kubernetes_client_watch_errors_total[__SCRAPE_INTERVAL_X_2__]) > 0
    for: 1m
    labels:
      severity_level: "5"
      d8_module: deckhouse
      d8_component: deckhouse
      tier: cluster
    annotations:
      plk_markup_format: "markdown"
      plk_protocol_version: "1"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: |
        Possible API server connection error in the client-go informer.
      description: |
        Deckhouse has detected an error in the client-go informer, possibly due to connection issues with the API server.

        Steps to investigate:
        
        1. Check Deckhouse logs for more information by running:

           ```bash
           kubectl -n d8-system logs deploy/deckhouse | grep error | grep -i watch
           ```

        1. This alert attempts to detect a correlation between the faulty snapshot invalidation and API server connection errors, specifically for the `handle-node-template` hook in the `node-manager` module.
           
           To compare the snapshot with the actual node objects for this hook, run the following command:

           ```bash
           diff -u <(kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'|sort) <(kubectl -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module snapshots node-manager -o json | jq '."040-node-manager/hooks/handle_node_templates.go"' | jq '.nodes.snapshot[] | .filterResult.Name' -r | sort)
           ```

  - alert: D8DeckhouseDeprecatedConfigmapManagedByArgoCD
    expr: |
      d8_deprecated_configmap_managed_by_argocd > 0
    labels:
      tier: cluster
      severity_level: "4"
    annotations:
      plk_markup_format: markdown
      plk_protocol_version: "1"
      for: "10m"
      summary: Deprecated Deckhouse ConfigMap managed by Argo CD.
      description: |
        The Deckhouse ConfigMap is no longer used.
        
        To resolve this issue, remove the `d8-system/deckhouse` ConfigMap from Argo CD.

  - alert: D8DeckhouseModuleUpdatePolicyNotFound
    expr: |
      increase(deckhouse_module_update_policy_not_found[__SCRAPE_INTERVAL_X_2__]) > 0
    for: 3m
    labels:
      tier: cluster
      d8_module: "{{ $labels.module }}"
      severity_level: "5"
    annotations:
      plk_markup_format: markdown
      plk_protocol_version: "1"
      summary: Module update policy not found for `{{ $labels.module_release }}`.
      description: |
        The module update policy for {{ $labels.module_release }} is missing.

        To resolve this issue, remove the label from the module release using the following command:
        
        ```bash
        kubectl label mr {{ $labels.module_release }} modules.deckhouse.io/update-policy-
        ```

        A new suitable policy will be detected automatically.
