- name: d8.deckhouse.availability
  rules:
  - alert: D8DeckhouseSelfTargetDown
    expr: max by (job) (up{job="deckhouse", scrape_source="self"} == 0)
    for: 2m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      plk_ignore_labels: "job"
      summary: Prometheus is unable to scrape Deckhouse metrics.

  - alert: D8DeckhouseCustomTargetDown
    expr: max by (job) (up{job="deckhouse", scrape_source="custom"} == 0)
    for: 10m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      plk_ignore_labels: "job"
      summary: Prometheus is unable to scrape custom metrics generated by Deckhouse hooks.

  - alert: D8DeckhouseSelfTargetAbsent
    expr: absent(up{job="deckhouse", scrape_source="self"}) == 1
    for: 2m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: There is no Deckhouse target in Prometheus.

  - alert: D8DeckhousePodIsNotReady
    expr: |
      min by (pod) (
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() kube_pod_status_ready{condition="true", namespace="d8-system"}
      ) != 1
    for: 10m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "pod"
      summary: The Deckhouse Pod is NOT Ready.

  - alert: D8DeckhousePodIsNotRunning
    expr: |
      absent(
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() kube_pod_status_phase{namespace="d8-system",phase="Running"}
      )
    for: 2m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      d8_ignore_on_update: "true"
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: The Deckhouse Pod is NOT Running.

  - alert: D8DeckhouseIsHung
    expr: max without (container, job) (increase(deckhouse_live_ticks[__SCRAPE_INTERVAL_X_4__])) < 1
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is down.
      description: |
        Deckhouse is probably down, since the `deckhouse_live_ticks` metric in Prometheus has stopped increasing.
        This metric is expected to increment every 10 seconds.

- name: d8.deckhouse.malfunctioning
  rules:
  - alert: D8DeckhousePodIsRestartingTooOften
    expr: |
      max by (pod) (
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() increase(kube_pod_container_status_restarts_total{namespace="d8-system"}[1h])
        and
        kube_controller_pod{namespace="d8-system", controller_type="Deployment", controller_name="deckhouse"}
        * on (pod) group_right() kube_pod_container_status_restarts_total{namespace="d8-system"}
      ) > 3
    labels:
      severity_level: "9"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "pod"
      summary: Excessive Deckhouse restarts detected.
      description: |
        Number of restarts in the last hour: {{ $value }}.

        Excessive Deckhouse restarts indicate a potential issue. Normally, Deckhouse should be up and running continuously.

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseHasNoAccessToRegistry
    expr: max by (pod, instance) (increase(deckhouse_registry_errors[__SCRAPE_INTERVAL_X_4__])) > 0
    for: 1h
    labels:
      severity_level: "7"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to connect to the registry.
      description: |
        Deckhouse can't connect to the registry (typically `registry.deckhouse.io`) to check for a new Docker image. These checks are performed every 15 seconds. Without access to the registry, automatic updates are unavailable.

        This alert often indicates that the Deckhouse Pod is experiencing connectivity issues with the Internet.

  - alert: D8DeckhouseQueueIsHung
    expr: max by (pod, instance, queue) (min_over_time(deckhouse_tasks_queue_length{queue!~"main-subqueue-kubernetes-.*|/modules/upmeter/update_selector.*|/modules/secret-copier|/modules/deckhouse/update_deckhouse_image"}[__SCRAPE_INTERVAL_X_3__])) != 0
    for: 10m
    labels:
      severity_level: "7"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: The `{{ $labels.queue }}` Deckhouse queue is stuck with {{ $value }} pending task(s).
      description: |
        Deckhouse cannot finish processing of the `{{ $labels.queue }}` queue, which currently has {{ $value }} pending task(s).

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseGlobalHookFailsTooOften
    for: 10m
    expr: |
      max by (pod, instance, hook) (
        increase(deckhouse_global_hook_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
        or
        increase(deckhouse_global_hook_allowed_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
      ) > 1
    labels:
      severity_level: "9"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: The `{{ $labels.hook }}` Deckhouse global hook is crashing too frequently.
      description: |
        The `{{ $labels.hook }}` hook has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:
        
        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseModuleHookFailsTooOften
    for: 10m
    expr: |
      max by (pod, instance, module, hook) (
        increase(deckhouse_module_hook_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
        or
        increase(deckhouse_module_hook_allowed_errors_total{job="deckhouse"}[__SCRAPE_INTERVAL_X_4__])
      ) > 1
    labels:
      severity_level: "9"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: The `{{ $labels.module }}/{{ $labels.hook }}` Deckhouse hook is crashing too frequently.
      description: |
        The `{{ $labels.hook }}` hook of the `{{ $labels.module }}` module has failed multiple times in the last `__SCRAPE_INTERVAL_X_4__`.

        To investigate the issue, check the logs by running the following command:

        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotDiscoverModules
    expr: max by (pod, instance) (increase(deckhouse_modules_discover_errors_total[__SCRAPE_INTERVAL_X_4__])) > 1
    for: 3m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to discover modules.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotRunModule
    expr: max(increase(deckhouse_module_run_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, module) > 1
    for: 3m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to start the `{{ $labels.module }}` module.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotDeleteModule
    expr: max(increase(deckhouse_module_delete_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, module) > 1
    for: 3m
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to delete the `{{ $labels.module }}` module.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotRunGlobalHook
    expr: max(increase(deckhouse_global_hook_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, hook) > 1
    for: 3m
    labels:
      severity_level: "5"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to run the `{{ $labels.hook }}` global hook.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseCouldNotRunModuleHook
    expr: max(increase(deckhouse_module_hook_errors_total[__SCRAPE_INTERVAL_X_4__])) by (pod, instance, module, hook) > 1
    for: 3m
    labels:
      severity_level: "7"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "instance,pod"
      summary: Deckhouse is unable to run the `{{ $labels.module }}`/`{{ $labels.hook }}` module hook.
      description: |
        To investigate the issue, check the logs by running the following command:
        
        ```bash
        d8 k -n d8-system logs -f -l app=deckhouse
        ```

  - alert: D8DeckhouseConfigInvalid
    expr: increase(deckhouse_config_values_errors_total[__SCRAPE_INTERVAL_X_2__]) > 0
    for: 1m
    labels:
      severity_level: "5"
      d8_module: deckhouse
      d8_component: deckhouse
      tier: cluster
    annotations:
      plk_markup_format: "markdown"
      plk_protocol_version: "1"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: |
        Deckhouse configuration is invalid.
      description: |
        Deckhouse configuration contains errors.

        Steps to troubleshoot:

        1. Check Deckhouse logs by running the following command:
        
           ```bash
           d8 k -n d8-system logs -f -l app=deckhouse
           ```

        1. Edit the Deckhouse global configuration:

           ```bash
           d8 k edit mc global
           ```
           
           Or edit configuration of a specific module:

           ```bash
           d8 k edit mc <MODULE_NAME>
           ```

  - alert: DeckhouseUpdating
    expr: max by (deployingRelease) (d8_is_updating) == 1
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      summary: Deckhouse is being updated to {{ $labels.deployingRelease }}.

  - alert: DeckhouseUpdatingFailed
    expr: max (d8_updating_is_failed) == 1
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      summary: Deckhouse update has failed.
      description: |
        The Deckhouse update has failed.

        Possible reasons:

        - The next minor or patch version of the Deckhouse image is not available in the registry.
        - The Deckhouse image is corrupted.

        Current version: {{ $labels.version }}.

        To resolve this issue, ensure that the next version of the Deckhouse image is available in the registry.

  - alert: DeckhouseModuleUpdatingFailedBrokenSequence
    expr: max by(module, registry, actual_version, version) (d8_module_updating_broken_sequence) > 0
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: "{{ $labels.module }}"
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      summary: Module update has failed. Updating sequence is broken.
      description: |
        The '{{ $labels.module }}' Module update has failed.

        Current version: {{ $labels.actual_version }}.
        Desired version: {{ $labels.version }}.

        Attempt to download interim releases failed.

        Possible reasons:
         - The necessary versions of the Module image is not available in the registry.

        To resolve this issue, ensure that all minor versions of the '{{ $labels.module }}' Module image between {{ $labels.actual_version }} and {{ $labels.version }} is available in the registry '{{ $labels.registry }}'.

  - alert: DeckhouseModuleUpdatingFailedModuleIsNotValid
    expr: max by(module, registry, version) (d8_module_updating_module_is_not_valid) > 0
    labels:
      severity_level: "4"
      tier: cluster
      d8_module: "{{ $labels.module }}"
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      summary: Module update has failed. Module is not valid.
      description: |
        The '{{ $labels.module }}' Module update has failed.

        Desired version: {{ $labels.version }}.

        Possible reasons:
         - The Module image is corrupted.

        To resolve this issue, ensure that {{ $labels.version }} image in the registry '{{ $labels.registry }}' is valid .

  - alert: D8DeckhouseWatchErrorOccurred
    expr: increase(deckhouse_kubernetes_client_watch_errors_total[__SCRAPE_INTERVAL_X_2__]) > 0
    for: 1m
    labels:
      severity_level: "5"
      d8_module: deckhouse
      d8_component: deckhouse
      tier: cluster
    annotations:
      plk_markup_format: "markdown"
      plk_protocol_version: "1"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      summary: |
        Possible API server connection error in the client-go informer.
      description: |
        Deckhouse has detected an error in the client-go informer, possibly due to connection issues with the API server.

        Steps to investigate:
        
        1. Check Deckhouse logs for more information by running:

           ```bash
           d8 k -n d8-system logs deploy/deckhouse | grep error | grep -i watch
           ```

        1. This alert attempts to detect a correlation between the faulty snapshot invalidation and API server connection errors, specifically for the `handle-node-template` hook in the `node-manager` module.
           
           To compare the snapshot with the actual node objects for this hook, run the following command:

           ```bash
           diff -u <(d8 k get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'|sort) <(d8 k -n d8-system exec svc/deckhouse-leader -c deckhouse -- deckhouse-controller module snapshots node-manager -o json | jq '."040-node-manager/hooks/handle_node_templates.go"' | jq '.nodes.snapshot[] | .filterResult.Name' -r | sort)
           ```

  - alert: D8DeckhouseDeprecatedConfigmapManagedByArgoCD
    expr: |
      d8_deprecated_configmap_managed_by_argocd > 0
    labels:
      tier: cluster
      severity_level: "4"
    annotations:
      plk_markup_format: markdown
      plk_protocol_version: "1"
      for: "10m"
      summary: Deprecated Deckhouse ConfigMap managed by Argo CD.
      description: |
        The Deckhouse ConfigMap is no longer used.
        
        To resolve this issue, remove the `d8-system/deckhouse` ConfigMap from Argo CD.

  - alert: D8DeckhouseModuleUpdatePolicyNotFound
    expr: |
      increase(deckhouse_module_update_policy_not_found[__SCRAPE_INTERVAL_X_2__]) > 0
    for: 3m
    labels:
      tier: cluster
      d8_module: "{{ $labels.module }}"
      severity_level: "5"
    annotations:
      plk_markup_format: markdown
      plk_protocol_version: "1"
      summary: Module update policy not found for `{{ $labels.module_release }}`.
      description: |
        The module update policy for {{ $labels.module_release }} is missing.

        To resolve this issue, remove the label from the module release using the following command:
        
        ```bash
        d8 k label mr {{ $labels.module_release }} modules.deckhouse.io/update-policy-
        ```

        A new suitable policy will be detected automatically.
  - alert: D8DeckhouseModuleValidationError
    expr: |
      deckhouse_module_configuration_error > 0
    for: 3m
    labels:
      tier: cluster
      d8_module: "{{ $labels.module }}"
      severity_level: "5"
    annotations:
      plk_markup_format: markdown
      plk_protocol_version: "1"
      summary: Module configuration failed for module {{ $labels.module }}.
      description: |
        Initial config for module {{ $labels.module }} is not valid. 
        
        You can get more details via
        
        ```bash
        d8 k get mr -l module={{ $labels.module }}
        ```

        Provided error: {{ $labels.error }}
  - alert: DeckhouseMigratedModuleNotFound
    expr: d8_migrated_module_not_found == 1
    labels:
      severity_level: "6"
      tier: cluster
      d8_module: deckhouse
      d8_component: deckhouse
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: "markdown"
      plk_create_group_if_not_exists__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_grouped_by__d8_deckhouse_malfunctioning: "D8DeckhouseMalfunctioning,tier=cluster,prometheus=deckhouse,kubernetes=~kubernetes"
      plk_labels_as_annotations: "module_name"
      summary: Migrated module {{ $labels.module_name }} not found in registry.
      description: |
        Module `{{ $labels.module_name }}` specified in DeckhouseRelease was moved to external source and not found in any available ModuleSource registry. DeckhouseRelease will be lock until found module in any available ModuleSource.
        
        This may indicate:
        - The module source registry is not properly configured.
        - The module has not been published to the registry yet.
        - There is a network issue preventing access to the registry.
        
        Please check:
        1. ModuleSource configurations: `d8 k get modulesources`.
        2. Registry accessibility and credentials.
        3. Module availability in the configured registries.
        
        To investigate, run the following commands:
        
        ```bash
        # Check ModuleSource status
        d8 k get modulesources
        
        # Check available modules in all sources
        d8 k -n d8-system exec -it deployment/deckhouse -c deckhouse -- deckhouse-controller registry get sources
        d8 k -n d8-system exec -it deployment/deckhouse -c deckhouse -- deckhouse-controller registry get modules <source_name>
        
        # Check Deckhouse logs for more details
        d8 k -n d8-system logs -f -l app=deckhouse
        ```
  - alert: DeckhouseEditionNotFound
    expr: |
      d8_edition_not_found > 0
    for: 3m
    labels:
      tier: cluster
      d8_module: "{{ $labels.module }}"
      severity_level: "6"
    annotations:
      plk_markup_format: markdown
      plk_protocol_version: "1"
      summary: Incorrect Deckhouse Kubernetes Platform edition.
      description: |
        Specify a Deckhouse Kubernetes Platform edition in `{{ $labels.module }}` ModuleConfig.

        ```bash
        d8 k patch moduleconfig {{ $labels.module }} --type merge --patch '{"spec": {"settings": {"license": {"edition": "DKP_EDITION"}}}}'
        ```

        Where `DKP_EDITION` is the edition you want to use: `CE`, `BE`, `EE`, `SE`, or `SE-plus`.
