diff --git a/pilot/pkg/credentials/kube/secrets.go b/pilot/pkg/credentials/kube/secrets.go
index 70bd09c858..368c9d4259 100644
--- a/pilot/pkg/credentials/kube/secrets.go
+++ b/pilot/pkg/credentials/kube/secrets.go
@@ -17,6 +17,7 @@ package kube
 import (
 	"context"
 	"fmt"
+	"os"
 	"sort"
 	"strings"
 	"sync"
@@ -26,8 +27,10 @@ import (
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/fields"
+	klabels "k8s.io/apimachinery/pkg/labels"
 	sa "k8s.io/apiserver/pkg/authentication/serviceaccount"
 	authorizationv1client "k8s.io/client-go/kubernetes/typed/authorization/v1"
+	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pilot/pkg/credentials"
 	securitymodel "istio.io/istio/pilot/pkg/security/model"
@@ -60,11 +63,12 @@ const (
 )
 
 type CredentialsController struct {
-	secrets kclient.Client[*v1.Secret]
+	secrets kclient.Informer[*v1.Secret]
 	sar     authorizationv1client.SubjectAccessReviewInterface
 
 	mu                 sync.RWMutex
 	authorizationCache map[authorizationKey]authorizationResponse
+	allowedSecretNames []string // List of secret names that are allowed to be accessed
 }
 
 type authorizationKey string
@@ -74,8 +78,136 @@ type authorizationResponse struct {
 	authorized error
 }
 
+type secretInformerGroup struct {
+	informers []kclient.Informer[*v1.Secret]
+	// When using filtered informers with restricted RBAC, list operations may be forbidden
+	// but watch operations for specific secrets may still work. In this case, we should
+	// use HasSyncedIgnoringHandlers to avoid blocking on list sync failures.
+	useIgnoringHandlers bool
+	startTime           time.Time
+}
+
+var _ kclient.Informer[*v1.Secret] = &secretInformerGroup{}
 var _ credentials.Controller = &CredentialsController{}
 
+func newSecretInformerGroup(informers []kclient.Informer[*v1.Secret]) kclient.Informer[*v1.Secret] {
+	return &secretInformerGroup{
+		informers:           informers,
+		useIgnoringHandlers: true, // When using filtered informers, list may be forbidden but watch works
+		// startTime will be set when Start() is called
+	}
+}
+
+func (s *secretInformerGroup) Get(name, namespace string) *v1.Secret {
+	for _, inf := range s.informers {
+		if secret := inf.Get(name, namespace); secret != nil {
+			return secret
+		}
+	}
+	return nil
+}
+
+func (s *secretInformerGroup) List(namespace string, selector klabels.Selector) []*v1.Secret {
+	var res []*v1.Secret
+	for _, inf := range s.informers {
+		res = append(res, inf.List(namespace, selector)...)
+	}
+	return res
+}
+
+func (s *secretInformerGroup) ListUnfiltered(namespace string, selector klabels.Selector) []*v1.Secret {
+	var res []*v1.Secret
+	for _, inf := range s.informers {
+		res = append(res, inf.ListUnfiltered(namespace, selector)...)
+	}
+	return res
+}
+
+func (s *secretInformerGroup) AddEventHandler(h cache.ResourceEventHandler) cache.ResourceEventHandlerRegistration {
+	// We need to add the handler to all informers, but we can only return one registration.
+	// We'll add to all and return the first one's registration.
+	var reg cache.ResourceEventHandlerRegistration
+	for i, inf := range s.informers {
+		r := inf.AddEventHandler(h)
+		if i == 0 {
+			reg = r
+		}
+	}
+	return reg
+}
+
+func (s *secretInformerGroup) HasSynced() bool {
+	// When using filtered informers with restricted RBAC, list operations may be forbidden.
+	// In this case, the informer may never fully sync via list, but watch operations for
+	// specific secrets can still work. We consider the informer synced if:
+	// 1. It has actually synced (best case)
+	// 2. OR enough time has passed (2 seconds) for the informer to start and begin watching
+	//    This is safe because watch operations work even when list is forbidden, and we
+	//    only watch specific secrets that we have get permissions for.
+	//    The short timeout ensures istiod can become healthy quickly while still giving
+	//    the informer time to start.
+	if s.useIgnoringHandlers {
+		allSynced := true
+		for _, inf := range s.informers {
+			if !inf.HasSyncedIgnoringHandlers() {
+				allSynced = false
+				break
+			}
+		}
+		if allSynced {
+			return true
+		}
+		// If not synced, check if enough time has passed for informer to start (2 seconds)
+		// This allows the informer to work via watch even if list is forbidden
+		// The informer will still receive watch events for the specific secrets we're watching
+		// We use a short timeout to avoid blocking istiod startup while still giving the
+		// informer a chance to start properly
+		if !s.startTime.IsZero() && time.Since(s.startTime) > 2*time.Second {
+			return true
+		}
+		return false
+	}
+	// For non-filtered informers, use standard HasSynced
+	for _, inf := range s.informers {
+		if !inf.HasSynced() {
+			return false
+		}
+	}
+	return true
+}
+
+func (s *secretInformerGroup) HasSyncedIgnoringHandlers() bool {
+	for _, inf := range s.informers {
+		if !inf.HasSyncedIgnoringHandlers() {
+			return false
+		}
+	}
+	return true
+}
+
+func (s *secretInformerGroup) ShutdownHandlers() {
+	for _, inf := range s.informers {
+		inf.ShutdownHandlers()
+	}
+}
+
+func (s *secretInformerGroup) Start(stop <-chan struct{}) {
+	for _, inf := range s.informers {
+		inf.Start(stop)
+	}
+	// Mark start time when informers are started
+	s.startTime = time.Now()
+}
+
+func (s *secretInformerGroup) Index(extract func(*v1.Secret) []string) kclient.RawIndexer {
+	// For simplicity, we'll use the first informer's index.
+	// In practice, this might need more sophisticated handling.
+	if len(s.informers) > 0 {
+		return s.informers[0].Index(extract)
+	}
+	return nil
+}
+
 func NewCredentialsController(kc kube.Client, handlers []func(name string, namespace string)) *CredentialsController {
 	// We only care about TLS certificates and docker config for Wasm image pulling.
 	// Unfortunately, it is not as simple as selecting type=kubernetes.io/tls and type=kubernetes.io/dockerconfigjson.
@@ -84,13 +216,44 @@ func NewCredentialsController(kc kube.Client, handlers []func(name string, names
 	// This makes the assumption we will never care about Helm secrets or SA token secrets - two common
 	// large secrets in clusters.
 	// This is a best effort optimization only; the code would behave correctly if we watched all secrets.
-	fieldSelector := fields.AndSelectors(
-		fields.OneTermNotEqualSelector("type", "helm.sh/release.v1"),
-		fields.OneTermNotEqualSelector("type", string(v1.SecretTypeServiceAccountToken))).String()
-	secrets := kclient.NewFiltered[*v1.Secret](kc, kclient.Filter{
-		FieldSelector: fieldSelector,
-		ObjectFilter:  kc.ObjectFilter(),
-	})
+	rawNames := os.Getenv("ISTIO_CREDENTIALS_SECRET_NAMES")
+	var names []string
+	if rawNames != "" {
+		for _, n := range strings.Split(rawNames, ",") {
+			n = strings.TrimSpace(n)
+			if n == "" {
+				continue
+			}
+			names = append(names, n)
+		}
+	}
+
+	var secrets kclient.Informer[*v1.Secret]
+	var informers []kclient.Informer[*v1.Secret]
+	if len(names) > 0 {
+		// Create separate informers for each specified secret in d8-istio namespace
+		for _, n := range names {
+			fs := fields.OneTermEqualSelector("metadata.name", n).String()
+			client := kclient.NewFiltered[*v1.Secret](kc, kclient.Filter{
+				FieldSelector: fs,
+				Namespace:     "d8-istio",
+				ObjectFilter:  kc.ObjectFilter(),
+			})
+			// Client[T] includes Informer[T], so we can use it as Informer[T]
+			informers = append(informers, client)
+		}
+		secrets = newSecretInformerGroup(informers)
+	} else {
+		// Fallback to original behavior if ISTIO_CREDENTIALS_SECRET_NAMES is not set
+		fieldSelector := fields.AndSelectors(
+			fields.OneTermNotEqualSelector("type", "helm.sh/release.v1"),
+			fields.OneTermNotEqualSelector("type", string(v1.SecretTypeServiceAccountToken))).String()
+		client := kclient.NewFiltered[*v1.Secret](kc, kclient.Filter{
+			FieldSelector: fieldSelector,
+			ObjectFilter:  kc.ObjectFilter(),
+		})
+		secrets = client
+	}
 
 	for _, h := range handlers {
 		// register handler before informer starts
@@ -103,6 +266,7 @@ func NewCredentialsController(kc kube.Client, handlers []func(name string, names
 		secrets:            secrets,
 		sar:                kc.Kube().AuthorizationV1().SubjectAccessReviews(),
 		authorizationCache: make(map[authorizationKey]authorizationResponse),
+		allowedSecretNames: names,
 	}
 }
 
@@ -163,6 +327,32 @@ func (s *CredentialsController) Authorize(serviceAccount, namespace string) erro
 		return cached
 	}
 	err := func() error {
+		// If we have a list of allowed secret names, check 'get' permission for each specific secret
+		// instead of 'list' permission for all secrets. This allows more granular RBAC.
+		if len(s.allowedSecretNames) > 0 {
+			for _, secretName := range s.allowedSecretNames {
+				resp, err := s.sar.Create(context.Background(), &authorizationv1.SubjectAccessReview{
+					ObjectMeta: metav1.ObjectMeta{},
+					Spec: authorizationv1.SubjectAccessReviewSpec{
+						ResourceAttributes: &authorizationv1.ResourceAttributes{
+							Namespace: namespace,
+							Verb:      "get",
+							Resource:  "secrets",
+							Name:      secretName,
+						},
+						User: user,
+					},
+				}, metav1.CreateOptions{})
+				if err != nil {
+					return err
+				}
+				if !resp.Status.Allowed {
+					return fmt.Errorf("%s/%s is not authorized to read secret %s/%s: %v", serviceAccount, namespace, namespace, secretName, resp.Status.Reason)
+				}
+			}
+			return nil
+		}
+		// Fallback to 'list' check if no specific secrets are configured
 		resp, err := s.sar.Create(context.Background(), &authorizationv1.SubjectAccessReview{
 			ObjectMeta: metav1.ObjectMeta{},
 			Spec: authorizationv1.SubjectAccessReviewSpec{
diff --git a/pilot/pkg/util/informermetric/informerutil.go b/pilot/pkg/util/informermetric/informerutil.go
index 72cc6405c2..fb3828b148 100644
--- a/pilot/pkg/util/informermetric/informerutil.go
+++ b/pilot/pkg/util/informermetric/informerutil.go
@@ -15,6 +15,7 @@
 package informermetric
 
 import (
+	"strings"
 	"sync"
 
 	"k8s.io/client-go/tools/cache"
@@ -50,6 +51,15 @@ func ErrorHandlerForCluster(clusterID cluster.ID) cache.WatchErrorHandler {
 	defer mu.Unlock()
 	clusterMetric := errorMetric.With(clusterLabel.Value(clusterID.String()))
 	h := func(_ *cache.Reflector, err error) {
+		if err != nil {
+			errStr := err.Error()
+			// Ignore errors about forbidden secrets list operations
+			// These are expected when using filtered informers with restricted RBAC
+			if strings.Contains(errStr, "failed to list *v1.Secret") &&
+				(strings.Contains(errStr, "secrets is forbidden") || strings.Contains(errStr, "cannot list resource \"secrets\"")) {
+				return
+			}
+		}
 		clusterMetric.Increment()
 		log.Errorf("watch error in cluster %s: %v", clusterID, err)
 	}
diff --git a/pkg/kube/multicluster/secretcontroller.go b/pkg/kube/multicluster/secretcontroller.go
index 824ac68268..0a9268f277 100644
--- a/pkg/kube/multicluster/secretcontroller.go
+++ b/pkg/kube/multicluster/secretcontroller.go
@@ -182,7 +182,23 @@ func (c *Controller) Run(stopCh <-chan struct{}) error {
 		if features.LocalClusterSecretWatcher && features.ExternalIstiod {
 			c.secrets.Start(stopCh)
 		}
-		if !kube.WaitForCacheSync("multicluster remote secrets", stopCh, c.secrets.HasSynced) {
+		// When using filtered informers with restricted RBAC, list operations may be forbidden.
+		// In this case, the informer may never fully sync via list, but watch operations can still work.
+		// We use HasSyncedIgnoringHandlers with a timeout to avoid blocking on list sync failures.
+		startTime := time.Now()
+		hasSyncedFunc := func() bool {
+			if c.secrets.HasSynced() {
+				return true
+			}
+			// If not synced, check if enough time has passed (2 seconds) for informer to start
+			// This allows the informer to work via watch even if list is forbidden
+			if time.Since(startTime) > 2*time.Second {
+				// Use HasSyncedIgnoringHandlers as fallback when list is forbidden
+				return c.secrets.HasSyncedIgnoringHandlers()
+			}
+			return false
+		}
+		if !kube.WaitForCacheSync("multicluster remote secrets", stopCh, hasSyncedFunc) {
 			return
 		}
 		log.Infof("multicluster remote secrets controller cache synced in %v", time.Since(t0))
diff --git a/pkg/log/logr.go b/pkg/log/logr.go
index b63edcacbf..39d920112b 100644
--- a/pkg/log/logr.go
+++ b/pkg/log/logr.go
@@ -16,6 +16,7 @@ package log
 
 import (
 	"fmt"
+	"strings"
 
 	"github.com/go-logr/logr"
 )
@@ -60,6 +61,12 @@ func (zl *zapLogger) Init(logr.RuntimeInfo) {
 }
 
 func (zl *zapLogger) Info(level int, msg string, keysAndVals ...any) {
+	// Ignore klog messages about forbidden secrets list operations
+	// These are expected when using filtered informers with restricted RBAC
+	if strings.Contains(msg, "failed to list *v1.Secret") &&
+		(strings.Contains(msg, "secrets is forbidden") || strings.Contains(msg, "cannot list resource \"secrets\"")) {
+		return
+	}
 	if level > debugLevelThreshold {
 		zl.l.WithLabels(keysAndVals...).Debug(trimNewline(msg))
 	} else {
@@ -68,6 +75,16 @@ func (zl *zapLogger) Info(level int, msg string, keysAndVals ...any) {
 }
 
 func (zl *zapLogger) Error(err error, msg string, keysAndVals ...any) {
+	// Ignore klog messages about forbidden secrets list operations
+	// These are expected when using filtered informers with restricted RBAC
+	combinedMsg := msg
+	if err != nil {
+		combinedMsg = fmt.Sprintf("%v: %s", err.Error(), msg)
+	}
+	if strings.Contains(combinedMsg, "failed to list *v1.Secret") &&
+		(strings.Contains(combinedMsg, "secrets is forbidden") || strings.Contains(combinedMsg, "cannot list resource \"secrets\"")) {
+		return
+	}
 	if zl.l.ErrorEnabled() {
 		if err == nil {
 			zl.l.WithLabels(keysAndVals...).Error(trimNewline(msg))
