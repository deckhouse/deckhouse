diff --git a/pkg/controller/daemonset/daemonset_controller.go b/pkg/controller/daemonset/daemonset_controller.go
index b0192208..ea8408a4 100644
--- a/pkg/controller/daemonset/daemonset_controller.go
+++ b/pkg/controller/daemonset/daemonset_controller.go
@@ -38,9 +38,6 @@ import (
 	"k8s.io/apimachinery/pkg/util/wait"
 	clientset "k8s.io/client-go/kubernetes"
 	v1core "k8s.io/client-go/kubernetes/typed/core/v1"
-	appslisters "k8s.io/client-go/listers/apps/v1"
-	corelisters "k8s.io/client-go/listers/core/v1"
-	"k8s.io/client-go/tools/cache"
 	"k8s.io/client-go/tools/record"
 	"k8s.io/client-go/util/flowcontrol"
 	"k8s.io/client-go/util/retry"
@@ -65,7 +62,6 @@ import (
 	"github.com/openkruise/kruise/pkg/client"
 	kruiseclientset "github.com/openkruise/kruise/pkg/client/clientset/versioned"
 	"github.com/openkruise/kruise/pkg/client/clientset/versioned/scheme"
-	kruiseappslisters "github.com/openkruise/kruise/pkg/client/listers/apps/v1alpha1"
 	kruiseutil "github.com/openkruise/kruise/pkg/util"
 	utilclient "github.com/openkruise/kruise/pkg/util/client"
 	utildiscovery "github.com/openkruise/kruise/pkg/util/discovery"
@@ -135,15 +131,12 @@ func Add(mgr manager.Manager) error {
 		return nil
 	}
 
-	r, err := newReconciler(mgr)
-	if err != nil {
-		return err
-	}
+	r := newReconciler(mgr)
 	return add(mgr, r)
 }
 
 // newReconciler returns a new reconcile.Reconciler
-func newReconciler(mgr manager.Manager) (reconcile.Reconciler, error) {
+func newReconciler(mgr manager.Manager) reconcile.Reconciler {
 	genericClient := client.GetGenericClientWithName("daemonset-controller")
 	eventBroadcaster := record.NewBroadcaster()
 	eventBroadcaster.StartLogging(klog.Infof)
@@ -152,27 +145,6 @@ func newReconciler(mgr manager.Manager) (reconcile.Reconciler, error) {
 	recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: "daemonset-controller"})
 	cacher := mgr.GetCache()
 
-	dsInformer, err := cacher.GetInformerForKind(context.TODO(), controllerKind)
-	if err != nil {
-		return nil, err
-	}
-	podInformer, err := cacher.GetInformerForKind(context.TODO(), corev1.SchemeGroupVersion.WithKind("Pod"))
-	if err != nil {
-		return nil, err
-	}
-	nodeInformer, err := cacher.GetInformerForKind(context.TODO(), corev1.SchemeGroupVersion.WithKind("Node"))
-	if err != nil {
-		return nil, err
-	}
-	revInformer, err := cacher.GetInformerForKind(context.TODO(), apps.SchemeGroupVersion.WithKind("ControllerRevision"))
-	if err != nil {
-		return nil, err
-	}
-
-	dsLister := kruiseappslisters.NewDaemonSetLister(dsInformer.(cache.SharedIndexInformer).GetIndexer())
-	historyLister := appslisters.NewControllerRevisionLister(revInformer.(cache.SharedIndexInformer).GetIndexer())
-	podLister := corelisters.NewPodLister(podInformer.(cache.SharedIndexInformer).GetIndexer())
-	nodeLister := corelisters.NewNodeLister(nodeInformer.(cache.SharedIndexInformer).GetIndexer())
 	failedPodsBackoff := flowcontrol.NewBackOff(1*time.Second, 15*time.Minute)
 	revisionAdapter := revisionadapter.NewDefaultImpl()
 
@@ -188,16 +160,13 @@ func newReconciler(mgr manager.Manager) (reconcile.Reconciler, error) {
 		},
 		lifecycleControl:            lifecycle.New(cli),
 		expectations:                kubecontroller.NewControllerExpectations(),
+		cache:                       cacher,
 		resourceVersionExpectations: kruiseExpectations.NewResourceVersionExpectation(),
-		dsLister:                    dsLister,
-		historyLister:               historyLister,
-		podLister:                   podLister,
-		nodeLister:                  nodeLister,
 		failedPodsBackoff:           failedPodsBackoff,
 		inplaceControl:              inplaceupdate.New(cli, revisionAdapter),
 		revisionAdapter:             revisionAdapter,
 	}
-	return dsc, err
+	return dsc
 }
 
 // add adds a new Controller to mgr with r as the reconcile.Reconciler
@@ -276,14 +245,8 @@ type ReconcileDaemonSet struct {
 	// A cache of pod resourceVersion expecatations
 	resourceVersionExpectations kruiseExpectations.ResourceVersionExpectation
 
-	// dsLister can list/get daemonsets from the shared informer's store
-	dsLister kruiseappslisters.DaemonSetLister
-	// historyLister get list/get history from the shared informers's store
-	historyLister appslisters.ControllerRevisionLister
-	// podLister get list/get pods from the shared informers's store
-	podLister corelisters.PodLister
-	// nodeLister can list/get nodes from the shared informer's store
-	nodeLister corelisters.NodeLister
+	// cache provides read access to cached objects.
+	cache runtimeclient.Reader
 
 	failedPodsBackoff *flowcontrol.Backoff
 
@@ -337,10 +300,18 @@ func (dsc *ReconcileDaemonSet) getDaemonPods(ctx context.Context, ds *appsv1alph
 
 	// List all pods to include those that don't match the selector anymore but
 	// have a ControllerRef pointing to this controller.
-	pods, err := dsc.podLister.Pods(ds.Namespace).List(labels.Everything())
-	if err != nil {
+	podList := &corev1.PodList{}
+	if err := dsc.cache.List(ctx, podList,
+		runtimeclient.InNamespace(ds.Namespace),
+		runtimeclient.MatchingLabelsSelector{Selector: labels.Everything()},
+	); err != nil {
 		return nil, err
 	}
+	pods := make([]*corev1.Pod, 0, len(podList.Items))
+	for i := range podList.Items {
+		pod := &podList.Items[i]
+		pods = append(pods, pod)
+	}
 
 	// If any adoptions are attempted, we should first recheck for deletion with
 	// an uncached quorum read sometime after listing Pods (see #42639).
@@ -363,14 +334,14 @@ func (dsc *ReconcileDaemonSet) getDaemonPods(ctx context.Context, ds *appsv1alph
 func (dsc *ReconcileDaemonSet) syncDaemonSet(ctx context.Context, request reconcile.Request) error {
 	logger := klog.FromContext(ctx)
 	dsKey := request.NamespacedName.String()
-	ds, err := dsc.dsLister.DaemonSets(request.Namespace).Get(request.Name)
-	if err != nil {
+	ds := &appsv1alpha1.DaemonSet{}
+	if err := dsc.cache.Get(ctx, request.NamespacedName, ds); err != nil {
 		if errors.IsNotFound(err) {
 			klog.V(4).InfoS("DaemonSet has been deleted", "daemonSet", request)
 			dsc.expectations.DeleteExpectations(logger, dsKey)
 			return nil
 		}
-		return fmt.Errorf("unable to retrieve DaemonSet %s from store: %v", dsKey, err)
+		return fmt.Errorf("unable to retrieve DaemonSet %s from cache: %v", dsKey, err)
 	}
 
 	// Don't process a daemon set until all its creations and deletions have been processed.
@@ -386,10 +357,15 @@ func (dsc *ReconcileDaemonSet) syncDaemonSet(ctx context.Context, request reconc
 		return nil
 	}
 
-	nodeList, err := dsc.nodeLister.List(labels.Everything())
-	if err != nil {
+	nodeObjs := &corev1.NodeList{}
+	if err := dsc.cache.List(ctx, nodeObjs, runtimeclient.MatchingLabelsSelector{Selector: labels.Everything()}); err != nil {
 		return fmt.Errorf("couldn't get list of nodes when syncing DaemonSet %#v: %v", ds, err)
 	}
+	nodeList := make([]*corev1.Node, 0, len(nodeObjs.Items))
+	for i := range nodeObjs.Items {
+		node := &nodeObjs.Items[i]
+		nodeList = append(nodeList, node)
+	}
 
 	// Construct histories of the DaemonSet, and get the hash of current history
 	cur, old, err := dsc.constructHistory(ctx, ds)
@@ -558,7 +534,7 @@ func (dsc *ReconcileDaemonSet) updateDaemonSetStatus(ctx context.Context, ds *ap
 		return fmt.Errorf("error storing status for DaemonSet %v: %v", ds.Name, err)
 	}
 
-	err = dsc.updateDaemonSetReplicas(ds, desiredNumberScheduled)
+	err = dsc.updateDaemonSetReplicas(ctx, ds, desiredNumberScheduled)
 	if err != nil {
 		return fmt.Errorf("error updating DaemonSet %v replicas: %v", ds.Name, err)
 	}
@@ -570,7 +546,7 @@ func (dsc *ReconcileDaemonSet) updateDaemonSetStatus(ctx context.Context, ds *ap
 	return nil
 }
 
-func (dsc *ReconcileDaemonSet) getEligibleNodesCount(ds *appsv1alpha1.DaemonSet) (int32, error) {
+func (dsc *ReconcileDaemonSet) getEligibleNodesCount(ctx context.Context, ds *appsv1alpha1.DaemonSet) (int32, error) {
 	if len(ds.Spec.Template.Spec.NodeSelector) > 0 {
 		nodeCount, err := dsc.kubeClient.CoreV1().Nodes().List(context.TODO(), metav1.ListOptions{LabelSelector: labels.SelectorFromSet(ds.Spec.Template.Spec.NodeSelector).String()})
 		if err != nil {
@@ -579,10 +555,15 @@ func (dsc *ReconcileDaemonSet) getEligibleNodesCount(ds *appsv1alpha1.DaemonSet)
 		return int32(len(nodeCount.Items)), nil
 	}
 
-	nodeList, err := dsc.nodeLister.List(labels.Everything())
-	if err != nil {
+	nodes := &corev1.NodeList{}
+	if err := dsc.cache.List(ctx, nodes, runtimeclient.MatchingLabelsSelector{Selector: labels.Everything()}); err != nil {
 		return 0, fmt.Errorf("couldn't get list of nodes when calculating the number of eligible nodes %#v: %v", ds, err)
 	}
+	nodeList := make([]*corev1.Node, 0, len(nodes.Items))
+	for i := range nodes.Items {
+		node := &nodes.Items[i]
+		nodeList = append(nodeList, node)
+	}
 
 	var count int32
 	for _, node := range nodeList {
@@ -593,8 +574,8 @@ func (dsc *ReconcileDaemonSet) getEligibleNodesCount(ds *appsv1alpha1.DaemonSet)
 	return count, nil
 }
 
-func (dsc *ReconcileDaemonSet) updateDaemonSetReplicas(ds *appsv1alpha1.DaemonSet, desiredNumberScheduled int) error {
-	nodeCount, err := dsc.getEligibleNodesCount(ds)
+func (dsc *ReconcileDaemonSet) updateDaemonSetReplicas(ctx context.Context, ds *appsv1alpha1.DaemonSet, desiredNumberScheduled int) error {
+	nodeCount, err := dsc.getEligibleNodesCount(ctx, ds)
 	if err != nil {
 		return err
 	}
@@ -753,7 +734,7 @@ func (dsc *ReconcileDaemonSet) manage(ctx context.Context, ds *appsv1alpha1.Daem
 func (dsc *ReconcileDaemonSet) syncNodes(ctx context.Context, ds *appsv1alpha1.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {
 	if ds.Spec.Lifecycle != nil && ds.Spec.Lifecycle.PreDelete != nil {
 		var err error
-		podsToDelete, err = dsc.syncWithPreparingDelete(ds, podsToDelete)
+		podsToDelete, err = dsc.syncWithPreparingDelete(ctx, ds, podsToDelete)
 		if err != nil {
 			return err
 		}
@@ -883,12 +864,13 @@ func (dsc *ReconcileDaemonSet) syncNodes(ctx context.Context, ds *appsv1alpha1.D
 	return utilerrors.NewAggregate(errors)
 }
 
-func (dsc *ReconcileDaemonSet) syncWithPreparingDelete(ds *appsv1alpha1.DaemonSet, podsToDelete []string) (podsCanDelete []string, err error) {
+func (dsc *ReconcileDaemonSet) syncWithPreparingDelete(ctx context.Context, ds *appsv1alpha1.DaemonSet, podsToDelete []string) (podsCanDelete []string, err error) {
 	for _, podName := range podsToDelete {
-		pod, err := dsc.podLister.Pods(ds.Namespace).Get(podName)
-		if errors.IsNotFound(err) {
-			continue
-		} else if err != nil {
+		pod := &corev1.Pod{}
+		if err := dsc.cache.Get(ctx, runtimeclient.ObjectKey{Namespace: ds.Namespace, Name: podName}, pod); err != nil {
+			if errors.IsNotFound(err) {
+				continue
+			}
 			return nil, err
 		}
 		if !lifecycle.IsPodHooked(ds.Spec.Lifecycle.PreDelete, pod) {
diff --git a/pkg/controller/daemonset/daemonset_history.go b/pkg/controller/daemonset/daemonset_history.go
index 807f1ec9..4469fc77 100644
--- a/pkg/controller/daemonset/daemonset_history.go
+++ b/pkg/controller/daemonset/daemonset_history.go
@@ -32,6 +32,7 @@ import (
 	"k8s.io/klog/v2"
 	kubecontroller "k8s.io/kubernetes/pkg/controller"
 	labelsutil "k8s.io/kubernetes/pkg/util/labels"
+	runtimeclient "sigs.k8s.io/controller-runtime/pkg/client"
 
 	appsv1alpha1 "github.com/openkruise/kruise/apis/apps/v1alpha1"
 	"github.com/openkruise/kruise/pkg/util"
@@ -106,9 +107,17 @@ func (dsc *ReconcileDaemonSet) controlledHistories(ctx context.Context, ds *apps
 
 	// List all histories to include those that don't match the selector anymore
 	// but have a ControllerRef pointing to the controller.
-	histories, err := dsc.historyLister.ControllerRevisions(ds.Namespace).List(labels.Everything())
-	if err != nil {
-		return nil, err
+	crList := &apps.ControllerRevisionList{}
+	if err := dsc.cache.List(ctx, crList,
+		runtimeclient.InNamespace(ds.Namespace),
+		runtimeclient.MatchingLabelsSelector{Selector: labels.Everything()},
+	); err != nil {
+		return nil, fmt.Errorf("couldn't get list of controller revisions for DaemonSet %#v: %v", ds, err)
+	}
+	histories := make([]*apps.ControllerRevision, 0, len(crList.Items))
+	for i := range crList.Items {
+		history := &crList.Items[i]
+		histories = append(histories, history)
 	}
 	// If any adoptions are attempted, we should first recheck for deletion with
 	// an uncached quorum read sometime after listing Pods (see #42639).
diff --git a/pkg/controller/daemonset/daemonset_update.go b/pkg/controller/daemonset/daemonset_update.go
index 72ddc2e1..751909c5 100644
--- a/pkg/controller/daemonset/daemonset_update.go
+++ b/pkg/controller/daemonset/daemonset_update.go
@@ -36,6 +36,7 @@ import (
 	clonesetutils "github.com/openkruise/kruise/pkg/controller/cloneset/utils"
 	"github.com/openkruise/kruise/pkg/util"
 	"github.com/openkruise/kruise/pkg/util/inplaceupdate"
+	runtimeclient "sigs.k8s.io/controller-runtime/pkg/client"
 )
 
 // rollingUpdate identifies the set of old pods to in-place update, delete, or additional pods to create on nodes,
@@ -52,7 +53,7 @@ func (dsc *ReconcileDaemonSet) rollingUpdate(ctx context.Context, ds *appsv1alph
 	}
 
 	// Advanced: filter the pods updated, updating and can update, according to partition and selector
-	nodeToDaemonPods, err = dsc.filterDaemonPodsToUpdate(ds, nodeList, hash, nodeToDaemonPods)
+	nodeToDaemonPods, err = dsc.filterDaemonPodsToUpdate(ctx, ds, nodeList, hash, nodeToDaemonPods)
 	if err != nil {
 		return fmt.Errorf("failed to filterDaemonPodsToUpdate: %v", err)
 	}
@@ -142,7 +143,7 @@ func (dsc *ReconcileDaemonSet) rollingUpdate(ctx context.Context, ds *appsv1alph
 
 		// Advanced: update pods in-place first and still delete the others
 		if ds.Spec.UpdateStrategy.RollingUpdate.Type == appsv1alpha1.InplaceRollingUpdateType {
-			oldPodsToDelete, err = dsc.inPlaceUpdatePods(ds, oldPodsToDelete, curRevision, oldRevisions)
+			oldPodsToDelete, err = dsc.inPlaceUpdatePods(ctx, ds, oldPodsToDelete, curRevision, oldRevisions)
 			if err != nil {
 				return err
 			}
@@ -283,7 +284,7 @@ func GetTemplateGeneration(ds *appsv1alpha1.DaemonSet) (*int64, error) {
 	return &generation, nil
 }
 
-func (dsc *ReconcileDaemonSet) filterDaemonPodsToUpdate(ds *appsv1alpha1.DaemonSet, nodeList []*corev1.Node, hash string, nodeToDaemonPods map[string][]*corev1.Pod) (map[string][]*corev1.Pod, error) {
+func (dsc *ReconcileDaemonSet) filterDaemonPodsToUpdate(ctx context.Context, ds *appsv1alpha1.DaemonSet, nodeList []*corev1.Node, hash string, nodeToDaemonPods map[string][]*corev1.Pod) (map[string][]*corev1.Pod, error) {
 	existingNodes := sets.NewString()
 	for _, node := range nodeList {
 		existingNodes.Insert(node.Name)
@@ -294,7 +295,7 @@ func (dsc *ReconcileDaemonSet) filterDaemonPodsToUpdate(ds *appsv1alpha1.DaemonS
 		}
 	}
 
-	nodeNames, err := dsc.filterDaemonPodsNodeToUpdate(ds, hash, nodeToDaemonPods)
+	nodeNames, err := dsc.filterDaemonPodsNodeToUpdate(ctx, ds, hash, nodeToDaemonPods)
 	if err != nil {
 		return nil, err
 	}
@@ -306,7 +307,7 @@ func (dsc *ReconcileDaemonSet) filterDaemonPodsToUpdate(ds *appsv1alpha1.DaemonS
 	return ret, nil
 }
 
-func (dsc *ReconcileDaemonSet) filterDaemonPodsNodeToUpdate(ds *appsv1alpha1.DaemonSet, hash string, nodeToDaemonPods map[string][]*corev1.Pod) ([]string, error) {
+func (dsc *ReconcileDaemonSet) filterDaemonPodsNodeToUpdate(ctx context.Context, ds *appsv1alpha1.DaemonSet, hash string, nodeToDaemonPods map[string][]*corev1.Pod) ([]string, error) {
 	var err error
 	var partition int32
 	var selector labels.Selector
@@ -343,8 +344,8 @@ func (dsc *ReconcileDaemonSet) filterDaemonPodsNodeToUpdate(ds *appsv1alpha1.Dae
 		}
 
 		if selector != nil {
-			node, err := dsc.nodeLister.Get(nodeName)
-			if err != nil {
+			node := &corev1.Node{}
+			if err := dsc.cache.Get(ctx, runtimeclient.ObjectKey{Name: nodeName}, node); err != nil {
 				return nil, fmt.Errorf("failed to get node %v: %v", nodeName, err)
 			}
 			if selector.Matches(labels.Set(node.Labels)) {
@@ -393,11 +394,11 @@ func (dsc *ReconcileDaemonSet) canPodInPlaceUpdate(pod *corev1.Pod, curRevision
 	return dsc.inplaceControl.CanUpdateInPlace(oldRevision, curRevision, getInPlaceUpdateOptions())
 }
 
-func (dsc *ReconcileDaemonSet) inPlaceUpdatePods(ds *appsv1alpha1.DaemonSet, podNames []string, curRevision *apps.ControllerRevision, oldRevisions []*apps.ControllerRevision) (podsNeedDelete []string, err error) {
+func (dsc *ReconcileDaemonSet) inPlaceUpdatePods(ctx context.Context, ds *appsv1alpha1.DaemonSet, podNames []string, curRevision *apps.ControllerRevision, oldRevisions []*apps.ControllerRevision) (podsNeedDelete []string, err error) {
 	var podsToUpdate []*corev1.Pod
 	for _, name := range podNames {
-		pod, err := dsc.podLister.Pods(ds.Namespace).Get(name)
-		if err != nil || !dsc.canPodInPlaceUpdate(pod, curRevision, oldRevisions) {
+		pod := &corev1.Pod{}
+		if err := dsc.cache.Get(ctx, runtimeclient.ObjectKey{Name: name, Namespace: ds.Namespace}, pod); err != nil || !dsc.canPodInPlaceUpdate(pod, curRevision, oldRevisions) {
 			podsNeedDelete = append(podsNeedDelete, name)
 			continue
 		}
diff --git a/pkg/controller/daemonset/daemonset_util.go b/pkg/controller/daemonset/daemonset_util.go
index 646778c8..d2ab6a26 100644
--- a/pkg/controller/daemonset/daemonset_util.go
+++ b/pkg/controller/daemonset/daemonset_util.go
@@ -17,6 +17,7 @@ limitations under the License.
 package daemonset
 
 import (
+	"context"
 	"fmt"
 	"sort"
 	"sync"
@@ -38,6 +39,7 @@ import (
 	kruiseutil "github.com/openkruise/kruise/pkg/util"
 	"github.com/openkruise/kruise/pkg/util/inplaceupdate"
 	"github.com/openkruise/kruise/pkg/util/lifecycle"
+	runtimeclient "sigs.k8s.io/controller-runtime/pkg/client"
 )
 
 var (
@@ -151,19 +153,24 @@ func getBurstReplicas(ds *appsv1alpha1.DaemonSet) int {
 // GetPodDaemonSets returns a list of DaemonSets that potentially match a pod.
 // Only the one specified in the Pod's ControllerRef will actually manage it.
 // Returns an error only if no matching DaemonSets are found.
-func (dsc *ReconcileDaemonSet) GetPodDaemonSets(pod *corev1.Pod) ([]*appsv1alpha1.DaemonSet, error) {
+func (dsc *ReconcileDaemonSet) GetPodDaemonSets(ctx context.Context, pod *corev1.Pod) ([]*appsv1alpha1.DaemonSet, error) {
 	if len(pod.Labels) == 0 {
 		return nil, fmt.Errorf("no daemon sets found for pod %v because it has no labels", pod.Name)
 	}
 
-	dsList, err := dsc.dsLister.DaemonSets(pod.Namespace).List(labels.Everything())
-	if err != nil {
-		return nil, err
+	dsList := &appsv1alpha1.DaemonSetList{}
+	if err := dsc.cache.List(ctx, dsList,
+		runtimeclient.InNamespace(pod.Namespace),
+		runtimeclient.MatchingLabelsSelector{Selector: labels.Everything()},
+	); err != nil {
+		return nil, fmt.Errorf("couldn't get list of daemonsets by pod %#v: %v", pod, err)
 	}
 
 	var selector labels.Selector
 	var daemonSets []*appsv1alpha1.DaemonSet
-	for _, ds := range dsList {
+	for i := range dsList.Items {
+		ds := &dsList.Items[i]
+		var err error
 		selector, err = kruiseutil.ValidatedLabelSelectorAsSelector(ds.Spec.Selector)
 		if err != nil {
 			// this should not happen if the DaemonSet passed validation
