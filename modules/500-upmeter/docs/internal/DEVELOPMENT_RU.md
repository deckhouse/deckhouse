---
title: "Информация для разработчиков"
---

Приложение состоит из трех частей:

- upmeter agent (ds/upmeter-agent)
- upmeter server (sts/upmeter)
- smoke-mini (sts/smoke-mini-{a,b,c,d,e})

## Agent

Агент измеряет доступность компонентов Deckhouse. Измерение состоит в проверке состояния объекта в апи кластера или
проверке делает ответа на HTTP-запрос к приложению. Например, что состояние пода Ready или что Prometheus отвечает
корректно на HTTP-запрос.

Логическая единица доступности называется пробой. Проба тестирует некоторую функциональность. Проба состоит из одной или
нескольких параллельных проверок. Например, проба `cluster-scaling` состоит из трех проверок, проверяющих статус подов
`cloud-controller-manager`, `machine-controller-manager` и `bashible-apiserver`.

Если одна из проверок выявила недоступность, компонента, статус пробы принимается `down`. Помимо недоступного, статус
может быть `up`, если нет недоступных результатов проверок, или `uncertain`, если все проверки не смогли установить
доступность или недоступность компонентов. Так может получиться, если не выполнены условия, заложенные в проверку.
Например, проверки, опирающиеся на статус пода, будут в статусе `uncertain`, если перед тем, как проверить под, не
подтвердится доступность аписервера.

Результат запуска пробы — статус доступности той функциональности, которую проба проверяет. Пробы запускаются
периодически с заранее заданным интервалом. Самая частая проба — `dns`, — запускается 5 раз в секунду. Самые
редкие пробы запускаются раз в минуту, например, проверка жизненного цикла `namespace`.

Результаты проверок снимаются с минимальной необходимой частотой — 5 раз в секунду и собираются в массив статусов проб.
Накопление статусов длится 30 секунд. Каждые 30 секунд собирается статистика для каждой пробы. Статистика состоит из
четырех чисел:

- длительность доступности (uptime)
- длительность недоступности (downtime)
- длительность неопределенности (uncertain)
- оставшееся время из 30 секунд, за которое изменения не проводилось (nodata).

Измерение не проводится только в том случае, если агент не запущен.

Пробы объединяются в группы доступности. На эти группы выдается SLA. Статистика доступности группы так же вычисляется
агентом во время сбора статистики проб. Статус группы вычисляется из статусов проб так же, как статус пробы вычисляется
из статусов проверок.

Статистика доступности проб и групп за 30 секунд отправляется в `upmeter server` по HTTP.

Agent — это DaemonSet, который запускается только на узлах control plane. Поды агента используют SQLite для «WAL».
Поэтому если `upmeter` недоступен, данные будут отправлены, когда тот поднимется. Данные в WAL ограничены последними 24
часами.

## Upmeter

- Хранит всё в базе данных SQLite, в файле `/db/dowtime.db.sqlite`.

- Умеет принимать данные от агентов:
  - Сразу дедуплицирует данные из 30-секундных таймслотов в 5-минутные (выбирая “лучший” результат);
  - Складывает данные в SQLite.

- Хранит 30-секундные таймслоты только за сутки, старое удаляет из таблицы.
- Хранит 5-минутные таймслоты постоянно, ничего не удаляет.

- (План, не имплементировано) Умеет читать CRD `Downtime`, в этих CRD:
  - startDate, endDate: время начала и время конца простоя в формате ISO,
  - type: тип простоя:
    - Accident – авария “по нашей вине”;
    - Maintenance – плановые работы;
    - InfrastructureMaintenance – плановые работы у провайдера инфраструктуры;
    - InfrastructureAccident – проблемы с инфраструктурой у провайдера;
  - description: информация для пользователей;
  - affected: перечень подсистем/компонентов, которых касается касается Downtime.

Пример Downtime:

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: Downtime
metadata:
  name: change-pod-cidr
  labels:
    heritage: deckhouse
    module: upmeter
spec:
- startDate: "2020-10-23T12:00:00Z"
  endDate: "2020-10-23T13:00:00Z"
  type: Maintenance
  description: "Change Pod's CIDR, ticket #33121"
  affected:
  - synthetic
  - control-plane
```

### API

Список подсистем, с информацией о списке компонентов для каждой подсистемы.

Уровень доступности по подсистеме/компоненту, в запросе передается:

- Период (с–по) и шаг. Например, чтобы получить месячный uptime, нужно передать последние 30 дней в качестве периода
  и `step 30d`, а чтобы получить данные по дням за последнюю неделю — нужно передать 7 дней и `step 1d`.
- Дополнительным параметром можно передать, какие виды простоя включить в расчет (Maintenance,
  InfrastructureMaintenance, InfrastructureAccident) – в этом случае уровень доступности рассчитывается без учета
  простоев этих типов.
- Состояние доступности по подсистеме/компоненту (для отрисовки “графика доступности”). Передаются step и период (с-по).
  Для каждого step возвращается состояние:
  - доступен;
  - недоступен;
  - если есть, uid Downtime с Accident;
  - недоступен, без нарушения SLA;
  - был Maintenance (+ uid Downtime);
  - был InfrastructureMaintenance (+ uid Downtime);
  - был InfrastructureAccident (+ uid Downtime);
  - нет данных.

### Алгоритм

- Подписывается на CR Downtime и на список Pod’ов измерятора.
- С информером работает web-интерфейс Deckhouse, а также “отправлятор” в “центральную штуку” (cronjob).

## Проверки

- control-plane — запросы к API-серверу кластера:
  - access
  - basic-functionality
  - control-plane-manager
  - namespace
  - scheduler
- synthetic — запросы к smokeMini:
  - access
  - dns
  - neighbor
  - neighbor-via-service
- nginx
- node-group
- monitoring-and-autoscaling
- extensions-availability

## smoke-mini

Пробы из группы "synthetic" опрашивают smoke-mini — приложение, имитирующее настоящее. Это позволяет оценить, как будут
вести себя настоящие приложения в кластере. `smoke-mini` запускает три `StatefulSet`, использующих `PV` и каждый имеющий
1 реплику, со специальным приложением, поднимающим HTTP-сервер и предоставляющим API для выполнения тестов. Ресурсы
одного из `StatefulSet` перешедуливаются раз в 10 минут на случайные узлы.

### Функционал тестирования

* `/` – return 200;
* `/error` – return 500;
* `/api` – проверяет доступ к API Kubernetes (запрашивается информация по Pod'у, из которого выполняется
  запрос `/api/v1/namespaces/d8-smoke-mini/pods/<POD_NAME>`);
* `/dns` – проверяет работу кластерного dns (выполняет резолв домена `kubernetes.default`);
* `/disk` – проверяет, что может создать и удалить файл;
* `/neighbor` – проверяет, есть ли доступ к "соседу" по HTTP;
* `/prometheus` – проверяет, что может отправить запрос в Prometheus `/api/v1/metadata?metric=prometheus_build_info`.

## Блок-схемы проб в группе доступности Control Plane

### Жизненный цикл объекта

Проба направлена на аписервер. Созданием и удалением объекта проверяется простой жизненный цикл объекта. Ошибки операций
с аписервером засчитываются как нерабочее состояние.

Пробы:

- `Basic Functionality`, создается и удаляется ConfigMap
- `Namespace`, создается и удаляется Namespace

![](01-single-object-lifecycle.png)

### Жизненный цикл состояния объекта

Проба направлена на определение состояние объекта в его жизненном цикле. Проба считается непрошедшей, если объект не
приобретает ожидаемого состояния. Например, если Pod не шедулится на ноду. Ошибки операций с аписервером считаются
условием для неопределенного результата пробы.

Пробы:

- `Scheduler`, Pod'у должен быть назначен узел

![](02-controller-object-lifecycle.png)

### Жизненный цикл дочернего объекта

Проба направлена на контроллер, который при создании одного объекта порождает существование другого. Проба проверяет,
что жизненный цикл дочернего объекта ожидаемо связан с жизненным циклом родительского.

Пробы:

- `Controller Manager`: StatefulSet → Pod,
- `Cert Manager`: Certificate → Secret.

![](03-parent-child-lifecycle.png)
