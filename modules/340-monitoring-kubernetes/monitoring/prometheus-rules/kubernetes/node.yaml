- name: kubernetes.node
  rules:
  - alert: CPUStealHigh
    expr: max by (node) (irate(node_cpu_seconds_total{mode="steal"}[30m]) * 100) > 10
    for: 30m
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      description: |-
        The CPU steal is too high on the {{ $labels.node }} Node in the last 30 minutes.

        Probably, some other component is stealing Node resources (e.g., a neighboring virtual machine). This may be the result of "overselling" the hypervisor. In other words, there are more virtual machines than the hypervisor can handle.
      summary: CPU Steal on the {{ $labels.node }} Node is too high.

  - alert: NodeSystemExporterDoesNotExistsForNode
    expr: sum by (node) (kubernetes_build_info{job="kubelet"}) unless (sum by (node) (up{node=~".+", job="kubelet"}) and sum by (node) (up{node=~".+", job="node-exporter"}))
    for: 5m
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |-
        Some of the Node system exporters don't work correctly for the {{ $labels.node }} Node.

        The recommended course of action:
        1. Find the Node exporter Pod for this Node: `kubectl -n d8-monitoring get pod -l app=node-exporter -o json | jq -r ".items[] | select(.spec.nodeName==\"{{$labels.node}}\") | .metadata.name"`;
        2. Describe the Node exporter Pod: `kubectl -n d8-monitoring describe pod <pod_name>`;
        3. Check that kubelet is running on the {{ $labels.node }} node.

  - alert: NodeConntrackTableFull
    expr: max by (node) ( node_nf_conntrack_entries / node_nf_conntrack_entries_limit * 100 > 70 )
    for: 5m
    labels:
      severity_level: "4"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |-
        The conntrack table on the {{ $labels.node }} is {{ $value }}% of the maximum size.

        There's nothing to worry about yet if the `conntrack` table is only 70-80 percent full. However, if it runs out, you will experience problems with new connections while the software will behave strangely.

        The recommended course of action is to identify the source of "excess" `conntrack` entries using Okmeter or Grafana charts.
      summary: The `conntrack` table is close to the maximum size.

  - alert: NodeConntrackTableFull
    expr: max by (node) ( node_nf_conntrack_entries / node_nf_conntrack_entries_limit * 100 > 95 )
    for: 1m
    labels:
      severity_level: "3"
      tier: cluster
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |-
        The `conntrack` table on the {{ $labels.node }} Node is full!

        No new connections are created or accepted on the Node; note that this may result in strange software issues.

        The recommended course of action is to identify the source of "excess" `conntrack` entries using Okmeter or Grafana charts.
      summary: The `conntrack` table is full.

  - alert: NodeUnschedulable
    expr: max by (node) (kube_node_spec_unschedulable) == 1
    for: 20m
    labels:
      severity_level: "8"
      tier: cluster
    annotations:
      plk_markup_format: "markdown"
      plk_protocol_version: "1"
      summary: The {{ $labels.node }} Node is cordon-protected; no new Pods can be scheduled onto it.
      description: |-
        The {{ $labels.node }} Node is cordon-protected; no new Pods can be scheduled onto it.

        This means that someone has executed one of the following commands on that Node:
        - `kubectl cordon {{ $labels.node }}`
        - `kubectl drain {{ $labels.node }}` that runs for more than 20 minutes

        Probably, this is due to the maintenance of this Node.
