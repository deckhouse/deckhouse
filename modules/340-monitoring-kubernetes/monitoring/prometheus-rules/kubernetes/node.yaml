- name: kubernetes.node
  rules:
  - record: node_ntp_offset_seconds:abs
    expr: abs(node_ntp_offset_seconds)

  - alert: NTPDaemonOnNodeDoesNotSynchronizeTime
    expr: (min by (node) (node_ntp_sanity)) == 0
    for: 2h
    labels:
      impact: marginal
      likelihood: certain
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |
        1. Please, check the NTP daemon's status by executing the following commands on the Node:
           * for `ntpd` - 'ntpq -p' or 'ntpdc -c sysinfo' or 'ntpdc -c sysstats';
           * for `chronyd` - 'chronyc sources -v' or 'chronyc tracking' or 'chronyc sourcestats -v' or 'chronyc ntpdata';
           * for `systemd-timesyncd` - 'timedatectl status' or 'systemctl status systemd-timesyncd'.
        2. Correct the time synchronization problems:
           * restart NTP daemon:
             - for `ntpd` - 'service ntp restart' or 'service ntpd restart' or 'systemctl restart ntp' or 'systemctl restart ntpd';
             - for `chronyd` - 'systemctl restart chronyd';
             - for `systemd-timesyncd` - 'systemctl restart systemd-timesyncd'.
           * correct network problems:
             - provide availability to upstream time synchronization servers defined in the NTP daemon configuration file;
             - eliminate large packet loss and excessive latency to upstream time synchronization servers.
           * correct errors in the NTP daemon configuration file:
             - for `ntpd` - '/etc/ntp.conf';
             - for `chronyd` - '/etc/chrony.conf';
             - for `systemd-timesyncd` - '/etc/systemd/timesyncd.conf'.
      summary: NTP daemon on node {{$labels.node}} have not synchronized time for too long.

  - alert: NodeTimeOutOfSync
    expr: max by (node) (abs(node_time_seconds - timestamp(node_time_seconds)) > 10)
    for: 5m
    labels:
      impact: critical
      likelihood: certain
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |
        Node's {{$labels.node}} time is out of sync from Prometheus Node by {{ $value }} seconds.
      summary: Node's {{$labels.node}} clock is drifting.

  - alert: CPUStealHigh
    expr: max by (node) (irate(node_cpu_seconds_total{mode="steal"}[30m]) * 100) > 10
    for: 30m
    labels:
      impact: marginal
      likelihood: certain
    annotations:
      plk_protocol_version: "1"
      description: |-
        The CPU steal is too high on the {{ $labels.node }} Node in the last 30 minutes.

        Probably, some other component is stealing Node resources (e.g., a neighboring virtual machine). This may be the result of "overselling" the hypervisor. In other words, there are more virtual machines than the hypervisor can handle.
      summary: CPU Steal on the {{ $labels.node }} Node is too high.

  - alert: NodeSystemExporterDoesNotExistsForNode
    expr: sum by (node) (kubernetes_build_info{job="kubelet"}) unless (sum by (node) (up{node=~".+", job="kubelet"}) and sum by (node) (up{node=~".+", job="node-exporter"}))
    for: 5m
    labels:
      impact: marginal
      likelihood: certain
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |-
        Some of the Node system exporters don't work correctly for the {{ $labels.node }} Node.

        The recommended course of action:
        1. Find the Node exporter Pod for this Node: `kubectl -n d8-monitoring get pod -l app=node-exporter -o json | jq -r ".items[] | select(.spec.nodeName==\"{{$labels.node}}\") | .metadata.name"`;
        2. Describe the Node exporter Pod: `kubectl -n d8-monitoring describe pod <pod_name>`;
        3. Check that kubelet is running on the {{ $labels.node }} node.

  - alert: NodeConntrackTableFull
    expr: max by (node) ( node_nf_conntrack_entries / node_nf_conntrack_entries_limit * 100 > 70 )
    for: 5m
    labels:
      impact: catastrophic
      likelihood: unlikely
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |-
        The conntrack table on the {{ $labels.node }} is {{ $value }}% of the maximum size.

        There's nothing to worry about yet if the `conntrack` table is only 70-80 percent full. However, if it runs out, you will experience problems with new connections while the software will behave strangely.

        The recommended course of action is to identify the source of "excess" `conntrack` entries using Okmeter or Grafana charts.
      summary: The `conntrack` table is close to the maximum size.

  - alert: NodeConntrackTableFull
    expr: max by (node) ( node_nf_conntrack_entries / node_nf_conntrack_entries_limit * 100 > 95 )
    for: 1m
    labels:
      impact: catastrophic
      likelihood: certain
    annotations:
      plk_protocol_version: "1"
      plk_markup_format: markdown
      description: |-
        The `conntrack` table on the {{ $labels.node }} Node is full!

        No new connections are created or accepted on the Node; note that this may result in strange software issues.

        The recommended course of action is to identify the source of "excess" `conntrack` entries using Okmeter or Grafana charts.
      summary: The `conntrack` table is full.

  - alert: NodeUnschedulable
    expr: max by (node) (kube_node_spec_unschedulable) == 1
    labels:
      severity_level: "8"
      tier: cluster
    annotations:
      plk_markup_format: "markdown"
      plk_protocol_version: "1"
      plk_pending_until_firing_for: "20m"
      summary: The {{ $labels.node }} Node is cordon-protected; no new Pods can be scheduled onto it.
      description: |-
        The {{ $labels.node }} Node is cordon-protected; no new Pods can be scheduled onto it.

        This means that someone has executed one of the following commands on that Node:
        - `kubectl cordon {{ $labels.node }}`
        - `kubectl drain {{ $labels.node }}` that runs for more than 20 minutes

        Probably, this is due to the maintenance of this Node.
