- name: kubernetes.drbd.device_state
  rules:
    - alert: D8LinstorVolumeIsNotHealthy
      expr: max by (exported_node, resource) (linstor_volume_state != 1 and linstor_volume_state != 4)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        summary: LINSTOR volume is not healthy
        description: |
          LINSTOR volume {{ $labels.resource }} on node {{ $labels.exported_node }} is not healthy

          The recommended course of action:
          1. Login into node with the problem:

             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app=linstor-node -o name) -c linstor-satellite -- bash
             ```

          2. Check the LINSTOR node state:

             ```
             linstor node list -n {{ $labels.exported_node }}
             ```

          3. Check the LINSTOR resource states:

             ```
             linstor resource list -r {{ $labels.resource }}
             ```

          4. View the status of the DRBD device and try to figure out why it is not UpToDate:

             ```
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```

    - alert: D8DrbdDeviceHasNoQuorum
      expr: max by (node, name) (drbd_device_quorum == 0)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        summary: DRBD device has no quorum
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} has no quorum.

          The recommended course of action:
          1. Login into the node with the problem:

             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app=linstor-node -o name) -c linstor-satellite -- bash
             ```

          2. Check the drbd resource state:

             ```
             drbd status {{ $labels.name }}
             ```

             If all the connections are green (`Diskless` or `UpToDate`) but you see `quorum:no blocked:upper`, then most probably you're faced with this [DRBD bug](https://lists.linbit.com/pipermail/drbd-user/2023-March/026373.html).
             To fix this it should be enough to remove and create an additional diskless replica:

             ```
             linstor resource create some-node {{ $labels.name }} -d
             linstor resource delete some-node {{ $labels.name }}
             ```

          3. Check the LINSTOR resource states:

             ```
             linstor resource list -r {{ $labels.name }}
             ```

          4. Make sure that {{ $labels.node }} persists in the output of the above command.

             If non-persists:
             1. Try to remove the resource from the kernel:

                ```
                drbdsetup down {{ $labels.name }}
                ```

             2. If command has stuck consider rebooting the node {{ $labels.node }}

             If persists:
             1. Check the LINSTOR node and peer node states:

                ```
                linstor node list -n {{ $labels.node }} {{ $labels.conn_name }}
                ```

             2. Check connectivity between {{ $labels.node }} and {{ $labels.conn_name }}.
             3. View the status of the DRBD device on the node and try to figure out why it has no quorum:

                ```
                drbdsetup status --verbose {{ $labels.name }}
                dmesg --color=always | grep 'drbd {{ $labels.name }}'
                ```

             4. Consider recreating failed resources in LINSTOR:

                ```
                linstor resource delete {{ $labels.node }} {{ $labels.name }}
                linstor resource-definition auto-place {{ $labels.name }}
                linstor resource-definition wait-sync {{ $labels.name }}
                ```

    - alert: D8DrbdDeviceIsUnintentionalDiskless
      expr: max by (node, name) (drbd_device_unintentionaldiskless == 1)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        summary: DRBD device is unintentional diskless
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} unintentionally switched to diskless mode

          The recommended course of action:
          1. Login into node with the problem:

             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app=linstor-node -o name) -c linstor-satellite -- bash
             ```

          2. Check the LINSTOR resource state on {{ $labels.node }}:

             ```
             linstor resource list -r {{ $labels.name }}
             ```

          3. Check the LINSTOR storage-pools on {{ $labels.node }}:

             ```
             linstor storage-pools list -r {{ $labels.name }}
             ```

          4. View the status of the DRBD device:

             ```
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```

          5. Check the backing storage device:

             ```
             lsblk
             ```

          6. Consider recreating failed resources in LINSTOR:

             ```
             linstor resource delete {{ $labels.node }} {{ $labels.name }}
             linstor resource-definition auto-place {{ $labels.name }}
             linstor resource-definition wait-sync {{ $labels.name }}
             ```

    - alert: D8DrbdPeerDeviceIsOutOfSync
      expr: max by (node, conn_name, name) (drbd_peerdevice_outofsync_bytes > 0)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        summary: DRBD device has out-of-sync data
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} has out-of-sync data with {{ $labels.conn_name }}

          The recommended course of action:
          1. Login into node with the problem:

             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app=linstor-node -o name) -c linstor-satellite -- bash
             ```

          2. Check the LINSTOR peer node state:

             ```
             linstor node list -n {{ $labels.conn_name }}
             ```

          3. Check the LINSTOR resource states:

             ```
             linstor resource list -r {{ $labels.name }}
             ```

          4. View the status of the DRBD device on the node and try to figure out why it has out-of-sync data with {{ $labels.conn_name }}:

             ```
             drbdsetup status {{ $labels.name }} --statistics
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```

          5. Consider reconnect resource with the peer node:

             ```
             drbdadm disconnect {{ $labels.name }}:{{ $labels.conn_name }}
             drbdadm connect {{ $labels.name }}:{{ $labels.conn_name }}
             ```

          6. Check if problem has solved, device should have no out-of-sync data:

             ```
             drbdsetup status {{ $labels.name }} --statistics
             ```

    - alert: D8DrbdDeviceIsNotConnected
      expr: max by (node, conn_name, name) (drbd_connection_state{drbd_connection_state!="UpToDate", drbd_connection_state!="Connected"} == 1)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse,kubernetes=~kubernetes"
        summary: DRBD device is not connected
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} is not connected with {{ $labels.conn_name }}

          The recommended course of action:
          1. Login into node with the problem:

             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app=linstor-node -o name) -c linstor-satellite -- bash
             ```

          2. Check the LINSTOR resource states:

             ```
             linstor resource list -r {{ $labels.name }}
             ```

          3. Make sure if {{ $labels.node }} persists in output of above command.

             If non-persists:
             1. Try to remove the resource from the kernel:

                ```
                drbdsetup down {{ $labels.name }}
                ```
             2. If command has stuck consider rebooting the node {{ $labels.node }}.

             If persists:
             1. Check the LINSTOR node and peer node states:

                ```
                linstor node list -n {{ $labels.node }} {{ $labels.conn_name }}
                ```

             2. Check connectivity between {{ $labels.node }} and {{ $labels.conn_name }}.
             3. View the status of the DRBD device on the node and try to figure out why it is not connected:

                ```
                drbdadm status {{ $labels.name }}
                dmesg --color=always | grep 'drbd {{ $labels.name }}'
                ```

             4. Consider recreating failed resources in LINSTOR:

                ```
                linstor resource delete {{ $labels.conn_name }} {{ $labels.name }}
                linstor resource-definition auto-place {{ $labels.name }}
                linstor resource-definition wait-sync {{ $labels.name }}
                ```
