---
title: "Модуль linstor: настройки"
---

Модуль по умолчанию **выключен**. Для включения добавьте в ConfigMap `deckhouse`:

```yaml
data:
  linstorEnabled: "true"
```

Сам модуль не требует настройки и не имеет параметров. 

После включения модуля кластер автоматически настраивается на использование LINSTOR и остается только сконфигурировать хранилище.

## Конфигурация хранилища LINSTOR

Конфигурация LINSTOR в Deckhouse осуществляется посредством назначения специального тега `linstor-<имя_пула>` на LVM-группу томов или LVMThin-пул.  

1. Выберите имя тега.

   Имя тега должно быть уникальным в пределах одного узла. Поэтому каждый раз, прежде чем назначить новый тег, убедитесь в отсутствии этого тега у других групп томов и пулов. 
   
   Выполните следующие команды, чтобы вывести список групп томов и пулов:

   ```shell
   vgs -o name,tags
   lvs -o name,vg_name,tags
   ```

1. Добавьте пулы.

   Создайте пулы хранения на всех узлах, где вы планируете хранить ваши данные. Используйте одинаковые имена пулов хранения на разных узлах, если хотите иметь для них общий StorageClass.

   - Чтобы добавить пул **LVM** создайте группу томов с тегом `linstor-<имя_пула>`, либо добавьте тег `linstor-<имя_пула>` существующей группе.
   
     Пример команды создания группы томов `data_project` с тегом `linstor-data`:
   
     ```shell
     vgcreate data_project /dev/nvme0n1 /dev/nvme1n1 --add-tag linstor-data
     ```
   
     Пример команды добавления тега `linstor-data` существующей группе томов `data_project`:
   
     ```shell
     vgchange data_project --add-tag linstor-data
     ```
 
   - Чтобы добавить пул **LVMThin** создайте LVMThin-пул с тегом `linstor-<имя_пула>`.

     Пример команды создания LVMThin-пула `data_project/thindata` с тегом `linstor-data`:
     
     ```shell
     vgcreate data_project /dev/nvme0n1 /dev/nvme1n1
     lvcreate -L 1.8T -T data_project/thindata --add-tag linstor-thindata
     ```

     > Обратите внимание, что сама группа томов не обязана содержать какой-либо тег.

1. Проверьте создание StorageClass.

   Когда все пулы хранения будут созданы, появятся три новых StorageClass'а. Проверьте что они создались, выполнив в кластере Kubernetes команду:

   ```shell
   kubectl get storageclass
   ```

   Пример вывода списка StorageClass:

   ```shell
   $ kubectl get storageclass
   NAME                   PROVISIONER                  AGE
   linstor-data-r1        linstor.csi.linbit.com       143s
   linstor-data-r2        linstor.csi.linbit.com       142s
   linstor-data-r3        linstor.csi.linbit.com       142s
   ```

   Каждый StorageClass можно использовать для создания томов соответственно с одной, двумя или тремя репликами в ваших пулах хранения.

При необходимости изучите пример [расширенной конфигурации LINSTOR](advanced_usage.html), но мы рекомендуем придерживаться приведенного выше упрощённого руководства.

## Дополнительные возможности по настройке приложений  

### Размещение приложения "поближе" к данным (data locality)

В случае гиперконвергентной инфраструктуры может возникнуть задача по приоритетному размещению Pod'а приложения на узлах, где необходимые ему данные хранилища расположены локально. Это позволит получить максимальную производительность хранилища.

Для решения этой задачи модуль linstor предоставляет специальный планировщик `linstor`, который учитывает размещение данных в хранилище и старается размещать Pod в первую очередь на тех узлах, где данные доступны локально.

Для использования планировщика `linstor`, необходимо в описании Pod'а приложения указать параметр `schedulerName: linstor`.

[Пример...](usage.html#использование-планировщика-linstor)

### Перенос приложения при проблемах с узлом (storage-based fencing)

В случае если приложение не умеет работать в режиме высокой доступности и работает в одном экземпляре, может возникнуть задача принудительного переноса приложения с узла, на котором возникли проблемы. Например, если возникли проблемы с сетевой, дисковой подсистемой и т.д.
  
Для решения этой задачи укажите лейбл `linstor.csi.linbit.com/on-storage-lost: remove` в описании Pod'а. Модуль linstor автоматически удалит такие Pod'ы с узла где возникли проблемы, что позволит Kubernetes перезапустить приложение на другом узле. 

[Пример...](usage.html#перенос-приложения-при-проблемах-с-узлом-storage-based-fencing)
