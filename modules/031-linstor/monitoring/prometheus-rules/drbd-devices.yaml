- name: kubernetes.drbd.device_state
  rules:
    - alert: D8LinstorVolumeIsNotHealthy
      expr: max by (exported_node, resource) (linstor_volume_state != 1 and linstor_volume_state != 4)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=cluster,d8_module=linstor,d8_component=linstor-drbd-devices"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse"
        summary: LINSTOR volume is not healthy
        description: |
          LINSTOR volume {{ $labels.resource }} on node {{ $labels.exported_node }} is not healthy

          The recommended course of action:
          1. Check the LINSTOR node state: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor node list -n {{ $labels.exported_node }}`
          2. Check the LINSTOR resource states: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor resource list -r {{ $labels.resource }}`
          3. View the status of the DRBD device and try to figure out why it is not UpToDate:
             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.exported_node }} -l app.kubernetes.io/instance=linstor,app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-node -o name) -c linstor-satellite -- bash
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```

    - alert: D8DrbdDeviceHasNoQuorum
      expr: max by (node, name) (drbd_device_quorum == 0)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=cluster,d8_module=linstor,d8_component=linstor-drbd-devices"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse"
        summary: DRBD device has no quorum
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} has no quorum

          The recommended course of action:
          1. Check the LINSTOR resource states: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor resource list -r {{ $labels.name }}`
          2. View the status of the DRBD device:
             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app.kubernetes.io/instance=linstor,app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-node -o name) -c linstor-satellite -- bash
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```
          3. Consider recreating failed resources in LINSTOR
             ```
             linstor resource delete {{ $labels.node }} {{ $labels.name }}
             linstor resource-definition auto-place {{ $labels.name }}
             linstor resource-definition wait-sync {{ $labels.name }}
             ```

    - alert: D8DrbdDeviceIsUnintentionalDiskless
      expr: max by (node, name) (drbd_device_unintentionaldiskless == 1)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=cluster,d8_module=linstor,d8_component=linstor-drbd-devices"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse"
        summary: DRBD device is unintentional diskless
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} unintentionally switched to diskless mode

          The recommended course of action:
          1. Check the LINSTOR resource state on {{ $labels.node }}: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor resource list -r {{ $labels.name }}`
          2. Check the LINSTOR storage-pools on {{ $labels.node }}: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor storage-pools list -r {{ $labels.name }}`
          3. View the status of the DRBD device:
             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.node }} -l app.kubernetes.io/instance=linstor,app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-node -o name) -c linstor-satellite -- bash
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```
          4. Check the backing storage device: `lsblk`
          5. Consider recreating failed resources in LINSTOR
             ```
             linstor resource delete {{ $labels.node }} {{ $labels.name }}
             linstor resource-definition auto-place {{ $labels.name }}
             linstor resource-definition wait-sync {{ $labels.name }}
             ```

    - alert: D8DrbdPeerDeviceIsOutOfSync
      expr: max by (node, conn_name, name) (drbd_peerdevice_outofsync_bytes > 0)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=cluster,d8_module=linstor,d8_component=linstor-drbd-devices"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse"
        summary: DRBD device has out-of-sync data
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} has out-of-sync data with {{ $labels.conn_name }}

          The recommended course of action:
          1. Check the LINSTOR peer node state: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor node list -n {{ $labels.conn_name }}`
          2. Check the LINSTOR resource states: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor resource list -r {{ $labels.name }}`
          3. Check connectivity between {{ $labels.node }} and {{ $labels.conn_name }}
          4. View the status of the DRBD device on the node and try to figure out why it is not UpToDate:
             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.conn_name }} -l app.kubernetes.io/instance=linstor,app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-node -o name) -c linstor-satellite -- bash
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```
          5. Consider recreating failed resources in LINSTOR
             ```
             linstor resource delete {{ $labels.conn_name }} {{ $labels.name }}
             linstor resource-definition auto-place {{ $labels.name }}
             linstor resource-definition wait-sync {{ $labels.name }}
             ```

    - alert: D8DrbdDeviceIsNotConnected
      expr: max by (node, conn_name, name) (drbd_connection_state{drbd_connection_state!="UpToDate", drbd_connection_state!="Connected"} == 1)
      for: 5m
      labels:
        severity_level: "6"
        tier: cluster
      annotations:
        plk_markup_format: "markdown"
        plk_protocol_version: "1"
        plk_create_group_if_not_exists__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=cluster,d8_module=linstor,d8_component=linstor-drbd-devices"
        plk_grouped_by__d8_drbd_device_health: "D8DrbdDeviceHealth,tier=~tier,prometheus=deckhouse"
        summary: DRBD device is not connected
        description: |
          DRBD device {{ $labels.name }} on node {{ $labels.node }} is not connected with {{ $labels.conn_name }}

          The recommended course of action:
          1. Check the LINSTOR peer node state: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor node list -n {{ $labels.conn_name }}`
          2. Check the LINSTOR resource states: `kubectl exec -n d8-linstor deploy/linstor-controller -- linstor resource list -r {{ $labels.name }}`
          3. Check connectivity between {{ $labels.node }} and {{ $labels.conn_name }}
          4. View the status of the DRBD device on the node and try to figure out why it is not connected:
             ```
             kubectl -n d8-linstor exec -ti $(kubectl -n d8-linstor get pod --field-selector=spec.nodeName={{ $labels.conn_name }} -l app.kubernetes.io/instance=linstor,app.kubernetes.io/managed-by=piraeus-operator,app.kubernetes.io/name=piraeus-node -o name) -c linstor-satellite -- bash
             drbdsetup status {{ $labels.name }} --verbose
             dmesg --color=always | grep 'drbd {{ $labels.name }}'
             ```
          5. Consider recreating failed resources in LINSTOR
             ```
             linstor resource delete {{ $labels.conn_name }} {{ $labels.name }}
             linstor resource-definition auto-place {{ $labels.name }}
             linstor resource-definition wait-sync {{ $labels.name }}
             ```
