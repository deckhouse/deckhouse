diff --git a/cluster-autoscaler/core/scale_down.go b/cluster-autoscaler/core/scale_down.go
index 7ac3654bf..a9af5b649 100644
--- a/cluster-autoscaler/core/scale_down.go
+++ b/cluster-autoscaler/core/scale_down.go
@@ -841,7 +841,7 @@ func (sd *ScaleDown) TryToScaleDown(
 
 		deletionsInProgress := sd.nodeDeletionTracker.GetDeletionsInProgress(nodeGroup.Id())
 		if size-deletionsInProgress <= nodeGroup.MinSize() {
-			klog.V(1).Infof("Skipping %s - node group min size reached", node.Name)
+			klog.V(1).Infof("Skipping %s on ScaleDown - node group min size reached", node.Name)
 			sd.addUnremovableNodeReason(node, simulator.NodeGroupMinSizeReached)
 			continue
 		}
diff --git a/cluster-autoscaler/processors/nodes/pre_filtering_processor.go b/cluster-autoscaler/processors/nodes/pre_filtering_processor.go
index a40e61e05..73d984b47 100644
--- a/cluster-autoscaler/processors/nodes/pre_filtering_processor.go
+++ b/cluster-autoscaler/processors/nodes/pre_filtering_processor.go
@@ -63,7 +63,7 @@ func (n *PreFilteringScaleDownNodeProcessor) GetScaleDownCandidates(ctx *context
 			continue
 		}
 		if size <= nodeGroup.MinSize() {
-			klog.V(1).Infof("Skipping %s - node group min size reached", node.Name)
+			klog.V(1).Infof("Skipping %s on GetCandidates - node group min size reached", node.Name)
 			continue
 		}
 		result = append(result, node)
diff --git a/cluster-autoscaler/processors/status/scale_down_status_processor.go b/cluster-autoscaler/processors/status/scale_down_status_processor.go
index de30a5c00..bc895f376 100644
--- a/cluster-autoscaler/processors/status/scale_down_status_processor.go
+++ b/cluster-autoscaler/processors/status/scale_down_status_processor.go
@@ -18,11 +18,12 @@ package status
 
 import (
 	apiv1 "k8s.io/api/core/v1"
+	klog "k8s.io/klog/v2"
+
 	"k8s.io/autoscaler/cluster-autoscaler/cloudprovider"
 	"k8s.io/autoscaler/cluster-autoscaler/context"
 	"k8s.io/autoscaler/cluster-autoscaler/simulator"
 	"k8s.io/autoscaler/cluster-autoscaler/utils/drain"
-	klog "k8s.io/klog/v2"
 )
 
 // ScaleDownStatus represents the state of scale down.
@@ -133,7 +134,7 @@ type ScaleDownStatusProcessor interface {
 
 // NewDefaultScaleDownStatusProcessor creates a default instance of ScaleUpStatusProcessor.
 func NewDefaultScaleDownStatusProcessor() ScaleDownStatusProcessor {
-	return &NoOpScaleDownStatusProcessor{}
+	return &CleanTaintsScaleDownStatusProcessor{}
 }
 
 // PodEvictionResult contains the result of an eviction of a pod.
diff --git a/cluster-autoscaler/processors/status/scale_down_status_taint_processor.go b/cluster-autoscaler/processors/status/scale_down_status_taint_processor.go
new file mode 100644
index 000000000..1bf1b043d
--- /dev/null
+++ b/cluster-autoscaler/processors/status/scale_down_status_taint_processor.go
@@ -0,0 +1,32 @@
+package status
+
+import (
+	apiv1 "k8s.io/api/core/v1"
+	"k8s.io/klog"
+
+	"k8s.io/autoscaler/cluster-autoscaler/context"
+	"k8s.io/autoscaler/cluster-autoscaler/simulator"
+	"k8s.io/autoscaler/cluster-autoscaler/utils/deletetaint"
+)
+
+// NoOpScaleDownStatusProcessor is a ScaleDownStatusProcessor implementations which removes ToBeDeleted taint from unremovable nodes with `node-group min group size reached` error.
+type CleanTaintsScaleDownStatusProcessor struct{}
+
+// Process processes the status of the cluster after a scale-down.
+func (p *CleanTaintsScaleDownStatusProcessor) Process(context *context.AutoscalingContext, status *ScaleDownStatus) {
+	unremovableNodes := make([]*apiv1.Node, 0, len(status.UnremovableNodes))
+	for _, un := range status.UnremovableNodes {
+		// remove taints only if NodeGroupMinSizeReached error, because it could be created during scaling down
+		if un.Reason != simulator.NodeGroupMinSizeReached {
+			klog.V(6).Infof("Node %s is unremovable because of '%s'.", un.Node.Name, un.Reason)
+			continue
+		}
+		klog.V(4).Infof("Node %s is unremovable because of '%s'. Cleaning taint ToBeDeleted", un.Node.Name, un.Reason)
+		unremovableNodes = append(unremovableNodes, un.Node)
+	}
+	deletetaint.CleanAllToBeDeleted(unremovableNodes, context.ClientSet, context.Recorder)
+}
+
+// CleanUp cleans up the processor's internal structures.
+func (p *CleanTaintsScaleDownStatusProcessor) CleanUp() {
+}
diff --git a/cluster-autoscaler/simulator/cluster.go b/cluster-autoscaler/simulator/cluster.go
index d8b710169..8215f96d0 100644
--- a/cluster-autoscaler/simulator/cluster.go
+++ b/cluster-autoscaler/simulator/cluster.go
@@ -98,6 +98,41 @@ const (
 	UnexpectedError
 )
 
+func (ur UnremovableReason) String() string {
+	switch ur {
+	case 0:
+		return "NoReason"
+	case 1:
+		return "ScaleDownDisabledAnnotation"
+	case 2:
+		return "NotAutoscaled"
+	case 3:
+		return "NotUnneededLongEnough"
+	case 4:
+		return "NotUnreadyLongEnough"
+	case 5:
+		return "NodeGroupMinSizeReached"
+	case 6:
+		return "MinimalResourceLimitExceeded"
+	case 7:
+		return "CurrentlyBeingDeleted"
+	case 8:
+		return "NotUnderutilized"
+	case 9:
+		return "NotUnneededOtherReason"
+	case 10:
+		return "RecentlyUnremovable"
+	case 11:
+		return "NoPlaceToMovePods"
+	case 12:
+		return "BlockedByPod"
+	case 13:
+		return "UnexpectedError"
+	}
+
+	return "UnknownReason"
+}
+
 // UtilizationInfo contains utilization information for a node.
 type UtilizationInfo struct {
 	CpuUtil float64
