{{- if (.Values.global.enabledModules | has "vertical-pod-autoscaler-crd") }}
---
apiVersion: autoscaling.k8s.io/v1beta2
kind: VerticalPodAutoscaler
metadata:
  name: d8-kube-dns
  namespace: kube-system
{{ include "helm_lib_module_labels" (list . (dict "app" "d8-kube-dns" "workload-resource-policy.deckhouse.io" "master")) | indent 2 }}
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: d8-kube-dns
  updatePolicy:
    updateMode: "Auto"
{{- end }}
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: d8-kube-dns
  namespace: kube-system
{{ include "helm_lib_module_labels" (list . (dict "k8s-app" "kube-dns")) | indent 2 }}
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: d8-kube-dns
  namespace: kube-system
{{ include "helm_lib_module_labels" (list . (dict "k8s-app" "kube-dns")) | indent 2 }}
spec:
# Policy #1: replicas
#    * If there are special nodes for kube-dns then deployment must fit there and on masters
#    * If there are system-nodes then deployment must fit there and on masters
#    * Else:
#      * there should be at least 2 replicas or more if someone configured it manually
#      * there must not be more replicas then non-specific nodes
  replicas: {{ .Values.kubeDns.internal.replicas }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
    spec:
      imagePullSecrets:
      - name: deckhouse-registry
      # hardcoded, because kube-dns is a critical component and system-cluster-critical priority class exists by default
      priorityClassName: system-cluster-critical
      serviceAccountName: d8-kube-dns
      # Policy #0: kube-dns must be able to work on every master and every specific node
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: node-role.kubernetes.io/master
      - key: node-role/system
      - key: dedicated.flant.com
        operator: Equal
        value: kube-dns
      - key: dedicated.deckhouse.io
        operator: Equal
        value: kube-dns
      - key: dedicated.flant.com
        operator: Equal
        value: system
      - key: dedicated.deckhouse.io
        operator: Equal
        value: system
      - key: dedicated.flant.com
        operator: Equal
        value: master
      - key: dedicated.deckhouse.io
        operator: Equal
        value: master
      containers:
      - name: coredns
        image: {{ .Values.global.modulesImages.registry }}/kube-dns/coredns:{{ .Values.global.modulesImages.tags.kubeDns.coredns }}
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
      - name: kube-rbac-proxy
        image: {{ $.Values.global.modulesImages.registry }}/common/kube-rbac-proxy:{{ $.Values.global.modulesImages.tags.common.kubeRbacProxy }}
        args:
        - "--secure-listen-address=$(KUBE_RBAC_PROXY_LISTEN_ADDRESS):9154"
        - "--client-ca-file=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
        - "--v=2"
        - "--logtostderr=true"
        - "--stale-cache-interval=1h30m"
        ports:
        - containerPort: 9154
          name: https-metrics
        env:
        - name: KUBE_RBAC_PROXY_LISTEN_ADDRESS
          value: "0.0.0.0"
        - name: KUBE_RBAC_PROXY_CONFIG
          value: |
            upstreams:
            - upstream: http://127.0.0.1:9153/metrics
              path: /metrics
              authorization:
                resourceAttributes:
                  namespace: kube-system
                  apiGroup: apps
                  apiVersion: v1
                  resource: deployments
                  subresource: prometheus-metrics
                  name: d8-kube-dns
      dnsPolicy: Default
      volumes:
      - name: config-volume
        configMap:
          name: d8-kube-dns
          items:
          - key: Corefile
            path: Corefile
      affinity:
        nodeAffinity:
{{- if .Values.kubeDns.internal.specificNodeType }}
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: "Exists"
            - matchExpressions:
              - key: "node-role.deckhouse.io/{{ .Values.kubeDns.internal.specificNodeType }}"
                operator: "Exists"
            - matchExpressions:
              - key: "node-role.flant.com/{{ .Values.kubeDns.internal.specificNodeType }}"
                operator: "Exists"
            - matchExpressions:
              - key: "node-role.kubernetes.io/{{ .Values.kubeDns.internal.specificNodeType }}"
                operator: "Exists"
{{- else }}
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: "Exists"
{{- end }}
        podAntiAffinity:
###
# Policy #2:
# * do not run more than one kube-dns on single node except cases with single master and lack of specific nodes
{{- if .Values.kubeDns.internal.enablePodAntiAffinity }}
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                k8s-app: "kube-dns"
{{- else }}
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                topologyKey: "kubernetes.io/hostname"
                labelSelector:
                  matchLabels:
                    k8s-app: "kube-dns"
{{- end }}
