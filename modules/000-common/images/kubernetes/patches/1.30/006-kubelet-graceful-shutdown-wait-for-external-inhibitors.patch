diff --git a/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go b/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go
index 8469cecdc0e..0ce167e8a52 100644
--- a/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go
+++ b/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go
@@ -22,6 +22,7 @@ package nodeshutdown
 
 import (
 	"fmt"
+	"os"
 	"path/filepath"
 	"sort"
 	"sync"
@@ -277,6 +278,12 @@ func (m *managerImpl) start() (chan struct{}, error) {
 					m.recorder.Event(m.nodeRef, v1.EventTypeNormal, kubeletevents.NodeShutdown, "Shutdown manager detected shutdown cancellation")
 				}
 
+				if isShuttingDown {
+					// Wait until external inhibitors are unlocked before proceeding with shutdown.
+					// Do not set m.nodeShuttingDownNow yet to keep Ready status for Node.
+					m.waitForExternalInhibitorsToUnlock()
+				}
+
 				m.nodeShuttingDownMutex.Lock()
 				m.nodeShuttingDownNow = isShuttingDown
 				m.nodeShuttingDownMutex.Unlock()
@@ -351,7 +358,16 @@ func (m *managerImpl) processShutdownEvent() error {
 		}()
 	}
 
-	groups := groupByPriority(m.shutdownGracePeriodByPodPriority, activePods)
+	groups := m.groupByPriority(m.shutdownGracePeriodByPodPriority, activePods)
+
+	m.logger.V(1).Info(fmt.Sprintf("Group pods: gracePeriods: %+v", m.shutdownGracePeriodByPodPriority))
+	for i, group := range groups {
+		m.logger.V(1).Info(fmt.Sprintf("Group pods: group %d: priority %d, gracePeriod %d", i, group.Priority, group.ShutdownGracePeriodSeconds))
+		for _, pod := range group.Pods {
+			m.logger.V(1).Info(fmt.Sprintf("Group pods: group %d: pod %s %s", i, pod.Namespace, pod.Name))
+		}
+	}
+
 	for _, group := range groups {
 		// If there are no pods in a particular range,
 		// then do not wait for pods in that priority range.
@@ -417,6 +433,47 @@ func (m *managerImpl) processShutdownEvent() error {
 	return nil
 }
 
+// waitForExternalInhibitorsToUnlock waits for external inhibitors to unlock before proceeding with shutdown.
+func (m *managerImpl) waitForExternalInhibitorsToUnlock() {
+	const (
+		inhibitorsCheckInterval = 5 * time.Second
+		delayLogInterval        = 2 * time.Second
+	)
+	// Do not wait if inhibitors not started.
+	_, err := os.Stat("/var/run/node-manager-graceful-shutdown/enabled")
+	if os.IsNotExist(err) {
+		m.logger.V(1).Info("No external inhibitor locks enabled, proceed with shutdown")
+		return
+	}
+
+	m.logger.V(1).Info("Wait for external inhibitors to unlock")
+
+	realClock := clock.RealClock{}
+	ticker := realClock.NewTicker(inhibitorsCheckInterval)
+	defer ticker.Stop()
+
+	lastLogTime := time.Now()
+
+	for {
+		_, err := os.Stat("/var/run/node-manager-graceful-shutdown/inhibited")
+		if os.IsNotExist(err) {
+			m.logger.V(1).Info("External inhibitors unlocked, proceed with shutdown")
+			return
+		}
+
+		now := time.Now()
+		if now.Sub(lastLogTime) > delayLogInterval {
+			lastLogTime = now
+			m.logger.V(1).Info("Shutdown still locked by external inhibitors")
+		}
+
+		<-ticker.C()
+	}
+
+	m.logger.V(1).Info("Timeout while waiting for external inhibitors, proceed with shutdown")
+	return
+}
+
 func (m *managerImpl) periodRequested() time.Duration {
 	var sum int64
 	for _, period := range m.shutdownGracePeriodByPodPriority {
@@ -449,25 +506,35 @@ func migrateConfig(shutdownGracePeriodRequested, shutdownGracePeriodCriticalPods
 	}
 }
 
-func groupByPriority(shutdownGracePeriodByPodPriority []kubeletconfig.ShutdownGracePeriodByPodPriority, pods []*v1.Pod) []podShutdownGroup {
+func (m *managerImpl) groupByPriority(shutdownGracePeriodByPodPriority []kubeletconfig.ShutdownGracePeriodByPodPriority, pods []*v1.Pod) []podShutdownGroup {
+	m.logger.V(1).Info(fmt.Sprintf("Group pods: gracePeriods: %+v", shutdownGracePeriodByPodPriority))
+
 	groups := make([]podShutdownGroup, 0, len(shutdownGracePeriodByPodPriority))
 	for _, period := range shutdownGracePeriodByPodPriority {
 		groups = append(groups, podShutdownGroup{
 			ShutdownGracePeriodByPodPriority: period,
 		})
 	}
+	m.logger.V(1).Info(fmt.Sprintf("Group pods: groups: %+v", groups))
 
 	for _, pod := range pods {
 		var priority int32
+		// Ignore pod without priotity (an experiment with kubernetes-api-proxy).
+		if pod.Spec.Priority == nil {
+			continue
+		}
 		if pod.Spec.Priority != nil {
 			priority = *pod.Spec.Priority
 		}
 
+		m.logger.V(1).Info(fmt.Sprintf("Group pods: pod %s %s, priority %d, spec.priority nil? %v", pod.Namespace, pod.Name, priority, pod.Spec.Priority == nil))
 		// Find the group index according to the priority.
 		index := sort.Search(len(groups), func(i int) bool {
 			return groups[i].Priority >= priority
 		})
 
+		m.logger.V(1).Info(fmt.Sprintf("Group pods: index after search %d", index))
+
 		// 1. Those higher than the highest priority default to the highest priority
 		// 2. Those lower than the lowest priority default to the lowest priority
 		// 3. Those boundary priority default to the lower priority
@@ -482,6 +549,8 @@ func groupByPriority(shutdownGracePeriodByPodPriority []kubeletconfig.ShutdownGr
 			index--
 		}
 
+		m.logger.V(1).Info(fmt.Sprintf("Group pods: index adjusted %d", index))
+
 		groups[index].Pods = append(groups[index].Pods, pod)
 	}
 	return groups
